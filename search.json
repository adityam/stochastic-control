[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Notes",
    "section": "",
    "text": "About the course\nThese notes started as the lecture notes for ECSE 506 (Stochastic Control and Decision Theory) that I teach in the Winter term of every even year. These notes are not meant to be exhaustive; rather my focus is to convey the key ideas in their simplest form. For a more exhaustive treatment of the subject, please refer to the reference books mentioned below.\nIf you find any typos/mistakes in the notes, please let me know. Pull requests are welcome."
  },
  {
    "objectID": "index.html#reference-books",
    "href": "index.html#reference-books",
    "title": "Course Notes",
    "section": "Reference books",
    "text": "Reference books\n\nKumar and Varaiya, Stochastic Systems: Estimation, Identification, and Adaptive Control, Prentice Hall, 1986. Reprinted by SIAM 2015  A gentle introduction which emphaisizes the key conceptual ideas.\nBertsekas, Dynamic programming and optimal control, vol 1 and 2, Athena Publications, 2005.  Perhaps the most comprehensive book of different topics in dynamic programming.\nPuterman, Markov decision processes: discrete time dynamic programming, Wiley 1994.  Excellent source algorithms for perfectly observed systems, in particular, infinite horizon dynamic programs.\nRoss, Introduction to Stochastic Dynamic Programming, Academic Press, 1983.  Excellent introduction to dynamic programming, from the point-of-view of applied mathematics.\nDernardo, Dynamic Programming: Models and Applications, Prentice Hall, 1982.  Excellent introduction to dynamic programming, from the point-of-view of operations research.\nPowell, Approximate Dynamic Programming, John Wiley and Sons, 2011.  Comprehensive overview of approximate dynamic programming\nKrishnamurty, Partially Observable Markov Decision Processes, Cambridge University Press, 2016.  Comprehensive overview of POMDPs\nSargent and Stachurski, Dynamic Programming, 2023.  Nice summary of DP ideas applied to economic models. Good mix of theory and numerical examples.\nKochenderfer, Wheeler, and Wray, Algorithms for decision making, MIT Press, 2022.  Broad introduction to decision making under uncertainty. Lots of nice examples."
  },
  {
    "objectID": "index.html#how-to-cite-these-notes",
    "href": "index.html#how-to-cite-these-notes",
    "title": "Course Notes",
    "section": "How to cite these notes",
    "text": "How to cite these notes\nTo cite these lecture notes, please use:\n@misc{506notes,\n  author        = {Aditya Mahajan},\n  title         = {Lecture notes on Stochastic Control and Decision Theory},\n  year          = {2023},\n  howpublished  = \"\\url{https://adityam.github.io/stochastic-control/}\",\n}"
  },
  {
    "objectID": "stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "href": "stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "title": "1  Introduction",
    "section": "1.1 The stochastic optimization problem",
    "text": "1.1 The stochastic optimization problem\nNow consider the simplest stochastic optimization problem. A decision maker has to choose an action \\(a \\in \\ALPHABET A\\). Upon choosing the action \\(a\\), the decision maker incurs a cost \\(c(a,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable with known probability distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(a, W) ]\\), where the expectation is with respect to the random variable \\(W\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:stochastic}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(a, W) ].\n\\end{equation}\\]\nDefine \\(J(a) = \\EXP[ c(a, W) ]\\). Then Problem \\eqref{eq:stochastic} is conceptually the same as Problem \\eqref{eq:basic} with the cost function \\(J(a)\\). Numerically, Problem \\eqref{eq:stochastic} is more difficult because computing \\(J(a)\\) involves evaluating an expectation, but we ignore the computational complexity for the time being."
  },
  {
    "objectID": "stochastic-optimization/intro.html#key-simplifying-idea",
    "href": "stochastic-optimization/intro.html#key-simplifying-idea",
    "title": "1  Introduction",
    "section": "1.2 Key simplifying idea",
    "text": "1.2 Key simplifying idea\nIn the stochastic optimization problems considered above, the decision maker does not observe any data before making a decision. In many situations, the decision maker does observe some data, which is captured by the following model. Suppose a decision maker observes a random variable \\(S \\in \\ALPHABET S\\) and then chooses an action \\(A \\in \\ALPHABET A\\) as a function of his observation according to a decision rule \\(π\\), i.e., \\[ A = π(S). \\]\nUpon choosing the action \\(A\\), the decision maker incurs a cost \\(c(S,A,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable. We assume that the primitive random variables \\((S,W)\\) are defined on a common probability space and have a known joint distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(S, π(S), W)]\\), where the expectation is taken with respect to the joint probability distribution of \\((S,W)\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:obs} \\tag{P1}\n  \\min_{π \\colon \\ALPHABET S \\to \\ALPHABET A} \\EXP[ c(S, π(S), W) ].\n\\end{equation}\\]\nDefine \\(J(π) = \\EXP[ c(S, π(S), W) ]\\). Then, Problem \\eqref{eq:obs} is conceptually the same as Problem \\eqref{eq:basic} with one difference: In Problem \\eqref{eq:basic}, the minimization is over a parameter \\(a\\), while in Problem \\eqref{eq:obs}, the minimization is over a function \\(π\\).\nWhen \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are finite sets, the optimal policy can be obtained by an exhaustive search over all policies as follows: for each policy \\(π\\) compute the performance \\(J(π)\\) and then pick the policy \\(π\\) with the smallest expected cost.\nSuch an exhaustive search is not satisfying for two reasons. First, it has a high computational cost. There are \\(| \\ALPHABET A |^{| \\ALPHABET S |}\\) policies and, for each policy, we have to evaluate an expectation, which can be expensive. Second, the above enumeration procedure does not work when \\(\\ALPHABET S\\) or \\(\\ALPHABET A\\) are continuous sets.\nThere is an alternative way of viewing the problem that simplifies it considerably. Instead of viewing the optimization problem before the system starts running (i.e., the ex ante view), imagine that the decision maker waits until they see the realization \\(s\\) of \\(S\\) (i.e., the interim view). they then asks what action \\(a\\) should they take to minimize the expected conditional cost \\(Q(s,a) := \\EXP[ c(s,a, W) | S = s]\\), i.e., they consider the problem\n\\[\\begin{equation} \\label{eq:cond-1} \\tag{P2}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a,W) | S = s], \\quad\n  \\forall s \\in \\ALPHABET S.\n\\end{equation}\\]\nThus, Problem \\eqref{eq:obs}, which is a functional optimization problem, has been reduced to a collection of parameter optimization problems (Problem \\eqref{eq:cond-1}), one for each possible of \\(s\\).\nNow define \\[ \\begin{equation} \\label{eq:cond} \\tag{P2-policy}\n  π^∘(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]\n\\end{equation} \\] where ties (in the minimization) are broken arbitrarily.\n\nTheorem 1.1 The decision rule \\(π^∘\\) defined in \\eqref{eq:cond} is optimal for Problem \\ref{eq:basic}.\n\n\n\n\n\n\n\nRemark\n\n\n\nWe restricted the proof to finite \\(\\ALPHABET S\\), \\(\\ALPHABET A\\), \\(\\ALPHABET W\\). This is to avoid any measurability issues. If \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are continuous sets, we need to restrict to measurable \\(π\\) in Problem \\ref{eq:basic} (otherwise the expectation is not well defined; of course the cost \\(c\\) also has to be measurable). However, it is not immediately obvious that \\(π^∘\\) defined in \\eqref{eq:cond} is measurable. Conditions that ensure this are known as measurable selection theorems.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(π\\) be any other decision rule. Then, \\[ \\begin{align*}\n  \\EXP[ c(S, π(S), W) ] &\\stackrel{(a)}= \\EXP[ \\EXP[c(S, π(S), W) | S ] ] \\\\\n  &\\stackrel{(b)}\\ge \\EXP[\\EXP[ c(S, π^∘(S), W) | S ] ] \\\\\n  &\\stackrel{(c)}= \\EXP[ c(S, π^∘(S), W) ],\n\\end{align*} \\] where \\((a)\\) and \\((c)\\) follow from the law of iterated expectations and \\((b)\\) follows from the definition of \\(π^∘\\) in \\eqref{eq:cond}.\n\n\n\nWe can also provide a partial converse of Theorem 1.1.\n\nTheorem 1.2 If \\(\\PR(S = s) &gt; 0\\) for all \\(s\\), then any optimal policy \\(π^∘\\) for Problem \\(\\ref{eq:basic}\\) must satisfy \\(\\eqref{eq:cond}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove this by contradiction. Suppose \\(π^*\\) is an optimal policy that does not satisfy \\eqref{eq:cond}. By definition of \\(π^∘\\), it must be the case that for all states \\[\\begin{equation}\n   \\EXP[ c(s, π^∘(s), W) | S = s ]\n   \\le\n   \\EXP[ c(s, π^*(s), W) | S = s ] .\n   \\label{eq:ineq:1}\n\\end{equation}\\] Now, since \\(π^*\\) does not satisfy \\eqref{eq:cond}, there exists some state \\(s^∘ \\in \\ALPHABET S\\) such that \\[\\begin{equation}\n   \\EXP[ c(s^∘, π^*(s^∘), W) | S = s^∘ ]\n   &gt;\n   \\EXP[ c(s^∘, π^∘(s^∘), W) | S = s^∘ ] .\n   \\label{eq:ineq:2}\n\\end{equation}\\] Therefore, \\[\\begin{align*}\n   \\EXP[ c(S, π^*(S), W) ]\n   &=\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^*(s), W) | S = s ] ]\n   \\\\\n   & \\stackrel{(a)}&gt;\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^∘(s), W) | S = s ] ]\n   \\\\\n   &=\n   \\EXP[ c(S, π^∘(S), W) ]\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:ineq:1} and \\eqref{eq:ineq:2} and the inequality is strict becase \\(\\PR(S = s^∘) &gt; 0\\). Thus, \\(J(π^*) &gt; J(π^∘)\\) and, hence, \\(π^*\\) cannot be an optimal policy."
  },
  {
    "objectID": "stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "href": "stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "title": "1  Introduction",
    "section": "1.3 Blackwell’s principle of irrelevant information",
    "text": "1.3 Blackwell’s principle of irrelevant information\nIn many scenarios, the decision maker may observe data which is irrelevant for evaluating performance. In such instances, the decision maker may ignore such information without affecting performance. Formally, we have the following result, which is known as Blackwell’s principle of irrelevant information.\n\nTheorem 1.3 (Blackwell’s principle of irrelevant information) Let \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), and \\(\\ALPHABET A\\) be standard Borel spaces and \\(S \\in \\ALPHABET S\\), \\(Y \\in \\ALPHABET Y\\), \\(W \\in \\ALPHABET W\\) be random variables defined on a common probability space.\nA decision maker observes \\((S,Y)\\) and chooses \\(A = π(S,Y)\\) to minimize \\(\\EXP[c(S,A,W)]\\), where \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET W \\to \\reals\\) is a measurable function.\nThen, if \\(W\\) is conditionally independent of \\(Y\\) given \\(S\\), then there is no loss of optimality in choosing \\(A\\) only as a function of \\(S\\).\nFormally, there exists a \\(π^* \\colon \\ALPHABET S \\to \\ALPHABET A\\) such that for all \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), \\[ \\EXP[c(S, π^*(S), W)] \\le \\EXP[ c(S, π(S,Y), W) ]. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result for the case when \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), \\(\\ALPHABET A\\) are finite.\nDefine \\[π^*(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]. \\] Then, by construction, for any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), we have that \\[ \\EXP[ c(s, π^*(s), W ) | S = s]  \\le \\EXP[ c(s,a,W) | S = s]. \\] Hence, for any \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), and for any \\(s \\in \\ALPHABET S\\) and \\(y \\in \\ALPHABET Y\\), we have \\[ \\begin{equation} \\label{eq:opt}\n  \\EXP[ c(s, π^*(s), W) | S = s] \\le \\EXP[ c(s, π(s,y),W) | S = s].\n\\end{equation} \\] The result follows by taking the expectation of both sides of \\eqref{eq:opt}.\n\n\n\nThe above proof doesn’t work for general Borel spaces because \\(π^*\\) defined above may not exist (inf vs min) or may not be measurable. See Blackwell (1964) for a formal proof."
  },
  {
    "objectID": "stochastic-optimization/intro.html#exercises",
    "href": "stochastic-optimization/intro.html#exercises",
    "title": "1  Introduction",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.1 (Computing optimal policies) Suppose \\(\\ALPHABET S = \\{1, 2 \\}\\), \\(\\ALPHABET A = \\{1, 2, 3\\}\\), and \\(\\ALPHABET W = \\{1, 2, 3\\}\\). Let \\((S,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET W\\) with joint distribution \\(P\\) shown below.\n\\[ P = \\MATRIX{ 0.25 & 0.15 & 0.05  \\\\ 0.30 & 0.10 & 0.15 } \\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S=2, W=1) = P_{21} = 0.30\\).\nThe cost function \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET W \\to \\reals\\) is shown below\n\\[\nc(\\cdot,\\cdot,1) = \\MATRIX{3 & 5 & 1 \\\\ 2 & 3 & 1 }, \\quad\nc(\\cdot,\\cdot,2) = \\MATRIX{4 & 3 & 1 \\\\ 1 & 2 & 8 }, \\quad\nc(\\cdot,\\cdot,3) = \\MATRIX{1 & 2 & 2 \\\\ 4 & 1 & 3 }.\n\\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(a\\). For example \\(c(s=1,a=2,w=1) = 5\\).\nFind the policy \\(π \\colon \\ALPHABET S \\to \\ALPHABET A\\) that minimizes \\(\\EXP[ c(S, π(S), W) ]\\).\n\n\nExercise 1.2 (Blackwell’s principle) Suppose \\(\\ALPHABET S = \\{1, 2\\}\\), \\(\\ALPHABET Y = \\{1, 2\\}\\), \\(\\ALPHABET A = \\{1, 2, 3\\}\\), and \\(\\ALPHABET W = \\{1, 2, 3\\}\\). Let \\((S,Y,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET Y × \\ALPHABET W\\), with joint distribution \\(Q\\) shown below. \\[\nQ_{Y = 1} = \\MATRIX{0.15 & 0.10 & 0.00 \\\\ 0.15 & 0.05 & 0.10}\n\\qquad\nQ_{Y = 2} = \\MATRIX{0.10 & 0.05 & 0.05 \\\\ 0.15 & 0.05 & 0.05}\n\\] For a fixed value of \\(y\\), the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S = 1, Y = 1, W = 3) = 0\\).\nThe cost function \\(c \\colon \\ALPHABET S × \\ALPHABET A × \\ALPHABET W \\to \\reals\\) is the same as the previous exercise.\n\nFind the policy \\(π \\colon \\ALPHABET S × \\ALPHABET Y \\to \\ALPHABET A\\) that minimizes \\(\\EXP[c(S, π(S,Y), W)]\\).\nCompare the solution with the solution of the previous exercise in view of Blackwell’s principle of irrelevant information. Clearly explain your observations.\n\n\n\n\nExercise 1.3 (Pollution monitoring) Consider the problem of monitoring the pollution level of a river. The river can have a high pollution level if there is a catastrophic failure of a factory upstream. There are then two “pollution states” indicating whether such a failure has not occured. We denote them by \\(S = 0\\) (indicating no failure) and \\(S = 1\\) (indicating catastrophic failure). Let \\([p, 1-p]\\) denote the prior probability mass function of \\(S\\).\nThe pollution monitoring system has a sensor which takes a measurement \\(y\\) of the pollution level. Let \\(f_s(y)\\) denote the probabiity density of the observation \\(y\\) conditional on the value of \\(s\\), \\(s \\in \\{0, 1\\}\\). Two actions are available at the monitoring system: raise an alarm or not raise an alarm. The cost of raising the alarm is \\(C_0\\) if the state \\(S\\) is \\(0\\) or zero if the state \\(S\\) is \\(1\\); the cost of not raising the alarm is zero if the state \\(S\\) is \\(0\\) or \\(C_1\\) if the state \\(S\\) is \\(1\\).\nShow that it is optimal to raise the alarm if \\[ p f_0(y) C_0 &lt; (1 - p) f_1(y) C_1. \\] That is, it is optimal to raise the alarm if the likelihood ratio \\(f_1(y)/f_0(y)\\) exceeds the threshold value \\(p C_0/(1-p) C_1\\)."
  },
  {
    "objectID": "stochastic-optimization/intro.html#notes",
    "href": "stochastic-optimization/intro.html#notes",
    "title": "1  Introduction",
    "section": "Notes",
    "text": "Notes\nTheorem 1.3 is due to Blackwell (1964) in a short 2.5 page paper. A similar result was used by Witsenhausen (1979) to show the structure of optimal coding strategies in real-time communication. Also see the blog post by Maxim Ragisnsky.\nExercise 1.3 is adaptive from Whittle (1996). It is a special instance of Bayesian hypothesis testing problem. We will study a generalization of this model later in sequential hypothesis testing\n\n\n\n\n\nBlackwell, D. 1964. Memoryless strategies in finite-stage dynamic programming. The Annals of Mathematical Statistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nWhittle, P. 1996. Optimal control: Basics and beyond. Wiley.\n\n\nWitsenhausen, H.S. 1979. On the structure of real-time source coders. Bell System Technical Journal 58, 6, 1437–1451."
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "href": "stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "title": "2  The newsvendor problem",
    "section": "2.1 Interlude with continuous version",
    "text": "2.1 Interlude with continuous version\nThe problem above has discrete action and discrete demand. To build intuition, we first consider the case where both the actions and demand are continuous. Let \\(f(w)\\) denote the probability density of the demand and \\(F(w)\\) denote the cumulative probability density. Then, the expected reward is \\[ \\begin{equation} \\label{eq:J}\nJ(a) = \\int_{0}^a [ q w - p a ] f(w) dw + \\int_{a}^\\infty [ q a - p a ] f(w) dw.\n\\end{equation}\\]\nTo fix ideas, we consider an example where \\(p = 0.5\\), \\(q = 1\\), and the demand is a :Kumaraswamy distribution with parameters \\((a,b) = (2,5)\\) and support \\([0,100]\\). The performance of a function of action is shown below.\n\np = 0.5\nq = 1\nr = function(w,a){ if(w&lt;=a) { return q*w - p*a } else { return q*a - p*a } }\n\na_opt = inverseCDF( (q-p)/q )\n\nconfig = ({\n  // Kumaraswamy Distribution: https://en.wikipedia.org/wiki/Kumaraswamy_distribution\n  a: 2,\n  b: 5,\n  max: 100\n})\n\npdf = {\n  const a = config.a\n  const b = config.b\n\n  return function(x) {\n    var normalized = x/config.max\n    return a*b*normalized**(a-1)*(1 - normalized**a)**(b-1)\n  }\n}\n\ninverseCDF= {\n  const a = config.a\n  const b = config.b\n  return function(y) {\n     // Closed form expression for inverse CDF of Kumaraswamy distribution \n     return config.max * (1 - (1-y)**(1/b))**(1/a)\n  }\n}\n\npoints = { \n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i &lt; n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,action) }\n  }\n  return points\n}\n\ncost_values = {\n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i &lt; n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\nJ = {\n  const a = config.a\n  const b = config.b\n\n  return function(action) {\n    const n = 1000\n    var cost = 0\n    var w = 0\n    for (var i = 0; i &lt; n; i++) {\n      w = config.max*i/n\n      if (w &lt;= action) {\n        cost += (q*w - p*action)*pdf(w)/n\n      } else {\n        cost += (q*action - p*action)*pdf(w)/n\n      }\n    }\n    return cost\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof action = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncost = Math.round(J(action)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJ = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [action, J(action)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_opt, J(a_opt)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_values, {x:\"x\", y:\"y\"})\n  ]\n})\n\nplotPDF = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [action,0], [action, pdf(action)] ], {stroke: \"blue\"}),\n    Plot.line(points,{x:\"x\", y:\"y\"}),\n    Plot.areaY(points.filter(pt =&gt; pt.x &lt;= action),{x:\"x\", y:\"y\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt =&gt; pt.x &gt; action),{x:\"x\", y:\"y\", fill: \"pink\"})\n  ]\n})\n\nplotReward = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {x:\"x\", y:\"reward\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 2.1: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn Figure 2.1(a), the plot of \\(J(a)\\) is concave. We can verify that this is true in general.\n\n\n\n\n\n\nVerify that \\(J(a)\\) is concave\n\n\n\n\n\nTo verify that the function \\(J(a)\\) is concave, we compute the second derivative: \\[\n  \\frac{d^2 J(a)}{da^2} = - p f(a) - (q - p) f(a) = -q f(a) \\le 0.\n\\]\n\n\n\nThis suggests that we can use calculus to find the optimal value. In particular, to find the optimal action, we need to compute the \\(a\\) such that \\(dJ(a)/da = 0\\).\n\nProposition 2.1 For the newsvendor problem with continuous demand, the optimal action is \\[\n    a = F^{-1}\\left( 1 - \\frac{p}{q} \\right).\n  \\] In the literature, the quantity \\(1 - (p/q)\\) is called the critical fractile.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\nLeibniz integral rule\n\n\n\n\n\n\\[ \\dfrac{d}{dx} \\left( \\int_{p(x)}^{q(x)} f(x,t) dt \\right)\n   = f(x, q(x)) \\cdot \\dfrac {d}{dx} q(x)\n   - f(x, p(x)) \\cdot \\dfrac {d}{dx} p(x)\n   + \\int_{p(x)}^{q(x)} \\dfrac{\\partial}{\\partial x} f(x,t) dt.\n\\]\n\n\n\nUsing the Leibniz integral rule, the derivative of the first term of \\(\\eqref{eq:J}\\) is \\[ [q a - p a ] f(a) + \\int_{0}^a [ -p ] f(w) dw\n= [q a - p a ] f(a) - p F(a).\n\\]\nSimilarly, the derivative of the second term of \\(\\eqref{eq:J}\\) is \\[ - [q a - p a] f(a) + \\int_{a}^{\\infty} (q-p)f(w)dw\n= - [q a - p a] f(a) + (q -p)[ 1 - F(a)].\n\\]\nCombining the two, we get that \\[ \\dfrac{dJ(a)}{da} = - p F(a) + (q - p) [ 1 - F(a) ]. \\]\nEquating this to \\(0\\), we get \\[ F(a) = \\dfrac{ q - p }{ q}\n\\quad\\text{or}\\quad\na = F^{-1} \\left( 1 - \\dfrac{  p }{ q } \\right).\n\\]"
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "href": "stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "title": "2  The newsvendor problem",
    "section": "2.2 Back to discrete version",
    "text": "2.2 Back to discrete version\nNow, we come back to the problem with discrete actions and discrete demand. Suppose \\(W\\) takes the values \\(\\ALPHABET W = \\{ w_1, w_2, \\dots, w_k \\}\\) (where \\(w_1 &lt; w_2 &lt; \\cdots &lt; w_k\\)) with probabilities \\(\\{ μ_1, μ_2, \\dots, μ_k \\}\\). It is ease to see that in this case the action \\(a\\) should be in the set \\(\\{ w_1, w_2, \\dots, w_k \\}\\).\nTo fix ideas, we repeat the above numerical example when \\(\\ALPHABET W = \\{0, 1, \\dots, 100\\}\\).\n\npointsD = { \n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i &lt; n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,actionD) }\n  }\n  return points\n}\n\ncost_valuesD = {\n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i &lt; n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\n\na_optD = Math.round(a_opt*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof actionD = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncostD = Math.round(J(actionD)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJD = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [actionD, J(actionD)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_optD, J(a_optD)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_valuesD, {x:\"x\", y:\"y\", curve: \"step-after\"})\n  ]\n})\n\nplotPDFD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [actionD,0], [actionD, pdf(actionD)] ], {stroke: \"blue\"}),\n    Plot.line(pointsD,{x:\"x\", y:\"y\", curve:\"step-after\"}),\n    Plot.areaY(points.filter(pt =&gt; pt.x &lt;= actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt =&gt; pt.x &gt; actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"pink\"})\n  ]\n})\n\nplotRewardD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(pointsD, {x:\"x\", y:\"reward\", curve: \"step-after\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 2.2: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn the discrete case, the brute force search is easier (because there are a finite rather than continuous number of values). We cannot directly use the ideas from calculus because functions over discrete domain are not differentiable. But we can use a very similar idea. Instead of checking if \\(dJ(a)/da = 0\\), we check the sign of \\(J(w_{i+1}) - J(w_i)\\).\n\nProposition 2.2 Let \\(\\{M_i\\}_{i \\ge 1}\\) denote the cumulative mass function of the demand. Then, the optimal action is the largest value of \\(w_i\\) such that \\[\n    M_i \\le 1 - \\frac{p}{q}.\n  \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe expected reward for choice \\(w_i\\) is \\[ \\begin{align*} J(w_i) &=\n\\sum_{j &lt; i} μ_j [ q w_j - p w_i ] + \\sum_{j \\ge i} μ_j [q w_i - p w_i]\n\\\\\n&= -p w_i + q \\Bigl[ \\sum_{j &lt; i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr].\n\\end{align*}\\]\nThus, \\[ \\begin{align*}\n  J(w_{i+1}) - J(w_i) &=\n  -p w_{i+1} + q \\Bigl[ \\sum_{j &lt; i+1}  μ_j w_j + \\sum_{j \\ge i+1} μ_j w_{i+1} \\Bigr]\n  \\\\\n  &\\quad + p w_i - q \\Bigl[ \\sum_{j &lt; i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr]\n  \\\\\n  &= -p (w_{i+1} - w_i) + q \\Bigl[ \\sum_{j \\ge i + 1} μ_j ( w_{i+1} - w_i) \\Bigr]\n  \\\\\n  &= \\big( - p + q [ 1 - M_i ] \\big) (w_{i+1} - w_i).\n\\end{align*}\\] Note that \\[\nM_i \\le \\dfrac{q-p}{q}\n\\iff\n-p + q [ 1 - M_i ] \\ge 0.\n\\] Thus, for all \\(i\\) such that \\(M_i \\le (q-p)/q\\), we have \\(J(w_{i+1}) \\ge J(w_i)\\). On the other hand, for all \\(i\\) such that \\(M_i &gt; (q-p)/q)\\), we have \\(J(w_{i+1}) &lt; J(w_i)\\). Thus, the optimal amount to order is the largest \\(w_i\\) such that \\(M_i \\le (q-p)/q\\).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that the structure of the optimal solution is the same for continuous and discrete demand distributions."
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#exercises",
    "href": "stochastic-optimization/newsvendor.html#exercises",
    "title": "2  The newsvendor problem",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 2.1 (Qualitative properties of optimal solution) Intuitively, we expect that if the purchase price of the newspaper increases but the selling price remains the same, then the newsvendor should buy less newspapers. Formally prove this statement.\nHint: The CDF of a distribution is a weakly increasing function.\n\n\nExercise 2.2 (Monotonicity of optimal action) Consider two scenarios for the case with continuous demand and actions. In scenario 1, the demand is distributed according to PDF \\(f_1\\). In scenario 2, it is distributed according to PDF \\(f_2\\). Suppose \\(F_1(w) \\le F_2(w)\\) for all \\(w\\). Show that the optimal action \\(a_1\\) for scenario 1 is greater than the optimal action \\(a_2\\) for scenario 2.\nHint: Plot the two CDFs and try to interpret the optimal decision rule graphically.\n\n\nExercise 2.3 (Selling random wind) The amount \\(W\\) of power generated by the wind turbine is a positive real-valued random variable with probability density function \\(f\\). The operator of the wind turbine has to commit to provide a certain amount of power in the day-ahead market. The price of power is \\(\\$p\\) per MW.\nIf the operator commits to provide \\(a\\) MW of power and the wind generation \\(W\\) is less than \\(a\\), then he has to buy the balance \\(a - W\\) from a reserves market at the cost of \\(\\$ q\\) per unit, where \\(q &gt; p\\). Thus, the reward of the operator is \\(r(a,W)\\) where \\[ r(a, w) = \\begin{cases}\n  p a, & \\text{if } w &gt; a \\\\\n  p a - q (a  - w), & \\text{if } w &lt; a.\n\\end{cases}\\]\nFind the value of commitment \\(a\\) that maximizes the expected reward."
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#notes",
    "href": "stochastic-optimization/newsvendor.html#notes",
    "title": "2  The newsvendor problem",
    "section": "Notes",
    "text": "Notes\nPerhaps the earliest model of the newsvendor problem appeared in Edgeworth (1888) in the context of a bank setting the level of cash reserves to cover demands from its customers. The solution to the basic model presented above and some of its variants was provided in Morse and Kimball (1951); Arrow et al. (1952); Whitin (1953). See Porteus (2008) for an accessible introduction.\nThe property \\(F_1(w) \\le F_2(w)\\) used in Exercise 2 is called stochastic dominance. Later in the course, we will study how stochastic dominance is useful to establish monotonicity properties of general MDPs.\n\nThe example of selling random wind in Exercise 2.3 is taken from Bitar et al. (2012).\n\n\n\n\n\nArrow, K.J., Harris, T., and Marschak, J. 1952. Optimal inventory policy. Econometrica 20, 1, 250–272. DOI: 10.2307/1907830.\n\n\nBitar, E., Poolla, K., Khargonekar, P., Rajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind. 2012 45th hawaii international conference on system sciences, IEEE, 1931–1937.\n\n\nEdgeworth, F.Y. 1888. The mathematical theory of banking. Journal of the Royal Statistical Society 51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nMorse, P. and Kimball, G. 1951. Methods of operations research. Technology Press of MIT.\n\n\nPorteus, E.L. 2008. Building intuition: Insights from basic operations management models and principles. In: D. Chhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nWhitin, S. 1953. The theory of inventory management. Princeton University Press."
  },
  {
    "objectID": "mdps/intro.html#performance",
    "href": "mdps/intro.html#performance",
    "title": "3  Finite horizon MDPs",
    "section": "3.1 Performance of Markov strategies",
    "text": "3.1 Performance of Markov strategies\nWe have shown that there is no loss of optimality to restrict attention to Markov strategies. One of the advantages of Markov strategies is that their performance can be computed recursively. In particular, given any Markov strategy \\(π = (π_1, \\dots, π_T)\\), define the cost-to-go functions as follows: \\[V^{π}_t(s) = \\EXP^π \\bigg[ \\sum_{τ = t}^{T} c_τ(S_τ, π_τ(S_τ)) \\biggm| S_t =\ns\\bigg]. \\] Note that \\(V^{π}_t(s)\\) only depends on the future strategy \\((π_t, \\dots, π_T)\\). These functions can be computed recursively as follows: \\[\\begin{align*}\n  V^{π}_t(s) &= \\EXP^π \\bigg[ \\sum_{τ = t}^{T} c_τ(S_τ, π_τ(S_τ)) \\biggm| S_t =\n  s \\bigg] \\\\\n  &= \\EXP^π \\bigg[ c_t(s, π_t(s)) + \\EXP^π \\bigg[ \\sum_{τ = t+1}^T\n    c_τ(S_τ, π_τ(S_τ)) \\biggm| S_{t+1} \\bigg] \\biggm| S_t = s \\bigg]\n  \\\\\n  &= \\EXP^π\\big[ c_t(s, π_t(s)) + V_{t+1}(S_{t+1}; π) \\big| S_t = s \\big].\n\\end{align*}\\]"
  },
  {
    "objectID": "mdps/intro.html#DP",
    "href": "mdps/intro.html#DP",
    "title": "3  Finite horizon MDPs",
    "section": "3.2 Dynamic Programming Decomposition",
    "text": "3.2 Dynamic Programming Decomposition\nNow we are ready to state the main result of MDPs\n\nTheorem 3.2 (Dynamic program) Recursive define value functions \\(\\{V_t\\}_{t = 1}^{T+1} \\colon \\ALPHABET S \\to \\reals\\) as follows: \\[ \\begin{equation} \\label{eq:DP-1}\n  V_{T+1}(s) = 0\n\\end{equation} \\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n   Q_t(s,a) &= c(s,a) + \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a]\n   \\nonumber \\\\\n   &= c(s,a) + \\EXP[ V_{t+1}(f_t(s,a,W_t)) ], \\label{eq:DP-2}\n\\end{align}\\] and define \\[ \\begin{equation} \\label{eq:DP-3}\n  V_t(s) = \\min_{a \\in \\ALPHABET A} Q_t(s,a).\n\\end{equation} \\] Then, a Markov policy is optimal if and only if it satisfies \\[ \\begin{equation} \\label{eq:verification}\n  π_t^*(s) = \\arg \\min_{a \\in \\ALPHABET A} Q_t(s,a).\n\\end{equation} \\]\n\nInstead of proving the above result, we prove a related result\n\nTheorem 3.3 (The comparison principle) For any Markov strategy \\(π\\) \\[ V^{π}_t(s) \\ge V_t(s) \\] with equality at \\(t\\) if and only if the future strategy \\(π_{t:T}\\) satisfies the verification step \\eqref{eq:verification}.\n\nNote that the comparison principle immediately implies that the strategy obtained using dynamic programming is optimal.\nThe comparison principle also allows us to interpret the value functions. The value function at time \\(t\\) is the minimum of all the cost-to-go functions over all future strategies. The comparison principle also allows us to interpret the optimal policy (the interpretation is due to Bellman and is colloquially called Bellman’s principle of optimality).\n\n\n\n\n\n\nBellman’s principle of optimality.\n\n\n\nAn optimal policy has the property that whatever the initial state and the initial decisions are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n\n\n\n\n\n\n\n\nProof of the comparison principle\n\n\n\n\n\nThe proof proceeds by backward induction. Consider any Markov strategy \\(π = (π_1, \\dots, π_T)\\). For \\(t = T\\), \\[ \\begin{align*}\n  V_T(s) &= \\min_{a \\in \\ALPHABET A} Q_T(s,a) \\\\\n  &\\stackrel{(a)}= \\min_{a \\in \\ALPHABET A} c_T(s,a) \\\\\n  &\\stackrel{(b)}\\le c_T(s, π_T(s)) \\\\\n  &\\stackrel{(c)}= V^{π}_T(s),\n\\end{align*} \\] where \\((a)\\) follows from the definition of \\(Q_T\\), \\((b)\\) follows from the definition of minimization, and \\((c)\\) follows from the definition of \\(J_T\\). Equality holds in \\((b)\\) iff the policy \\(π_T\\) is optimal. This result forms the basis of induction.\nNow assume that the statement of the theorem is true for \\(t+1\\). Then, for \\(t\\) \\[ \\begin{align*}\n  V_t(s) &= \\min_{a \\in \\ALPHABET A} Q_t(s,a) \\\\\n  &\\stackrel{(a)}= \\min_{a \\in \\ALPHABET A} \\Big\\{\n  c_t(s,a) + \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a]\n  \\Big\\}\n  \\\\\n  &\\stackrel{(b)}\\le  \\Big\\{\n  c_t(s,π_t(s)) + \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = π_t(s)]\n  \\Big\\} \\\\\n  &\\stackrel{(c)}\\le  \\Big\\{\n  c_t(s,π_t(s)) + \\EXP[ J_{t+1}(S_{t+1}; π) | S_t = s, A_t = π_t(s)]\n  \\Big\\} \\\\\n  &\\stackrel{(d)}= V^{π}_t(s),\n\\end{align*} \\] where \\((a)\\) follows from the definition of \\(Q_t\\), \\((b)\\) follows from the definition of minimization, \\((c)\\) follows from the induction hypothesis, and \\((d)\\) follows from the definition of \\(J_t\\). We have equality in step \\((b)\\) iff \\(π_t\\) satisfies the verification step \\eqref{eq:verification} and have equality in step \\((c)\\) iff \\(π_{t+1:T}\\) is optimal (this is part of the induction hypothesis). Thus, the result is true for time \\(t\\) and, by the principle of induction, is true for all time."
  },
  {
    "objectID": "mdps/intro.html#variations-of-a-theme",
    "href": "mdps/intro.html#variations-of-a-theme",
    "title": "3  Finite horizon MDPs",
    "section": "3.3 Variations of a theme",
    "text": "3.3 Variations of a theme\n\n3.3.1 Cost depends on next state\nIn the basic model that we have considered above, we assumed that the per-step cost depends only on the current state and current actions. In some applications, such as the inventory management model considered in class, it is more natural to have a cost function where the cost depends on the current state, current action, and the next state. Conceptually, such problems can be treated in the same way as the standard model.\nIn particular, suppose we have a per-step cost given by \\(c_t(S_t,A_t,S_{t+1})\\), where the objective is to minimize \\[ J(π) = \\EXP\\Bigl[ \\sum_{t=1}^T c_t(S_t, A_t, S_{t+1}) \\Bigr]. \\]\nDefine \\[ \\tilde c_t(s, a) = \\EXP[ c_t(s, a, S_{t+1}) | S_t = s, A_t = a ]\n= \\EXP[ c_t(s,a, f_t(s,a, W_t) ]. \\] Then, by the towering property of conditional expectation, we can write\n\\[ \\begin{align*}\nJ(π) &= \\EXP\\Bigl[ \\sum_{t=1}^T \\EXP[ c_t(S_t, A_t, S_{t+1}) | S_t, A_t] \\Bigr] \\\\\n&= \\EXP\\Bigl[ \\sum_{t=1}^T \\tilde c_t(S_t, A_t) \\Bigr].\n\\end{align*} \\]\nThus, we can equivalently consider this as our standard model with the per-step cost given by \\(\\tilde c_t(S_t, A_t)\\). We can write the recursive step of the dynamic program as follows: \\[ Q_t(s,a) = \\EXP[ c_t(s,a, S_{t+1}) + V_{t+1}(S_{t+1}) | S_t = s, A_t = a ].\\]\nFor numerically solving the dynamic program when the cost is time-homogeneous (i.e., does not depend on \\(t\\)), it is more efficient to compute \\(\\tilde c\\) once and recuse that in the dynamic program recursion.\n\n\n3.3.2 Discounted cost\nIn some applications, it is common to consider a discounted expected cost given by \\[ J(π) = \\EXP\\Bigl[ \\sum_{t=1}^T γ^{t-1} c_t(S_t, A_t) \\Bigr] \\] where \\(γ \\in (0,1)\\) is called the discount factor.\n\n\n\n\n\n\nDiscount factor\n\n\n\nThere are two interpretations of the discount factor \\(γ\\). The first interpretation is an economic interpretation to determine the present value of a utility that will be received in the future. For example, suppose a decision maker is indifferent between receiving 1 dollar today or \\(s\\) dollars tomorrow. This means that the decision maker discounts the future at a rate \\(1/s\\), so \\(γ = 1/s\\).\nThe second interpretation is that of an absorbing state. Suppose we are operating a machine that generates a value of $1 each day. However, there is a probability \\(p\\) that the machine will break down at the end of the day. Thus, the expected return for today is $1 while the expected return for tomorrow is \\((1-p)\\) (which is the probability that the machine is still working tomorrow). In this case, the discount factor is defined as \\((1-p)\\). See Shwartz (2001) for a detailed discussion of this alternative.\n\n\nThe recursive step of the dynamic program for such models can be written as \\[ Q_t(s,a) = c_t(s,a) + γ \\, \\EXP[ V_{t+1}( S_{t+1}) | S_t = s, A_t = a ].\\]\n\n\n3.3.3 Multiplicative cost\nSo far, we have assumed that the cost is additive. The dynamic proramming decomposition also works for models with multiplicative cost. In particular, suppose that the performance of any policy is given by \\[ J(π) = \\EXP\\Bigl[ \\prod_{t=1}^T c_t(S_t, A_t) \\Bigr] \\] where the per-step cost function is positive. Then, it can be shown that the optimal policy is given by the following dynamic program.\n\nProposition 3.1 (Dynamic Program for multiplicative cost) Initialize \\(V_{T+1}(s) = 1\\) and recursively compute \\[ \\begin{align*}\nQ_t(s,a) &= c_t(s,a) \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\\\\nV_t(s) &= \\min_{a \\in \\ALPHABET A} Q_t(s,a).\n\\end{align*} \\]\n\n\n\n3.3.4 Exponential cost function\nA special class of multiplicative cost function is exponential of sum: \\[J(π) = \\EXP\\Bigl[ \\exp\\Bigl( \\theta \\sum_{t=1}^T c_t(S_t, A_t) \\Bigr) \\Bigr]. \\]\nWhen \\(\\theta &gt; 0\\), the above captures risk-averse preferences and when \\(\\theta &lt; 0\\), it corresponds to risk-seeking preferences. This is equivalent to a multiplicative cost \\[J(π) = \\EXP\\Bigl[ \\prod_{t=1}^T \\exp( \\theta c_t(S_t, A_t)) \\Bigr]. \\] Therefore, the dynamic program for multiplicative cost is also applicable for this model.\n\n\n3.3.5 Optimal stopping\nLet \\(\\{S_t\\}_{t \\ge 1}\\) be a Markov chain. At each time \\(t\\), a decision maker observes the state \\(S_t\\) of the Markov chain and decides whether to continue or stop the process. If the decision maker decides to continue, he incurs a continuation cost \\(c_t(S_t)\\) and the state evolves. If the DM decides to stop, he incurs a τtopping cost of \\(d_t(S_t)\\) and the problem is terminated. The objective is to determine an optimal τtopping time \\(\\tau\\) to minimize \\[J(\\tau) := \\EXP\\bigg[ \\sum_{t=1}^{\\tau-1} c_t(S_t) + d_\\tau(S_\\tau)\n\\bigg].\\]\nSuch problems are called Optimal stopping problems.\nDefine the cost-to-go function of any stopping rule as \\[V^{\\tau}_t(s) = \\EXP\\bigg[ \\sum_{τ = t}^{\\tau - 1} c_{\\tau}(S_t) +\nd_\\tau(S_\\tau) \\,\\bigg|\\, \\tau &gt; t \\bigg]\\] and the value function as \\[V_t(s) = \\inf_{\\tau} V^{\\tau}_t(s). \\] Then, it can be shown that the value functions satisfy the following recursion:\n\nProposition 3.2 Dynamic Program for optimal stopping \\[ \\begin{align*}\nV_T(s) &= s_T(s) \\\\\nV_t(s) &= \\min\\{ s_t(s), c_t(s) + \\EXP[ V_{t+1}(S_{t+1}) | S_t = s].\n\\end{align*}\\]\n\nFor more details on the optimal stopping problems, see Ferguson (2008).\n\n\n3.3.6 Minimax setup\nTo be written"
  },
  {
    "objectID": "mdps/intro.html#sec-mdp-cts-spaces",
    "href": "mdps/intro.html#sec-mdp-cts-spaces",
    "title": "3  Finite horizon MDPs",
    "section": "3.4 Continuous state and action spaces",
    "text": "3.4 Continuous state and action spaces\nThe fundamental ideas discussed above also hold for continuous state and action spaces provided one carefully deals with measurability. We first fix some notation:\n\nFor a set \\(\\ALPHABET S\\), let \\(\\mathscr B(\\ALPHABET S)\\) denote the Borel sigma-algebra on \\(\\ALPHABET S\\).\nWe use \\(\\ALPHABET M(\\ALPHABET X, \\ALPHABET Y)\\) to denote the set of measurable functions from the space \\(\\ALPHABET X\\) to the space \\(\\ALPHABET Y\\) (we implicitly assume that the sigma-algebras on \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) is the respective Borel sigma-algebras). When \\(\\ALPHABET Y\\) is \\(\\reals\\), we will sometimes use the notation \\(\\ALPHABET M(\\ALPHABET X)\\).\n\n\nIn order to talk about expected cost, in continuous state spaces we have to assume that the per-step cost is measurable, i.e., \\(c \\in \\ALPHABET M(\\ALPHABET S × \\ALPHABET A)\\), and the \\(P \\colon \\ALPHABET S × \\ALPHABET A × \\mathscr B(\\ALPHABET S) \\to [0,1]\\) is a stochastic kernel, i.e., for every \\((s,a) \\in \\ALPHABET S × \\ALPHABET A\\), \\(p(s,a,\\cdot)\\) is probability measure on \\(\\ALPHABET S\\), and for every \\(B \\in \\ALPHABET S\\), the function \\(p(\\cdot, \\cdot, B) \\in \\ALPHABET M(\\ALPHABET S × \\ALPHABET A)\\). These assumption imply that for every measurable policy \\(π \\in \\ALPHABET M(\\ALPHABET S, \\ALPHABET A)\\), the performance \\(J(\\pi)\\) is well defined. However, these assumptions are not sufficient to establish the optimality of dynamic programmming.\nTo highlight the technical issues, let us consider an MDP with \\(T = 1\\), i.e., a stochastic optimization problem. Define \\[\n  V^*(s) = \\inf_{a \\in \\ALPHABET A} c(s,a)\n  \\quad\\text{and}\\quad\n  π^*(s) = \\arg\\inf_{a \\in \\ALPHABET A} c(s,a).\n\\]\nWe present a few examples below from Blackwell (1965) to highlight technical issues with non-finite state models.\n\nExample 3.1 (No optimal policy) Let \\(\\ALPHABET S = \\{s_\\circ\\}\\), \\(\\ALPHABET A = \\integers_{\\ge 0}\\) Consider \\(c(s_\\circ,a) = (a+1)/a\\). Here \\(v^*(s_\\circ) = 1\\) but there is no policy which achieves this cost.\n\nIn the above example, there is no optimal policy, but given any \\(ε &gt; 0\\), we can identify an \\(ε\\)-optimal policy. The next example shows that we can have a much severe situation where the value function is not measurable. The example relies on the following fact:\n\nThere exist Borel sets in \\(\\reals^2\\) whose projection on \\(\\reals\\) is not Borel. See :the wikipedia article on projections for a discussion.\n\n\nExample 3.2 (Non-measurable value function) Let \\(\\ALPHABET S = \\ALPHABET A = [0,1]\\) and let \\(B \\subset \\ALPHABET S × \\ALPHABET A\\) such that \\(B\\) is measurable but its projection on \\(D\\) onto \\(\\ALPHABET S\\) is not. Consider \\(c(s,a) = -\\IND\\{ (s,a) \\in B \\}\\). Note that \\[\n  v(s) = \\inf_{a \\in \\ALPHABET A} c(s,a) =\n  \\inf_{a \\in \\ALPHABET A} -\\IND\\{ (s,a) \\in B \\} =\n  -\\IND\\{s \\in D \\}\n\\] which is not Borel measurable.\n\nSee Piunovskiy (2011) for various examples on what can go wrong in MDPs. In particular, Example 1.4.15 of Piunovskiy (2011) extends Example 3.2 to provide an example where there is no \\(ε\\)-optimal policy! (Blackwell (1965) also has such an example, but it is much harder to parse).\n\nThere are two ways to resolve the issue with non-existence of optimal policies: either assume that \\(\\ALPHABET A\\) is compact or that the function that we are minimizing is coercive, so that the \\(\\arg\\inf\\) can be replaced by an \\(\\arg\\min\\). Or work with \\(ε\\)-optimal policies.\nResolving the measurability issue is more complicated. There are various measurable selection theorems in the literature to identify sufficient conditions for measurability of the value function and optimal policy. See Hernández-Lerma and Lasserre (1996) and (1999) for an accessible treatment of continous state MDPs."
  },
  {
    "objectID": "mdps/intro.html#notes",
    "href": "mdps/intro.html#notes",
    "title": "3  Finite horizon MDPs",
    "section": "Notes",
    "text": "Notes\nThe proof idea for the optimality of Markov strategies is based on a proof by Witsenhausen (1979) on the structure of optimal coding strategies for real-time communication. Note that the proof does not require us to find a dynamic programming decomposition of the problem. This is in contrast with the standard textbook proof where the optimality of Markov strategies is proved as part of the dynamic programming decomposition.\n\n\n\n\nBlackwell, D. 1964. Memoryless strategies in finite-stage dynamic programming. The Annals of Mathematical Statistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nBlackwell, D. 1965. Discounted dynamic programming. The Annals of Mathematical Statistics 36, 1, 226–235. DOI: 10.1214/aoms/1177700285.\n\n\nFerguson, T.S. 2008. Optimal stopping and applications. Available at: http://www.math.ucla.edu/~tom/Stopping/Contents.html.\n\n\nHernández-Lerma, O. and Lasserre, J.B. 1996. Discrete-time markov control processes. Springer New York. DOI: 10.1007/978-1-4612-0729-0.\n\n\nHernández-Lerma, O. and Lasserre, J.B. 1999. Further topics on discrete-time markov control processes. Springer New York. DOI: 10.1007/978-1-4612-0561-6.\n\n\nPiunovskiy, A.B. 2011. Examples in markov decision processes. Imperial College Proess. DOI: 10.1142/p809.\n\n\nShwartz, A. 2001. Death and discounting. IEEE Transactions on Automatic Control 46, 4, 644–647. DOI: 10.1109/9.917668.\n\n\nWitsenhausen, H.S. 1979. On the structure of real-time source coders. Bell System Technical Journal 58, 6, 1437–1451."
  },
  {
    "objectID": "mdps/gambling.html#computational-experiment",
    "href": "mdps/gambling.html#computational-experiment",
    "title": "4  Optimal gambling",
    "section": "4.1 Computational experiment",
    "text": "4.1 Computational experiment\nTo fix ideas, let’s try to find the optimal policy on our own. An example strategy is given below.\n\nviewof code = Inputs.textarea({label: \"\", height:800, rows:11, width: 800, submit: true,\n   value: `// function bet(t, states, outcomes) {\n// t: current time\n// states: Array of states\n// outcomes: Array of outcomes\n// \n// modify the (javascript) code between the lines:\n// ===============================\n     // As an illustration, we implement the policy to bet\n     //  half of the wealth as long as one is winning. \n     if(t == 0) { \n        return 0.5*states[t] \n     } else { \n        return outcomes[t-1] == 1 ? 0.5*states[t] : 0\n     }\n// ================================\n//}`\n                              })\nviewof strategy = Inputs.radio([\"user code\", \"optimal\"], {value: \"user code\", label: \"Select strategy\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT = 100\nn = 25\nS1 = 100\n\nBernoulli = function(p) { return Math.random() &lt;= p ? 1 : -1 }\n\nuser_strategy = new Function('t', 'states', 'outcomes', code)\n\noptimal_strategy = function(t,states,outcomes) {\n  return p &lt; 0.5 ? 0 : (2*p - 1)*states[t]\n}\n\nbet = function(t, states, outcomes) {\n  return strategy == \"optimal\" ? optimal_strategy(t, states, outcomes) : user_strategy(t, states, outcomes) \n}\n\ndata = { \n  run;\n  var states = new Array(T+1)\n  var outcomes = new Array(T+1)\n  var trajectory = new Array(T+1)\n  var sum = 0\n\n  const initial = 100\n  var idx = 0\n\n  for (var i = 0; i &lt; n; i++) {\n      // Initialize the array to NaN values.\n      for (var t = 0; t &lt; T+1; t++) {\n        states[t] = NaN\n        outcomes[t] = Bernoulli(p)\n      }\n    \n      states[0] = initial\n      var action = 0\n    \n      for (var t = 0; t &lt; T; t++, idx++) {\n        action = bet(t, states, outcomes)\n        states[t+1] = states[t] + outcomes[t] * action\n        trajectory[idx] = { \n          time: t+1, \n          state: states[t],\n          action: action, \n          outcome: outcomes[t],\n          reward: Math.log10(states[t]),\n          sample: i,\n        }\n      }\n      sum += Math.log10(states[T])\n  }\n  return { trajectories: trajectory, mean: sum/n }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssuming that $S_1 = $ , we plot the performance of this policy below. Choosing “optimal” in the radio button above gives the performance of the optimal policy (derived below).\n\nviewof p = Inputs.range([0, 1], {value: 0.6, label: \"p\", step: 0.01})\n\nviewof run = Inputs.button(\"Re-run simulation\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrewardPlot = Plot.plot({\n  grid: true,\n  marginRight: 40,\n  marks: [\n    // Data\n    Plot.line(data.trajectories, {x: \"time\", y: \"reward\", z: \"sample\", stroke: \"gray\", curve: \"step-after\"}),\n    Plot.line(data.trajectories, Plot.groupX({y: \"mean\"}, {x:\"time\", y: \"reward\", stroke: \"red\", strokeWidth: 2, curve: \"step-after\"})),\n\n    // Final value\n    Plot.dot([ [T,data.mean] ], { fill: \"red\"}),\n    Plot.text([ [T,data.mean] ], { text: Math.round(data.mean*100)/100, dx:18, fill:\"red\", fontWeight:\"bold\" }),\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n  ]\n})\n\n\n\n\n\nFigure 4.1: Plot of the performance of the strategy for a horizon of \\(T=\\) . The curves in gray show the performance over $n = $  difference sample paths and the red curve shows its mean. For ease of visualization, we are plotting the utility at each stage (i.e., \\(\\log s_t\\)), even though the reward is only received at the terminal time step. The red line shows the mean performance over the \\(n\\) sample paths. The final mean value of the reward is shown in red. You can toggle the select strategy button to see how the optimal strategy performs (and how close you came to it).\n\n\n\nAs we can see, most intuitive policies do not do so well. We will now see how to compute the optimal policy using dynamic programming."
  },
  {
    "objectID": "mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "href": "mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "title": "4  Optimal gambling",
    "section": "4.2 Optimal gambling strategy and value functions",
    "text": "4.2 Optimal gambling strategy and value functions\nThe above model of optimal gambling is a Markov decision process. Therefore, the optimal solution is given by dynamic programming.\n\nProposition 4.1 (Dynamic programming decomposition) Define the following value function \\(V_t \\colon \\reals_{\\ge 0} \\to \\reals\\) \\[ V_T(s) = \\log s \\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[ \\begin{align*}\nQ_t(s,a) &= \\EXP[ r_t(s,a) + V_{t+1}(S_{t+1}) \\,|\\, S_t = s, A_t = a] \\\\\n&= p V_{t+1}(s+a) + (1-p) V_{t+1}(s-a),\n\\end{align*}\n\\] and \\[ \\begin{align*}\nV_t(s) &=  \\max_{a \\in [0, s]} Q_t(s,a), \\\\\nπ_t(s) &= \\arg \\max_{a \\in [0, s]} Q_t(s,a). \\\\\n\\end{align*}\n\\]\nThen the strategy \\(π = (π_1, \\dots, π_{T-1})\\) is optimal.\n\n\n\n\n\n\n\nRemark\n\n\n\nThe above model is one of the rare instances when the optimal strategy and the optimal strategy and value function of an MDP can be identified in closed form.\n\n\n\nTheorem 4.1 (Optimal gambling strategy) When \\(p \\le 0.5\\):\n\nthe optimal strategy is to not gamble, specifically \\(π_t(s) = 0\\);\nthe value function is \\(V_t(s) = \\log s\\).\n\nWhen \\(p &gt; 0.5\\):\n\nthe optimal strategy is to bet a fraction of the current fortune, specifically \\(π_t(s) = (2p - 1)s\\);\nthe value function is \\(V_t(s) = \\log s + (T - t) C\\), where \\[ C = \\log 2 + p \\log p + (1-p) \\log (1-p).\\]\n\n\nThe constant \\(C\\) defined in Theorem 4.1 is equal to the capacity of a binary symmetric channel! In fact, the above model was introduced by Kelly (1956) to show a gambling interpretation of information rates.\nWe prove the two cases separately.\n\n\n\n\n\n\nProof when \\(p \\le 0.5\\)\n\n\n\n\n\nLet \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p \\le 0.5\\) implies that \\(p \\le 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V_{t+1}(s) = \\log s\\). Now consider\n\\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { - (q - p) s - a } {s^2 - a^2 }\n   \\\\\n   &&lt; 0.\n  \\end{align*}   \n\\]\nThis implies that \\(Q_t(s,a)\\) is decreasing in \\(a\\). Therefore,\n\\[ π_t(s) = \\arg\\max_{a \\in [0, s]} Q_t(s,a) = 0. \\]\nMoreover, \\[ V_t(s) = Q_t(s, π_t(s)) = \\log s.\\]\nThis completes the induction step.\n\n\n\n\n\n\n\n\n\nProof when \\(p &gt; 0.5\\)\n\n\n\n\n\nAs in the previous case, let \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p &gt; 0.5\\) implies that \\(p &gt; 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V_{t+1}(s) = \\log s + (T -t - 1)C\\). Now consider\n\\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { (p - q) s - a } {s^2 - a^2 }\n  \\end{align*}   \n\\]\nSetting \\(\\partial Q_t(s,a)/\\partial a = 0\\), we get that the optimal action is\n\\[ π_t(s) = (p-q) s. \\]\nNote that \\((p-q) \\in (0,1]\\)\n\\[\n  \\frac { \\partial^2 Q_t(s,a) } {\\partial a^2} =\n   - \\frac p { (s + a)^2 } - \\frac q { (s - a)^2 }\n  &lt; 0;\n\\] hence the above action is indeed the maximizer. Moreover, \\[ \\begin{align*}\n  V_t(s) &= Q_t(s, π_t(s))  \\\\\n  &= p V_{t+1}(s + π_t(s)) + q V_{t+1}( s - π_t(s) )\\\\\n  &= \\log s + p \\log (1 + (p-q)) + q \\log (1 - (p-q)) + (T - t -1)C \\\\\n  &= \\log s + p \\log 2p + q \\log 2q + (T - t + 1)C \\\\\n  &= \\log s + (T - t) C\n  \\end{align*}   \n\\]\nThis completes the induction step."
  },
  {
    "objectID": "mdps/gambling.html#generalized-model",
    "href": "mdps/gambling.html#generalized-model",
    "title": "4  Optimal gambling",
    "section": "4.3 Generalized model",
    "text": "4.3 Generalized model\nSuppose that the terminal reward \\(r_T(s)\\) is monotone increasing2 in \\(s\\).2 I use the convention that increasing means weakly increasing. The alternative term non-decreasing implicitly assumes that we are talking about a totally ordered set.\n\nTheorem 4.2 For the generalized optimal gambling problem:\n\nFor each \\(t\\), the value function \\(V_t(s)\\) is monotone increasing in \\(s\\).\nFor each \\(s\\), the value function \\(V_t(s)\\) is monotone decreasing in \\(t\\).\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(s\\)\n\n\n\n\n\nWe proceed by backward induction. \\(V_T(s) = r_T(s)\\) which is monotone increasing in \\(s\\). Assume that \\(V_{t+1}(s)\\) is increasing in \\(s\\). Now, consider \\(V_t(s)\\). Consider \\(s_1, s_2 \\in \\reals_{\\ge 0}\\) such that \\(s_1 \\le s_2\\). Then for any \\(a \\le s_1\\), we have that\n\\[ \\begin{align*}\n    Q_t(s_1, a) &= p V_{t+1}(s_1+a) + q V_{t+1}(s_1-a) \\\\\n    & \\stackrel{(a)}{\\le} p V_{t+1}(s_2 + a) + q V_{t+1}(s_2  - a) \\\\\n    & = Q_t(s_2, a),\n  \\end{align*}\n\\] where \\((a)\\) uses the induction hypothesis. Now consider\n\\[ \\begin{align*}\n  V_t(s_1) &= \\max_{a \\in [0, s_1]} Q_t(s_1, a) \\\\\n  & \\stackrel{(b)}{\\le} \\max_{a \\in [0, s_1]} Q_t(s_2, a) \\\\\n  & \\le \\max_{a \\in [0, s_2]} Q_t(s_2, a) \\\\\n  &= V_t(s_2),\n  \\end{align*}\n\\] where \\((b)\\) uses monotonicity of \\(Q_t\\) in \\(s\\). This completes the induction step.\n\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(t\\)\n\n\n\n\n\nThis is a simple consequence of the following:\n\\[V_t(s) = \\max_{a \\in [0, s]} Q_t(s,a) \\ge Q_t(s,0) = V_{t+1}(s).\\]"
  },
  {
    "objectID": "mdps/gambling.html#exercises",
    "href": "mdps/gambling.html#exercises",
    "title": "4  Optimal gambling",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nNote\n\n\n\nThe purpose of these series of exercises is to generalize the basic result to a model where the gambler can bet on many mutually exclusive outcomes (think of betting on multiple horses in a horse race).\n\n\n\nExercise 4.1 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log w_i\\] subject to:\n\n\\(w_i \\ge 0\\)\n\\(\\sum_{i=1}^n w_i \\le s\\).\n\nShow that the optimal solution is given by \\[ w_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 4.2 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log (s - a + na_i)\\] subject to:\n\n\\(a_i \\ge 0\\)\n\\(a = \\sum_{i=1}^n a_i \\le s\\).\n\nShow that the optimal solution is given by \\[ a_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 4.3 Consider an alternative of the optimal gambling problem where, at each time, the gambler can place bets on many mutually exclusive outcomes. Suppose there are \\(n\\) outcomes, with success probabilities \\((p_1, \\dots, p_n)\\). Let \\((A_{1,t}, \\dots, A_{n,t})\\) denote the amount that the gambler bets on each outcome. The total amount \\(A_t := \\sum_{i=1}^n A_{i,t}\\) must be less than the gambler’s fortune \\(S_t\\). If \\(W_t\\) denotes the winning outcome, then the gambler’s wealth evolves according to \\[ S_{t+1} = S_t - A_t + nU_{W_t, t}.\\] For example, if there are three outcomes, gambler’s current wealth is \\(s\\), the gambler bets \\((a_1, a_2, a_3)\\), and outcome 2 wins, then the gambler wins \\(3 a_2\\) and his fortune at the next time is \\[ s - (a_1 + a_2 + a_3) + 3 a_2. \\]\nThe gambler’s utility is \\(\\log S_T\\), the logarithm of his final wealth. Find the strategy that maximizes the gambler’s expected utility.\nHint: Argue that the value function is of the form \\(V_t(s) = \\log s + (T -t)C\\), where  \\[C = \\log n - H(p_1, \\dots, p_n)\\] where \\(H(p_1, \\dots, p_n) = - \\sum_{i=1}^n p_i \\log p_i\\) is the entropy of a random variable with pmf \\((p_1, \\dots, p_n)\\).The constant \\(C\\) is the capacity of a symmetric discrete memoryless with \\(n\\) outputs and for every input, the output probabilities are a permutation of \\((p_1, \\dots, p_n)\\)."
  },
  {
    "objectID": "mdps/gambling.html#notes",
    "href": "mdps/gambling.html#notes",
    "title": "4  Optimal gambling",
    "section": "Notes",
    "text": "Notes\nThe above model (including the model described in the exercise) was introduced by Kelly (1956). However, Kelly restricted attention to “bet a constant fraction of your fortune” betting strategy and found the optimal fraction. This strategy is sometimes referred to as :Kelly criteria. As far as I know, the dynamic programming treatment of the problem is due to Ross (1974). Ross also considered variations where the objective was to maximize the probability of reaching a preassigned fortune or maximizing the time until becoming broke.\nA generalization of the above model to general logarithmic and exponential utilities is presented in Ferguson and Gilstein (2004).\n\n\n\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004. Optimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nKelly, J.L., Jr. 1956. A new interpretation of information rate. Bell System Technical Journal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nRoss, S.M. 1974. Dynamic programming and gambling models. Advances in Applied Probability 6, 3, 593–606. DOI: 10.2307/1426236."
  },
  {
    "objectID": "mdps/inventory-management.html#dynamic-programming-decomposition",
    "href": "mdps/inventory-management.html#dynamic-programming-decomposition",
    "title": "5  Inventory Management",
    "section": "5.1 Dynamic programming decomposition",
    "text": "5.1 Dynamic programming decomposition\n\\(\\def\\S{\\mathbb{S}}\\)\nThe above model is a Markov decision process.1 Therefore, the optimal solution is given by dynamic programming.1 Part of the per-step cost depends on the future state \\(S_{t+1}\\). It is easy to show that the standard MDP model works even when the per-step cost is a function of \\((S_t, A_t, S_{t+1})\\)\nInstead of \\(\\integers\\), we use \\(\\S\\) to denote the possible values of states. The reason is that we will later consider the case when the state space is the set of reals, and we can still use the same equations.\n\nProposition 5.1 (Dynamic programming) Define the following value functions \\(V_t \\colon \\S \\to \\reals\\) \\[V_{T+1}(s) = 0\\] and for \\(t \\in \\{T, \\dots, 1\\}\\) \\[ Q_t(s, a) = p a + \\EXP[ h(s + a - W_t) + V_{t+1}( s + a - W_t ) ]\\] and \\[ \\begin{align*}\n  V_t(s) &= \\min_{a \\in \\S_{\\ge 0}} Q_t(s,a) \\\\\n  π_t(s) &= \\arg \\min_{a \\in \\S_{\\ge 0}} Q_t(s,a)\n  \\end{align*}\n\\] Then the strategy \\(π = (π_1, \\dots, π_T)\\) is optimal.\n\nIt is possible to simplify the above dynamic program by exploiting a feature of the model. Notice that the dynamics can be split into two parts: \\[ \\begin{align*}\n    Z_t &= S_t + A_t,  \\\\\n    S_{t+1} &= Z_t - W_t.\n   \\end{align*}\n\\] The first part, \\(Z_t\\), depends only on the current state and action. The second part depends only on \\(Z_t\\) and a primitive random variable. In this particular model, \\(Z_t\\) is a deterministic function of \\(S_t\\) and \\(A_t\\); but, in general, it could be stochastic as well; what is important is that the second part should only depend on \\(Z_t\\) and a primitive random variable. The variable \\(Z_t\\) is sometimes called the post-decision state.\nNow write the dynamic program in terms of the post-decision state as follows. Define \\[ H_t(z) = \\EXP[ h(z - W) + V_{t+1}(z-W) ].\\] Then the value function and optimal policy at time \\(t\\) can be written as: \\[ \\begin{align*}\n  V_t(s) &= \\min_{a \\in \\S_{\\ge 0}} \\bigl\\{ pa + H_t(s + a) \\bigr\\}, \\\\\n  π_t(s) &= \\arg \\min_{a \\in \\S_{\\ge 0}} \\bigl\\{ pa + H_t(s + a) \\bigr\\}.\n\\end{align*} \\]\nNote that the problem at each step is similar to the newsvendor problem. So, similar to that model, we try to see if we can establish qualitative properties of the optimal solution.\n\nn = 50\ns = 0.4\n\n// Model parameters\np0 = 5\nch = 4\ncs = 2\n\n\nh = function(s) {\n  if(s &gt;= 0) \n    return ch*s\n  else\n    return -cs*s\n}\n\n// Horizon\nT = 15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbinomial = {\n  // From https://stackoverflow.com/a/37715980/193149\n  const logf = [0, 0, 0.6931471805599453, 1.791759469228055, 3.1780538303479458, 4.787491742782046, 6.579251212010101, 8.525161361065415, 10.60460290274525, 12.801827480081469, 15.104412573075516, 17.502307845873887, 19.987214495661885, 22.552163853123425, 25.19122118273868, 27.89927138384089, 30.671860106080672, 33.50507345013689, 36.39544520803305, 39.339884187199495, 42.335616460753485, 45.38013889847691, 48.47118135183523, 51.60667556776438, 54.78472939811232, 58.00360522298052, 61.261701761002, 64.55753862700634, 67.88974313718154, 71.25703896716801, 74.65823634883016, 78.0922235533153, 81.55795945611504, 85.05446701758152, 88.58082754219768, 92.1361756036871, 95.7196945421432, 99.33061245478743, 102.96819861451381, 106.63176026064346, 110.32063971475739, 114.0342117814617, 117.77188139974507, 121.53308151543864, 125.3172711493569, 129.12393363912722, 132.95257503561632, 136.80272263732635, 140.67392364823425, 144.5657439463449, 148.47776695177302, 152.40959258449735, 156.3608363030788, 160.3311282166309, 164.32011226319517, 168.32744544842765,  172.3527971391628, 176.39584840699735, 180.45629141754378, 184.53382886144948, 188.6281734236716, 192.7390472878449, 196.86618167289, 201.00931639928152, 205.1681994826412, 209.34258675253685, 213.53224149456327, 217.73693411395422, 221.95644181913033, 226.1905483237276, 230.43904356577696, 234.70172344281826, 238.97838956183432, 243.2688490029827, 247.57291409618688, 251.8904022097232, 256.22113555000954, 260.5649409718632, 264.9216497985528, 269.2910976510198, 273.6731242856937, 278.0675734403661, 282.4742926876304, 286.893133295427, 291.3239500942703, 295.76660135076065, 300.22094864701415, 304.6868567656687, 309.1641935801469, 313.65282994987905, 318.1526396202093, 322.66349912672615, 327.1852877037752, 331.7178871969285, 336.26118197919845, 340.815058870799, 345.37940706226686, 349.95411804077025, 354.5390855194408, 359.1342053695754, 363.73937555556347]\n\n  return function(n, k, p) {\n      return Math.exp(logf[n] - logf[n-k] - logf[k]) * p**k * (1-p)**(n-k)\n  }\n}\n\nW = {\n  var points = new Array(n+1)\n  var cumulative = 0\n  var probability = 0\n\n  for(var k = 0; k &lt;= n; k++) {\n    probability = binomial(n,k,s)\n    cumulative += probability\n    points[k] = { demand:k, probability: probability, cumulative: cumulative }\n  }\n\n  return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo fix ideas, let’s solve this dynamic program for a specific instance.\n\ndemandPlot = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(W, {x:\"demand\", y:\"probability\",curve:\"step-after\"})\n  ]\n})\n\n\n\n\n\n\nFigure 5.1: Demand Distribution\n\n\nWe assume that the demand is distributed according to a Binomial(, ) distribution, as shown in Figure 5.1. We assume that the model parameters are as given below:\n\n\\(p =\\) , \\(c_h =\\) , \\(c_s =\\) \n\nWe consider a horizon \\(T =\\) , and solve the dynamic program shown above. The optimal value function and policy are shown below:\n\nDP = {\n   var DP = new Array()\n   var idx = 0\n\n  \n   const Smax =  (W.length - 1)*T\n   var V = new Array(2*Smax + 1)  // -Smax to Smax\n   var H = new Array(3*Smax + 1)  // -Smax to 2Smax\n\n   \n  \n   // Initialize the terminal value function\n   for( var s = -Smax; s &lt;= Smax; s++) {\n      V[s + Smax] = 0\n   }\n\n   // Dynamic programming\n   for( var t = T; t &gt;= 0; t--) {\n     // H(z) = EXP[ h(z-W) + V(z-W) ]\n     for( var z = -Smax; z &lt;= 2*Smax; z++) {\n       H[z + Smax] = 0\n       for( var i = 0; i &lt; W.length; i++ ) {\n         var w = W[i].demand\n         H[z + Smax] += (h(z - w) + V[Math.min(Math.max(z-w, -Smax), Smax) + Smax])*W[i].probability\n       }\n     }\n\n     // V(s) = min_{a in S_{&gt;= 0}} { p*a + H(s+a) }\n     for(var s = -Smax; s &lt;= Smax; s++) {\n       var opt = 0\n       var val = H[s + Smax]\n       for(var a = 1; a &lt;= Smax; a++) {\n         var newVal = p*a + H[Math.min(s+a, 2*Smax) + Smax ]\n         if(newVal &lt;= val) {\n           opt = a\n           val = newVal\n         }\n       }\n       V[s + Smax] = val\n       DP[idx++] = { time: t, state: s, value: val, action: opt }\n     }\n   }\n   \n   \n   return DP\n     \n}\n\n\n\n\n\n\n\nviewof time = Inputs.range([1, T], {label: \"t\", step: 1, value: 1 })\nviewof p = Inputs.range([1, 15], {label: \"Purchase cost\", step: 0.5, value: p0 })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvaluePlot = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(DP.filter(d =&gt; d.time == time && Math.abs(d.state) &lt;= 75), {x:\"state\", y:\"value\", curve:\"step-after\"})\n  ]\n})\n\nactionPlot = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(DP.filter(d =&gt; d.time == time && Math.abs(d.state) &lt;= 20), {x:\"state\", y:\"action\", curve:\"step-after\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Value function\n\n\n\n\n\n\n\n(b) Optimal policy\n\n\n\nFigure 5.2: Dynamic programming solution for the example\n\n\n\nThe plots above suggest that the optimal policy has a structure. Play around with the value of the purchase cost to see if that structure is retained.\nWe will now see how to prove the structure of optimal policy."
  },
  {
    "objectID": "mdps/inventory-management.html#structure-of-optimal-solution",
    "href": "mdps/inventory-management.html#structure-of-optimal-solution",
    "title": "5  Inventory Management",
    "section": "5.2 Structure of optimal solution",
    "text": "5.2 Structure of optimal solution\nFor ease of exposition, we assume that the state space \\(\\S\\) is equal to \\(\\reals\\) (instead of \\(\\integers\\)). See exercise 1 at the end to extend the argument to \\(\\integers\\).\nFor this setting, the optimal policy is then characterized as follows.\n\nTheorem 5.1 Define \\[ s^*_t = \\arg \\min_{z \\in \\reals} \\bigl\\{ p z + H_t(z) \\bigr\\} . \\] Then, \\[\\begin{equation} \\label{eq:V}\nV_t(s) = \\begin{cases}\n  H_t(s_t) + p (s_t - s), &\\text{if } s \\le s^*_t \\\\\n  H_t(s)   , & \\text{otherwise }\n\\end{cases}\n\\end{equation}\\] and \\[\\begin{equation}\\label{eq:pi}\n  π_t(s) = \\begin{cases}\n  s^*_t - s, &\\text{if } s \\le s^*_t \\\\\n  0, & \\text{otherwise }\n\\end{cases}\\end{equation}\\]\nFurthermore, for all \\(t\\), \\(H_t(z)\\) and \\(V_t(s)\\) are convex in \\(z\\) and \\(s\\), respectively.\n\n\n\n\n\n\n\nBase-stock policy\n\n\n\n\n\nThe optimal policy given by \\eqref{eq:pi} is called a base-stock policy. It states that there is a base-stock level \\(\\{s^*_t\\}_{t \\ge 1}\\) for every time step. If, at the beginning of time \\(t\\), the value of the current stock is below the base stock level \\(s^*_t\\), then the optimal decision is to order more goods so that the level of the stock equals the base stock level.\n\n\n\n\npoints = {\n  const n = 100\n  const Smax = 10\n\n  const f = function(s) { return (s-5)**2 }\n\n  var points = new Array()\n  var W = [0, 1, 2 ]\n  var Pw = [1/3, 1/3, 1/3]\n\n  var idx = 0\n  for( var i = 0; i &lt; n; i++) {\n    var s = Smax*i/n\n    var sum = 0\n    var min = 10000 // CHECK: Large positive number\n    for(var w = 0; w &lt; W.length; w++) {\n      sum += f(s + W[w])\n      min = Math.min(min, f(s+W[w]))\n      points[idx++] = { state: s, value: f(s + W[w]), noise:W[w], type: \"noise\" }\n    }\n    points[idx++] = { state: s, value: sum/W.length, type: \"average\" }\n    points[idx++] = { state: s, value: min, type: \"minimum\" }\n  }\n  return points\n  }\n\n\n\n\n\n\nWe first establish some preliminary results.\n\naveragePlot = Plot.plot({\n  grid: true,\n  y: { domain: [0, 25] },\n  marginRight: 40,\n  marginTop: 40,\n\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {filter: d =&gt;  d.value &lt;= 25 && d.type == \"noise\", \n                       x:\"state\", y:\"value\", z:\"noise\", stroke:\"gray\"}),\n    Plot.text(points, Plot.selectLast({\n      filter: d =&gt; d.value &lt;= 25 && d.type == \"noise\", \n      x: \"state\",\n      y: \"value\",\n      z: \"noise\",\n      text: d =&gt; \"w = \" + d.noise,\n      textAnchor: \"start\",\n      dx: 3\n    })),\n    Plot.line(points, {filter: d =&gt; d.value &lt;= 25 && d.type == \"average\", \n                       x:\"state\", y:\"value\", stroke:\"red\", strokeWidth: 4}),\n    Plot.text(points, Plot.selectLast({\n      filter: d =&gt; d.value &lt;= 25 && d.type == \"average\", \n      x: \"state\",\n      y: \"value\",\n      z: \"noise\",\n      text: \"type\",\n      textAnchor: \"start\",\n      fill:\"red\",\n      dy: -10\n    }))\n  ]\n})\n\n\n\n\n\n\nFigure 5.3: An example showing that the average of convex functions is convex\n\n\n\nFor any convex function \\(f \\colon \\reals \\to \\reals\\), \\(F(s) = \\EXP[ f(s - W) ]\\) is convex.\nProof For any realization of \\(W\\), \\(f(s - w)\\) is convex in \\(s\\). The expectation w.r.t. \\(W\\) is simply the sum of convex functions and is, therefore, convex.\nFor any convex function \\(f \\colon \\reals \\to \\reals\\), let \\(s^* = \\arg \\min_{s \\in \\reals} f(s)\\). Then, \\[\\arg \\min_{a \\in \\reals_{\\ge 0}} f(s + a) = \\begin{cases}\n0, & \\text{if } s &gt; s^*, \\\\\ns^* - s, & \\text{if } s \\le s^*\n\\end{cases}\\] and \\[F(s) = \\min_{a \\in \\reals_{\\ge 0}} f(s+a) = \\begin{cases}\nf(s), & \\text{if } s &gt; s^* \\\\\nf(s^*), & \\text{if } s \\le s^*\n\\end{cases}\\] and \\(F(s)\\) is convex in \\(s\\).\n\n\nminimumPlot = Plot.plot({\n  grid: true,\n  y: { domain: [0, 25] },\n  marginRight: 60,\n  marginTop: 40,\n\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {filter: d =&gt;  d.value &lt;= 25 && d.type == \"noise\", \n                       x:\"state\", y:\"value\", z:\"noise\", stroke:\"gray\"}),\n    Plot.text(points, Plot.selectLast({\n      filter: d =&gt; d.value &lt;= 25 && d.type == \"noise\", \n      x: \"state\",\n      y: \"value\",\n      z: \"noise\",\n      text: d =&gt; \"f(s +\" + d.noise + \")\",\n      textAnchor: \"start\",\n      dx: 3\n    })),\n    Plot.line(points, {filter: d =&gt; d.value &lt;= 25 && d.type == \"minimum\", \n                       x:\"state\", y:\"value\", stroke:\"red\", strokeWidth: 4}),\n    Plot.text(points, Plot.selectLast({\n      filter: d =&gt; d.value &lt;= 25 && d.type == \"minimum\", \n      x: \"state\",\n      y: \"value\",\n      text: \"type\",\n      textAnchor: \"start\",\n      fill:\"red\",\n      dy: -10\n    }))\n  ]\n})\n\n\n\n\n\n\nFigure 5.4: An example showing the minimum of \\(f(s)\\), \\(f(s+1)\\), \\(f(s+2)\\).\n\n\nWe first see an illustration of \\(F(s) = \\min\\{ f(s), f(s+1), f(s+2) \\}\\) in Figure 5.4. Note that the resulting function is not convex because \\(a\\) takes only discrete values. But the plot shows that the minimum will look like when we allow \\(a\\) to take continuous values.\nIf there were no constraint on \\(a\\), then the minimizer will be \\(a = s^* - s\\). If \\(s \\le s^*\\), then \\(a = s^* -s \\in \\reals_{\\ge 0}\\) is the minimizer for the constrained problem as well. On the other hand, if \\(s \\ge s^*\\), then the function \\(f(s + a)\\) is increasing as a function of \\(a\\). Hence, the minimizer for the constrained problem is \\(a = 0\\).\n\n\n\n\n\n\nProof of the structural result\n\n\n\n\n\nNow to prove the result, we define \\[ f_t(z) = py + H_t(z). \\] Then, \\[ V_t(s) = \\min_{a \\in \\reals_{\\ge 0}} \\bigl\\{ p(s + a) + H_t(s + a)\n\\bigr\\} - p s\n= \\min_{a \\in \\reals_{\\ge 0}} f_t(s+a) - p s.\n\\] As usual, we prove the result by backward induction. For \\(t=T\\), \\[\\bar Q_T(z) = \\EXP[ h(z - W) ] \\] which is convex because \\(h(z)\\) is convex. \\(f_T(z) = p z + Q_T(z)\\) is the sum of a linear function and convex function and is, therefore, convex. Then, by fact 2 above, \\[π_T(s) = \\arg \\min_{a \\in \\reals_{\\ge 0}} f_T(s+a) = \\max(s^*_T - s, 0)\n\\] and \\[V_T(s) = \\min_{a \\in \\reals_{\\ge 0}} f_T(s + a) - px =\n  \\begin{cases}\n    f_T(s) - p s, & \\text{if } s &gt; s^*_T \\\\\n    f_T(s^*_T) - px, & \\text{if } s \\le s^*_T.\n  \\end{cases}\n\\] Substituting \\(f_t(z) = p z + H_t(z)\\), we get that both \\(V_T\\) and \\(π_T\\) have the desired form and \\(V_T\\) is convex. This forms the basis of induction.\nNow assume that \\(V_{t+1}(s)\\) is convex and of the form \\eqref{eq:V}. Now note that, by fact 1, \\[ H_t(z) = \\EXP[ h(z - W) + V_{t+1}(z - W) ]\\] is convex. Hence, \\(f_t(z)\\) is convex. Therefore, by fact 2 above, \\[ π_t(s) = \\max(s^*_t - s, 0)\\] and \\(V_t(s)\\) is of the desired form and convex.\nThus, the result is holds by induction."
  },
  {
    "objectID": "mdps/inventory-management.html#exercises",
    "href": "mdps/inventory-management.html#exercises",
    "title": "5  Inventory Management",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 5.1 Consider the setting when \\(\\S = \\integers\\). Show that there exists a sequence \\(\\{s_t\\}_{t \\ge 1}\\) of numbres such that policy given by \\[ π_t(s) = \\begin{cases}\n   n, & \\text{if } s_t - n \\le s \\le s_t - n + 1, \\\\\n   0, & \\text{if } s_t \\ge s_t.\n  \\end{cases} \\] is optimal."
  },
  {
    "objectID": "mdps/inventory-management.html#notes",
    "href": "mdps/inventory-management.html#notes",
    "title": "5  Inventory Management",
    "section": "Notes",
    "text": "Notes\nInventory management models with deterministic demand were introduced by Harris (1913). The mathematical model of inventory management considered here was originally proposed by Arrow et al. (1952). The optimality of base-stock policy was established by Bellman et al. (1955). See the notes on infinite horizon version of this model to see how to find the threshold in closed form.\nThe problem for Exercise 1 is from Veinott (1965). See Tsitsiklis (1984) for a partial characterization of the optimal policy with non-zero ordering costs.\n\n\n\n\n\nArrow, K.J., Harris, T., and Marschak, J. 1952. Optimal inventory policy. Econometrica 20, 1, 250–272. DOI: 10.2307/1907830.\n\n\nBellman, R., Glicksberg, I., and Gross, O. 1955. On the optimal inventory equation. Management Science 2, 1, 83–104. DOI: 10.1287/mnsc.2.1.83.\n\n\nHarris, F.W. 1913. How many parts to make at once. The magazine of management 10, 2, 135–152. DOI: 10.1287/opre.38.6.947.\n\n\nTsitsiklis, J.N. 1984. Periodic review inventory systems with continuous demand and discrete order sizes. Management Science 30, 10, 1250–1254. DOI: 10.1287/mnsc.30.10.1250.\n\n\nVeinott, A.F. 1965. The optimal inventory policy for batch ordering. Operations Research 13, 3, 424–432. DOI: 10.1287/opre.13.3.424."
  },
  {
    "objectID": "mdps/monotone-mdps.html#stochastic-dominance",
    "href": "mdps/monotone-mdps.html#stochastic-dominance",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.1 Stochastic dominance",
    "text": "6.1 Stochastic dominance\n Let \\(\\ALPHABET S\\) be a totally ordered finite set, say \\(\\{1, \\dots, n\\}\\).Stochastic dominance is a partial order on random variables defined on totally ordered sets\n\n\n\n\n\n\n(First order) stochastic dominance\n\n\n\nSuppose \\(S^1\\) and \\(S^2\\) are \\(\\ALPHABET S\\) valued random variables where \\(S^1 \\sim \\mu^1\\) and \\(S^2 \\sim \\mu^2\\). We say \\(S^1\\) stochastically dominates \\(S^2\\) if for any \\(s \\in \\ALPHABET S\\), \\[\\begin{equation}\\label{eq:inc-prob}\n  \\PR(S^1 \\ge s) \\ge \\PR(S^2 \\ge s).\n\\end{equation}\\]\nStochastic domination is denoted by \\(S^1 \\succeq_s S^2\\) or \\(\\mu^1 \\succeq_s \\mu^2\\).\n\n\nLet \\({\\rm M}^1\\) and \\({\\rm M}^2\\) denote the CDF of \\(\\mu^1\\) and \\(\\mu^2\\). Then \\eqref{eq:inc-prob} is equivalent to the following: \\[\\begin{equation}\\label{eq:cdf}\n  {\\rm M}^1_s \\le {\\rm M}^2_s, \\quad \\forall s \\in \\ALPHABET S.\n\\end{equation}\\] Thus, visually, \\(S^1 \\succeq_s S^2\\) means that the CDF of \\(S^1\\) lies below the CDF of \\(S^2\\).\n\n\n\n\n\n\nExample\n\n\n\n\\(\\left[0, \\frac 14, \\frac 14, \\frac 12\\right] \\succeq_s \\left[\\frac 14, 0, \\frac 14, \\frac 12 \\right] \\succeq_s \\left[\\frac 14, \\frac 14, \\frac 14, \\frac 14 \\right].\\)\n\n\nStochastic dominance is important due to the following property.\n\nTheorem 6.1 Let \\(f \\colon \\ALPHABET S \\to \\reals\\) be a (weakly) increasing function and \\(S^1 \\sim \\mu^1\\) and \\(S^2 \\sim \\mu^2\\) are random variables defined on \\(\\ALPHABET S\\). Then \\(S^1 \\succeq_s S^2\\) if and only if \\[\\begin{equation}\\label{eq:inc-fun}\n  \\EXP[f(S^1)] \\ge \\EXP[f(S^2)].\n\\end{equation}\\]\n\n\n\n\n\n\n\nAbel’s lemma or summation by parts\n\n\n\nFor any two sequences \\(\\{f_k\\}_{k \\ge 1}\\) and \\(\\{g_k\\}_{k \\ge 1}\\), \\[\\sum_{k=m}^n f_k(g_{k+1} - g_{k}) =\n(f_n g_{n+1} - f_m g_m) + \\sum_{k=m+1}^n g_k(f_{k+1} - f_k).\\]\nSummation by parts may be viewed as the discrete analog of integration by parts: \\[\n\\int_a^b f(x) g'(x) dx = f(x) g(x)\\Big|_{a}^{b} - \\int_{a}^{b} f'(x)g(x)dx.\n\\] An alternative form which is sometimes useful is: \\[\nf_n g_n - f_m g_m =\n\\sum_{k=m}^{n-1} f_k \\Delta g_k\n+\n\\sum_{k=m}^{n-1} g_k \\Delta f_k\n+\n\\sum_{k=m}^{n-1} \\Delta f_k \\Delta g_k.\n\\]\n\n\n\n\n\n\n\n\nProof (stochastic dominance implies monotone expectations)\n\n\n\n\n\nFor the ease of notation, let \\(f_i\\) to denote \\(f(i)\\) and define \\({\\rm M}^1_0 = {\\rm M}^2_0 = 0\\). Consider the following: \\[\\begin{align*}\n    \\sum_{i=1}^n f_i \\mu^1_i\n    &= \\sum_{i=1}^n f_i ({\\rm M}^1_i - {\\rm M}^1_{i-1})\n    \\\\\n    &\\stackrel{(a)}= \\sum_{i=1}^n {\\rm M}^1_{i-1} (f_{i-1} - f_{i}) + f_n {\\rm M}^1_n\n    \\\\\n    &\\stackrel{(b)}{\\ge}\n    \\sum_{i=1}^n {\\rm M}^2_{i-1} (f_{i-1} - f_{i}) + f_n {\\rm M}^2_n\n    \\\\\n    &\\stackrel{(a)}= \\sum_{i=1}^n f_i ({\\rm M}^2_i - {\\rm M}^2_{i-1})\n    \\\\\n    &= \\sum_{i=1}^n f_i \\mu_i,\n\\end{align*}\\] which completes the proof. In the above equations, both steps marked \\((a)\\) use summation by parts and \\((b)\\) uses the following facts:\n\nFor any \\(i\\), \\({\\rm M}^1_{i-1} \\le {\\rm M}^2_{i-1}\\) (because of \\eqref{eq:cdf}) and \\(f_{i-1} - f_{i} &lt; 0\\) (because \\(f\\) is increasing function). Thus, \\[{\\rm M}^1_{i-1}(f_{i-1} - f_i) \\ge {\\rm M}^2_{i-1}(f_{i-1} - f_i). \\]\n\\({\\rm M}^1_n = {\\rm M}^2_n = 1\\).\n\n\n\n\n\n\n\n\n\n\nProof (monotone expectations implies stochastic monotonicity)\n\n\n\n\n\nSuppose for any increasing function \\(f\\), \\eqref{eq:inc-fun} holds. Given any \\(i \\in \\{1, \\dots, n\\}\\), define the function \\(f_i(k) = \\IND\\{k &gt; i\\}\\), which is an increasing function of \\(k\\). Then, \\[ \\EXP[f_i(S)] = \\sum_{k=1}^n f_i(k) \\mu^1_k = \\sum_{k &gt; i} \\mu^1_k = 1 - {\\rm M}^1_i.\n\\] By a similar argument, we have \\[ \\EXP[f_i(S^2)] = 1 - {\\rm M}^2_i. \\] Since \\(\\EXP[f_i(S)] \\ge \\EXP[f_i(S^2)]\\), we have that \\({\\rm M}^1_i \\le {\\rm M}^2_i\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#stochastic-monotonicity",
    "href": "mdps/monotone-mdps.html#stochastic-monotonicity",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.2 Stochastic monotonicity",
    "text": "6.2 Stochastic monotonicity\nStochastic monotonicity extends the notion of stochastic dominance to Markov chains. Suppose \\(\\ALPHABET S\\) is a totally ordered set and \\(\\{S_t\\}_{t \\ge 1}\\) is a time-homogeneous Markov chain on \\(\\ALPHABET S\\) with transition probability matrix \\(P\\). Let \\(P_i\\) denote the \\(i\\)-th row of \\(P\\). Note that \\(P_i\\) is a PMF.\n\n\n\n\n\n\nStochastic monotonicity\n\n\n\nA Markov chain with transition matrix \\(P\\) is stochastically monotone if \\[ P_i \\succeq_s P_j, \\quad \\forall i &gt; j. \\]\n\n\nAn immediate implication is the following.\n\nTheorem 6.2 Let \\(\\{S_t\\}_{t \\ge 1}\\) be a Markov chain with transition matrix \\(P\\) and \\(f \\colon \\ALPHABET S \\to \\reals\\) is a weakly increasing function. Then, for any \\(s^1, s^2 \\in \\ALPHABET S\\) such that \\(s^1 &gt; s^2\\), \\[ \\EXP[f(S_{t+1}) | S_t = s^1] \\ge \\EXP[ f(S_{t+1}) | S_t = s^2], \\] if and only if \\(P\\) is stochatically monotone."
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotonicity-of-value-functions",
    "href": "mdps/monotone-mdps.html#monotonicity-of-value-functions",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.3 Monotonicity of value functions",
    "text": "6.3 Monotonicity of value functions\n\nTheorem 6.3 Consider an MDP where the state space \\(\\ALPHABET S\\) is totally ordered. Suppose the following conditions are satisfied.\nC1. For every \\(a \\in \\ALPHABET A\\), the per-step cost \\(c_t(s,a)\\) is weakly inceasing in \\(s\\).\nC2. For every \\(a \\in \\ALPHABET A\\), the transition matrix \\(P(a)\\) is stochastically monotone.\nThen, the value function \\(V_t(s)\\) is weakly increasing in \\(s\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe result above also applies to models with continuous (and totally ordered) state space provided the measurable selection conditions hold so that the arg min at each step of the dynamic program is attained.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe proceed by backward induction. By definition, \\(V_{T+1}(s) = 0\\), which is weakly increasing. This forms the basis of induction. Assume that \\(V_{t+1}(s)\\) is weakly increasing. Now consider, \\[Q_t(s,a) = c_t(s,a) + \\EXP[V_{t+1}(S_{t+1}) | S_t = s, A_t = a].\\] For any \\(a \\in \\ALPHABET A\\), \\(Q_t(s,a)\\) is a sum of two weakly increasing functions in \\(s\\); hence \\(Q_t(s,a)\\) is weakly increasing in \\(s\\).\nNow consider \\(s_1, s_2 \\in \\ALPHABET S\\) such that \\(s_1 &gt; s_2\\). Suppose \\(a_1^*\\) is the optimal action at state \\(s_1\\). Then \\[\n  V_t(s^1) = Q_t(s^1, a_1^*) \\stackrel{(a)}\\ge Q_t(s^2,a_1^*) \\stackrel{(b)}\\ge V_t(s_2),\n\\] where \\((a)\\) follows because \\(Q_t(\\cdot, u^*)\\) is weakly increasing and \\((b)\\) follows from the definition of the value function."
  },
  {
    "objectID": "mdps/monotone-mdps.html#submodularity",
    "href": "mdps/monotone-mdps.html#submodularity",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.4 Submodularity",
    "text": "6.4 Submodularity\n\n\n\n\n\n\nSubmodularity\n\n\n\nLet \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be partially ordered sets. A function \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) is called submodular if for any \\(x^+ \\ge x^-\\) and \\(y^+ \\ge y^-\\), we have \\[\\begin{equation}\\label{eq:submodular}\n  f(x^+, y^+) + f(x^-, y^-) \\le f(x^+, y^-) + f(x^-, y^+).\n\\end{equation}\\]\nThe function is called supermodular if the inequality in \\eqref{eq:submodular} is reversed.\n\n\nA continuous and differentiable function on \\(\\reals^2\\) is submodular iff \\[ \\frac{ \\partial^2 f(x,y) }{ \\partial x \\partial y } \\le 0,\n  \\quad \\forall x,y.\n\\] If the inequality is reversed, then the function is supermodular.\nSubmodularity is a useful property because it implies monotonicity of the arg min.\n\nTheorem 6.4 Let \\(\\ALPHABET X\\) be a partially ordered set, \\(\\ALPHABET Y\\) be a totally ordered set, and \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) be a submodular function. Suppose that for all \\(x\\), \\(\\arg \\min_{y \\in \\ALPHABET Y} f(x,y)\\) exists. Then, \\[\n  π(x) := \\max \\{ y^* \\in \\arg \\min_{y \\in \\ALPHABET Y} f(x,y) \\}\n\\] is weakly increasing in \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(x^+, x^- \\in \\ALPHABET X\\) such that \\(x^+ \\ge x^-\\). Since \\(f\\) is submodular, for any \\(y \\le π(x^-)\\), we have \\[\\begin{equation}\\label{eq:1}\n  f(x^+, π(x^-)) - f(x^+, y) \\le f(x^-, π(x^-)) - f(x^-, y) \\le 0,\n\\end{equation}\\] where the last inequality follows because \\(π(x^-)\\) is the arg min of \\(f(x^-, y)\\). Eq. \\eqref{eq:1} implies that for all \\(y \\le π(x^-)\\), \\[\n  f(x^+, π(x^-)) \\le f(x^+, y).\n\\] Thus, \\(π(x^+) \\ge π(x^-)\\).\n\n\n\nThe analogue of Theorem 6.4 for supermodular functions is as follows.\n\nTheorem 6.5 Let \\(\\ALPHABET X\\) be a partially ordered set, \\(\\ALPHABET Y\\) be a totally ordered set, and \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) be a supermodular function. Suppose that for all \\(x\\), \\(\\arg \\min_{y \\in \\ALPHABET Y} f(x,y)\\) exists. Then, \\[\n  π(x) := \\min \\{ y^* \\in \\arg \\min_{y \\in \\ALPHABET Y} f(x,y) \\}\n\\] is weakly decreasing in \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof is similar to Theorem 6.4.\nConsider \\(x^+, x^- \\in \\ALPHABET X\\) such that \\(x^+ \\ge x^-\\). Since \\(f\\) is supermodular, for any \\(y \\ge π(x^-)\\), we have \\[\\begin{equation}\\label{eq:2}\n  f(x^+, y) - f(x^+, π(x^-)) \\ge f(x^-, y) - f(x^-, π(x^-)) \\ge 0,\n\\end{equation}\\] where the last inequality follows because \\(π(x^-)\\) is the arg min of \\(f(x^-, y)\\). Eq. \\eqref{eq:2} implies that for all \\(y \\ge π(x^-)\\), \\[\n  f(x^+, y) \\ge f(x^+, π(x^-)).\n\\] Thus, \\(π(x^+) \\le π(x^-)\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotonicity-of-optimal-policy",
    "href": "mdps/monotone-mdps.html#monotonicity-of-optimal-policy",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.5 Monotonicity of optimal policy",
    "text": "6.5 Monotonicity of optimal policy\n\nTheorem 6.6 Consider an MDP where the state space \\(\\ALPHABET S\\) and the action space \\(\\ALPHABET A\\) are totally ordered. Suppose that, in addition to (C1) and (C2), the following condition is satisfied.\nC3. For any weakly increasing function \\(v\\), \\[ c_t(s,a) + \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\] is submodular in \\((s,a)\\).\nLet \\(π^*_t(s) = \\max\\{ a^* \\in \\arg \\min_{a \\in \\ALPHABET A} Q_t(s,a) \\}\\). Then, \\(π^*(s)\\) is weakly increasing in \\(s\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConditions (C1) and (C2) imply that the value function \\(V_{t+1}(s)\\) is weakly increasing. Therefore, condition (C3) implies that \\(Q_t(s,a)\\) is submodular in \\((s,a)\\). Therefore, the arg min is weakly increasing in \\(x\\)\n\n\n\nIt is difficult to verify condition (C3). The following conditions are sufficient for (C3).\n\nLemma 6.1 Consider an MDP with totally ordered state and action spaces. Suppose\n\n\\(c_t(s,a)\\) is submodular in \\((s,a)\\).\nFor all \\(s' \\in \\ALPHABET S\\), \\(H(s' | s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a)\\) is submodular in \\((s,a)\\).\n\nThe condition (C3) of the previous theorem holds.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s^+, s^- \\in \\ALPHABET S\\) and \\(a^+, a^- \\in \\ALPHABET A\\) such that \\(s^+ &gt; s^-\\) and \\(a^+ &gt; a^-\\). Define\n\\[\\begin{align*}\n  μ_1(s) &= \\tfrac 12 P_{s^- s}(a^-) + \\tfrac 12 P_{s^+ s}(a^+), \\\\\n  μ_2(s) &= \\tfrac 12 P_{s^- s}(a^+) + \\tfrac 12 P_{s^+ s}(a^-).\n\\end{align*}\\] Since \\(H(s' | s,a)\\) is submodular, we have \\[ H(s' | s^+, a^+) + H(s' | s^-, a^-) \\le H(s' | s^+, a^-) + H(s' | s^-, a^+) \\] or equivalently, \\[\\sum_{z \\le s'} \\big[ P_{s^+ z}(a^+) + P_{s^- z}(a^-) \\big]\n  \\ge\n  \\sum_{z \\le s'} \\big[ P_{s^+ z}(a^-) + P_{s^- z}(a^+) \\big]. \\] which implies \\[ M_1(s') \\ge M_2(s')\\] where \\(M_1\\) and \\(M_2\\) are the CDFs of \\(μ_1\\) and \\(μ_2\\). Thus, \\(μ_1 \\preceq_s μ_2\\).\nHence, for any weakly increasing function \\(v \\colon \\ALPHABET S \\to \\reals\\), \\[ \\sum_{s' \\in \\ALPHABET S} μ_1(s') v(s') \\le\n   \\sum_{s' \\in \\ALPHABET S} μ_2(s') v(s').\\] Or, equivalently, \\[H(s^+, a^+) + H(s^-, a^-) \\le H(s^-, a^+) + H(s^+, a^-)\\] where \\(H(s,a) = \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\).\nTherefore, \\(c_t(s,a) + H_t(s,a)\\) is submodular in \\((s,a)\\).\n\n\n\nThe analogue of Theorem 6.6 for supermodular functions is as follows.\n\nTheorem 6.7 Consider an MDP where the state space \\(\\ALPHABET S\\) and the action space \\(\\ALPHABET A\\) are totally ordered. Suppose that, in addition to (C1) and (C2), the following condition is satisfied.\nC4. For any weakly increasing function \\(v\\), \\[ c_t(s,a) + \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\] is supermodular in \\((s,a)\\).\nLet \\(π^*_t(s) = \\min\\{ a^* \\in \\arg \\min_{a \\in \\ALPHABET S} Q_t(s,a) \\}\\). Then, \\(π^*(s)\\) is weakly decreasing in \\(s\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConditions (C1) and (C2) imply that the value function \\(V_{t+1}(s)\\) is weakly increasing. Therefore, condition (C4) implies that \\(Q_t(s,a)\\) is supermodular in \\((s,a)\\). Therefore, the arg min is decreasing in \\(s\\)\n\n\n\nIt is difficult to verify condition (C4). The following conditions are sufficient for (C4).\n\nLemma 6.2 Consider an MDP with totally ordered state and action spaces. Suppose\n\n\\(c_t(s,a)\\) is supermodular in \\((s,a)\\).\nFor all \\(s' \\in \\ALPHABET S\\), \\(H(s' | s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a)\\) is supermodular in \\((s,a)\\).\n\nThe condition (C4) of the previous theorem holds.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s^+, s^- \\in \\ALPHABET S\\) and \\(a^+, a^- \\in \\ALPHABET A\\) such that \\(s^+ &gt; s^-\\) and \\(a^+ &gt; a^-\\). Define\n\\[\\begin{align*}\n  μ_1(s) &= \\tfrac 12 P_{s^- s}(a^-) + \\tfrac 12 P_{s^+ s}(a^+), \\\\\n  μ_2(s) &= \\tfrac 12 P_{s^- s}(a^+) + \\tfrac 12 P_{s^+ s}(a^-).\n\\end{align*}\\] Since \\(H(s' | s,a)\\) is supermodular, we have \\[ H(s' | s^+, a^+) + H(s' | s^-, a^-) \\ge H(s' | s^+, a^-) + H(s' | s^-, a^+) \\] or equivalently, \\[\\sum_{s' \\le s'} \\big[ P_{s^+ s'}(a^+) + P_{s^- s'}(a^-) \\big]\n  \\le\n  \\sum_{s' \\le s'} \\big[ P_{s^+ s'}(a^-) + P_{s^- s'}(a^+) \\big]. \\] which implies \\[ M_1(s') \\le M_2(s')\\] where \\(M_1\\) and \\(M_2\\) are the CDFs of \\(μ_1\\) and \\(μ_2\\). Thus, \\(μ_1 \\succeq_s μ_2\\).\nHence, for any weakly increasing function \\(v \\colon \\ALPHABET S \\to \\reals\\), \\[ \\sum_{s' \\in \\ALPHABET S} μ_1(s') v(s') \\ge\n   \\sum_{s' \\in \\ALPHABET S} μ_2(s') v(s').\\] Or, equivalently, \\[H(s^+, a^+) + H(s^-, a^-) \\ge H(s^-, a^+) + H(s^+, a^-)\\] where \\(H(s,a) = \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\).\nTherefore, \\(c_t(s,a) + H_t(s,a)\\) is supermodular in \\((s,a)\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#constraints-on-actions",
    "href": "mdps/monotone-mdps.html#constraints-on-actions",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.6 Constraints on actions",
    "text": "6.6 Constraints on actions\nIn the results above, we have assumed that the action set \\(\\ALPHABET A\\) is the same for all states. The results also extend to the case when the action at state \\(s\\) must belong to some set \\(\\ALPHABET A(s)\\) provided the following conditions are satisfied:\n\nFor any \\(s \\ge s'\\), \\(\\ALPHABET A(s) \\supseteq \\ALPHABET A(s')\\)\nFor any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A(s)\\), \\(a' &lt; a\\) implies that \\(a' \\in \\ALPHABET A(s)\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotone-dynamic-programming",
    "href": "mdps/monotone-mdps.html#monotone-dynamic-programming",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.7 Monotone dynamic programming",
    "text": "6.7 Monotone dynamic programming\nIf we can establish that the optimal policy is monontone, then we can use this structure to implement the dynamic program more efficient. Suppose \\(\\ALPHABET S = \\{1, \\dots, n\\}\\) and \\(\\ALPHABET A = \\{1, \\dots. m\\}\\). The main idea is as follows. Suppose \\(V_{t+1}(\\cdot)\\) has been caclulated. Insead of computing \\(Q_t(s,a)\\) and \\(V_t(s)\\), proceed as follows:\n\nSet \\(s = 1\\) and \\(α = 1\\).\nFor all \\(u \\in \\{α, \\dots, m\\}\\), compute \\(Q_t(s,a)\\) as usual.\nCompute\n\\[V_t(s) = \\min_{ α \\le a \\le m } Q_t(s,a)\\]\nand set\n\\[π_t^*(s) = \\max \\{ a \\in \\{α, \\dots, m\\} : V_t(s) = Q_t(s,a) \\}.\\]\nIf \\(s = n\\), then stop. Otherwise, set \\(α = π_t^*(s)\\) and \\(s = s+1\\) and go to step 2."
  },
  {
    "objectID": "mdps/monotone-mdps.html#example-a-machine-replacement-model",
    "href": "mdps/monotone-mdps.html#example-a-machine-replacement-model",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.8 Example: A machine replacement model",
    "text": "6.8 Example: A machine replacement model\nLet’s revisit the machine replacement problem presented at the beginning of this section. For simplicity, we’ll assume that \\(n = ∞\\), i.e., the state space is countable. In this case, the transition matrices are given by \\[ P_{sz}(0) = \\begin{cases}\n  0, & z &lt; s \\\\\n  μ_{z - s}, & z \\ge s\n\\end{cases}\n\\quad\\text{and}\\quad\nP_sz(1) = μ_z.\n\\] where \\(μ\\) is the PMF of \\(W\\).\n\nProposition 6.1 For the machine replacement problem, there exist a series of thresholds \\(\\{s^*_t\\}_{t = 1}^T\\) such that the optimal policy at time \\(t\\) is a threshold policy with threshold \\(s_t\\), i.e., \\[\n  π_t(s) = \\begin{cases}\n  0 & \\text{if $s &lt; s_t^*$} \\\\\n  1 & \\text{otherwise}\n\\end{cases}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by verifying conditions (C1)–(C4) to establish that the optimal policy is monotone.\nC1. For \\(a = 0\\), \\(c(s,0) = h(s)\\), which is weakly increasing by assumption. For \\(a = 1\\), \\(c(s,1) = K\\), which is trivially weakly increasing.\nC2. For \\(a = 0\\), \\(P(0)\\) is stochastically monotone (because the CDF of \\(P(\\cdot | s, 0)\\) lies above the CDF of \\(P(\\cdot | s+1, 0)\\)). For \\(a = 1\\), all rows of \\(P(1)\\) are the same; therefore \\(P(1)\\) is stochastically monotone.\nSince (C1) and (C2) are satisfied, by Theorem 6.3, we can assert that the value function is weakly increasing.\nC3. \\(c(s,1) - c(s,0) = K - h(s)\\), which is weakly decreasing in \\(s\\). Therefore, \\(c(s,a)\\) is submodular in \\((s,a)\\).\nC4. Recall that \\(H(s'|s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a).\\) Therefore,\n\\[H(s'|s,0) = 1 - \\sum_{z = s}^{s'} μ_{z -s} = 1 - \\sum_{k = 0}^{s' - s} μ_k\n= 1 - M_{s' - s},\\] where \\(M\\) is the CMF of \\(μ\\), and \\[H(s'|s,1) = 1 - \\sum_{z \\le s'} μ_z = 1 - M_{s'},\\]\nTherefore, \\(H(s'|s,1) - H(s'|s,0) = M_{s'-s} - M_{s'}\\). For any fixed \\(s'\\), \\(H(s'|s,1) - H(s'|s,0)\\) is weakly decreasing in \\(s\\). There \\(H(s'|s,a)\\) is submodular in \\((s,a)\\).\nSince (C1)–(C4) are satisfied, the optimal policy is weakly increasing in~\\(s\\). Since there are only two actions, it means that for every time, there exists a state \\(s^*_t\\) with the property that if \\(s\\) exceeds \\(s^*_t\\), the optimal decision is to replace the machine; and if \\(s \\le s^*_t\\), then the optimal decision is to operate the machine for another period."
  },
  {
    "objectID": "mdps/monotone-mdps.html#exercises",
    "href": "mdps/monotone-mdps.html#exercises",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 6.1 Let \\(T\\) denote a upper triangular matrix with 1’s on or below the diagonal and 0’s above the diagonal. Then \\[ T^{-1}_{ij} = \\begin{cases}\n  1, & \\text{if } i = j, \\\\\n-1, & \\text{if } i = j + 1, \\\\\n  0, & \\text{otherwise}.\n\\end{cases}\\]\nFor example, for a \\(4 \\times 4\\) matrix \\[\n  T = \\MATRIX{1 & 1 & 1 & 1 \\\\ 0 & 1 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 1},\n  \\quad\n  T^{-1} = \\MATRIX{1 & -1 & 0 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 1 & -1 \\\\\n  0 & 0 & 0 & 1 }.\n\\]\nShow the following:\n\nFor any two PMFs \\(μ^1\\) and \\(μ^2\\), \\(\\mu^1 \\succeq_s \\mu^2\\) iff \\(\\mu^1 T \\ge \\mu^2 T\\).\nA Markov transition matrix \\(P\\) is stochastic monotone iff \\(T^{-1} P T \\ge 0\\).\n\n\n\nExercise 6.2 Show that the following are equivalent:\n\nA transition matrix \\(P\\) is stochastically monotone\nFor any two PMFs \\(μ^1\\) and \\(μ^2\\), if \\(\\mu^1 \\succeq_s \\mu^2\\) then \\(\\mu^1P \\succeq_s \\mu^2P\\).\n\n\n\nExercise 6.3 Show that if two transition matrices \\(P\\) and \\(Q\\) have the same dimensions and are stochastically monotone, then so are:\n\n\\(\\lambda P + (1 - \\lambda) Q\\), where \\(\\lambda \\in (0,1)\\).\n\\(P Q\\)\n\\(P^k\\), for \\(k \\in \\integers_{&gt; 0}\\).\n\n\n\nExercise 6.4 Let \\(\\mu_t\\) denote the distribution of a Markov chain at time \\(t\\). Suppose \\(\\mu_0 \\succeq_s \\mu_1\\). Then \\(\\mu_t \\succeq_s \\mu_{t+1}\\).\n\n\n\nExercise 6.5 Consider the example of machine repair presented in notes on matrix formulation of MDPs. Prove that the optimal policy for that model is weakly increasing.\n\n\nExercise 6.6 Suppose the state space \\(\\ALPHABET S\\) is a symmetric subset of integers of the form \\(\\{-L, -L + 1, \\dots, L-1, L\\}\\) and the action space \\(\\ALPHABET A\\) is discrete. Let \\(\\ALPHABET S_{\\ge 0}\\) denote the set \\(\\{0, \\dots, L\\}\\).\nLet \\(P(a)\\) denote the controlled transition matrix and \\(c_t(s,a)\\) denote the per-step cost. To avoid ambiguity, we define the optimal policy as \\[\nπ^*_t(s) = \\begin{cases}\n    \\max\\bigl\\{ a' \\in \\arg\\min_{a \\in \\ALPHABET A} Q_t(s,a) \\bigr\\},\n    & \\text{if } s \\ge 0 \\\\\n    \\min\\bigl\\{ a' \\in \\arg\\min_{a \\in \\ALPHABET A} Q_t(s,a) \\bigr\\},\n    & \\text{if } s &lt; 0\n\\end{cases}\\] The purpose of this exercise is to identify conditions under which the value function and the optimal policy are even and :quasi-convex. We do so using the following steps.\n\nWe say that the transition probability matrix \\(P(a)\\) is even if for all \\(s, s' \\in \\ALPHABET S\\), \\(P(s'|s,a) = P(-s'|-s,a)\\). Prove the following result.\n\n\nProposition 6.2 Suppose the MDP satisfies the following properties:\n(A1) For every \\(t\\) and \\(a \\in \\ALPHABET A\\), \\(c_t(s,a)\\) is even function of \\(s\\).\n(A2) For every \\(a \\in \\ALPHABET A\\), \\(P(a)\\) is even.\nThen, for all \\(t\\), \\(V_t\\) and \\(π_t\\) are even functions.\n\n\nGiven any probability mass function \\(μ\\) on \\(\\ALPHABET S\\), define the folded probability mass function \\(\\tilde μ\\) on \\(\\ALPHABET S_{\\ge 0}\\) as follows: \\[ \\tilde μ(s) = \\begin{cases}\n   μ(0), & \\text{if } s = 0 \\\\\n   μ(s) + μ(-s), & \\text{if } s &gt; 0.\n\\end{cases} \\]\n\nFor ease of notation, we use \\(\\tilde μ = \\mathcal F μ\\) to denote this folding operation. Note that an immediate consequence of the definition is the following (you don’t have to prove this).\n\nLemma 6.3 If \\(f \\colon \\ALPHABET S \\to \\reals\\) is even, then for any probability mass function \\(μ\\) on \\(\\ALPHABET S\\) and \\(\\tilde μ = \\mathcal F μ\\), we have \\[\n  \\sum_{s \\in \\ALPHABET S} f(s) μ(s) =\n  \\sum_{s \\in \\ALPHABET S_{\\ge 0}} f(s) \\tilde μ(s). \\]\n\nThus, the expectation of the function \\(f \\colon \\ALPHABET S \\to \\reals\\) with respect to the PMF \\(μ\\) is equal to the expectation of the function \\(f \\colon \\ALPHABET S_{\\ge 0} \\to \\reals\\) with respect to the PMF \\(\\tilde μ = \\mathcal F μ\\).\nNow given any probability transition matrix \\(P\\) on \\(\\ALPHABET S\\), we can define a probability transition matrix \\(\\tilde P\\) on \\(\\ALPHABET S_{\\ge 0}\\) as follows: for any \\(s \\in \\ALPHABET S\\), \\(\\tilde P_s = \\mathcal F P_s\\), where \\(P_s\\) denotes the \\(s\\)-th row of \\(P\\). For ease of notation, we use \\(\\tilde P = \\mathcal F P\\) to denote this relationship.\nNow prove the following:\n\nProposition 6.3 Given the MDP \\((\\ALPHABET S, \\ALPHABET A, P, \\{c_t\\})\\), define the folded MDP as \\((\\ALPHABET S_{\\ge 0}, \\ALPHABET A, \\tilde P, \\{c_t\\})\\), where \\(\\tilde P(a) = \\mathcal F P(a)\\) for all \\(a \\in \\ALPHABET A\\). Let \\(\\tilde Q_t \\colon \\ALPHABET S_{\\ge 0} \\times \\ALPHABET A \\to \\reals\\), \\(\\tilde V_t \\colon \\ALPHABET S_{\\ge 0} \\to \\reals\\) and \\(\\tilde π_t^* \\colon \\ALPHABET S_{\\ge 0} \\to \\ALPHABET A\\) denote the action-value function, value function and the policy of the folded MDP. Then, if the original MDP satisfies conditions (A1) and (A2) then, for any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), \\[ Q_t(s,a) = \\tilde Q_t(|s|, a),\n\\quad\n  V_t(s) = \\tilde V_t(|s|),\n\\quad\n  π_t^*(s) = \\tilde π_t^*(|s|).\n\\]\n\n\nThe result of the previous part implies that if the value function \\(\\tilde V_t\\) and the policy \\(\\tilde π^*_t\\) are monotone increasing, then the value function \\(V_t\\) and the policy \\(π^*_t\\) are even and quasi-convex. This gives us a method to verify if the value function and optimal policy are even and quasi-convex.\nNow, recall the model of the Internet of Things presented in Q2 of Assignment 3. The numerical experiments that you did in Assignment 3 suggest that the value function and the optimal policy are even and quasi-convex. Prove that this is indeed the case.\nNow suppose the distribution of \\(W_t\\) is not Gaussian but is some general probability density \\(\\varphi(\\cdot)\\) and the cost function is \\[ c(e,a) = \\lambda a + (1 - a) d(e). \\] Find conditions on \\(\\varphi\\) and \\(d\\) such that the value function and optimal policy are even and quasi-convex."
  },
  {
    "objectID": "mdps/monotone-mdps.html#notes",
    "href": "mdps/monotone-mdps.html#notes",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "Notes",
    "text": "Notes\nStochastic dominance has been employed in various areas of economics, finance, and statistics since the 1930s. See Levy (1992) and Levy (2015) for detailed overviews. The notion of stochastic monotonicity for Markov chains is due to Daley (1968). For a generalization of stochastic monotonicity to continuous state spaces, see Serfozo (1976). The characterization of stochastic monotonicity in Exercise 6.1–Exercise 6.4 are due to Keilson and Kester (1977).\nRoss (1974) has an early treatment of monotonicity of optimal policies. The general theory was developed by Topkis (1998). The presentation here follows Puterman (2014). Exercise 6.6 is from Chakravorty and Mahajan (2018).\n\n\n\n\nChakravorty, J. and Mahajan, A. 2018. Sufficient conditions for the value function and optimal strategy to be even and quasi-convex. IEEE Transactions on Automatic Control 63, 11, 3858–3864. DOI: 10.1109/TAC.2018.2800796.\n\n\nDaley, D.J. 1968. Stochastically monotone markov chains. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 10, 4, 305–317. DOI: 10.1007/BF00531852.\n\n\nKeilson, J. and Kester, A. 1977. Monotone matrices and monotone markov processes. Stochastic Processes and their Applications 5, 3, 231–241.\n\n\nLevy, H. 1992. Stochastic dominance and expected utility: Survey and analysis. Management Science 38, 4, 555–593. DOI: 10.1287/mnsc.38.4.555.\n\n\nLevy, H. 2015. Stochastic dominance: Investment decision making under uncertainty. Springer. DOI: 10.1007/978-3-319-21708-6.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887.\n\n\nRoss, S.M. 1974. Dynamic programming and gambling models. Advances in Applied Probability 6, 3, 593–606. DOI: 10.2307/1426236.\n\n\nSerfozo, R.F. 1976. Monotone optimal policies for markov decision processes. In: Mathematical programming studies. Springer Berlin Heidelberg, 202–215. DOI: 10.1007/bfb0120752.\n\n\nTopkis, D.M. 1998. Supermodularity and complementarity. Princeton University Press."
  },
  {
    "objectID": "mdps/power-delay-tradeoff.html#dynamic-program",
    "href": "mdps/power-delay-tradeoff.html#dynamic-program",
    "title": "7  Power-delay tradeoff in wireless communication",
    "section": "7.1 Dynamic program",
    "text": "7.1 Dynamic program\nWe can assume \\(Y_t = X_t - A_t\\) as a post-decision state in the above model and write the dynamic program as follows:\n\\[ V_{T+1}(x,s) = 0 \\] and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\\begin{align*}\n  H_t(y,s) &= \\lambda d(y) + \\EXP[ V_{t+1}(y + W_t, S_{t+1}) | S_t = s ], \\\\\n  V_t(x,s) &= \\min_{0 \\le a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\big\\}\n\\end{align*}\\]\n\n7.1.1 Monotonicity of value functions\n\nLemma 7.1 For all \\(t\\), \\(V_t(x,s)\\) and \\(H_t(y,s)\\) are increasing in both variables.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that the constraint set \\(\\ALPHABET A(x) = \\{0, \\dots, x\\}\\) satisfies the conditions that generalize the result of monotonicity to constrained actions.\nWe prove the two monotonicity properties by backward induction. First note that \\(V_{T+1}(x,s)\\) is trivially monotone. This forms the basis of induction. Now suppose \\(V_{t+1}(x,s)\\) is increasing in \\(x\\) and \\(s\\). Since \\(\\{S_t\\}_{t \\ge 1}\\) is stochastically monotone, \\[H_t(y,s) = \\lambda d(y) + \\EXP[ V_{t+1}(y + W_t, S_{t+1}) | S_t = s ]\\] is increasing in \\(s\\). Moreover, since both \\(d(y)\\) and \\(V_{t+1}(y + w, s)\\) are increasing in \\(y\\), so is \\(H_t(y,s)\\).\nNow, for every \\(a\\), \\(p(a) q(s)\\) and \\(H_t(x-a, s)\\) is increasing in \\(x\\) and \\(s\\). So, the pointwise minima over \\(a\\) is also increasing in \\(x\\) and \\(s\\).\n\n\n\n\n\n7.1.2 Convexity of value functions\n\nLemma 7.2 For all time \\(t\\) and channel state \\(s\\), \\(V_t(x,s)\\) and \\(H_t(y,s)\\) are convex in the first variable.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe proceed by backward induction. First note that \\(V_{T+1}(x,s)\\) is trivially convex in \\(x\\). Now assume that \\(V_{t+1}(x,s)\\) is convex in \\(x\\). Then, \\(\\EXP[V_{t+1}(y + W_t, S_{t+1}) | S_t = s]\\) is weighted sum of convex functions and is, therefore, convex in \\(y\\). Therefore, \\(H_t(y,s)\\) is a sum of two convex functions and, therefore, convex in \\(y\\).\nWe cannot directly show the convexity of \\(V_t(x,s)\\) because the pointwise minimum of convex functions is not convex. So, we consider the following argument. Fix \\(s\\) and pick \\(x &gt; 1\\). Let \\(\\underline a = π^*_t(x-1,s)\\) and \\(\\bar a = π^*_t(x+1,s)\\). Let \\(\\underline v = \\lfloor (\\underline a + \\bar a)/2 \\rfloor\\) and \\(\\bar v = \\lceil (\\underline a + \\bar a)/2 \\rceil\\). Note that both \\(\\underline v\\) and \\(\\bar v\\) are feasible at \\(x\\). Then, \\[ \\begin{align*}\n  \\hskip 2em & \\hskip -2em\n  V_t(x-1, s) + V_t(x+1, s)\n  \\\\\n  &=\n  [ p(\\underline a) + p(\\bar a) ] q(s) + H_t(x - 1 - \\underline a, s)\n  + H_t(x + 1 - \\bar a, s)\n  \\\\\n  &\\stackrel{(a)}\\ge [ p(\\underline v) + p(\\bar v)] q(s) +\n    H_t(x - \\underline v, s) + H_t(x - \\bar v, s) \\\\\n  &\\ge 2 \\min_{a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\\\\n  &= 2 V_t(x,s),\n\\end{align*} \\] where \\((a)\\) follows from convexity of \\(p(\\cdot)\\) and \\(H_t(\\cdot, s)\\). Thus, \\(V_t(x,s)\\) is convex in \\(x\\). This completes the induction step.\n\n\n\n\n\n7.1.3 Monotonicity of optimal policy in queue length\n\nTheorem 7.1 For all time \\(t\\) and channel state s\\(s\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is increasing in the queue length \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn the previous lemma, we have shown that \\(H_t(y,s)\\) is convex in \\(y\\). Therefore, \\(H_t(x-a, s)\\) is submodular in \\((x,a)\\).\n\nThus, for a fixed \\(s\\), \\(p(a)q(s) + H_t(x-a, s)\\) is submodular in \\((x,a)\\). Therefore, the optimal policy is increasing in \\(x\\).\n\n\n\nOne can show submodularity by finite difference, but for simplicity, we assume that \\(H_t(y,s)\\) is twice differentiable. Then, \\(\\partial^2 H_t(x - a, s)/ \\partial x \\partial a \\le 0\\) (by convexity of \\(H_t\\)).\n\n7.1.4 Monotonicity of optimal policy in channel state\nIt is natural to expect that for a fixed \\(x\\) the optimal policy is decreasing in \\(s\\). However, it is not possible to obtain the monotonicity of optimal policy in channel state in general. To see why this is difficult, let us impose a mild assumption on the arrival distribution.\n\nThe packet arrival distribution is weakly decreasing, i.e., for any \\(v,w \\in \\integers_{\\ge 0}\\) such that \\(v \\le w\\), we have that \\(P_W(v) \\ge P_W(w)\\).\n\nWe first start with a slight generalization of stochastic monotonicity result.\n\nLemma 7.3 Let \\(\\{p_i\\}_{i \\ge 0}\\) and \\(\\{q_i\\}_{i \\ge 0}\\) be real-valued non-negative sequences satisfying \\[ \\sum_{i \\le j} p_i \\le \\sum_{i \\le j} q_i, \\quad \\forall j.\\] Then, for any increasing sequence \\(\\{v_i\\}_{i \\ge 0}\\), we have \\[ \\sum_{i = 0}^\\infty p_i v_i \\ge \\sum_{i=0}^\\infty q_i v_i. \\]\n\nThe proof is similar to the proof for stochastic monotonicity.\n\nLemma 7.4 Under (asm-power-delay-density?), for all \\(t\\), \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nThe idea of the proof is similar to Lemma 1 of the notes on monotone MDPs.\nFix \\(y^+, y^- \\in \\integers_{\\ge 0}\\) and \\(s^+, s^- \\in \\ALPHABET S\\) such that \\(y^+ &gt; y^-\\) and \\(s^+ &gt; s^-\\). Now, for any \\(y' \\in \\integers_{\\ge 0}\\) and \\(s' \\in \\ALPHABET S\\) define \\[\\begin{align*}\n  π(y',s') = P_W(y' - y^+)P_S(s'|s^+) +\n             P_W(y' - y^-)P_S(s'|s^-),\n             \\\\\n  μ(y',s') = P_W(y' - y^-)P_S(s'|s^+) +\n             P_W(y' - y^+)P_S(s'|s^-).\n\\end{align*}\\]\nSince \\(P_S\\) is stochastically monotone, we have that for any \\(σ \\in \\ALPHABET S\\), \\[ \\sum_{s'=1}^{σ} P_S(s'|s^+) \\le \\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Moreover, due to (asm-power-delay-density?), we have that \\(P_W(y' - y^-) \\le P_W(y' - y^+)\\). Thus, \\[ [P_W(y' - y^+) - P_W(y' - y^-)] \\sum_{s'=1}^{σ} P_S(s'|s^+)\n\\le [P_W(y' - y^+) - P_W(y' - y^-)]\\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Rearranging terms, we get \\[ \\sum_{s'=1}^σ π(y',s') \\le \\sum_{s'=1}^σ μ(y',s'). \\] Thus, for any \\(y'\\), the sequence \\(π(y',s')\\) and \\(ν(y',s')\\) satisfy the condition of Lemma 7.3.\nNow, in Lemma 7.1, we have established that for any \\(y'\\), \\(V_{t+1}(y',s')\\) is increasing in \\(s'\\). Thus, from Lemma 7.3, we have \\[  \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Summing up over \\(y'\\), we get \\[  \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Or equivalently, \\[\\begin{align*}\n\\hskip 2em & \\hskip -2em\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^-) ]\n\\\\\n& \\ge\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^-) ] .\n\\end{align*}\\] Thus, \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\n\n\nEven under (asm-power-delay-density?), we cannot establish the monotonicity of \\(π^*_t(x,s)\\) is \\(s\\).\n\n\n\nNote that we have established that \\(H_t(y,s)\\) is supermodular in \\((y,s)\\). Thus, for any fixed \\(x\\), \\(H_t(x-a,s)\\) is submodular in \\((a,s)\\). Furthermore the function \\(p(a)q(s)\\) is increasing in both variables and therefore supermodular in \\((a,s)\\). Therefore, we cannot say anything specific about \\(p(a)q(s) + H_t(x-a, s)\\) which is a sum of submodular and supermodular functions."
  },
  {
    "objectID": "mdps/power-delay-tradeoff.html#exercises",
    "href": "mdps/power-delay-tradeoff.html#exercises",
    "title": "7  Power-delay tradeoff in wireless communication",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 7.1 In this exercise, we provide sufficient conditions for the optimal policy to be monotone in the channel state. Suppose that the channel state \\(\\{S_t\\}_{t \\ge 1}\\) is an i.i.d. process. Then prove that for all time \\(t\\) and queue state \\(x\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is decreasing in channel state \\(s\\)."
  },
  {
    "objectID": "mdps/power-delay-tradeoff.html#notes",
    "href": "mdps/power-delay-tradeoff.html#notes",
    "title": "7  Power-delay tradeoff in wireless communication",
    "section": "Notes",
    "text": "Notes\nThe mathematical model of power-delay trade-off is taken from Berry (2000), where the monotonicty results were proved using first principles. More detailed characterization of the optimal transmission strategy when the average power or the average delay goes to zero are provided in Berry and Gallager (2002) and Berry (2013). A related model is presented in Ding et al. (2016).\nFor a broader overview of power-delay trade offs in wireless communication, see Berry et al. (2012) and Yeh (2012).\nThe remark after Lemma 7.4 shows the difficulty in establishing monotonicity of optimal policies for a multi-dimensional state space. In fact, sometimes even when monotonicity appears to be intuitively obvious, it may not hold. See Sayedana and Mahajan (2020) for an example. For general discussions on monotonicity for multi-dimensional state spaces, see Topkis (1998) and Koole (2006). As an example of using such general conditions to establish monotonicity, see Sayedana et al. (2020).\n\n\n\n\nBerry, R.A. 2000. Power and delay trade-offs in fading channels. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay tradeoffs in fading channels—small-delay asymptotics. IEEE Transactions on Information Theory 59, 6, 3939–3952. DOI: 10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002. Communication over fading channels with delay constraints. IEEE Transactions on Information Theory 48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M. 2012. Energy-efficient scheduling under delay constraints for wireless networks. Synthesis Lectures on Communication Networks 5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A. 2016. On monotonicity of the optimal transmission policy in cross-layer adaptive \\(m\\) -QAM modulation. IEEE Transactions on Communications 64, 9, 3771–3785. DOI: 10.1109/TCOMM.2016.2590427.\n\n\nKoole, G. 2006. Monotonicity in markov reward and decision chains: Theory and applications. Foundations and Trends in Stochastic Systems 1, 1, 1–76. DOI: 10.1561/0900000002.\n\n\nSayedana, B. and Mahajan, A. 2020. Counterexamples on the monotonicity of delay optimal strategies for energy harvesting transmitters. IEEE Wireless Communications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E. 2020. Cross-layer communication over fading channels with adaptive decision feedback. International symposium on modeling and optimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nTopkis, D.M. 1998. Supermodularity and complementarity. Princeton University Press.\n\n\nYeh, E.M. 2012. Fundamental performance limits in cross-layer wireless optimization: Throughput, delay, and energy. Foundations and Trends in Communications and Information Theory 9, 1, 1–112. DOI: 10.1561/0100000014."
  },
  {
    "objectID": "mdps/reward-shaping.html#generalization-to-discounted-models",
    "href": "mdps/reward-shaping.html#generalization-to-discounted-models",
    "title": "8  Reward Shaping",
    "section": "8.1 Generalization to discounted models",
    "text": "8.1 Generalization to discounted models\nNow consider a finite horizon discounted cost problem, where the performance of a policy \\(π\\) is given by \\[\nJ(π) = \\EXP\\Bigl[ \\sum_{t=1}^{T-1} γ^{t-1} c_t(S_t, A_t) + γ^T c_T(S_T)\n       \\Bigr].\n\\] As argued in the introduction to discounted models, the dynamic prgram for this case is given by\n\\[ V_{T}(s) = c_T(s) \\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[ \\begin{align*}\n  Q_t(s,a) &= c(s,a) + γ \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\\\\n  V_t(s) &= \\min_{a \\in \\ALPHABET A} Q_t(s,a).\n\\end{align*} \\]\nFor such models, we have the following.\n\nCorollary 8.2 For discounted cost models, the results of Theorem 8.1 and Corollary 8.1 continue to hold if condition 2 is replaced by\n\nFor \\(t \\in \\{1, \\dots, T-1\\}\\),\n\\[ c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) + γ Φ_{t+1}(s_{+}) - Φ_t(s). \\]\n\n\n\n\n\n\n\n\nInfinite horizon models\n\n\n\nIf the cost function is time homogeneous, Corollary 8.2 extends naturally to infinite horizon models with a time-homogeneous potential function. A remarkable feature is that if the potential function is chosen as the value function, i.e., \\(Φ(s) = V(s)\\), then the value function of the modified cost \\(\\tilde c(s,a,s_{+})\\) is zero!"
  },
  {
    "objectID": "mdps/reward-shaping.html#examples",
    "href": "mdps/reward-shaping.html#examples",
    "title": "8  Reward Shaping",
    "section": "8.2 Examples",
    "text": "8.2 Examples\nAs an example of reward shaping, see the notes on inventory management. Also see the notes on martingale approach to stochastic control for an iteresting relationship between reward shaping and martingales."
  },
  {
    "objectID": "mdps/reward-shaping.html#notes",
    "href": "mdps/reward-shaping.html#notes",
    "title": "8  Reward Shaping",
    "section": "Notes",
    "text": "Notes\nThe idea of reward shaping was proposed by Skinner (1938) to synthesize complex behavior by guiding animals to perform simple functions (see :Skinner’s Box Experiment). The formal description of reward shaping comes from Porteus (1975), who established a result similar to Ng et al. (1999), and called it the transformation method. Porteus (1975) also describes transformations of the dynamics which preserve the optimal policy.\nCorollary 8.2 was also re-established by Ng et al. (1999), who aslo provided a partial converse. The results of Porteus (1975) and Ng et al. (1999) were restricted to time-homogeneous potential functions. The generalization to time-varying potential functions was presented in Devlin and Kudenko (2012).\nThe partial converse of Corollary 8.1 established by Ng et al. (1999) states that the shaping presented in Theorem 8.1 is the only additive cost transformation that that preserves the set of optimal policy. However, this converse was derived under the assumption that the transition dynamics are complete (see Ng et al. (1999)). A similar converse under a weaker set of assumptions on the transition dynamics is established in Jenner et al. (2022).\nFor a discussion on practical considerations in using reward shaping in reinforcement learning, see Grzes and Kudenko (2009) and Devlin (2014). As a counter-point, Wiewiora (2003) shows that the advantages of reward shaping can also be achieved by simply adding the potential function to the \\(Q\\)-function initialization.\n\n\n\n\n\nDevlin, S. 2014. Potential based reward shaping tutorial. Available at: http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf.\n\n\nDevlin, S. and Kudenko, D. 2012. Dynamic potential-based reward shaping. Proceedings of the 11th international conference on autonomous agents and multiagent systems, International Foundation for Autonomous Agents; Multiagent Systems, 433–440.\n\n\nGrzes, M. and Kudenko, D. 2009. Theoretical and empirical analysis of reward shaping in reinforcement learning. International conference on machine learning and applications, 337–344. DOI: 10.1109/ICMLA.2009.33.\n\n\nJenner, E., Hoof, H. van, and Gleave, A. 2022. Calculus on MDPs: Potential shaping as a gradient. Available at: https://arxiv.org/abs/2208.09570v1.\n\n\nNg, A.Y., Harada, D., and Russell, S. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. ICML, 278–287. Available at: http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf.\n\n\nPorteus, E.L. 1975. Bounds and transformations for discounted finite markov decision chains. Operations Research 23, 4, 761–784. DOI: 10.1287/opre.23.4.761.\n\n\nSkinner, B.F. 1938. Behavior of organisms. Appleton-Century.\n\n\nWiewiora, E. 2003. Potential-based shaping and q-value initialization are equivalent. Journal of Artificial Intelligence Research 19, 1, 205–208."
  },
  {
    "objectID": "mdps/inf-horizon.html#performance-of-a-time-homogeneous-markov-policy",
    "href": "mdps/inf-horizon.html#performance-of-a-time-homogeneous-markov-policy",
    "title": "9  Infinite horizon MDPs",
    "section": "9.1 Performance of a time-homogeneous Markov policy",
    "text": "9.1 Performance of a time-homogeneous Markov policy\nFor any \\(π \\colon \\ALPHABET S \\to \\ALPHABET A\\), consider the time homogeneous policy \\((π, π, \\dots)\\). For ease of notation, we denote this policy simply by \\(π\\). The expected discounted cost under this policy is given by \\[ V^π(s) = \\EXP^π\\bigg[ \\sum_{t=1}^∞ γ^{t-1} c(S_t, A_t) \\biggm| S_1 = s\n\\bigg].\\]\nTo get a compact expression for this, define a \\(n × 1\\) vector \\(c_π\\) and a \\(n × n\\) matrix \\(P_π\\) as follows: \\[ [c_π]_s = c(s, π(s))\n   \\quad\\text{and}\\quad\n   [P_π]_{ss'} = P_{ss'}(π(s)).\n\\] Then the dynamics under policy \\(π\\) are Markovian with transition probability matrix \\(P_π\\) and a cost function \\(c_π\\). Then \\[ \\begin{align*}\n\\EXP^π\\big[ c(S_t, π(S_t)) \\bigm| S_1 = s \\big]\n  &= \\sum_{s' \\in \\ALPHABET S} \\PR^π(S_t = s' | S_1 = s) c(s', π(s'))\n  \\\\\n  &= \\sum_{s' \\in \\ALPHABET S} [P_π^{t-1}]_{ss'} [c_π]_y\n  \\\\\n  &= δ_s P_π^{t-1} c_π.\n\\end{align*} \\]\nLet \\(V^π\\) denote the \\(n × 1\\) vector given by \\([V^π]_s = V^π(s)\\). Then, \\[ \\begin{align*}\nV^π &= c_π + γ P_π c_π + γ^2 P_π^2 c_π + \\cdots \\\\\n    &= c_π + γ P_π \\big( c_π + γ P_π c_π + \\cdots \\big) \\\\\n    &= c_π + γ P_π V^π,\n\\end{align*} \\] which can be rewritten as \\[ (I - γ P_π) V^π = c_π. \\]\nThe :spectral radius \\(ρ(γ P_d)\\) of a matrix is upper bounded by its :spectral norm \\(\\lVert γ P_d \\rVert = γ &lt; 1\\). Therefore, the matrix \\((I - γ P_π)\\) has an inverse and by left multiplying both sides by \\((I - γ P_π)^{-1}\\), we get \\[ V^π = (I - γP_π)^{-1} c_π. \\]\nThe equation \\[ V^π = c_π + γ P_π V^π \\] is sometimes also written as \\[ V^π = \\BELLMAN^π V^π \\] where the operator \\(\\BELLMAN^π\\), which is called the Bellman operator, is an operator from \\(\\reals^n\\) to \\(\\reals^n\\) given by \\[ \\BELLMAN^π v = c_π + γ P_π v.\\]"
  },
  {
    "objectID": "mdps/inf-horizon.html#bellman-operators",
    "href": "mdps/inf-horizon.html#bellman-operators",
    "title": "9  Infinite horizon MDPs",
    "section": "9.2 Bellman operators",
    "text": "9.2 Bellman operators\n\nDefinition 9.1 Define the Bellman operator \\(\\BELLMAN^* : \\reals^n \\to \\reals^n\\) as follows: for any \\(v \\in \\reals^n\\) \\[ [\\BELLMAN^* v]_s = \\min_{a \\in \\ALPHABET A}\n\\Big\\{ c(s,a) + γ \\sum_{s' \\in \\ALPHABET S} P_{ss'}(a) v_y \\Big\\}.\n\\]\n\nNote that the above may also be written as1 \\[ \\BELLMAN^* v = \\min_{π \\in \\Pi} \\BELLMAN^π v, \\] where \\(\\Pi\\) denotes the set of all deterministic Markov policies.1 This is true for general models only when the arg min at each state exists.\n\nProposition 9.1 For any \\(v \\in \\reals^n\\), define the norm \\(\\NORM{V} := \\sup_{s \\in \\ALPHABET S} \\ABS{V^s}\\). Then, the Bellman operator is a contraction, i.e., for any \\(v, w \\in \\reals^n\\), \\[ \\NORM{\\BELLMAN^* v - \\BELLMAN^* w} \\le γ \\NORM{v - w}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix a state \\(s \\in \\ALPHABET S\\) and consider \\([\\BELLMAN^* v](s) - [\\BELLMAN w](s)\\). In particular, let \\(a^*\\) be the optimal action in the right hand side of \\([\\BELLMAN^* w](s)\\). Then, \\[\\begin{align*}\n  [\\BELLMAN^* v - \\BELLMAN^* w](s) &=\n  \\min_{a \\in \\ALPHABET A}\\bigl\\{ c(s,a) + γ \\sum_{s' \\in \\ALPHABET S}\n  P_{ss'}(a) v(s') \\bigr\\} -\n  \\min_{a \\in \\ALPHABET A}\\bigl\\{ c(s,a) + γ \\sum_{s' \\in \\ALPHABET S}\n  P_{ss'}(a) w(s') \\bigr\\}\n  \\\\\n  &\\le c(s,a^*) + γ \\sum_{s'\\in \\ALPHABET S} P_{ss'}(a^*) v(s') -\n       c(s,a^*) - γ \\sum_{s'\\in \\ALPHABET S} P_{ss'}(a^*) w(s')\n  \\\\\n  &\\le γ \\sum_{s' \\in \\ALPHABET S} P_{ss'}(a^*) \\| v - w \\|\n  \\\\\n  &= γ \\| v - w \\|.\n\\end{align*} \\]\nBy a similar argument, we can show that \\([\\BELLMAN^* w - \\BELLMAN^* v](s) \\le γ \\| v - w \\|\\), which proves the other side of the inequality.\n\n\n\nAn immediate consequence of the contraction property is the following.\n\nTheorem 9.1 There is a unique bounded \\(V^* \\in \\reals^n\\) that satisfies the Bellman equation \\[ V = \\BELLMAN^* V \\]\nMoreover, if we start from any \\(V^0 \\in \\reals^n\\) and recursively define \\[ V^n = \\BELLMAN^* V^{n-1} \\] then \\[ \\lim_{n \\to ∞} V^n = V^*. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis follows immediately from the Banach fixed point theorem."
  },
  {
    "objectID": "mdps/inf-horizon.html#optimal-time-homogeneous-policy",
    "href": "mdps/inf-horizon.html#optimal-time-homogeneous-policy",
    "title": "9  Infinite horizon MDPs",
    "section": "9.3 Optimal time-homogeneous policy",
    "text": "9.3 Optimal time-homogeneous policy\n\n\nProposition 9.2 Define \\[ V^{\\text{opt}}_∞(s) := \\min_{π} \\EXP^π \\bigg[ \\sum_{t=1}^∞ γ^{t-1} c(S_t, A_t)\n\\biggm| S_1 = s \\bigg], \\] where the minimum is over all (possibly randomized) history dependent policies. Then, \\[ V^{\\text{opt}}_∞ = V^*, \\] where \\(V^*\\) is the solution of the Bellman equation.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the state and action space are finite, without loss of optimality, we can assume that \\(0 \\le c(s,a) \\le M\\).\nConsider the finite horizon truncation \\[ V^{\\text{opt}}_T(s) =  \\min_{π} \\EXP^π\\bigg[ \\sum_{t=1}^T γ^{t-1} c(S_t, A_t) | S_1 = s \\bigg].\n\\] From the results for finite horizon MDP, we have that \\[ V^{\\text{opt}}_T = \\BELLMAN^*^T V^0 \\] where \\(V^0\\) is the all zeros vector.\nNow by construction, \\[V^{\\text{opt}}_∞(s) \\ge V^{\\text{opt}}_T(s) = [\\BELLMAN^*^T V^0](s). \\] Taking limit as \\(T \\to ∞\\), we get that \\[\\begin{equation}\\label{eq:1}\n  V^{\\text{opt}}_∞(s) \\ge \\lim_{T \\to ∞} [\\BELLMAN^*^T V^0](s) = V^*(s).\n\\end{equation}\\]\nSince \\(0 \\le c(s,a) \\le M\\), for any \\(T\\), \\[ \\begin{align*}\nV^{\\text{opt}}_∞(s) &\\le \\min_π \\EXP^π \\bigg[ \\sum_{t=1}^T γ^{t-1} c(S_t, A_t)\n\\biggm| S_1 = s \\bigg] + \\sum_{t=T+1}^∞ γ^{t-1} M \\\\\n&= V^{\\text{opt}}_T(s) + γ^T M / (1 - γ) \\\\\n&= [\\BELLMAN^*^T V^0](s) + γ^T M / (1-γ).\n\\end{align*} \\] Taking limit as \\(T \\to ∞\\), we get that \\[\\begin{equation}\\label{eq:2}\n  V^{\\text{opt}}_∞(s) \\le \\lim_{T \\to ∞}\n  \\big\\{ [\\BELLMAN^*^T V^0](s) + γ^T M / (1-γ) \\big\\} = V^*(s).\n\\end{equation}\\]\nFrom \\eqref{eq:1} and \\eqref{eq:2}, we get that \\(V^{\\text{opt}}_∞ = V^*\\)."
  },
  {
    "objectID": "mdps/inf-horizon.html#properties-of-bellman-operator",
    "href": "mdps/inf-horizon.html#properties-of-bellman-operator",
    "title": "9  Infinite horizon MDPs",
    "section": "9.4 Properties of Bellman operator",
    "text": "9.4 Properties of Bellman operator\n\nProposition 9.3 The Bellman operator satisfies the following properties\n\nMonotonicity. For any \\(v, w \\in \\reals^n\\), if \\(v \\le w\\), then \\(\\BELLMAN^π v \\le \\BELLMAN^π w\\) and \\(\\BELLMAN^* v \\le \\BELLMAN^* w\\).\nDiscounting. For any \\(v \\in \\reals^n\\) and \\(m \\in \\reals\\), \\(\\BELLMAN^π (v + m \\ONES) = \\BELLMAN^π v + γ m \\ONES\\) and \\(\\BELLMAN^* (v + m \\ONES) = \\BELLMAN^* v + γ m \\ONES\\).\n\n\n\n\n\n\n\n\nProof of monotonicity property\n\n\n\n\n\nRecall that \\[ \\BELLMAN^π v = c_π + γ P_π v. \\] So, monotonicity of \\(\\BELLMAN^π\\) follows immediately from monotonicity of matrix multiplication for positive matrices.\nLet \\(μ\\) be such that \\(\\BELLMAN^* w = \\BELLMAN^μ w\\). Then, \\[ \\BELLMAN^* v \\le \\BELLMAN^μ v\n\\stackrel{(a)} \\le \\BELLMAN^μ w = \\BELLMAN^* w,\n\\] where \\((a)\\) uses the monotonicity of \\(\\BELLMAN^μ\\).\n\n\n\n\n\n\n\n\n\nProof of discounting property\n\n\n\n\n\nRecall that \\[ \\BELLMAN^π v = c_π + γ P_π v. \\] Thus, \\[ \\BELLMAN^π(v+m \\ONES) = c_π + γ P_π (v+m \\ONES) = c_π + γ P_π v + γ m\n\\ONES = \\BELLMAN^π\nv + γ m \\ONES.\\] Thus, \\(\\BELLMAN^π\\) is discounting. Now consider \\[ \\BELLMAN^* (v + m \\ONES ) = \\min_{π} \\BELLMAN^π (v+m \\ONES)\n= \\min_π \\mathcal (B_π v + γ m \\ONES) = \\BELLMAN^* v + γ m \\ONES.\\] Thus, \\(\\BELLMAN^*\\) is discounting.\n\n\n\n\nProposition 9.4 For any \\(V \\in \\reals^n\\),\n\nIf \\(V \\ge \\BELLMAN^* V\\), then \\(V \\ge V^*\\);\nIf \\(V \\le \\BELLMAN^* V\\), then \\(V \\le V^*\\);\nIf \\(V = \\BELLMAN^* V\\), then \\(V\\) is the only vector with this property and \\(V = V^*\\).\n\nThe same bounds are true when \\((\\BELLMAN^*, V^*)\\) is replaced with \\((\\BELLMAN^π, V^π)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the first part. The proof of the other parts is similar.\nWe are given that \\[V \\ge \\BELLMAN^* V.\\] Then, by monotonicity of the Bellman operator, \\[ \\BELLMAN^* V \\ge \\BELLMAN^*^2 V.\\] Continuing this way, we get \\[ \\BELLMAN^*^k V \\ge \\BELLMAN^*^{k+1} V.\\] Adding the above equations, we get \\[ V \\ge \\BELLMAN^*^{k+1} V.\\] Taking limit as \\(k \\to ∞\\), we get \\[V \\ge V^*.\\]\n\n\n\n\nProposition 9.5 For any \\(V \\in \\reals^n\\) and \\(m \\in \\reals\\),\n\nIf \\(V + m \\ONES \\ge \\BELLMAN^* V\\), then \\(V + m \\ONES/(1-γ) \\ge V^*\\);\nIf \\(V + m \\ONES \\le \\BELLMAN^* V\\), then \\(V + m \\ONES/(1-γ) \\le V^*\\);\n\nThe same bounds are true when \\((\\BELLMAN^*, V^*)\\) is replaced with \\((\\BELLMAN^π, V^π)\\).\n\n\n\n\n\n\n\nRemark\n\n\n\nThe above result can also be stated as follows:\n\n\\(\\displaystyle \\| V^π - V \\| \\le \\frac{1}{1-γ}\\| \\BELLMAN^π V - V \\|\\).\n\\(\\displaystyle \\| V^* - V \\| \\le \\frac{1}{1-γ}\\| \\BELLMAN^* V - V \\|\\).\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAgain, we only prove the first part. The proof of the second part is the same. We have that \\[ V + m \\ONES \\ge \\BELLMAN^* V. \\] From discounting and monotonicity properties, we get \\[ \\BELLMAN^* V + γ m \\ONES \\ge \\BELLMAN^*^2 V. \\] Again, from discounting and monotonitiy properties, we get \\[ \\BELLMAN^*^2 V + γ^2 m \\ONES \\ge \\BELLMAN^*^3 V. \\] Continuing this way, we get \\[ \\BELLMAN^*^k V + γ^k m \\ONES \\ge \\BELLMAN^*^{k+1} V. \\] Adding all the above equations, we get \\[ V + \\sum_{\\ell = 0}^k γ^\\ell m \\ONES \\ge \\BELLMAN^*^{k+1} V. \\] Taking the limit as \\(k \\to ∞\\), we get \\[ V + m \\ONES/(1-γ) \\ge V^*. \\]"
  },
  {
    "objectID": "mdps/inf-horizon.html#countable-or-continuous-state-and-action-spaces",
    "href": "mdps/inf-horizon.html#countable-or-continuous-state-and-action-spaces",
    "title": "9  Infinite horizon MDPs",
    "section": "9.5 Countable or continuous state and action spaces",
    "text": "9.5 Countable or continuous state and action spaces\nThe discussion above was restricted to finite state and action spaces. In general, when working with non-finite state and action spaces, we need to be careful about existence of solutions. See Section 3.4 for a discussion.\nFor infinite horizon problems, there are additional technical challenges. We first point out via examples that the Bellman equation may not have a unique fixed point equation.\n\nExample 9.1 (Phantom solutions of DP (Bertsekas 2011)) Consider an MDP with \\(\\ALPHABET S = [0,∞)\\), \\(\\ALPHABET A = \\{a_\\circ\\}\\) (dummy action), \\(p(ds'\\mid s,a) = δ_{s' - s/γ}\\) (so the state deterministically transitions to \\(s/γ\\)) and \\(c(s,a) ≡ 0\\). It is clear that the optimal value function \\(V^*(s) ≡ 0\\). However, the dynamic programming equation is \\[\n  V^*(s) = γ V^*(s/γ)\n\\] and is satisfied by any linear function \\(V^*(s) = α s\\)!\n\n\n\n\n\n\n\nDid we just violate the Banach fixed point theorem?\n\n\n\nTo apply the Banach fixed point theorem, we need to be in a Banach space, i.e., a complete normed metric space. So, what was the Banach space in Theorem 9.1? Implicitly, we had used the space \\[\n  \\ALPHABET V^M \\coloneqq \\{ v \\colon \\ALPHABET S \\to \\reals \\text{ such that } \\| v \\|_{∞} \\le M \\}\n\\] where \\(M\\) is any constant greater than \\(\\|c\\|_{∞}/(1-γ)\\). Banach fixed point theorem says that DP has a unique fixed point in \\(\\ALPHABET V^M\\). It does not say anything about solutions outside \\(\\ALPHABET V^M\\)!\nAmong all the solutions, only the one corresponding to \\(α = 0\\) is bounded, which is also the optimal value function.\n\n\n\nExample 9.2 Consider a birth-death Markov chain with \\(0\\) as an aborbing state. In particular, \\(\\ALPHABET S = \\integers_{\\ge 0}\\), \\(\\ALPHABET A = \\{a_\\circ\\}\\) (dummy action), and \\(c(s,a) = \\IND\\{s &gt; 0\\}\\). The transition dynamics are given by: for \\(s = 0\\), \\(P(s'|0,a) = \\IND\\{s' = 0\\}\\) and for \\(s &gt; 0\\), we have \\[\nP(s'|s,a) = p \\IND\\{ s' = s+1\\} + (1-p) \\IND\\{s' = s-1\\}.\n\\] So, the DP is given by \\(v(0) = 0\\) and for \\(s &gt; 0\\) \\[\nV^*(s) = 1 + γ p  V^*(s+1) + γ(1-p) V^*(s-1),\n\\] which is a harmonic equation and has a generic solution of the form: \\[\n  V^*(s) = \\frac{1}{1-γ} -\n  \\biggl[ \\frac{1}{1-γ} + C \\biggr] q_1^s +\n  C q_2^s\n\\] where \\(C\\) is an arbitrary constant and \\[\n  q_{1,2} = \\frac{1 \\pm \\sqrt{1 - 4 γ^2 p (1-p)}}{2 γp}.\n\\] It can be verified that \\(q_1 \\in (0,1)\\) and \\(q_2 \\in (1,∞)\\). So, every solution except \\(C = 0\\) is unbounded.\nAgain we have multiple solutions of the DP and the correct solution is the only bounded solution (which corresponds to \\(C = 0\\))."
  },
  {
    "objectID": "mdps/inf-horizon.html#exercises",
    "href": "mdps/inf-horizon.html#exercises",
    "title": "9  Infinite horizon MDPs",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 9.1 (One-step look-ahead error bounds.) Given any \\(V \\in \\reals^n\\), let \\(π\\) be such that \\(\\BELLMAN^* V = \\mathcal B_π V\\). Moreover, let \\(V^*\\) denote the unique fixed point of \\(\\BELLMAN^*\\) and \\(V^π\\) denote the unique fixed point of \\(\\BELLMAN^π\\). Then, show that\n\n\\[ \\| V^* - V \\| \\le \\frac{1}{1-γ} \\| \\BELLMAN^* V - V \\|. \\]\n\\[ \\| V^* - \\BELLMAN^* V \\| \\le \\frac{γ}{1-γ} \\| \\BELLMAN^* V - V \\|. \\]\n\\[ \\| V^π - V \\| \\le \\frac{1}{1-γ} \\| \\BELLMAN^π V - V \\|. \\]\n\\[ \\| V^π - \\BELLMAN^π V \\| \\le \\frac{γ}{1-γ} \\| \\BELLMAN^π V - V \\|. \\]\n\\[ \\| V^π - V^* \\| \\le \\frac{2}{1-γ} \\| \\BELLMAN^* V - V \\|. \\]\n\\[ \\| V^π - V^* \\| \\le \\frac{2γ}{1 - γ} \\| V - V^* \\|. \\]"
  },
  {
    "objectID": "mdps/inf-horizon.html#notes",
    "href": "mdps/inf-horizon.html#notes",
    "title": "9  Infinite horizon MDPs",
    "section": "Notes",
    "text": "Notes\nThe material included here is referenced from different sources. Perhaps the best sources to study this material are the books by Puterman (2014), Whittle (1982), and Bertsekas (2011).\n\n\n\n\nBertsekas, D.P. 2011. Dynamic programming and optimal control. Athena Scientific. Available at: http://www.athenasc.com/dpbook.html.\n\n\nBlackwell, D. 1965. Discounted dynamic programming. The Annals of Mathematical Statistics 36, 1, 226–235. DOI: 10.1214/aoms/1177700285.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887.\n\n\nShwartz, A. 2001. Death and discounting. IEEE Transactions on Automatic Control 46, 4, 644–647. DOI: 10.1109/9.917668.\n\n\nWhittle, P. 1982. Optimization over time: Dynamic programming and stochastic control. Vol. 1 and 2. Wiley."
  },
  {
    "objectID": "mdps/mdp-algorithms.html#value-iteration-algorithm",
    "href": "mdps/mdp-algorithms.html#value-iteration-algorithm",
    "title": "10  MDP algorithms",
    "section": "10.1 Value Iteration Algorithm",
    "text": "10.1 Value Iteration Algorithm\n\n\n\n\n\n\n Value Iteration Algorithm\n\n\n\n\nStart with any \\(V_0 \\in \\reals^n\\).\nRecursively compute \\(V_{k+1} = \\mathcal B V_k = \\mathcal B_{π_k} V_k.\\)\nDefine \\[ \\begin{align*}\n   \\underline δ_k &= \\frac{γ}{1-γ} \\min_s \\{ V_k(s) - V_{k-1}(s) \\}, \\\\\n   \\bar δ_k &=       \\frac{γ}{1-γ} \\max_s \\{ V_k(s) - V_{k-1}(s) \\}.\n\\end{align*} \\]\n\nThen, for all \\(k\\)\n\n\\(V_k + \\underline δ_k \\ONES \\le V^* \\le V_k + \\bar δ_k \\ONES\\).\n\\((\\underline δ_k - \\bar δ_k) \\ONES \\le V_{π_k} - V^* \\le (\\bar δ_k - \\underline δ_k) \\ONES.\\)\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy construction, \\[ \\begin{align*}\n   \\mathcal B V_k - V_k &= \\mathcal B V_k - \\mathcal B V_{k-1} \\\\\n   & \\le \\mathcal B_{π_{k-1}} V_k - \\mathcal B_{π_{k-1}} V_{k-1}\\\\\n   & \\le γ P_{π_{k-1}}[ V_k - V_{k-1} ] \\\\\n   &= (1-γ) \\bar δ_k \\ONES.\n\\end{align*} \\] Thus, by Proposition 9.5, we have \\[\\begin{equation} \\label{eq:VI-1}\n  V^* \\le V_k + \\bar δ_k \\ONES.\n\\end{equation}\\] Note that \\(\\mathcal B V_k = \\mathcal B_{π_k} V_k\\). So, we have also show that \\(\\mathcal B_{π_k} V_k - V_k \\le (1-γ) \\bar δ_k \\ONES\\). Thus, again by Proposition 9.5, we have \\[\\begin{equation}\\label{eq:VI-2}\n  V_{π_k} \\le V_k + \\bar δ_k \\ONES.\n\\end{equation}\\]\nBy a similar argument, we can show \\[\\begin{equation}\\label{eq:VI-3}\n  V^* \\ge V_k + \\underline δ_k \\ONES\n\\quad\\text{and}\\quad\nV_{π_k} \\ge V_k + \\underline δ_k \\ONES.\n\\end{equation}\\]\nEq. \\eqref{eq:VI-1} and \\eqref{eq:VI-3} imply the first relationship of the result. To establish the second relationship, note that the triangle inequality \\[ \\max\\{ V_{π_k} - V^* \\} \\le\n   \\max\\{ V_{π_k} - V_k \\} + \\max\\{ V_{k} - V^* \\}\n   \\le (\\bar δ_k - \\underline δ_k) \\ONES.\n\\] Similarly, \\[\n  \\max\\{ V^* - V_{π_k} \\} \\le\n   \\max \\{ V^* - V_k \\} + \\max\\{ V_k - V_{π_k} \\}\n   \\le (\\bar δ_k - \\underline δ_k) \\ONES.\n\\] Combining the above two equation, we get the second relationship of the result."
  },
  {
    "objectID": "mdps/mdp-algorithms.html#policy-iteration-algorithm",
    "href": "mdps/mdp-algorithms.html#policy-iteration-algorithm",
    "title": "10  MDP algorithms",
    "section": "10.2 Policy Iteration Algorithm",
    "text": "10.2 Policy Iteration Algorithm\n\nLemma 10.1 (Policy improvement) Suppose \\(V_π\\) is the fixed point of \\(\\mathcal B_π\\) and \\[\\mathcal B_{μ} V_π = \\mathcal B V_π. \\] Then, \\[V_{μ}(s) \\le V_π(s), \\quad \\forall s \\in \\ALPHABET S. \\] Moreover, if \\(π\\) is not optimal, then at least one inequality is strict.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[ V_π = \\mathcal B_π V_π \\ge \\mathcal B V_π = \\mathcal B_{μ} V_π.\\] Thus, \\[ V_π \\ge V_{μ}. \\]\nFinally, suppose \\(V_μ = V_π\\). Then, \\[ V_μ = \\mathcal B_μ V_μ = \\mathcal B_μ V_π = \\mathcal B V_π = \\mathcal B\nV_μ. \\] Thus, \\(V_μ\\) (and \\(V_π\\)) is the unique fixed point of \\(\\mathcal B\\). Hence \\(μ\\) and \\(π\\) are optimal.\n\n\n\n\n\n\n\n\n\n Policy Iteration Algorithm\n\n\n\n\nStart with an arbitrary policy \\(π_0\\). Compute \\(V_0 = \\mathcal B_{π_0} V_0\\).\nRecursively compute a policy \\(π_k\\) such that \\[\\mathcal B V_{k-1} = \\mathcal B_{π_k} V_{k-1}\\] and compute the performance of the policy using \\[ V_k = \\mathcal B_{π_k} V_k.\\]\nStop if \\(V_k = V_{k-1}\\) (or \\(π_k = π_{k-1}\\)).\n\n\n\nThe policy improvement lemma (Lemma 10.1) implies that \\(V_{k-1} \\ge V_k\\). Since the state and action spaces are finite, there are only a finite number of policies. The value function improves at each step. So the process must converge in finite number of iterations. At convergence, \\(V_k = V_{k-1}\\) and the policy improvement lemma implies that the corresponding policies \\(π_k\\) or \\(π_{k-1}\\) are optimal.\n\n10.2.1 Policy iteration as Newton-Raphson algoritm\n\nRecall that the main idea behind Newton-Raphson is as follows. Suppose we want to solve a fixed point equation \\(V = \\mathcal B V\\) and we have an approximate solution \\(V_k\\). Then we can search for an improved soluiton \\(V_{k+1} = V_k + Δ_k\\) by setting \\[\\begin{equation} \\label{eq:NR}\nV_k + Δ_k = \\mathcal{B}( V_k + Δ_k ),\n\\end{equation} \\] expanding the right-hand side as far as first-order terms in \\(Δ_k\\) and solving the consequent linear equation for \\(Δ_k\\).\nNow, let’s try to apply this idea to find the fixed point of the Bellman equation. Suppose we have identified a guess \\(V_k\\) and \\(\\mathcal B V_k = \\mathcal B_{π_{k+1}} V_k\\). Because the choice of control action \\(a\\) is optimization out in \\(\\mathcal B\\), the varation of \\(a\\) induced by the variation \\(Δ_k\\) of \\(V_k\\) has no first-order effect on the value of \\(\\mathcal B(V_k + Δ_k)\\). Therefore, \\[\n  \\mathcal{B}(V_k + Δ_k) = \\mathcal B_{π_{k+1}}(V_k + Δ_k) + o(Δ_k).\n\\] It follows that the linearized version of \\eqref{eq:NR} is just \\[\n  V_{k+1} = \\mathcal B_{π_{k+1}} V_{k+1}.\n\\] That is, \\(V_{k+1}\\) is just the value function for the policy \\(π_{k+1}\\), where \\(π_{k+1}\\) was deduced from the value function \\(V_k\\) exactly by the policy improvement procedure. Therefore, we can conclude the following.\n\nTheorem 10.1 The policy improvement algorithm is equivalent to the application of Newton-Raphson algorithm to the fixed point equation \\(V = \\mathcal B V\\) of dynamic programming.\n\nThe equivalence between policy iteration and Newton-Raphson partily explains why policy iteration approaches converge in few iterations."
  },
  {
    "objectID": "mdps/mdp-algorithms.html#optimistic-policy-iteration",
    "href": "mdps/mdp-algorithms.html#optimistic-policy-iteration",
    "title": "10  MDP algorithms",
    "section": "10.3 Optimistic Policy Iteration",
    "text": "10.3 Optimistic Policy Iteration\n\n\n\n\n\n\n Optimistic Policy Iteration Algorithm\n\n\n\n\nFix a sequence of integers \\(\\{\\ell_k\\}_{k \\in \\integers_{\\ge 0}}\\).\nStart with an initial guess \\(V_0 \\in \\reals^n\\).\nFor \\(k=0, 1, 2, \\dots\\), recursively compute a policy \\(π_k\\) such that \\[ \\mathcal B_{π_k} V_k = \\mathcal B V_k \\] and then update the value function \\[ V_{k+1} = \\mathcal B_{π_k}^{\\ell_k} V_k. \\]\n\n\n\nNote that if \\(\\ell_k = 1\\), the optimistic policy iteration is equivalent to value iteration and if \\(\\ell_k = \\infty\\), then optimistic policy iteration is equal to policy iteration.\nIn the remainder of this section, we state the modifications of the main results to establish the convergence bounds for optimistic policy iteration.\n\nProposition 10.1 For any \\(V \\in \\reals^n\\) and \\(m \\ONES \\in \\reals_{\\ge 0}\\)\n\nIf \\(V + m \\ONES \\ge \\mathcal B V = \\mathcal B_π V\\), then for any \\(\\ell \\in \\integers_{&gt; 0}\\), \\[ \\mathcal B V + \\frac{γ}{1 - γ} m \\ONES \\ge \\mathcal B_π^\\ell V \\] and \\[ \\mathcal B_π^\\ell V + γ^\\ell m \\ONES \\ge \\mathcal B( \\mathcal B_π^\\ell V). \\]\n\n\nThe proof is left as an exercise.\n\nProposition 10.2 Let \\(\\{(V_k, π_k)\\}_{k \\ge 0}\\) be generated as per the optimistic policy iteration algorithm. Define \\[ \\alpha_k = \\begin{cases}\n  1, & \\text{if } k = 0 \\\\\n  γ^{\\ell_0 + \\ell_1 + \\dots + \\ell_{k-1}}, & \\text{if } k &gt; 0.\n\\end{cases}\\] Suppose there exists an \\(m \\in \\reals\\) such that \\[ \\| V_0 - \\mathcal B V_0 \\| \\le m. \\] Then, for all \\(k \\ge 0\\), \\[ \\mathcal B V_{k+1} - \\alpha_{k+1} m \\le V_{k+1} \\le\n\\mathcal B V_k + \\frac{γ}{1-γ} \\alpha_k m.\n\\] Moreover, \\[ V_{k} - \\frac{(k+1) γ^k}{1-γ} m \\le\n    V^* \\le\n    V_k + \\frac{\\alpha_k}{1 - γ} m \\le\n    V_k + \\frac{γ^k}{1 - γ} m. \\]"
  },
  {
    "objectID": "mdps/mdp-algorithms.html#exercises",
    "href": "mdps/mdp-algorithms.html#exercises",
    "title": "10  MDP algorithms",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 10.1 Show that the error bound for value iteration is monotone with the number of iterations, i.e, \\[ V_k + \\underline δ_k \\ONES \\le V_{k+1} + \\underline δ_{k+1} \\ONES\n\\le V^*\n\\le V_{k+1} + \\bar δ_{k+1} \\ONES \\le V_k + \\bar δ_k \\ONES. \\]"
  },
  {
    "objectID": "mdps/mdp-algorithms.html#notes",
    "href": "mdps/mdp-algorithms.html#notes",
    "title": "10  MDP algorithms",
    "section": "Notes",
    "text": "Notes\nThe techniques for value iteration and policy improvement were formalized by Howard (1960). The equivalence of policy improvement and the Newton-Raphson algorithm was demonstrated in the LQ case by Whittle and Komarova (1988), for which it holds in a tighter sense.\n\n\n\n\nHoward, R.A. 1960. Dynamic programming and markov processes. The M.I.T. Press.\n\n\nWhittle, P. and Komarova, N. 1988. Policy improvement and the newton-raphson algorithm. Probability in the Engineering and Informational Sciences 2, 2, 249–255. DOI: 10.1017/s0269964800000760."
  },
  {
    "objectID": "mdps/inventory-management-revisited.html#exercises",
    "href": "mdps/inventory-management-revisited.html#exercises",
    "title": "11  Inventory management (revisted)",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 11.1 Suppose that the arrival process is exponential with rate \\(1/\\mu\\), i.e., the density of \\(W\\) is given by \\(e^{-s/\\mu}/\\mu\\). Show that the optimal threshold is given by \\[ s^* = \\mu \\log \\left[ \\frac{ c_h + c_s} { c_h + p (1-γ)/γ} \\right]. \\]\nHint: Recall that the CDF the exponential distribution is \\(F(s) = 1 - e^{-s/μ}\\)."
  },
  {
    "objectID": "mdps/inventory-management-revisited.html#notes",
    "href": "mdps/inventory-management-revisited.html#notes",
    "title": "11  Inventory management (revisted)",
    "section": "Notes",
    "text": "Notes\nThe idea of using reward shaping to derive a closed form expression for inventory management is taken from Whittle (1982). It is interesting to note that Whittle (1982) uses the idea of reward shaping more than 17 years before the paper by Ng et al. (1999) on reward shaping. It is possible that Whittle was using the results of Porteus (1975).\nAs established in the notes on Lipschitz MDPs, it can be shown that the optimal value function for the inventory management model above is Lipschitz continuous.\n\n\n\n\nNg, A.Y., Harada, D., and Russell, S. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. ICML, 278–287. Available at: http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf.\n\n\nPorteus, E.L. 1975. Bounds and transformations for discounted finite markov decision chains. Operations Research 23, 4, 761–784. DOI: 10.1287/opre.23.4.761.\n\n\nWhittle, P. 1982. Optimization over time: Dynamic programming and stochastic control. Vol. 1 and 2. Wiley."
  },
  {
    "objectID": "mdps/mobile-edge-computing.html#structure-of-the-optimal-policy",
    "href": "mdps/mobile-edge-computing.html#structure-of-the-optimal-policy",
    "title": "12  Service Migration in Mobile edge computing",
    "section": "12.1 Structure of the optimal policy",
    "text": "12.1 Structure of the optimal policy\nWe provide a basic characterization of the optimal policy.\n\nProposition 12.1 Let \\(π^*\\) denote the optimal policy. Then for any \\((x,s) \\in \\ALPHABET X × \\ALPHABET S\\), we have \\[ \\| x - π^*(x,s) \\| \\le \\| x - s \\|. \\]\n\nProposition 12.1 states that the optimal policy always migrates the user to a server which is closer than the one already serving the user.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result using an interchange argument. Suppose we are given a service migration policy \\(π\\) such that the service is migrated to a location farther away from the user, i.e., \\(\\|x - a\\| &gt; \\| x - s \\|\\). We will show that for an arbitrary sample path of the user locations \\(\\{ x_t\\}_{t \\ge 1}\\), we can find a (possibly history dependent) policy \\(μ\\) that does not migrate to locations further away from the user in any time slot and performs no worse than policy \\(π\\).\nGiven a arbitrary sample path of user locations \\(\\{x_t\\}_{t \\ge 1}\\) let \\(t_0\\) denote the first timeslot in which the service is migrated to somewhere farther away from the user when following policy \\(π\\). The state of \\(t_0\\) is \\((x_{t_0}, s_{t_0})\\) and the policy \\(π\\) moves the service to server \\(a_{t_0} = π(x_{t_0}, s_{t_0})\\), where \\(\\|x_{t_0} - a_{t_0}\\| &gt; \\| x_{t_0} - s_{t_0} \\|\\). Let \\(\\{a^π_t\\}_{t \\ge t_0}\\) denote the subsequent locations of the server (after migration) under policy \\(π\\).\nNow, we define a policy \\(μ\\) such that the following conditions are satisfied for the given sample path \\(\\{x_t\\}_{t \\ge 1}\\) of the user locations as follows. The policy \\(μ\\) chooses the same migration actions as policy \\(π\\) in timeslots \\(t &lt; t_0\\).\nNow, suppose \\[\\begin{equation} \\label{eq:1}\n  \\| x_{t} - s^π_{t_0} \\| \\le \\| x_{t} - a^π_{t} \\|, \\quad \\forall t &gt; t_0.\n\\end{equation}\\] Then, the policy \\(μ\\) does not choose any migrations from time \\(t_0\\) onwards. Hence, \\(a^h_t = s^π_{t_0}\\) for all \\(t \\ge t_0\\). Note that from time \\(t_0\\) onwards, policy \\(μ\\) doesn’t incur any migration cost and always incurs a transmission cost which is less than \\(π\\). Hence, policy \\(μ\\) performs at least as well as policy \\(π\\).\nNow suppose \\eqref{eq:1} does not hold. Then define \\(t_m\\) to be the first timeslot after \\(t_0\\) such that \\[\n  \\| x_{t_m} - s^π_{t_0} \\| &gt; \\| x_{t_m} - a^π_{t_m} \\|.\n\\]\nNow, we define policy \\(μ\\) as a policy which does not specify any migrations for time \\(t \\in [t_0, t_m - 1]\\), migrates to location \\(a^π_{t_m}\\) at timeslot \\(t_m\\), and follows policy \\(π\\) from \\(t_m\\) onwards.\nNote that policies \\(π\\) and \\(μ\\) agree on \\([1, t_0 -1]\\) and \\([t_m + 1, ∞)\\). In the interval \\([t_0, t_m]\\), \\[\n  \\| x_{t} - a^h_t \\| \\le \\| x_t - a^π_t \\|.\n\\] Thus, the transmission cost of policy \\(μ\\) is no more than the transmission cost of policy \\(π\\).\nNow, the migration cost incurred by policy \\(π\\) in the interval \\([t_0, t_m]\\) can be lower bounded by the migration cost incurred by policy \\(μ\\) as follows: \\[\\begin{align*}\n  \\hskip 2em & \\hskip -2em\n  γ^{t_0 - 1}  b(\\| s^π_{t_0} - a^π_{t_0} \\|) +\n  γ^{t_0 } b(\\| a^π_{t_0} - a^π_{t_0 + 1} \\|) + \\cdots  +\n  γ^{t_m - 1} b(\\| a^π_{t_m -1} - a^π_{t_m} \\|) \\\\\n  &\\ge\n  γ^{t_m - 1}\\bigl[\n    b(\\| s^π_{t_0} - a^π_{t_0} \\|) +\n   b(\\| a^π_{t_0} - a^π_{t_0 + 1} \\|) + \\cdots  +\n   b(\\| a^π_{t_m -1} - a^π_{t_m} \\|)  \\bigr]\n  \\\\\n  &\\ge\n  γ^{t_m - 1} b(\\| s^π_{t_0} - a^π_{t_m} \\|),\n\\end{align*}\\] where the first inequality follows because \\(γ &lt; 1\\) and the second follows from the triangle inequality.\nHence, policy \\(μ\\) performs at least as well as policy \\(π\\). The above procedure can be repeated so that all the mitigation actions to a location farther away from the user can be removed without increasing the overall cost. \nNote that the policy \\(μ\\) constructed above is a history dependent policy. From the result for infinite horizon MDP, we know that a history dependent policy cannot outperform Markovian policies. Therefore, there exists a Markovian policy that does not migrate to a location farther away from the user, which does not perform worse than \\(π\\)."
  },
  {
    "objectID": "mdps/mobile-edge-computing.html#notes",
    "href": "mdps/mobile-edge-computing.html#notes",
    "title": "12  Service Migration in Mobile edge computing",
    "section": "Notes",
    "text": "Notes\nThe model and results presented here are taken from Wang et al. (2019). See Urgaonkar et al. (2015) for a variation of this model.\n\n\n\n\nUrgaonkar, R., Wang, S., He, T., Zafer, M., Chan, K., and Leung, K.K. 2015. Dynamic service migration and workload scheduling in edge-clouds. Performance Evaluation 91, 205–228. DOI: 10.1016/j.peva.2015.06.013.\n\n\nWang, S., Urgaonkar, R., Zafer, M., He, T., Chan, K., and Leung, K.K. 2019. Dynamic service migration in mobile edge computing based on Markov decision process. IEEE/ACM Transactions on Networking 27, 3, 1272–1288. DOI: 10.1109/tnet.2019.2916577."
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#span-norm-contraction",
    "href": "mdps/computational-complexity-vi.html#span-norm-contraction",
    "title": "13  Computational complexity of value interation",
    "section": "13.1 Span norm contraction",
    "text": "13.1 Span norm contraction\nLet \\(\\SPAN(v) = \\max(v) - \\min(v)\\) denotes the span semi-norm. We start by stating some basic properties of span semi-norm.\n\n\\(\\SPAN(v) \\ge 0\\) for all \\(v \\in \\reals^n\\)\n\\(\\SPAN(v + w) \\le \\SPAN(v) + \\SPAN(w)\\) for all \\(v, w \\in \\reals^n\\).\n\\(\\SPAN(m v) \\le |m| \\SPAN(v)\\) for all \\(v \\in \\reals^n\\) and \\(m \\in \\reals\\).\n\\(\\SPAN(v + m \\mathbf{1}) = \\SPAN(v)\\) for all \\(m \\in \\reals\\).\n\\(\\SPAN(v) = \\SPAN(-v)\\).\n\\(\\SPAN(v) \\le 2\\|v\\|\\).\n\nProperties 1–3 imply that \\(\\SPAN(v)\\) is a semi-norm. However, it is not a norm because of property 4; that is, \\(\\SPAN(v) = 0\\) does not imply that \\(v = 0\\). If \\(\\SPAN(v) = 0\\), then \\(v = m \\mathbf{1}\\) for some scalar \\(m\\).\nA basic result for our analysis is the following:\n\nProposition 13.1 Let \\(v \\in \\reals^n\\) and \\(P\\) be any matrix of compatible dimensions. Then, \\[ \\SPAN(P v) \\le β_P \\SPAN(v), \\] where \\[\\begin{equation} \\label{eq:span-matrix}\n  β_P = 1 - \\min_{s, s' \\in \\ALPHABET S}\n  \\sum_{z \\in \\ALPHABET S} \\min\\{ P_{sz}, P_{s'z} \\}.\n\\end{equation}\\] Furhermore, \\(β_P \\in [0, 1]\\) and there exists a \\(v \\in \\reals^n\\) such that \\(\\SPAN(Pv) = β_P \\SPAN(v)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNote that for any \\(v \\in \\reals^n\\) \\[\\begin{align*}\n\\SPAN(Pv) &= \\max_{s \\in \\ALPHABET S} \\sum_{z \\in \\ALPHABET S} P_{sz} v_z\n- \\min_{s' \\in \\ALPHABET S} \\sum_{z \\in \\ALPHABET S} P_{s'z} v_z \\\\\n&= \\max_{s, s' \\in \\ALPHABET S}\n  \\sum_{z \\in \\ALPHABET S} P_{sz} v_z - \\sum_{z \\in \\ALPHABET S} P_{s'z} v_z\n\\end{align*}\\] Let \\(B(z; s,s') = \\min\\{ P_{sz}, P_{s'z} \\}\\). Then consider \\[\\begin{align*}\n  \\sum_{z \\in \\ALPHABET S} P_{sz} v_z - \\sum_{z \\in \\ALPHABET S} P_{s'z} v_z\n  &=\n  \\sum_{z \\in \\ALPHABET S} [ P_{sz} - B(z; s,s') ] v_z -\n  \\sum_{z \\in \\ALPHABET S} [ P_{s'z} - B(z; s,s') ] v_z \\\\\n  &\\le\n  \\sum_{z \\in \\ALPHABET S} [ P_{sz} - B(z; s,s') ] \\max(v) -\n  \\sum_{z \\in \\ALPHABET S} [ P_{s'z} - B(z; s,s') ] \\min(v)\n  \\\\\n  &= \\biggl[ 1 - \\sum_{z \\in \\ALPHABET S} B(z; s, s') \\biggr] \\SPAN(v).\n\\end{align*}\\] Hence, \\[ \\SPAN(Pv) \\le \\max_{s, s' \\in \\ALPHABET S} \\biggl[\n  1 - \\sum_{z \\in \\ALPHABET S} B(z; s, ) \\biggr] \\SPAN(v). \\]\nNow we show that there exists a \\(v\\) such that \\eqref{eq:span-matrix} holds with equality. If \\(β_P = 0\\), then \\(P\\) has equal rows, so that \\(\\SPAN(Pv) = 0 = 0 \\cdot \\SPAN(v)\\) for all \\(v \\in \\reals^n\\). Suppose \\(β_P &gt; 0\\). Using the identity \\([ a - b]^{+} = a - \\min(a,b)\\), we can write \\[\nβ_P = \\max_{s,s' \\in \\ALPHABET S}\n\\sum_{z \\in \\ALPHABET  S} \\bigl[ P_{sz} - P_{s'z} \\bigr]^{+}.\n\\] Let \\(s^*\\) and \\(s'^*\\) be such that \\[\n  β_P = \\sum_{z \\in \\ALPHABET S}  \n\\sum_{z \\in \\ALPHABET  S} \\bigl[ P_{s^*z} - P_{s'^*z} \\bigr]^{+}.\n\\] Define \\(v\\) by \\[\n  v_z = \\IND\\{ P_{s^*z} &gt; P_{s'^*z} \\}.\n\\] Then, note that \\(\\SPAN(v) = 1\\) and \\[\\begin{align*}\n  \\SPAN(Pv) &\\ge\n  \\sum_{z \\in \\ALPHABET S} P_{s^* z} v_z -\n  \\sum_{z \\in \\ALPHABET S} P_{s'^* z} v_z \\\\\n  &=\n  \\sum_{z \\in \\ALPHABET  S} \\bigl[ P_{s^*z} - P_{s'^*z} \\bigr]^{+}\n  \\\\\n  &= β_P \\SPAN(v).\n\\end{align*}\\] Combining with \\eqref{eq:span-matrix}, we get \\(\\SPAN(Pv) = β_P \\SPAN(v)\\).\n\n\n\nProposition 13.1 illustrates the “averaging” property of a transition matrix. By multiplying a vector by a transition matrix, the resulting vector has components which are more nearly equal. When \\(P\\) is a square matrix, the quantity \\(β_P\\) is called the ergodicity coefficient, which is often written in an alternative form by using the relation \\(|a - b| = (a + b) - 2 \\min(a,b)\\): \\[\n  β_P = \\frac12\n  \\max_{s,s' \\in \\ALPHABET S} \\sum_{z \\in \\ALPHABET S}\n  \\bigl| P_{sz} - P_{s'z} \\bigr|. \\] The ergodicity coefficient is an upper bound on the second largest eigenvalue of \\(P\\). \\(β_P\\) equals \\(0\\) if all rows of \\(P\\) are equal and equals \\(1\\) if at least two rows of \\(P_d\\) are orthogonal. From a different perspective, \\(β_P &lt; 1\\) if for each pair of states there exists at least one state which they both can reach with positive probability in one step.\n\n\n\n\n\n\nRemark\n\n\n\nNote that \\[\\begin{equation}\\label{eq:ergodicity-bound}\nβ_P \\le 1 - \\sum_{z \\in \\ALPHABET S} \\min_{s \\in \\ALPHABET S} P_{sz}\n=: β_P'\n\\end{equation}\\] which is easier to compute.\n\n\nDefine the contraction factor \\[\\begin{equation}\\label{eq:contraction}\n  β = \\max_{\\substack{ s,s' \\in \\ALPHABET S \\\\ a \\in \\ALPHABET A(s), w \\in\n  \\ALPHABET A(s')}}\n  \\biggl[ 1 - \\sum_{z \\in \\ALPHABET S}\n    \\min\\{ P(z | s,a), P(z | s', w) \\biggr].\n\\end{equation}\\] Note that \\(β \\in [0, 1]\\).\n\nTheorem 13.1 For any \\(V_1, V_2 \\in \\reals^n\\), \\[ \\SPAN(\\mathcal B V_1 - \\mathcal B V_2) \\le γ β\\, \\SPAN(V_1 - V_2). \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(π_i\\) be such that \\(\\mathcal B V_i = \\mathcal B_{π_i}V_i\\). Let \\[\\begin{align*}\n  s^* &= \\arg \\max_{s \\in \\ALPHABET S}(\\mathcal B V_1(s) - \\mathcal B V_2(s)),\n  \\\\\n  s_* &= \\arg \\min_{s \\in \\ALPHABET S}(\\mathcal B V_1(s) - \\mathcal B V_2(s)),\n\\end{align*}\\] Then, \\[\n  \\mathcal B V_1(s^*) - \\mathcal B V_2(s^*) \\le\n  \\mathcal B_{π_2} V_1(s^*) - \\mathcal B_{π_2} V_2(s^*)\n  = γ P_{π_2}(V_1 - V_2)(s^*)\n\\] and \\[\n  \\mathcal B V_1(s_*) - \\mathcal B V_2(s_*) \\ge\n  \\mathcal B_{π_1} V_1(s^*) - \\mathcal B_{π_1} V_2(s^*)\n  = γ P_{π_1}(V_1 - V_2)(s^*).\n\\] Therefore, \\[\\begin{align*}\n  \\SPAN(\\mathcal B V_1 - \\mathcal B V_2) &\\le\n  γ P_{π_2}(V_1 - V_2)(s^*) - γ P_{π_1}(V_1 - V_2)(s_*) \\\\\n  &\\le \\max_{s \\in \\ALPHABET S} γ P_{π_2} (V_1 - V_2)(s) -\n  \\min_{s \\in \\ALPHABET S} γ P_{π_1}(V_1 - V_2)(s)\n  \\\\\n  &\\le \\SPAN(γ\\, \\ROWS(P_{π_2}, P_{π_1})(V_1 - V_2).\n\\end{align*}\\] By applying Proposition 13.1, we get \\[ \\SPAN(\\mathcal B V_1 - \\mathcal B V_2) \\le γβ_{\\bar P} \\SPAN(V_1 - V_2), \\] where \\(β_{\\bar P}\\) is given by \\eqref{eq:span-matrix} with \\(\\bar P = \\ROWS(P_{π_2}, P_{π_1})\\). The result follows by noting that \\(β_{\\bar P}\\) is at most \\(β\\)."
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#computational-complexity-of-value-iteration",
    "href": "mdps/computational-complexity-vi.html#computational-complexity-of-value-iteration",
    "title": "13  Computational complexity of value interation",
    "section": "13.2 Computational complexity of value iteration",
    "text": "13.2 Computational complexity of value iteration\nNote that \\(β \\in [0, 1]\\). We will first rule out the case \\(β = 0\\). If \\(β = 0\\), then \\(P(z | s, a) = P(z | s', w)\\) for all \\(s, s', z \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A(s)\\) and \\(w \\in \\ALPHABET A(s')\\), which implies that all deterministic policies have the same transition probabilities. Therefore, a deterministic policy \\(π\\) is optimal if and only if it minimize the one-step cost. Thus, the case with \\(β = 0\\) is trivial. So, in the rest of the analysis, we assume that \\(β \\in (0, 1)\\).\n\nTheorem 13.2 Start with an abritrary \\(V_0\\). If \\(Δ_1 = 0\\), then we obtain an optimal policy in iteration 1. Otherwise, for any \\(ε &gt; 0\\), value iteration finds an \\(ε\\)-optimal policy in no more than \\(K^*(γ)\\) iterations, where \\[\\begin{equation}\\label{eq:K*}\n  K^*(γ) =  \\left\\lceil\n  \\frac{ \\log \\frac{(1-γ) ε β}{Δ_1} } {\\log(γβ)}\n  \\right\\rceil.\n\\end{equation}\\] In addition, each iteration uses at most \\(O(nM)\\) operations.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf follows from the definition of Bellman operator that each iteration uses at most \\(O(nM)\\) iterations (to compute the \\(Q\\) function, for each state-action pair, we need to compute a sum over \\(n\\) terms).\nFrom Theorem 13.1, we get that \\(Δ_k \\le (γβ)^{k-1} Δ_1\\). Therefore, the minimum number of iterations required to achieve \\(Δ_k \\le \\frac{1-γ}{γ}ε\\) is given \\(K^*(γ)\\). \\(\\Box\\)\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that finding the value of \\(β\\) requires computing the sum in \\eqref{eq:contraction} for all couples \\(\\{ (s,a), (s',b) \\}\\) of state action pairs such that \\((s,a) \\neq (s',b)\\). The total number of such pairs are \\(M(M-1)/2 = O(M^2)\\). Therefore, the number of arithematic operators in \\eqref{eq:contraction}, which are additions, is \\(n\\) for each couple. Therefore, computation of \\(β\\) requires \\(O(nM^2)\\) operations, which can be significantly larger than the complexity of computing an \\(ε\\)-optimal policy which is given by Theorem 13.2! Based on \\eqref{eq:ergodicity-bound} we can replace \\(β\\) by in \\eqref{eq:K*} by \\[\n  β' = 1 - \\sum_{z \\in \\ALPHABET S} \\min_{s \\in \\ALPHABET S, a \\in \\ALPHABET A}\n       P(z | s,a)\n\\] which requies \\(O(nM)\\) operations."
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#the-bound-may-be-exact",
    "href": "mdps/computational-complexity-vi.html#the-bound-may-be-exact",
    "title": "13  Computational complexity of value interation",
    "section": "13.3 The bound may be exact",
    "text": "13.3 The bound may be exact\nWe now present an example (due to Feinberg and He (2020)) to show that the bound in Theorem 13.2 may be exact. Consider an MDP with \\(\\ALPHABET S = \\{1, 2, 3\\}\\), \\(\\ALPHABET A = \\{1,2 \\}\\), with \\(\\ALPHABET A(1) = \\{1, 2\\}\\) and \\(\\ALPHABET A(2) = \\ALPHABET A(3) = \\{1\\}\\). The per-step transitions are \\[\n  P(1) = \\MATRIX{0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 },\n  \\quad\\text{and}\\quad\n  P(2) = \\MATRIX{0 & 1 & 0 \\\\ * & * & * \\\\ * & * & *},\n\\] where \\(*\\) indicates that the corresponding action is infeasible. The reward matrix is \\[\n  r = \\MATRIX{1 & 0 \\\\ 1 & * \\\\ -1 & *}.\n\\]\nSuppose we start with an initial \\(V_0 = \\VEC(1, 2, -2)\\). Then elementary calculations show that \\[\n  V_k = \\MATRIX{ γ^k \\\\ γ^k \\\\ -γ^k} +\n  \\MATRIX{ \\sum_{\\ell = {\\color{red} 1}}^{k} γ^\\ell \\\\\n           \\sum_{\\ell = 0}^{k} γ^\\ell \\\\\n           \\sum_{\\ell = 0}^{k} γ^\\ell }.\n\\] Thus, \\[\n  V_k - V_{k-1} = \\MATRIX{\n  2 γ^k - γ^{k-1} \\\\\n  2 γ^k - γ^{k-1} \\\\\n  - 2 γ^k + γ^{k-1} }.\n\\] Hence, \\[\n\\SPAN(V_k - V_{k-1}) = 2γ^{k-1} |2γ - 1| = γ^{k-1} \\SPAN(V_1 - V_0).\n\\]\nThus, for this model, the expression \\eqref{eq:K*} is exact.\n\n\n\n\n\n\nRemark\n\n\n\nThe exact number of iterations need not be monotone in \\(γ\\)! In the above example, let \\(ε = 0.02\\), then \\[\n  K^*(0.24) = 3, \\quad\n  K^*(0.47) = 4, \\quad\n  K^*(0.48) = 3.\n\\]\nThus, the number of iterations is not monotone in \\(γ\\)."
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#notes",
    "href": "mdps/computational-complexity-vi.html#notes",
    "title": "13  Computational complexity of value interation",
    "section": "Notes",
    "text": "Notes\nThe discussion on span semi-norm and Theorem 13.1 is from Puterman (2014). Theorem 13.2 is from Feinberg and He (2020).\n\n\n\n\nFeinberg, E.A. and He, G. 2020. Complexity bounds for approximately solving discounted MDPs by value iterations. Operations Research Letters. DOI: 10.1016/j.orl.2020.07.001.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887."
  },
  {
    "objectID": "mdps/martingale.html#notes",
    "href": "mdps/martingale.html#notes",
    "title": "14  Thirfty and equalizing policies",
    "section": "Notes",
    "text": "Notes\nSee Davis (1979) for a historical review of martingale methods for stochastic control.\nThe idea of thirfty and equalizing policies is due to Dubins and Savage (2014). It was used by Blackwell (1970) for characterizing optimal solutions of total reward problems and is a commonly used technique in that literature. The “translation” to discounted cost problems is taken from Karatzas and Sudderth (2010).\n\n\n\n\n\nBlackwell, D. 1970. On stationary policies. Journal of the Royal Statistical Society. Series A (General) 133, 1, 33. DOI: 10.2307/2343810.\n\n\nDavis, M.H.A. 1979. Martingale methods in stochastic control. In: Stochastic control theory and stochastic differential systems. Springer-Verlag, 85–117. DOI: 10.1007/bfb0009377.\n\n\nDubins, L.E. and Savage, L.J. 2014. How to gamble if you must: Inequalities for stochastic processes. Dover Publications.\n\n\nKaratzas, I. and Sudderth, W.D. 2010. Two characterizations of optimality in dynamic programming. Applied Mathematics and Optimization 61, 3, 421–434. DOI: 10.1007/s00245-009-9093-x."
  },
  {
    "objectID": "mdps/linear-programming.html#constrained-mdps",
    "href": "mdps/linear-programming.html#constrained-mdps",
    "title": "15  Linear programming formulation",
    "section": "15.1 Constrained MDPs",
    "text": "15.1 Constrained MDPs\nSuppose, in addition to the per-step cost function \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\to \\reals\\), we have a per-step constraint function \\(d \\colon \\ALPHABET S \\times \\ALPHABET A \\to \\reals\\) and we are interested in the following constrained optimization problem:\n\\[\\begin{gather*}\n\\min \\EXP\\Bigl[ \\sum_{t=1}^\\infty γ^{t-1} c(S_t, A_t) \\Bigr] \\\\\n\\text{subject to}\\quad\n\\EXP\\Bigl[ \\sum_{t=1}^\\infty γ^{t-1} d(S_t, A_t) \\Bigr] \\le D.\n\\end{gather*}\\]\nThe dual LP in this case is given by \\[\\begin{gather}\n  \\min \\sum_{s \\in \\ALPHABET S} \\sum_{a \\in \\ALPHABET A} \\mu(s,a) c(s,a)\n  \\notag \\\\\n  \\text{subject to}\\quad\n  \\mu(s,a) - γ\\sum_{s \\in \\ALPHABET S} \\sum_{a \\in \\ALPHABET A}\n  P(s | z, a) \\mu(s,a) = p(s), \\quad \\forall s \\in \\ALPHABET S, \\notag \\\\\n  \\sum_{s \\in \\ALPHABET S} \\sum_{a \\in \\ALPHABET A} \\mu(s,a) d(s,a) \\le D \\notag \\\\\n  \\mu(s,a) \\ge 0, \\quad \\forall s \\in \\ALPHABET S, a \\in \\ALPHABET A.\n\\end{gather}\\]\nIf we interpret \\(\\mu(s,a)\\) as the occupation measure of any policy, then this formulation follows immediately."
  },
  {
    "objectID": "mdps/linear-programming.html#notes",
    "href": "mdps/linear-programming.html#notes",
    "title": "15  Linear programming formulation",
    "section": "Notes",
    "text": "Notes\nThe material in this section is taken from Puterman (2014). See Altman (1999) for a detailed treatment of constrained MDPs.\n\n\n\n\nAltman, Eitan. 1999. Constrained markov decision processes. CRC Press. Available at: http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#preliminaries",
    "href": "mdps/lipschitz-mdps.html#preliminaries",
    "title": "16  Lipschitz MDPs",
    "section": "16.1 Preliminaries",
    "text": "16.1 Preliminaries\n\nLipschitz continuous functions\nGiven two metric spaces \\((\\ALPHABET X, d_X)\\) and \\((\\ALPHABET Y, d_Y)\\), the Lipschitz constant of function \\(f \\colon \\ALPHABET X \\to \\ALPHABET Y\\) is defined by \\[ \\| f\\|_{L} = \\sup_{x_1 \\neq x_2}\n    \\left\\{ \\frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } :\n    x_1, x_2 \\in \\ALPHABET X \\right\\} \\in [0, ∞]. \\] The function is called Lipschitz continuous if its Lipschitz constant is finite.\nIntuitively, a Lipschitz continuous function is limited by how fast it can change. For example, the following image from Wikipedia shows that for a Lipschitz continuous function, there exists a double cone (white) whose origin can be moved along the graph so that the whole graph always stays outside the double cone.\n\n\n\nImage credit: https://en.wikipedia.org/wiki/File:Lipschitz_Visualisierung.gif\n\n\nLet \\(\\ALPHABET Z\\) be an arbitrary set. A function \\(f \\colon \\ALPHABET X × \\ALPHABET Z \\to \\ALPHABET Y\\) is said to be uniformly Lipschitz in \\(u\\) if \\[ \\sup_{z \\in \\ALPHABET Z} \\| f(\\cdot, z) \\|_L  =\n  \\sup_{z \\in \\ALPHABET Z} \\sup_{x_1 \\neq x_2}\n  \\dfrac{ d_Y(f(x_1,z), f(x_2, z)) }{ d_X(x_1, x_2) } &lt; ∞. \\]\n\n\nSome examples\nA function \\(f \\colon \\reals \\to \\reals\\) is Lipschitz continuous if and only if it has bounded first derivative. The Lipschitz constant of such a function is equal to the maximum absolute value of the derivative.\nHere are some examples of Lipschitz continuous functions:\n\nThe function \\(f(x) = \\sqrt{x^2 + 1}\\) defined over \\(\\reals\\) is Lipschitz continuous because it is everywhere differentiable and the maximum value of the derivative is \\(L = 1\\).\nThe function \\(f(x) = |x|\\) defined over \\(\\reals\\) is Lipschitz continuous with Lipschitz constant equal to \\(1\\). Note that this function is continuous but not differentiable.\nThe function \\(f(x) = x + \\sin x\\) defined over \\(\\reals\\) is Lipschitz continuous with a Lipschitz constant equal to \\(1\\).\nThe function \\(f(x) = \\sqrt{x}\\) defined over \\([0,1]\\) is not Lipschitz continuous because the function becomes infinitely steep as \\(x\\) approaches \\(0\\).\nThe function \\(f(x) = x^2\\) defined over \\(\\reals\\) is not Lipschitz continuous because it becomes arbitrarily steep as \\(x\\) approaches infinity.\nThe function \\(f(x) = \\sin(1/x)\\) is bounded but not Lipschitz because becomes infinitely steep as \\(x\\) approaches \\(0\\).\n\n\n\nProperties of Lipschitz functions\n\nProposition 16.1 Lipschitz continuous functions have the following properties:\n\nIf a function \\(f \\colon (\\ALPHABET X, d_X) \\to (\\ALPHABET Y, d_Y)\\) is Lipschitz continuous, then \\(f\\) is uniformly continuous and measurable.\n\\(\\| f\\|_L = 0\\) if and only if \\(f\\) is a constant.\nIf \\(f \\colon (\\ALPHABET X, d_X) \\to (\\ALPHABET Y, d_Y)\\) and \\(g \\colon (\\ALPHABET Y, d_Y) \\to (\\ALPHABET Z, d_Z)\\) are Lipschitz continuous, then \\[ \\| f \\circ g \\|_L \\le \\| f \\|_L \\cdot \\| g \\|_L. \\]\nThe \\(\\| \\cdot \\|_{L}\\) is a seminorm on the vector space of Lipschitz functions from a metric space \\((\\ALPHABET X, d_X)\\) to \\((\\ALPHABET Y, d_Y)\\). In particular, \\(\\| \\cdot \\|_L\\) has the following properties: \\(\\| f \\|_L \\in [0, ∞]\\), \\(\\| α f\\|_L = |α| \\cdot \\|f\\|_L\\) for any \\(α \\in \\reals\\), and \\(\\| f_1 + f_2 \\|_L \\le \\|f_1 \\|_L + \\|f_2 \\|_L\\).\nGiven a family of functions \\(f_i\\), \\(i \\in I\\), on the same metric space such that \\(\\sup_{i \\in I} f_i &lt; ∞\\), \\[ \\| \\sup_{i \\in I} f_i \\|_{L} \\le \\sup_{i \\in I} \\| f_i \\|_{L}. \\]\nLet \\(f_n\\), \\(n \\in \\integers_{\\ge 1}\\), and \\(f\\) be functions from \\((\\ALPHABET X, d_X)\\) to \\((\\ALPHABET Y, d_Y)\\). If \\(f_n\\) converges pointwise to \\(f\\) for \\(n \\to ∞\\), then \\[ \\| f \\|_{L} \\le \\lim\\inf_{n \\to ∞} \\| f_i \\|_{L}. \\]"
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#kantorovich-distance",
    "href": "mdps/lipschitz-mdps.html#kantorovich-distance",
    "title": "16  Lipschitz MDPs",
    "section": "16.2 Kantorovich distance",
    "text": "16.2 Kantorovich distance\nLet \\(\\mu\\) and \\(\\nu\\) be probability measures on \\((\\ALPHABET X, d_X)\\). The Kantorovich distance between distributions \\(\\mu\\) and \\(\\nu\\) is defined as: \\[ K(\\mu,\\nu) = \\sup_{f : \\| f\\|_L \\le 1 }\n   \\left| \\int_{\\ALPHABET X} f\\, d\\mu - \\int_{\\ALPHABET X} f\\, d\\nu \\right|. \\]\nThe next results follow immediately from the definition of Kantorovich distance.\n\nProposition 16.2 For any Lipschitz function \\(f \\colon (\\ALPHABET X, d_X) \\to (\\reals, \\lvert \\cdot \\rvert)\\), and \\(μ,ν\\) are probability measures on \\((\\ALPHABET X, d_X)\\), \\[ \\left|\n  \\int_{\\ALPHABET X} f\\, dμ - \\int_{\\ALPHABET X} f\\, dν \\right| \\le\n  \\| f \\|_L \\cdot K(μ,ν). \\]\n\nThe Kantorivich distance is a special class metrics on probability spaces known as integral probability meterics (IPMs). Proposition 16.2 is a special case of a similar general result for IPMs (Proposition 31.4).\n\nSome examples\n\nLet \\((\\ALPHABET X, d_X)\\) be a metric space and for any \\(x,y \\in \\ALPHABET X\\), let \\(δ_x\\) and \\(δ_y\\) denote the Dirac delta distributions centered at \\(x\\) and \\(y\\). Then, \\[ K(δ_x, δ_y) = d_X(x,y). \\]\nLet \\((\\ALPHABET X, d_X)\\) be a Euclidean space with Euclidean norm. Let \\(μ \\sim \\mathcal{N}(m_1, \\Sigma_1)\\) and \\(ν \\sim \\mathcal{N}(m_2, \\Sigma_2)\\) be two Gaussian distributions on \\(\\ALPHABET X\\). Then, \\[K(μ,ν) = \\sqrt{ \\| m_1 - m_2 \\|_2^2\n+ \\text{Tr}( \\Sigma_1 + \\Sigma_2 - 2(\\Sigma_2^{1/2} \\Sigma_1 \\Sigma_2^{1/2})^{1/2} ) }. \\] If the two covariances commute, i.e. \\(\\Sigma_1\\Sigma_2 = \\Sigma_2 \\Sigma_1\\), then, \\[K(μ,ν) = \\sqrt{ \\| m_1 - m_2 \\|_2^2\n+ \\| \\Sigma_1^{1/2} - \\Sigma_2^{1/2} \\|^2_F},\\] where \\(\\| ⋅ \\|_{F}\\) denotes the Frobeinus norm of a matrix.\nWhen \\(\\Sigma_1 = \\Sigma_2\\), we have \\[K(μ,ν) = \\| m_1 - m_2 \\|_2. \\]\nIf \\(\\ALPHABET X = \\reals\\) and \\(d_X = | \\cdot |\\), then for any two distributions \\(μ\\) and \\(ν\\), \\[\\begin{equation}\\label{eq:Kantorovich-CDF}\nK(μ,ν) = \\int_{-∞}^∞ \\left| F_μ(x) - F_ν(x) \\right| dx,\n\\end{equation}\\] where \\(F_μ\\) and \\(F_ν\\) denote the CDF of \\(μ\\) and \\(ν\\).\nFurthermore, if \\(μ\\) is stochastically dominated by \\(ν\\), then \\(F_μ(x) \\ge F_ν(x)\\). Thus, \\[\\begin{equation}\\label{eq:Kantorovich-stochastic-dominance}\nK(μ, ν) = \\bar μ - \\bar ν\n\\end{equation}\\] where \\(\\bar μ\\) and \\(\\bar ν\\) are the means of \\(μ\\) and \\(ν\\)."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#lipschitz-mdps",
    "href": "mdps/lipschitz-mdps.html#lipschitz-mdps",
    "title": "16  Lipschitz MDPs",
    "section": "16.3 Lipschitz MDPs",
    "text": "16.3 Lipschitz MDPs\nConsider an MDP where the state and action spaces are metric spaces. We use \\(d_S\\) and \\(d_A\\) to denote the corresponding metric. For ease of exposition, we define a metric \\(d\\) on \\(\\ALPHABET S × \\ALPHABET A\\) by \\[ d( (s_1, a_1), (s_2, a_2) ) = d_S(s_1, s_2) + d_A(a_1, a_2). \\]\nWe allow for randomized policies. Thus, given any state \\(s \\in \\ALPHABET S\\), \\(π(\\cdot | s)\\) is a probability distribution on \\(\\ALPHABET A\\). We say that a (possibly) randomized policy \\(π\\) has a Lipschitz constant of \\(L_π\\) if for any \\(s_1, s_2 \\in \\ALPHABET S\\), \\[ K(π(\\cdot| s_1), π(\\cdot | s_2)) \\le L_π d_S(s_1, s_2). \\]\nNote that if \\(π\\) is deterministic, then due to property of Kantorovich distance between delta distributions, the above relationship simplifies to \\[ d_A(π(s_1), π(s_2)) \\le L_π d_S(s_1, s_2). \\]\n\nDefinition 16.1 An MDP is \\((L_c, L_p)\\)-Lipschitz if for all \\(s_1, s_2 \\in \\ALPHABET S\\) and \\(a_1, a_2 \\in \\ALPHABET A\\),\n\n\\(| c(s_1, a_1) - c(s_2, a_2) | \\le L_c\\bigl( d_S(s_1, s_2) + d_A(a_1, a_2) \\bigr)\\).\n\\(K(p(\\cdot | s_1, a_1), p(\\cdot | s_2, a_2)) \\le L_p\\bigl( d_S(s_1, s_2) + d_A(a_1, a_2) \\bigr)\\).\n\n\n\nExample 16.1 As an example, consider the inventory management problem considered earlier. We assume that \\(\\ALPHABET S = \\reals\\) and \\(\\ALPHABET A = \\reals_{\\ge 0}\\); the cost function and the dynamics are the same as before. We will show that this model is \\((L_c, L_p)\\) Lipschitz with \\[\n  L_c = p + \\max\\{ c_h, c_s \\}\n  \\quad\\text{and}\\quad\n  L_p = 1.\n\\]\n\n\n\n\n\n\n\nProof of Lipschitz continuity of the inventory model\n\n\n\n\n\nNote that in this model, the per-step cost depends on the next stage, so we need to make the appropriate changes to compute \\(L_c\\).\nWe first consider \\(L_p\\). For random variables \\(X \\sim μ\\) and \\(Y \\sim ν\\), we will use the notation \\(K(X,Y)\\) to denote \\(K(μ,ν)\\). Let \\(y_1 = s_1 +a_1\\) and \\(y_2 = s_2 + a_2\\). Then, \\[\n  K(p(\\cdot | s_1, a_1), p( \\cdot | s_2, a_2))\n  =\n  K( y_1 - W, y_2 - W )\n  =\n  K( W - y_1, W - y_2)\n\\] where we have used the following fact that \\(K(X,Y) = K(-X,-Y)\\). Now observe that if \\(y_1 &gt; y_2\\), the CDF of the RV \\(W - y_1\\) lies above the CDF of the RV \\(W - y_2\\); thus \\(W - y_2\\) [stochastically dominates] \\(W - y_1\\), hence from \\(\\eqref{eq:Kantorovich-stochastic-dominance}\\), \\(K(W - y_1, W - y_2) = y_1 - y_2\\). By symmetry, if \\(y_1 &lt; y_2\\), \\(K(W - y_1, W - y_2) = y_2 - y_1\\). Thus, \\[\n  K( W - y_1, W - y_2) = | y_1 - y_2 |\n  \\le | s_1 - s_2 | + | a_1 - a_2|\n\\] The above relationship implies \\(L_p = 1\\).\nNow consider \\[\n  \\bar c(s,a) = \\EXP[ c(s,a,S_{+}) \\mid S = s, A = a]\n  = pa + \\EXP[ h(s+a - W) ]\n\\] Then \\[\\begin{align*}\n  | \\bar c(s_1, a_1) - \\bar c(s_2, a_2) |\n  &\\le\n  p| a_1 - a_2 | + \\| h \\|_L K(s_1 + a_1 - W, s_2 + a_2 - W)\n  \\\\\n  &\\stackrel{(a)}\\le\n  p| a_1 - a_2 | + \\| h \\|_L | s_1 + a_1 - s_2 - a_2 |\n  \\\\\n  &\\le\n  (p + \\| h\\|_L)[ |s_1 - s_2| + |a_1 - a_2| ]\n\\end{align*}\\] where \\((a)\\) follows from Proposition 16.2. Thus, \\(L_c = p + \\|h\\|_L\\).\n\n\n\n\nLipschitz continuity of Bellman updates\nWe now prove a series of results for the Lipschitz continuity of Bellman updates.\n\nLemma 16.1 Let \\(V \\colon \\ALPHABET S \\to \\reals\\) be \\(L_V\\)-Lipschitz continuity. Define \\[ Q(s,a) = c(s,a) + γ \\int V(y) p(y|s,a)dy. \\] Then \\(Q\\) is \\((L_c + γ L_p L_V)\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider, \\[\\begin{align*}\n| Q(s_1, a_1) - Q(s_2, a_2) | &\\stackrel{(a)}\\le\n| c(s_1, a_1) - c(s_2, a_2) | \\\\\n& \\quad +\nγ \\left|\\int V(y) p(y|s_1, a_1) dy -\n             \\int V(y) p(y|s_2, a_2) dy \\right|\n  \\\\\n  &\\stackrel{(b)}\\le  L_c d( (s_1, a_1), (s_2, a_2) ) +\n  γ L_V L_p d( (s_1, a_1), (s_2, a_2) ),\n\\end{align*}\\] where \\((a)\\) follows from the triangle inequality and \\((b)\\) follows from Proposition 16.2. Thus, \\(L_Q = L_c + γ L_p L_V\\).\n\n\n\n\nLemma 16.2 Let \\(Q \\colon \\ALPHABET S × \\ALPHABET A \\to \\reals\\) be \\(L_Q\\)-Lipschitz continuous. Define \\[V(s) = \\min_{a \\in \\ALPHABET A} Q(s,a).\\] Then \\(V\\) is \\(L_Q\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s_1, s_2 \\in \\ALPHABET S\\) and let \\(a_1\\) and \\(a_2\\) denote the corresponding optimal action. Then, \\[ \\begin{align*}\nV(s_1) - V(s_2) &= Q(s_1, a_1) - Q(s_2, a_2) \\\\\n&\\stackrel{(a)}\\le Q(s_1, a_2) - Q(s_2, a_2) \\\\\n&\\stackrel{(b)}\\le L_Q( d_S(s_1, s_2) + d_A(a_2, a_2) )\\\\\n&= L_Q d_S(s_1, s_2).\n\\end{align*} \\]\nBy symmetry, \\[ V(s_2) - V(s_1) \\le L_Q d_S(s_2, s_1). \\] Thus, \\[ | V(s_1) - V(s_2) | \\le L_Q d_S(s_1, s_2). \\] Thus, \\(V\\) is \\(L_Q\\)-Lipschitz continuous.\n\n\n\n\nLemma 16.3 Let \\(Q \\colon \\ALPHABET S × \\ALPHABET A \\to \\reals\\) be \\(L_Q\\)-Lipschitz continuous and \\(π\\) be a (possibly randomized) \\(L_π\\)-Lipschitz policy. Define \\[V_π(s) = \\int Q(s, a) π(a | s) du.\\] Then, \\(V_π\\) is \\(L_Q( 1 + L_π)\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(s_1, s_2 \\in \\ALPHABET S\\), consider \\[ \\begin{align}\n| V_π(s_1) - V_π(s_2) | &=\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_2) du \\right|\n\\notag \\\\\n&\\stackrel{(a)}\\le\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_1) du \\right|\n\\notag \\\\\n& \\quad +\n\\left| \\int Q(s_2, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_2) du \\right|\n\\label{eq:split}\n\\end{align} \\] where \\((a)\\) follows from the triangle inequality. Now we consider both terms separately.\nThe first term of \\eqref{eq:split} simplifies as follows: \\[\\begin{align}\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_1) du \\right|\n&\\stackrel{(b)}\\le\n\\int \\left|Q(s_1, a) - Q(s_2, a)\\right| π(a | s_1) du \\notag \\\\\n&\\stackrel{(c)}\\le\n\\int L_Q d_S(s_1, s_2) π(a | s_1) du \\notag \\\\\n&= L_Q d_S(s_1, s_2), \\label{eq:first}\n\\end{align} \\] where \\((b)\\) follows from the triangle inequality and \\((c)\\) follows from Lipschitz continuity of \\(Q\\).\nThe second term of \\eqref{eq:split} simplifies as follows: \\[ \\begin{align}\n  \\left| \\int Q(s, a) π(a | s_1) du - \\int Q(s,a) π(a | s_2) du \\right|\n  &\\stackrel{(d)}\\le L_Q K (π(\\cdot | s_1), π(\\cdot | s_2))\n  \\notag \\\\\n  &\\stackrel{(e)}\\le L_Q L_π d_S(s_1, s_2),\n  \\label{eq:second}\n  \\end{align}\n\\] where the \\((d)\\) inequality follows from Proposition 16.2 and \\((e)\\) follows from the definition of Lipschitz continuous policy.\nSubstituting \\eqref{eq:first} and \\eqref{eq:second} in \\eqref{eq:split}, we get \\[ \\begin{align*}\n| V_π(s_1) - V_π(s_2) | &\\le L_Q d_S(s_1, s_2) + L_Q L_π d_S(s_1, s_2)\n\\\\\n&= L_Q(1 + L_π) d_S(s_1, s_2).\n\\end{align*} \\] Thus, \\(V\\) is Lipschitz continuous with Lipschitz constant \\(L_Q(1 + L_π)\\)."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#lipschitz-continuity-of-value-iteration",
    "href": "mdps/lipschitz-mdps.html#lipschitz-continuity-of-value-iteration",
    "title": "16  Lipschitz MDPs",
    "section": "16.4 Lipschitz continuity of value iteration",
    "text": "16.4 Lipschitz continuity of value iteration\n\nLemma 16.4 Consider a discounted infinite horizon MDP which is \\((L_c, L_p)\\)-Lipschitz. Start with \\(V^{(0)} = 0\\) and recursively define\n\n\\(\\displaystyle Q^{(n+1)}(s,a) = c(s,a) + γ \\int V^{(n)}(y) p(y|s,a) dy.\\)\n\\(\\displaystyle V^{(n+1)}(s) = \\min_{a \\in \\ALPHABET A} Q^{(n+1)}(s,a).\\)\n\nThen, \\(V^{(n)}\\) is Lipschitz continuous and its Lipschitz constant \\(L_{V^{(n)}}\\) satisfies the following recursion: \\[L_{V^{(n+1)}} = L_c + γ L_p L_{V^{(n)}}.\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by induction. For \\(n=1\\), \\(Q^{(1)}(s,a) = c(s,a)\\), which is Lipschitz with Lipschitz constant \\(L_{Q^{(1)}} = L_c\\). Then, by Lemma 16.2, \\(V^{(1)}\\) is Lipschitz with Lipschitz constant \\(L_{V^{(1)}} = L_{Q^{(1)}} = L_c\\). This forms the basis of induction. Now assume that \\(V^{(n)}\\) is \\(L_{V^{(n)}}\\)-Lipschitz. Then, by Lemma 16.1, \\(Q^{(n+1)}\\) is \\((L_c + γL_p L_{V^{(n)}})\\)-Lipschitz. Therefore, by Lemma 16.2, \\(V^{(n+1)}\\) is Lipschitz with constant \\[ L_{V^{(n+1)}} = L_c + γ L_p L_{V^{(n)}}.\\]\n\n\n\n\nLemma 16.5 Consider a discounted infinite horizon MDP which is \\((L_c, L_p)\\)-Lipschitz and let \\(π\\) be any randomized time-homogeneous policy which is \\(L_π\\)-Lipschitz. Start with \\(V^{(0)} = 0\\) and then recursively define\n\n\\(V^{(n)}_π(s) = \\int Q^{(n)}_π(s,a)π(a|s) du.\\)\n\\(\\displaystyle Q^{(n+1)}_π(s,a) = c(s,a) + γ \\int V^{(n)}_π(y) p(y|s,a) dy.\\)\n\nThen, then \\(Q^{(n)}_π\\) is Lipschitz continuous and its Lipschitz constant \\(L_{Q^{(n)}_π}\\) satisfies the follwoing recursion: \\[ L_{Q^{(n+1)}_π} + L_c + γ(1 + L_π)L_p L_{Q^{(n)}_π}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by induction. For \\(n=1\\), \\(Q^{(1)}_π(s,a) = c(s,a)\\), which is Lipschitz with Lipschitz constant \\(L_{Q^{(1)}_π} = L_c\\). This forms the basis of induction. Now assume that \\(Q^{(n)}_π\\) is \\(L_{Q^{(n)}_π}\\)-Lipschitz. Then, by Lemma 16.3, \\(V^{(n)}_π\\) is Lipschitz with Lipschitz constant \\(L_{V^{(n)}_π} = L_{Q^{(n)}_π}(1 + L_π)\\) and by Lemma 16.1, \\(Q^{(n+1)}_π\\) is Lipschitz with Lipschitz constant \\(L_{Q^{(n+1)}_π} = L_c + γL_p L_{V^{(n)}_π}.\\) Combining these two we get \\[ L_{Q^{(n+1)}_π} + L_c + γ(1 + L_π)L_p L_{Q^{(n)}_π}. \\]\n\n\n\n\nTheorem 16.1 Given any \\((L_c, L_p)\\)-Lipschitz MDP, if \\(γ L_p &lt; 1\\), then the infinite horizon \\(γ\\)-discounted value function \\(V\\) is Lipschitz continuous with Lipschitz constant \\[ L_{V} = \\frac{L_c}{1 - γ L_p} \\] and the action-value function \\(Q\\) is Lipschitz with Lipschitz constant \\[ L_Q = L_V = \\frac{L_c}{1 - γ L_p}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the sequence of \\(L_n = L_{V^{(n)}}\\) values. For simplicity write \\(α = γ L_p\\). Then the sequence \\(\\{L_n\\}_{n \\ge 1}\\) is given by: \\(L_1 = L_c\\) and for \\(n \\ge 1\\), \\[ L_{n+1} = L_c + α L_n. \\] Hence, \\[ L_n = L_c + α L_c + \\dots + α^{n-1} L_c = \\frac{1 - α^n}{1 - α} L_c. \\] This sequence converges if \\(|α| &lt; 1\\). Since \\(α\\) is non-negative, this is equivalent to \\(α &lt; 1\\), which is true by hypothesis. Hence \\(L_n\\) is a convergent sequence. At convergence, the limit \\(L_V\\) must satisfy the fixed point of the recursion relationship introduced in Lemma 16.4, hence \\[ L_V = L_c + γ L_p L_V. \\] Consequently, the limit is equal to \\[ L_V = \\frac{L_c}{1 - γ L_p}. \\] The Lipschitz constant of \\(Q\\) follows from Lemma 16.1.\n\n\n\n\n\n\n\n\n\nExample 16.1 (continued)\n\n\n\nAs discussed in Example 16.1, the inventory management example is \\((p + \\max\\{c_h,c_s\\}, 1)\\)-Lipschitz. Therefore, Theorem 16.1 implies that the value function of the inventory management problem is \\(L_V\\)-Lipschitz with \\[\n  L_V = \\frac{p + \\max\\{ c_h + c_s \\}}{1 - γ}.\n\\] To understand the tightness of this bound, we consider a specific instance of inventory management problem where the demand is \\(\\text{Exp}(1)\\), \\(c_h = 4\\), \\(c_s = 2\\), and \\(p = 5\\). The theoretical maximum value of the Lipschitz constant (for \\(γ = 0.9\\)) is \\(L_V = 90\\). In Figure 16.1, we show the animation of this upper bound, in the style of the wikipedia animation shown at the beginning of this lecture.\n\n\n\n\n\nFigure 16.1: Animation showing the upper bound on the Lipschitz constant of the value function, computed via Theorem 16.1.\n\n\n\nSource: inventory-management.ipynb\nNote that since the demand is \\(\\text{Exp}(1)\\), most of the mass of the demand is in the range \\([0,10]\\). So, the region of the value function of interest is perhaps \\([-20,20]\\) or so. We plot a larger region to highlight the fact that the bound on the Lipschitz constant has to capture the Lipschitz constant of the value function over the entire real line.\n\n\n\nTheorem 16.2 Given any \\((L_c, L_p)\\)-Lipschitz MDP and an \\(L_π\\)-Lipschitz (possibly randomized) time-homogeneous policy \\(π\\), if \\(γ (1 + L_π) L_p &lt; 1\\), then the infinite horizon \\(γ\\)-discounted value-action function \\(Q_π\\) is Lipschitz continuous with Lipschitz constant \\[ L_{Q_π} = \\frac{L_c}{1 - γ(1 + L_π) L_p} \\] and the value function \\(V_π\\) is Lipschitz with Lipschitz constant \\[ L_{V_π} = L_{Q_π}(1 + L_π) =\n   \\frac{L_c(1 + L_π)}{1 - γ(1 + L_π) L_p}. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThe restrictive assumption in the result is that \\(γ(1 + L_π)L_p &lt; 1\\). For a specific model, even when this assumption does not hold, it may be possible to directly check if the \\(Q\\)-function is Lipschitz continuous. Such a direct check often gives a better Lipschitz constant.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the sequence of \\(L_n = L_{Q^{(n)}_π}\\) values. For simplicity, write \\(α = γ(1 + L_π)L_p\\). Then, the sequence \\(\\{L_n\\}_{n \\ge 1}\\) is given by: \\(L_1 = L_c\\) and for \\(n \\ge 1\\), \\[L_{n+1} = L_c + α L_n. \\] Hence, \\[ L_n = L_c + α L_c + \\dots + α^{n-1} L_c = \\frac{1 - α^n}{1 - α} L_c. \\] This sequence converges if \\(|α| &lt; 1\\). Since \\(α\\) is non-negative, this is equivalent to \\(α &lt; 1\\), which is true by hypothesis. Hence \\(L_n\\) is a convergent sequence. At convergence, the limit \\(L_{Q_π}\\) must satisfy the fixed point of the recursion relationship introduced in Lemma 16.5, hence \\[ L_{Q_π} = L_c + γ(1 + L_π)L_p L_{Q_π}. \\] Consequently, the limit is equal to \\[ L_{Q_π} = \\frac{L_c}{1 - γ(1 + L_π) L_p}. \\]\nThe Lipschitz constant of \\(V_π\\) follows from Lemma 16.3."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#influence-radius",
    "href": "mdps/lipschitz-mdps.html#influence-radius",
    "title": "16  Lipschitz MDPs",
    "section": "16.5 Influence Radius",
    "text": "16.5 Influence Radius\nWhen the \\(Q\\)-function of an MDP is Lipschitz continuous, then the optimal action does not change too abruptly. More precisely, suppose an action \\(a\\) is optimal at state \\(s\\). Then, we can identify a hyperball \\(B(s, ρ(s))\\) of radius \\(ρ(s)\\) centered around \\(s\\) such that \\(a\\) is guaranteed to be the dominating action in \\(ρ(s)\\). This radius \\(ρ(s)\\) is called the influence radius.\nLet \\(π\\) denote the optimal policy, i.e., \\[ π(s) = \\arg \\min_{a \\in \\ALPHABET A} Q(s,a) \\] and \\(h\\) denote the second best action, i.e., \\[ h(s) = \\arg \\min_{a \\in \\ALPHABET A \\setminus \\{π(s)\\}} Q(s,a). \\] Define the domination value of state \\(s\\) to be \\[ Δ(s) = Q(s, h(s)) - Q(s, π(s)). \\]\n\nTheorem 16.3 For a Lipschitz continuous \\(Q\\)-function, the influence radius at state \\(s\\) is given by \\[ ρ(s) = \\frac{ Δ(s) }{ 2 L_Q }. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nCombining Theorem 16.2 and Theorem 16.3 implies that under the condition of Theorem 16.2, the influence radius at state \\(s\\) is at least \\[ ρ(s) = Δ(s)(1 - γ(1 + L_π)L_p)/2L_c. \\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe intuition behind the proof is the following. The value of the action \\(π(s)\\) can only decrease by \\(L_Q ρ(s)\\) in \\(B(s, ρ(s))\\), while the value of the second best action \\(h(s)\\) can only increase by \\(L_Q ρ(s)\\). So, the shortest distance \\(ρ(s)\\) from \\(s\\) needed for an action \\(h(s)\\) to “catch-up” with action \\(π(s)\\) should satisfy \\(2 L_Q ρ(s) = Δ(s)\\) or \\(ρ(s) = Δ(s)/2L_Q\\).\nFormally, for any \\(s' \\in B(s,ρ(s))\\), \\(d_S(s,s') \\le ρ(s)\\). Thus, for any action \\(a \\in \\ALPHABET A\\), \\[ | Q(s,a) - Q(s',a)| \\le L_Q d_S(s,s') \\le L_Q ρ(s). \\] Equivalently, \\[ Q(s,a) - L_Q ρ(s) \\le Q(s',a) \\le Q(s,a) + L_Q ρ(s) \\] which states that as \\(s'\\) moves away from \\(s\\), the value of \\(Q(s',a)\\) remains within a symmetric bound that depends on the radius \\(ρ(s)\\). Since this bound holds for all \\(a\\), they also hold for \\(a = π(s)\\). Thus, \\[ Q(s, π(s)) - L_Q ρ(s) \\le Q(s', π(s)) \\le Q(s, π(s)) + L_Q ρ(s). \\]\nSince \\(π(s)\\) is the optimal action, for any other action \\(a \\neq π(s)\\), \\[ Q(s,π(s)) \\le Q(s,a). \\] Thus, the action \\(π(s)\\) is optimal as long as the upper bound on \\(Q(s', π(s))\\) is lower than the lower bound on \\(Q(s',a)\\), i.e., \\[ Q(s, π(s)) + L_Q ρ(s) \\le Q(s,a) - L_Q ρ(s).  \\] Thus, the maximum value of \\(ρ(s)\\) is when the relationship holds with equality, i.e., \\[ ρ(s) = \\frac{Q(s,a) - Q(s,π(s))}{2 L_Q} \\ge \\frac{Δ(s)}{2 L_Q}. \\]"
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#exercises",
    "href": "mdps/lipschitz-mdps.html#exercises",
    "title": "16  Lipschitz MDPs",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 16.1 Let \\((\\ALPHABET S, d_S)\\) be a metric space and \\(s, s' \\in \\ALPHABET S\\). Consider two Bernoulli measures \\[ μ = a δ_s + (1-a) δ_{s'}, \\qquad\n      ν = b δ_s + (1-b) δ_{s'}. \\]\nShow that \\[ K(μ,ν) = |a - b| d(s,s'). \\]"
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#notes",
    "href": "mdps/lipschitz-mdps.html#notes",
    "title": "16  Lipschitz MDPs",
    "section": "Notes",
    "text": "Notes\nThe material in this section is taken from Rachelson and Lagoudakis (2010) and Hinderer (2005).\nThe proof of Lipschitz continuity for the inventory management problem in Example 16.1 is adapted from Müller (1997).\n\n\n\n\nHinderer, K. 2005. Lipschitz continuity of value functions in Markovian decision processes. Mathematical Methods of Operations Research 62, 1, 3–22. DOI: 10.1007/s00186-005-0438-1.\n\n\nMüller, A. 1997. How does the value function of a markov decision process depend on the transition probabilities? Mathematics of Operations Research 22, 4, 872–885. DOI: 10.1287/moor.22.4.872.\n\n\nRachelson, E. and Lagoudakis, M.G. 2010. On the locality of action domination in sequential decision making. Proceedings of 11th international symposium on artificial intelligence and mathematics. Available at: https://oatao.univ-toulouse.fr/17977/."
  },
  {
    "objectID": "pomdps/intro.html#history-dependent-dynamic-program",
    "href": "pomdps/intro.html#history-dependent-dynamic-program",
    "title": "17  Introduction",
    "section": "17.1 History dependent dynamic program",
    "text": "17.1 History dependent dynamic program\nOur first step to develop an efficient dynamic programming decomposition is to simply ignore efficiency and develop a dynamic programming decomposition. We start by deriving a recursive formula to compute the performance of a generic history dependent strategy \\(π = (π_1, \\dots, π_T)\\).\n\nPerformance of history-dependent strategies\nLet \\(H_t = (Y_{1:t}, A_{1:t-1})\\) denote all the information available to the decision maker at time \\(t\\). Thus, given any history dependent strategy \\(π\\), we can write \\(A_t = π_t(H_t)\\). Define the cost-to-go functions as follows: \\[\n  J_t(h_t; π) = \\EXP^π\\biggl[ \\sum_{s=t}^T c_s(S_s, A_s) \\biggm| H_t = h_t\n  \\biggr].\n\\] Note that \\(J_t(h_t; π)\\) only depends on the future strategy \\((π_t, \\dots, π_T)\\). These functions can be computed recursively as follows: \\[\\begin{align*}\n  J_t(h_t; π) &= \\EXP^π\\biggl[ \\sum_{s=t}^T c_s(H_s, π_s(H_s)) \\biggm|\n    H_t = h_t \\biggr] \\\\\n    &\\stackrel{(a)}= \\EXP^π \\biggl[ c_t(h_t, π_t(h_t)) + \\EXP^π\\biggl[\n    \\sum_{s=t+1}^T c_s(S_s, π_s(S_s)) \\biggm| H_{t+1} \\biggr] \\biggm|\n    H_t = h_t \\biggr]  \\\\\n    &= \\EXP^π[ c_t(h_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid H_t = h_t ],\n\\end{align*}\\] where \\((a)\\) follows from the towering property of conditional expectation and the fact that \\(H_t \\subseteq H_{t+1}\\).\nThus, we can use the following dynamic program to recursively compute the performance of a history-dependent strategy: \\(J_{T+1}(h_{T+1}) = 0\\) and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\nJ_t(h_t; π) = \\EXP^π [ c_t(h_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid\n  H_t = h_t ].\n\\]\n\n\nHistory-dependent dynamic programming decomposition\nWe can use the above recursive formulation for performance evaluation to derive a history-dependent dynamic program.\n\nTheorem 17.1 Recursively define _value functions \\(\\{V_t\\}_{t = 1}^{T+1}\\), where \\(V_t \\colon \\ALPHABET H_t \\to \\reals\\) as follows: \\[\\begin{equation}\n  V_{T+1}(h_{T+1}) = 0\n\\end{equation}\\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n  Q_t(h_t, a_t) &= \\EXP[ c_t(S_t, a_t) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = a_t ] \\\\\n  V_t(h_t) &= \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t)\n\\end{align}\\] Then, a history-dependent policy \\(π\\) is optimal if and only if it satisfies \\[\\begin{equation} \\label{eq:history-verification}\n  π_t(h_t) \\in \\arg \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t).\n\\end{equation}\\]\n\nThe proof idea is similar to the proof for MDPs. Instead of proving the above result, we prove a related result.\n\nTheorem 17.2 (The comparison principle) For any history-dependent strategy \\(π\\) \\[ J_t(h_t; π) \\ge V_t(h_t) \\] with equality at \\(t\\) if and only if the future straegy \\(π_{t:T}\\) satisfies the verification step \\eqref{eq:history-verification}.\n\nNote that the comparison principle immediately implies that the strategy obtained using dynamic program of Theorem 17.1 is optimal. The proof of the comparison principle is almost identical to the proof for MDPs.\n\n\n\n\n\n\nProof of the comparison principle\n\n\n\n\n\nThe proof proceeds by backward induction. Consider any history dependent policy \\(π = (π_1, \\dots, π_T)\\). For \\(t = T+1\\), the comparison principle is satisfied by definition and this forms the basis of induction. We assume that the result holds for time \\(t+1\\), which is the induction hypothesis. Then for time \\(t\\), we have \\[\\begin{align*}\n  V_t(h_t) &= \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t) \\\\\n  &\\stackrel{(a)}= \\min_{a_t \\in \\ALPHABET A}\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = π_t(h_t) ]\n  \\\\\n  &\\stackrel{(b)}\\le\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = π_t(h_t)]\n  \\\\\n  &\\stackrel{(c)}\\le\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid\n  H_t = h_t, A_t = π_t(h_t)]\n  \\\\\n  &= J_t(h_t, π).\n\\end{align*}\\] where \\((a)\\) follows from the definition of the \\(Q\\)-function; \\((b)\\) follows from the definition of minimization; and \\((c)\\) follows from the induction hyothesis. We have the equality at step \\((b)\\) iff \\(π_t\\) satisfies the verification step \\eqref{eq:history-verification} and have the equality in step \\((c)\\) iff \\(π_{t+1:T}\\) is optimal (this is part of the induction hypothesis). Thus, the result is true for time \\(t\\) and, by the principle of induction, is true for all time."
  },
  {
    "objectID": "pomdps/intro.html#the-notion-of-an-information-state",
    "href": "pomdps/intro.html#the-notion-of-an-information-state",
    "title": "17  Introduction",
    "section": "17.2 The notion of an information state",
    "text": "17.2 The notion of an information state\nNow that we have obtained a dynamic programming decomposition, let’s try to simplify it. To do so, we define the notion of an information state.\n\n\n\n\n\n\nInformation state\n\n\n\nA stochastic process \\(\\{Z_t\\}_{t = 1}^T\\), \\(Z_t \\in \\ALPHABET Z\\), is called an information state if \\(Z_t\\) be a function of \\(H_t\\) (which we denote by \\(Z_t = φ_t(H_t)\\)) and satisfies the following two properties:\nP1. Sufficient for performance evaluation, i.e., \\[ \\EXP^π[ c_t(S_t, A_t) \\mid H_t = h_t, A_t = a_t]\n    =  \\EXP[ c_t(S_t, A_t) \\mid Z_t = φ_t(h_t), A_t = a_t ] \\]\nP2. Sufficient to predict itself, i.e., for any Borel measurable subset \\(B\\) of \\(\\ALPHABET Z\\), we have \\[ \\PR^π(Z_{t+1} \\in B \\mid H_t = h_t, A_t = a_t) =\n       \\PR(Z_{t+1} \\in B \\mid Z_t = φ_t(h_t), A_t = a_t).\n    \\]\n\n\nInstead of (P2), the following sufficient conditions are easier to verify in some models:\n\n\n\n\n\n\nAn equivalent characterization\n\n\n\nP2a. Evolves in a state-like manner, i.e., there exist measurable functions \\(\\{ψ_t\\}_{t=1}^T\\) such that \\[ Z_{t+1} = ψ_t(Z_t, Y_{t+1}, A_t). \\]\nP2b. Is sufficient for predicting future observations, i.e., for any Borel subset \\(B\\) of \\(\\ALPHABET Y\\), \\[ \\PR^π(Y_{t+1} \\in B | H_t = h_t, A_t = a_t) =\n        \\PR(Y_{t+1} \\in B | Z_t = φ_t(h_t), A_t = a_t).\n     \\]\n\n\n\n\n\n\n\n\nRemark\n\n\n\nThe right hand sides of (P1) and (P2) as well as (P2a) and (P2b) do not depend on the choice of the policy \\(π\\).\n\n\n\nProposition 17.1 : (P2a) and (P2b) imply (P2).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any Borel measurable subset \\(B\\) of \\(\\ALPHABET Z\\), we have \\[\\begin{align*}\n  \\hskip 1em & \\hskip -1em\n  \\PR(Z_{t+1} \\in B \\mid H_t = h_t, A_t = a_t)  \n  \\stackrel{(a)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(Y_{t+1} = y_{t+1}, Z_{t+1} \\in B\n  \\mid H_t = h_t, A_t = a_t ]\n  \\\\\n  &\\stackrel{(b)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\IND\\{ ψ_t(φ_t(h_t), y_{t+1}, a_t) \\}\n  \\PR(Y_{t+1} = y_{t+1} \\mid H_t = h_t, A_t = a_t)\n  \\\\\n  &\\stackrel{(c)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\IND\\{ ψ_t(φ_t(h_t), y_{t+1}, a_t) \\}\n  \\PR(Y_{t+1} = y_{t+1} \\mid Z_t = φ_t(h_t), A_t = a_t)\n  \\\\\n  &\\stackrel{(d)}=\n  \\PR(Z_{t+1} \\in B \\mid Z_t = φ_t(h_t), A_t = a_t)  \n\\end{align*}\\] where \\((a)\\) follows from the law of total probability, \\((b)\\) follows from (P2a), \\((c)\\) follows from (P2b), and \\((d)\\) from the law of total probability."
  },
  {
    "objectID": "pomdps/intro.html#examples-of-an-information-state",
    "href": "pomdps/intro.html#examples-of-an-information-state",
    "title": "17  Introduction",
    "section": "17.3 Examples of an information state",
    "text": "17.3 Examples of an information state\nWe start by define the belief state \\(b_t \\in Δ(\\ALPHABET S)\\) as follows: for any \\(s \\in \\ALPHABET S\\) \\[ b_t(s) = \\PR^π(S_t = s \\mid H_t = h_t). \\] The belief state is a function of the history \\(h_t\\). When we want to explicitly show the dependence of \\(b_t\\) on \\(h_t\\), we write it as \\(b_t[h_t]\\).\n\nLemma 17.1 The belief state \\(b_t\\) does not depend on the policy \\(π\\).\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is an extremely important result which has wide-ranging implications in stochastic control. For a general discussion of this point, see Witsenhausen (1975).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the law of total probability and Bayes rule, we have \\[\\begin{equation} \\label{eq:belief}\n  \\PR(s_t | y_{1:t}, a_{1:t-1})\n  = \\sum_{s_{1:t-1}} \\PR(s_{1:t} | y_{1:t}, a_{1:t-1})\n  = \\sum_{s_{1:t-1}}\n   \\frac{\\PR(s_{1:t}, y_{1:t}, a_{1:t-1})}\n   {\\sum_{s'_{1:t}} \\PR(s'_{1:t}, y_{1:t}, a_{1:t-1})}\n\\end{equation}\\]\nNow consider \\[\\begin{align*}\n  \\PR(s_{1:t}, y_{1:t}, a_{1:t-1}) &=\n  \\PR(s_1) \\PR(y_1 | s_1) \\IND\\{ a_1 = π_1(y_1) \\} \\\\\n  & \\times\n  \\PR(s_2 | s_1, a_1) \\PR(y_2 | s_2) \\IND \\{ a_2 = π_2(y_{1:2}, a_1)\\} \\\\\n  & \\times \\cdots \\\\\n  & \\times\n  \\PR(s_{t-1} | s_{t-2}, a_{t-2}) \\PR(y_{t-1} | s_{t-1}) \\IND \\{ a_{t-1} =\n  π_{t-1}(y_{1:t-1}, a_{1:t-2}) \\} \\\\\n  & \\times\n  \\PR(s_{t} | s_{t-1}, a_{t-1}) \\PR(y_{t} | s_{t}).\n\\end{align*}\\] Substitute the above expression in both the numerator and the denominator of \\eqref{eq:belief}. Observe that the terms of the form \\(\\IND\\{ a_s = π_s(y_{1:s}, a_{1:s-1})\\) are common to both the numerator and the denominator and cancel each other. Thus, \\[\\begin{equation} \\label{eq:belief-fn}\n  \\PR(s_t | y_{1:t}, a_{1:t-1}) = \\sum_{s_{1:t-1}}\n  \\frac{ \\prod_{s=1}^t \\PR(s_s \\mid s_{s-1}, a_{s-1}) \\PR(y_s \\mid s_s) }\n  { \\sum_{s'_{1:t}} \\prod_{s=1}^t \\PR(s'_s \\mid s'_{s-1}, a_{s-1}) \\PR(y_s \\mid s'_s) }.\n\\end{equation}\\] None of the terms here depend on the policy \\(π\\). Hence, the belief state does not depend on the policy \\(π\\).\n\n\n\n\nLemma 17.2 The belief state \\(b_t\\) updates in a state like manner. In particular, for any \\(s_{t+1} \\in \\ALPHABET S\\), we have \\[\n  b_{t+1}(s_{t+1}) = \\sum_{s_t \\in \\ALPHABET S}\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n   { \\sum_{s'_{t:t+1}} \\PR(y_{t+1} | s'_{t+1}) \\PR(s'_{t+1} | s'_t, a_t) b_t(s'_t) }.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(s_{t+1} \\in \\ALPHABET S\\), consider\n\\[\\begin{align}\nb_{t+1}(s_{t+1}) &= \\PR(s_{t+1} | y_{1:t+1}, a_{1:t}) \\notag \\\\\n&= \\sum_{s_t} \\PR(s_{t:t+1} | y_{1:t+1}, a_{1:t}) \\notag \\\\\n&= \\sum_{s_t} \\frac{ \\PR(s_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) }\n  {\\sum_{s'_{t:t+1}}\\PR(s'_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) }.\n\\label{eq:update-1}\n\\end{align}\\]\nNow consider \\[\\begin{align}\n\\hskip 1em & \\hskip -1em\n\\PR(s_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) \\notag \\\\\n&= \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t)\n   \\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\n   \\PR(s_t | y_{1:t}, a_{1_t-1}) \\notag \\\\\n&= \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t)\n   \\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\n   b_t(s_t). \\label{eq:belief-2}\n\\end{align}\\] Substitute the above expression in both the numerator and the denominator of \\eqref{eq:update-1}. Observe that \\(\\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\\) is common to both the numerator and the denominator and cancels out. Thus, we get the result of the lemma.\n\n\n\nNow, we present three examples of information state here. See the Exercises for more examples.\n\nExample 17.1 The complete history \\(H_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove that \\(Z_t = H_t\\) satisfies properties (P1), (P2a), and (P2b).\nP1. \\(\\displaystyle \\EXP^π[ c_t(S_t, A_t) | H_t = h_t, A_t = a_t ] = \\sum_{s_t \\in \\ALPHABET S} c_t(s_t, a_t) b_t[h_t](s_t)\\).\nP2a. \\(H_{t+1} = (H_t, Y_{t+1}, A_t)\\)\nP2b. \\(\\displaystyle \\PR^π(y_{t+1} | y_{1:t}, a_{1:t}) = \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR( s_{t+1} | s_t, a_t) \\PR(s_t | y_{1:t}, a_{1:t})\\). Note that in the last term \\(\\PR^π(s_t | y_{1:t}, a_{1:t})\\) we can drop \\(a_t\\) from the conditioning because it is a function of \\((y_{1:t}, a_{1:t-1})\\). Thus, \\[ \\PR^π(s_t | y_{1:t}, a_{1:t}) = \\PR^π(s_t | y_{1:t}, a_{1:t-1}) =\nb_t[h_t](s_t).\\] Note that in the last step, we have used Lemma 17.1. Thus, \\(\\displaystyle \\PR^π(y_{t+1} | y_{1:t}, a_{1:t}) = \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR( s_{t+1} | s_t, a_t) b_t[h_t](s_t)\\).\n\n\n\n\nExample 17.2 The belief state \\(b_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe belief state \\(b_t\\) is a function of the history \\(h_t\\). (The exact form of this function is given by \\eqref{eq:belief-fn}). In the proof of Example 17.1, we have already shown that \\(b_t\\) satisfies (P1) and (P2b). Moreover Lemma 17.2 implies that the belief update satisfies (P2a).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nBoth the above information states are generic information states which work for all models. For specific models, it is possible to identify other information states as well. We present some examples of such an information state below.\n\n\n\nExample 17.3 An MDP is a special case of a POMDP where \\(Y_t = S_t\\). For an MDP \\(Z_t = S_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will show that \\(Z_t = S_t\\) satisfies (P1) and (P2).\n(P1) is satisfied because the per-step cost is a function of the \\((S_t, A_t)\\). (P2) is equivalent to the control Markov property."
  },
  {
    "objectID": "pomdps/intro.html#information-state-based-dynamic-program",
    "href": "pomdps/intro.html#information-state-based-dynamic-program",
    "title": "17  Introduction",
    "section": "17.4 Information state based dynamic program",
    "text": "17.4 Information state based dynamic program\nThe main feature of an information state is that one can always write a dynamic program based on an information state.\n\nTheorem 17.3 Let \\(\\{Z_t\\}_{t=1}^T\\) be any information state, where \\(Z_t = φ_t(H_t)\\). Recursively define value functions \\(\\{ \\hat V_t \\}_{t=1}^T\\), where \\(\\hat V_t \\colon \\ALPHABET Z \\to \\reals\\), as follows: \\[ \\hat V_{T+1}(z_{T+1}) = 0 \\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n  \\hat Q_t(z_t, a_t) &= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}(Z_{t+1}) \\mid\n  Z_t = z_t, A_t = a_t] \\\\\n  \\hat V_t(z_t) &= \\min_{a_t \\in \\ALPHABET A} \\hat Q_t(z_t, a_t).\n\\end{align}\\] Then, we have the following: for any \\(h_t\\) and \\(a_t\\), \\[\\begin{equation} \\label{eq:history-info}\n  Q_t(h_t, a_t) = \\hat Q_t(φ_t(h_t), a_t)\n  \\quad\\text{and}\\quad\n  V_t(h_t) = \\hat V_t(φ_t(h_t)).\n\\end{equation}\\] Any strategy \\(\\hat π = (\\hat π_1, \\dots, \\hat π_T)\\), where \\(\\hat π_t \\colon \\ALPHABET Z \\to \\ALPHABET A\\), is optimal if and only if \\[\\begin{equation}\\label{eq:info-verification}\n    \\hat π_t(z_t) \\in \\arg\\min_{a_t \\in \\ALPHABET A} \\hat Q_t(z_t, a_t).\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs usual, we prove the result by backward induction. By construction, Eq. \\eqref{eq:history-info} is true at time \\(T+1\\). This forms the basis of induction. Now assume that \\eqref{eq:history-info} is true at time \\(t+1\\) and consider the system at time \\(t\\). Then, \\[\\begin{align*}\nQ_t(h_t, a_t) &= \\EXP[ c_t(S_t, A_t) + V_{t+1}(H_{t+1}) | H_t = h_t, A_t = a_t\n] \\\\\n&\\stackrel{(a)}= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}( φ_t(H_{t+1}) ) | H_t =\nh_t, A_t = a_t ]  \\\\\n&\\stackrel{(b)}= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}( φ_t(H_{t+1}) ) | Z_t =\nφ_t(h_t), A_t = a_t ]  \\\\\n&\\stackrel{(c)}= \\hat Q_t(φ_t(h_t), a_t),\n\\end{align*}\\] where \\((a)\\) follows from the induction hypothesis, \\((b)\\) follows from the properties (P1) and (P2) of the information state, and \\((c)\\) follows from the definition of \\(\\hat Q_t\\). This shows that the action value functions are equal. By minimizing over the actions, we get that the value functions are also equal."
  },
  {
    "objectID": "pomdps/intro.html#belief-state-based-dynamic-program",
    "href": "pomdps/intro.html#belief-state-based-dynamic-program",
    "title": "17  Introduction",
    "section": "17.5 Belief state based dynamic program",
    "text": "17.5 Belief state based dynamic program\nAs shown in Example 17.2, the belief state \\(b_t\\) is an information state. Therefore, Theorem 17.3 implies that we can write a dynamic program based on \\(b_t\\). This is an important and commonly used formulation, so we study it separately and present some properties of the value functions. The belief state based dynamic program is given by: \\(V_{T+1}(b_{T+1}) = 0\\) and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\n  Q_t(b_t, a_t) =\n  \\EXP [ c_t(S_t, A_t) + V_{t+1}(B_{t+1}) \\mid B_t = b_t, A_t = a_t ].\n\\] and \\[ V_t(b_t) = \\min_{a_t \\in \\ALPHABET A} Q_t(b_t, a_t). \\]\nDefine \\[ \\PR(y_{t+1} | b_t, a_t) =\n   \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t) b_t(s_t).\n\\] Then, the belief update expression in Lemma 17.2 can be written as: \\[\n  b_{t+1}(s_{t+1}) =\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n  { \\PR(y_{t+1} | b_t, a_t) }.\n\\] For the ease of notation, we write this expression as \\(b_{t+1} = ψ(b_t, y_{t+1}, a_t)\\).\n\\[\\begin{align*}\n  Q_t(b_t, a_t) &= \\sum_{s_t \\in \\ALPHABET S} c_t(s_t, a_t) b_t(s_t) \\\\\n  & \\quad +  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  V_{t+1}( φ(b_t, y_{t+1}, a_t) ).\n\\end{align*}\\]\nA key property of the belief-state based value functions is the following.\n\nTheorem 17.4 The belief-state based value functions are piecewise linear and concave.\n\n\n\n\n\n\nAn illustration of a piecewise linear and concave function. Move the points around to see how the shape of the function changes.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs usual, we prove the result using backward induction. For any \\(a_T\\), \\[ Q_T(b_T, a_T) = \\sum_{s_T \\in \\ALPHABET S} c_T(s_T, a_T) b_T(s_T) \\] is linear in \\(b_T\\). Therefore, \\[ V_T(b_T) = \\min_{a_T \\in \\ALPHABET A} Q_T(b_T, a_T) \\] is the minimum of a finite number of linear functions. Hence \\(V_T(b_T)\\) is piecewise linear and concave.\nNow assume that \\(V_{t+1}(b_{t+1})\\) is piecewise linear and concave (PWLC). Any PWLC function can be represented as a minimum of a finite number of hyperplanes. Therefore, we can find a finite set of vectors \\(\\{ A_i \\}_{i \\in I}\\) indexed by finite set \\(I\\) such that \\[\n  V_{t+1}(b) = \\min_{i \\in I} \\langle A_i, b \\rangle.\n\\]\nWe need to show that \\(V_t(b_t)\\) is piecewise linear and concave (PWLC). We first show that \\(Q_t(b_t, a_t)\\) is PWLC. For any fixed \\(a_t\\), the first term \\(\\sum_{s_t} c_t(s_t, a_t) b_t(s_t)\\) is linear in \\(b_t\\). Now consider the second term: \\[\\begin{align*}\n  \\hskip 1em & \\hskip -1em\n  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  V_{t+1}( φ(b_t, y_{t+1}, a_t) ) \\\\\n  &=\n  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  \\min_{i \\in I}\n  \\left\\langle A_i,\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n  { \\PR(y_{t+1} | b_t, a_t) } \\right\\rangle \\\\\n  &=\n  \\sum_{y_{t+1} \\in \\ALPHABET Y}\n  \\min_{i \\in I}\n  \\Big\\langle A_i,\n   \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t)\n   \\Big\\rangle\n\\end{align*}\\] which is the sum of PWLC functions of \\(b_t\\) and therefore PWLC in \\(b_t\\).\nThus, \\(Q_t(b_t, a_t)\\) is PWLC. Hence, \\(V_t(b_t)\\) which is the pointwise minimum of PWLC functions is PWLC. Hence, the result holds due to principle of induction.\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nSince the value function is PWLC, we can identify a finite index set \\(I_t\\), and a set of vectors \\(\\{ A^i_t \\}_{i \\in I_t}\\) such that \\[\n    V_t(b) = \\min_{i \\in I_t} \\langle A^i_t, b \\rangle.\n\\] Smallwood and Sondik (1973) presented a “one-pass” algorithm to recursively compute \\(I_t\\) and \\(\\{ A^i_t \\}_{i \\in I_t}\\) which allows us to exactly compute the value function. Various efficient refinements of these algorithms have been presented in the literature, e.π., the linear-support algorithm (Cheng 1988), the witness algorithm (Cassandra et al. 1994), incremental pruning (Zhang and Liu 1996; Cassandra et al. 1997), duality based approach (Zhang 2009), and others. See https://pomdp.org/ for an accessible introduction to these algorithms."
  },
  {
    "objectID": "pomdps/intro.html#exercises",
    "href": "pomdps/intro.html#exercises",
    "title": "17  Introduction",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 17.1 Consider an MDP where the state space \\(\\ALPHABET S\\) is a symmetric subset of integers of the form \\(\\{-L, -L + 1, \\dots, L -1 , L\\}\\) and the action space \\(\\ALPHABET A\\) is discrete. Suppose the transition matrix \\(P(a)\\) and the cost function \\(c_t(s,a)\\) satisfy properties (A1) and (A2) of Exercise 6.6. Show that \\(Z_t = |S_t|\\) is an information state.\n\n\nExercise 17.2 Consider a linear system with state \\(x_t \\in \\reals^n\\), observations \\(y_t \\in \\reals^p\\), and action \\(u_t \\in \\reals^m\\). Note that we will follow the standard notation of linear systems and denote the system variables by lower case letters \\((x,u)\\) rather than upper case letter \\((S,A)\\). The dynamics of the system are given by \\[\\begin{align*}\n  x_{t+1} &= A x_t + B u_t + w_t  \\\\\n  y_t &= C x_t + n_t\n\\end{align*}\\] where \\(A\\), \\(B\\), and \\(C\\) are matrices of appropriate dimensions. The per-step cost is given by \\[\n  c(x_t, u_t) = x_t^\\TRANS Q x_t + u_t^\\TRANS R u_t,\n\\] where \\(Q\\) is a positive semi-definite matrix and \\(R\\) is a positive definite matrix. We make the standard assumption that the primitive random variables \\(\\{s_1, w_1, \\dots, w_T, n_1, \\dots, n_T \\}\\) are independent.\nShow that if the primitive variables are Guassian, then the conditional estimate of the state \\[\n  \\hat x_t = \\EXP[ x_t | y_{1:t}, u_{1:t-1} ]\n\\] is an information state.\n\n\nExercise 17.3 Consider a machine which can be in one of \\(n\\) ordered state where the first state is the best and the last state is the worst. The production cost increases with the state of the machine. The state evolves in a Markovian manner. At each time, an agent has the option to either run the machine or stop and inspect it for a cost. After inspection, the agent may either repair the machine (at a cost that depends on the state) or replace it (at a fixed cost). The objective is to identify a maintenance policy to minimize the cost of production, inspection, repair, and replacement.\nLet \\(τ\\) denote the time of last inspection and \\(S_τ\\) denote the state of the machine after inspection, repair, or replacement. Show that \\((S_τ, t-τ)\\) is an information state."
  },
  {
    "objectID": "pomdps/intro.html#notes",
    "href": "pomdps/intro.html#notes",
    "title": "17  Introduction",
    "section": "Notes",
    "text": "Notes\nThe discussion in this section is taken from Subramanian et al. (2022). Information state may be viewed as a generalization of the traditional notion of state Nerode (1958), which is defined as a statistic (i.e., a function of the observations) sufficient for input-output mapping. In contrast, we define an information state as a statistic sufficient for performance evaluation (and, therefore, for dynamic programming). Such a definition is hinted in Witsenhausen (1976). The notion of information state is also related to sufficient statistics for optimal control defined in Striebel (1965) for systems with state space models.\nAs far as we are aware, the informal definition of information state was first proposed by Kwakernaak (1965) for adaptive control systems. Formal definitions for linear control systems were given by Bohlin (1970) for discrete time systems and by Davis and Varaiya (1972) for continuous time systems. Kumar and Varaiya (1986) define an information state as a compression of past history which satisfies property (P2a) but do not formally show that such an information state always leads to a dynamic programming decomposition.\n\n\n\n\nBohlin, T. 1970. Information pattern for linear discrete-time models with stochastic coefficients. IEEE Transactions on Automatic Control (TAC) 15, 1, 104–106.\n\n\nCassandra, A., Littman, M.L., and Zhang, N.L. 1997. Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes. Proceedings of the thirteenth conference on uncertainty in artificial intelligence.\n\n\nCassandra, A.R., Kaelbling, L.P., and Littman, M.L. 1994. Acting optimally in partially observable stochastic domains. AAAI, 1023–1028.\n\n\nCheng, H.-T. 1988. Algorithms for partially observable markov decision processes.\n\n\nDavis, M.H.A. and Varaiya, P.P. 1972. Information states for linear stochastic systems. Journal of Mathematical Analysis and Applications 37, 2, 384–402.\n\n\nKumar, P.R. and Varaiya, P. 1986. Stochastic systems: Estimation identification and adaptive control. Prentice Hall.\n\n\nKwakernaak, H. 1965. Theory of self-adaptive control systems. In: Springer, 14–18.\n\n\nNerode, A. 1958. Linear automaton transformations. Proceedings of American Mathematical Society 9, 541–544.\n\n\nSmallwood, R.D. and Sondik, E.J. 1973. The optimal control of partially observable markov processes over a finite horizon. Operations Research 21, 5, 1071–1088. DOI: 10.1287/opre.21.5.1071.\n\n\nStriebel, C. 1965. Sufficient statistics in the optimal control of stochastic systems. Journal of Mathematical Analysis and Applications 12, 576–592.\n\n\nSubramanian, J., Sinha, A., Seraj, R., and Mahajan, A. 2022. Approximate information state for approximate planning and reinforcement learning in partially observed systems. Journal of Machine Learning Research 23, 12, 1–83. Available at: http://jmlr.org/papers/v23/20-1165.html.\n\n\nWitsenhausen, H.S. 1975. On policy independence of conditional expectation. Information and Control 28, 65–75.\n\n\nWitsenhausen, H.S. 1976. Some remarks on the concept of state. In: Y.C. Ho and S.K. Mitter, eds., Directions in large-scale systems. Plenum, 69–75.\n\n\nZhang, H. 2009. Partially observable Markov decision processes: A geometric technique and analysis. Operations Research.\n\n\nZhang, N. and Liu, W. 1996. Planning in stochastic domains: Problem characteristics and approximation. Hong Kong Univeristy of Science; Technology."
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#dynamic-programming-decomposition",
    "href": "pomdps/sequential-hypothesis.html#dynamic-programming-decomposition",
    "title": "18  Sequential hypothesis testing",
    "section": "18.1 Dynamic programming decomposition",
    "text": "18.1 Dynamic programming decomposition\nWe use the belief-state as an information state to obtain a dynamic programming decomposition. Recall that the beief state is two-dimensional pdf where \\[ b_t(h) = \\PR(H = h | Y_{1:t}), \\quad h \\in \\{h_0, h_1\\}. \\]\n\nRemarks\n\n\nWe are only conditioning on \\(Y_{1:t}\\) and not adding \\(A_{1:t-1}\\) in the conditioning. This is because we are taking the standard approach used in optimal stopping problems where we are only defining the state for case when the stopping decision hasn’t been taken so far and all previous actions are continue. Taking a continue action does not effect the observations. For this reason, we do not condition on \\(A_{1:t-1}\\).\nIt is possible to exploit the fact that \\(b_t = [p_t, 1 - p_t]^T\\) and write a simplified DP in terms of \\(p_t\\). In these notes, I don’t make this simplification so that we can see how these results will extend to the case of non-binary hypothesis.\n\n\n\nThe dynamic program for the above model is then given by \\[\n  V_T(b_T) = \\min\\{ \\EXP[ \\ell(h_0, H) | B_T = b_T],\n                    \\EXP[ \\ell(h_1, H) | B_T = b_T] \\}\n\\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\), \\[ V_t(b_t) = \\min \\{ \\EXP[ \\ell(h_0, H) | B_t = b_t],\n                      \\EXP[ \\ell(h_1, H) | B_t = b_t],\n                     c + \\EXP[V_{t+1}(ψ(b_t, Y_{t+1})) | B_t = b_t] \\},\n\\] where \\(ψ(b, y)\\) denotes the standard non-linear filtering update (there is no dependence on \\(a\\) here because there are no state dynamics in this model).\nWe introduce some notation to simplify the discussion. Define\n\n\\(L_i(b) = \\EXP[ \\ell(h_i, H) | B = b] = \\sum_{h \\in \\{h_0, h_1\\}} \\ell(h_i, h) b(h)\\).\n\\(W_t(b_t) = c + \\EXP[V_{t+1}(ψ(b_t, Y_{t+1})) | B_t = b_t]\\).\n\nThen, the above DP can be written as \\[\n  V_T(b_T) = \\min\\{ L_0(b_T), L_1(b_T) \\}\n\\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\), \\[ V_t(b_t) = \\min \\{ L_0(b_t), L_1(b_t), W_t(b_t) \\}. \\]"
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#structure-of-the-optimal-policy",
    "href": "pomdps/sequential-hypothesis.html#structure-of-the-optimal-policy",
    "title": "18  Sequential hypothesis testing",
    "section": "18.2 Structure of the optimal policy",
    "text": "18.2 Structure of the optimal policy\nWe start by establishing simple properties of the different functions defined above.\n\nLemma 18.1 The above functions statisfy the following properties:\n\n\\(L_i(b)\\) is linear in \\(b\\).\n\\(V_t(b)\\) and \\(W_t(b)\\) is concave in \\(b\\).\n\\(V_t(b)\\) and \\(W_t(b)\\) are increasing in \\(t\\).\n\n\n\n\n\n\n\nAn illustration of the minimum of two straight lines and a concave function. Move the points around to see how the shape of the function changes.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe linearity of \\(L_i(b)\\) follows from definition. From the discussion on POMDPs, we know that \\(V_{t+1}(b)\\) is concave in \\(b\\) and so is \\(\\EXP[V_{t+1}(ψ(b, Y_{t+1})) | B_t = b]\\). Therefore \\(W_t(b)\\) is concave in \\(b\\).\nFinally, by construction, we have that \\(V_{T-1}(b) \\le V_T(b)\\). The monotonicity in time then follows from Q2 of Assignment 2. Sincen \\(V_t\\) is monotone in time, it implies that \\(W_t\\) is also monotone.\n\n\n\nNow define stopping sets \\(D_t(h) = \\{ b \\in Δ^2 : π_t(b) = h \\}\\) for \\(h \\in \\{h_0, h_1\\}\\). The key result is the following.\n\nTheorem 18.1 For all \\(t\\) and \\(h \\in \\{h_0, h_1\\}\\), the set \\(D_t(h)\\) is convex. Moreover, \\(D_t(h_i) \\subseteq D_{t+1}(h_i)\\).\n\n\n\n\n\n\nAn illustration of the stopping sets. Move the points around to see how the shape of the stopping set changes.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNote that we can write \\(D_t(h) = A_t(h) \\cap B_t(h)\\), where \\[ A_t(h_i) = \\{ b \\in Δ^2 : L_i(b)  \\le L_j(b) \\}\n   \\quad\\text{and}\\quad\n   B_t(h_i) = \\{ b \\in Δ^2 : L_i(b) \\le W_t(b)  \\}. \\]\n\\(A_t(h_i)\\) is a the set of \\(b\\) where one linear function of \\(b\\) is less than or equal to another linear function of \\(b\\). Therefore, \\(A_t(h_i)\\) is a convex set.\nSimilarly, \\(B_t(h_i)\\) is the set of \\(b\\) where a linear function of \\(b\\) is less than or equal to a concave function of \\(b\\). Therefore \\(B_t(h_i)\\) is also a convex set.\n\\(D_t(h_i)\\) is the intersection of two convex sets, and hence convex.\nThe monotonicty of \\(D_t(h_i)\\) in time follows from the monotonicity of \\(W_t\\) in time.\n\n\n\n\nTheorem 18.2 Suppose the stopping cost satisfy the following: \\[\\begin{equation} \\label{eq:cost-ass}\n\\ell(h_0, h_0) \\le c \\le \\ell(h_0, h_1)\n  \\quad\\text{and}\\quad\n  \\ell(h_1, h_1) \\le c \\le \\ell(h_1, h_0).\n\\end{equation}\\] Then, \\(e_i \\in D_t(h_i)\\), where \\(e_i\\) denotes the standard unit vector (i.e., \\(e_0 = [1, 0]^T\\) and \\(e_1 = [0, 1]^T\\)).\n\n\n\n\n\n\n\nRemark\n\n\n\nThe assumption on observation cost states that: (i) the cost of observation is greater than the cost incurred when the DM chooses the right hypothesis, and (ii) the cost of observation is less than the cost incurred when the DM chooses the wrong hypothesis. Both these assumptions are fairly natural.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNote that \\(L_i(e_0) = \\ell(h_i, h_0)\\) and \\(L_i(e_1) = \\ell(h_1, h_1)\\). Moreover, by construction, \\(W_t(b) \\ge c\\). Thus, under the above assumption on the cost, \\[ L_0(e_0) = \\ell(h_0, h_0) \\le c \\le W_t(e_0) \\] and \\[ L_0(e_0) = \\ell(h_0, h_0) \\le \\ell(h_1, h_0) = L_1(e_0). \\] Thus, \\(e_0 \\in D_t(h_0)\\).\nBy a symmetric argument, we can show that \\(e_1 \\in D_t(h_1)\\).\n\n\n\nTheorem 18.1 and Theorem 18.2 imply that the optimal stopping regions are convex and include the “corner points” of the simplex. Note that although we formulated the problem for binary hypothesis, all the steps of the proof hold in general as well.\n\n\n\n\n\nStopping regions for multiple hypothesis\n\n\nFor binary hypothesis, we can present a more concerete characterizatin of the optimal policy. Note that the two-dimensional simplex is equivalent to the interval \\([0,1]\\). In particular, any \\(b = Δ^2\\) is equal to \\([p, 1-p]\\), where \\(p \\in [0,1]\\). Now define:\n\n\\(\\displaystyle b_t = \\min\\left\\{ p \\in [0, 1] :  π_t\\left(\\begin{bmatrix} p \\\\ 1-p \\end{bmatrix}\\right) = h_0 \\right\\}.\\)\n\\(\\displaystyle a_t = \\max\\left\\{ p \\in [0, 1] :  π_t\\left(\\begin{bmatrix} p \\\\ 1-p \\end{bmatrix}\\right) = h_1 \\right\\}.\\)\n\nThen, by definition, the optimal policy has the following threshold property:\n\nProposition 18.1 Let \\(\\bar π_t(p) = π_t([p, 1-p]^T)\\). Then, under \\eqref{eq:cost-ass}, \\[ \\bar π_t(p) = \\begin{cases}\n   h_1, & \\text{if } p \\le a_t \\\\\n   \\mathsf{C}, & \\text{if } a_t &lt; p &lt; b_t \\\\\n   h_0, & \\text{if } b_t \\le p.\n  \\end{cases} \\]\nFurthermore, the decision thresholds are monotone in time. In particular, for all \\(t\\), \\[ a_t \\le a_{t+1} \\le b_{t+1} \\le b_t. \\]\n\nThe above property is simplies stated slighted in terms of the likelihood ratio. In particular, define \\(λ_t = b_t(0)/b_t(1) = p_t/(1 - p_t)\\). Then, we have the following:\n\nProposition 18.2 Let \\(\\hat π_t(λ) = π_t([λ/(1+λ), 1/(1+λ)]^T)\\). Then, under \\eqref{eq:cost-ass}, \\[ \\hat π_t(λ) = \\begin{cases}\n   h_1, & \\text{if } λ \\le a_t/(1 - a_t) \\\\\n   \\mathsf{C}, & \\text{if } a_t/(1 - a_t) &lt; λ &lt; b_t/(1 - b_t)_t \\\\\n   h_0, & \\text{if } b_t/(1 - b_t)_t \\le λ.\n  \\end{cases} \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(a, b \\in [0, 1]\\), \\[ a \\le b \\iff \\frac{a}{1-a} \\le \\frac{b}{1-b}.\\]\n\n\n\nThe result of Proposition 18.2 is called the sequential likelihood ratio test (SLRT) or sequential probability ratio test (SPRT) to contrast it with the standard :likelihood ratio test in hypotehsis testing."
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#infinite-horizon-setup",
    "href": "pomdps/sequential-hypothesis.html#infinite-horizon-setup",
    "title": "18  Sequential hypothesis testing",
    "section": "18.3 Infinite horizon setup",
    "text": "18.3 Infinite horizon setup\nAssume that \\(T = ∞\\) so that the continuation alternative is always available. Then, we have the following.\n\nTheorem 18.3 Under \\eqref{eq:cost-ass}, an optimal decision rule always exists, is time-homogeneous, and is given by the solution of the following DP: \\[ V(b) = \\min\\{ L_0(b) , L_1(b) , W(b) \\} \\] where \\[ W(b) = c + \\int_{y} [ pf_0(y) + (1-p)f_1(y)] V(ψ(b,y)) dy. \\]\nTherefore, the optimal thresholds \\(a\\) and \\(b\\) are time-homogeneous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe result follows from standard results on non-negative dynamic programming. We did not cover non-negative DP. Essentially it determines conditions under which undiscounted infinite horizon problems have a solution when the per-step cost is non-negative."
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#upper-bound-on-the-expected-number-of-measurements",
    "href": "pomdps/sequential-hypothesis.html#upper-bound-on-the-expected-number-of-measurements",
    "title": "18  Sequential hypothesis testing",
    "section": "18.4 Upper bound on the expected number of measurements",
    "text": "18.4 Upper bound on the expected number of measurements\nFor simplicity, we assume that \\(\\ell(h_0, h_0) = \\ell(h_1, h_1) = 0\\). For the infinite horizon model, we can get upper bound on the expected number of measurements that an optimal policy will take. Let \\(τ\\) denote the number of measurements taken under policy \\(π\\) and \\(A_τ\\) denote the terminal action after stopping. Then, the performance of policy \\(π\\) is given by \\[\n  J(π) = \\EXP[ c τ + \\ell(H, A_\\tau) \\mid \\Pi = b ].\n\\] Note that \\(\\ell(H, A_\\tau) \\ge 0\\). Therefore, the performance of the optimal policy is lower bounded by \\[\n  J^* \\ge c\\, \\EXP^{π^*}[  τ \\mid \\Pi = b] .\n\\] Now, consider a policy \\(\\tilde π\\) which does not consider continuation action and takes the best stopping decision. The performance of \\(\\tilde π\\) is given by \\[ J(\\tilde π) = \\min \\{ \\ell(h_1, h_0) b_1, \\ell(h_0, h_1) b_0 \\}. \\] Since \\(J(\\tilde π) \\ge J^*\\), we get \\[\n  \\EXP^{π^*}[ τ  \\mid \\Pi = b ] \\le \\frac 1c\n  \\min \\{ \\ell(h_1, h_0) b_1, \\ell(h_0, h_1) b_0 \\}. \\]"
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#exercises",
    "href": "pomdps/sequential-hypothesis.html#exercises",
    "title": "18  Sequential hypothesis testing",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 18.1 Consider the following modification of the sequential hypothesis testing. As in the model discussed above, there are two hypothesis \\(h_0\\) and \\(h_1\\). The a priori probability that the hypothesis is \\(h_0\\) is \\(p\\).\nIn contrast to the model discussed above, there are \\(N\\) sensors. If the underlying hypothesis is \\(h_i\\) and sensor \\(m\\) is used at time \\(t\\), then the observation \\(Y_t\\) is distrubted according to pdf (or pmf) \\(f^m_i(y)\\). The cost of using sensor \\(m\\) is \\(c_m\\).\nWhenever the decision maker takes a measurement, he picks a sensor \\(m\\) uniformly at random from \\(\\{1, \\dots, N\\}\\) and observes \\(Y_t\\) according to the distribution \\(f^m_i(\\cdot)\\) and incurs a cost \\(c_m\\).\nThe system continues for a finite time \\(T\\). At each time \\(t &lt; T\\), the decision maker has three options: stop and declare \\(h_0\\), stop and declare \\(h_1\\), or continue to take another measurement. At time \\(T\\), the continue alternative is unavailable.\n\nFormulate the above problem as a POMDP. Identify an information state and write the dynamic programming decomposition for the problem.\nShow that the optimal control law has a threshold property, similar to the threshold propertly for the model described above.\n\n\n\nExercise 18.2 In this exercise, we will derive an approximate method to compute the performance of a given threshold based policy for infinite horizon sequential hypothesis testing problem. Let \\[ θ_i(π,p) = \\EXP^{π}[ τ | H = h_i] \\] denote the expected number of samples when using stopping rule \\(π\\) assuming that the true hypothesis is \\(h_i\\). Note that for any belief state based stopping rule, \\(θ_i\\) depends on the initial belief \\([p, 1-p]\\). Furthermore, let \\[ ξ_i(h_k ;π, p) = \\PR^π(A_τ = h_k | H = h_i) \\] denote the probability that the stopping action is \\(h_k\\) when using stopping rule \\(π\\) assuming that the true hypothesis is \\(h_i\\).\n\nArgue that the performance of any policy \\(π\\) can be written as \\[\\begin{align*}\n  V_π(p) &= c [ p θ_0(π, p) + (1-p) θ_1(π,p) ] \\\\\n  & \\quad + p \\sum_{a \\in \\{h_0, h_1\\}} \\ell(a, h_0) ξ_0(a; π, p) \\\\\n  & \\quad + (1-p) \\sum_{a \\in \\{h_0, h_1\\}} \\ell(a, h_1) ξ_1(a; π, p).\n  \\end{align*}\\] Thus, approximately computing \\(θ_i\\) and \\(ξ_i\\) gives an approximate value of \\(V_π(p)\\).\nNow assume that the policy \\(π\\) is of a threshold form with thresholds \\(a\\) and \\(b\\). To avoid trivial cases, we assume that \\(p \\in (a,b)\\). The key idea to compute \\(θ_i\\) and \\(ξ_i\\) is that the evolution of \\(p_t =  \\PR(H = h_t | Y_{1:t})\\) is a Markov chain which starts at a state \\(p  \\in (a,b)\\) and stops the first time \\(p_t\\) goes below \\(a\\) or above \\(b\\).\n\n\n\nDiscretization of the state space\n\n\nSuppose we discretize the state space space \\([0, 1]\\) into \\(n+1\\) grid points \\(\\ALPHABET D_n = \\{0, \\frac1n, \\dots, 1\\}\\). Assume that \\(p\\), \\(a\\), and \\(b\\) lie on this discrete grid. Discreteize \\(p_t\\) to the closest grid point and let \\(P_i\\) denote the transition matrix of the discretized \\(p_t\\) when the true hypothesis is \\(h_i\\). Partition the \\(P_i\\) as \\[ \\left[\\begin{array}{c|c|c}\n     A_i & B_i & C_i \\\\\n     \\hline\n     D_i & E_i & F_i \\\\\n     \\hline\n     G_i & H_i & J_i\n    \\end{array}\\right] \\] where the lines correspond to the index for \\(a\\) and \\(b\\). The transition matrix of the absorbing Markov chain is given by \\[ \\hat P_i = \\left[\\begin{array}{c|c|c}\n       I & 0 & I \\\\\n       \\hline\n       D_i & E_i & F_i \\\\\n       \\hline\n       I & 0 & I\n      \\end{array}\\right] \\] Now suppose \\(j\\) is the index of \\(p\\) in \\(\\ALPHABET D_n\\). Using properties of absorbing Markov chains, show that\n\n\\(ξ_i(h_0; \\langle a, b \\rangle, p) \\approx  [ (I - E_i)^{-1} F_i \\mathbf{1} ]_j\\)\n\\(ξ_i(h_1; \\langle a, b \\rangle, p) \\approx  [ (I - E_i)^{-1} D_i \\mathbf{1} ]_j\\)\n\\(θ_i(\\langle a, b \\rangle, p) \\approx  [ (I - E_i)^{-1} \\mathbf{1} ]_j\\)"
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#notes",
    "href": "pomdps/sequential-hypothesis.html#notes",
    "title": "18  Sequential hypothesis testing",
    "section": "Notes",
    "text": "Notes\nFor more details on sequential hypothesis testing, incuding an approximate method to determine the thresholds, see Wald (1945). The optimal of sequential likelihood ratio test was proved in Wald and Wolfowitz (1948). The model described above was first considered by Arrow et al. (1949). See DeGroot (1970).\nThe upper bound on expected number of measurements is adapted from an argument presented in Hay et al. (2012).\nExercise 18.1 is from Bai et al. (2015). Exercise 18.2 is from Woodall and Reynolds (1983).\n\n\n\n\n\nArrow, K.J., Blackwell, D., and Girshick, M.A. 1949. Bayes and minimax solutions of sequential decision problems. Econometrica 17, 3/4, 213. DOI: 10.2307/1905525.\n\n\nBai, C.-Z., Katewa, V., Gupta, V., and Huang, Y.-F. 2015. A stochastic sensor selection scheme for sequential hypothesis testing with multiple sensors. IEEE transactions on signal processing 63, 14, 3687–3699.\n\n\nDeGroot, M. 1970. Optimal statistical decisions. Wiley-Interscience, Hoboken, N.J.\n\n\nHay, N., Russell, S., Tolpin, D., and Shimony, S.E. 2012. Selecting computations: Theory and applications. UAI. Available at: http://www.auai.org/uai2012/papers/123.pdf.\n\n\nWald, A. 1945. Sequential tests of statistical hypotheses. The Annals of Mathematical Statistics 16, 2, 117–186. DOI: 10.1214/aoms/1177731118.\n\n\nWald, A. and Wolfowitz, J. 1948. Optimum character of the sequential probability ratio test. The Annals of Mathematical Statistics 19, 3, 326–339. DOI: 10.1214/aoms/1177730197.\n\n\nWoodall, W.H. and Reynolds, M.R. 1983. A discrete markov chain representation of the sequential probability ratio test. Communications in Statistics. Part C: Sequential Analysis 2, 1, 27–44. DOI: 10.1080/07474948308836025."
  },
  {
    "objectID": "approx-mdps/approx-DP.html#approximate-value-iteration",
    "href": "approx-mdps/approx-DP.html#approximate-value-iteration",
    "title": "19  Approximate dynamic programming",
    "section": "19.1 Approximate value iteration",
    "text": "19.1 Approximate value iteration\n\nTheorem 19.1 Generate \\(\\{V_k\\}_{k \\ge 0}\\) and \\(\\{π_k\\}_{k \\ge 0}\\) such that \\[\\NORM{V_{k+1} - \\mathcal B V_k} \\le δ\n\\quad\\text{and}\\quad\n  \\NORM{\\mathcal B_{π_k} V_k - \\mathcal B V_k} \\le ε. \\] Then,\n\n\\(\\displaystyle \\lim_{k \\to ∞} \\NORM{V_k - V^*} \\le  \\frac {δ}{(1-γ)}.\\)\n\\(\\displaystyle \\lim_{k \\to ∞} \\NORM{V_{π_k} - V^*} \\le  \\frac {ε}{(1-γ)} + \\frac{2γδ}{(1-γ)^2}.\\)\n\n\nIf we use a periodic policy with period \\(M\\), then the above bound can be improved by a factor of \\(1/(1-γ)\\).\n\n\n\n\n\n\nProof\n\n\n\n\n\nTo prove the first part, note that repeatedly combining the contraction property of the Bellman operator (Proposition 9.1) with the fact that \\(\\NORM{V_{k+1} - \\mathcal B V_k} \\le δ\\), we get that \\[\\begin{equation}\\label{eq:B1}\n  \\NORM{\\mathcal B^m V_{k+1} - \\mathcal B^{m+1} V_k} \\le γ^m δ.\n\\end{equation}\\]\nNow, from the triangle inequality, we have that \\[\\begin{align*}\n\\NORM{V_k - \\mathcal B^k V_0} &\\le\n  \\NORM{V_k - \\mathcal B V_{k-1}} + \\NORM{\\mathcal B V_{k-1} - \\mathcal B^2 V_{k-2}}\n  + \\cdots + \\NORM{B^{k-1} V_1 - \\mathcal B^k V_0} \\\\\n  &\\stackrel{(a)}\\le δ + γδ + \\dots + γ^{k-1}δ \\\\\n  &= \\left(\\frac{1 - γ^k}{1-γ}\\right) δ,\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:B1}. Taking the limit as \\(k \\to ∞\\) gives the first result.\nNow, to prove the second part, we again apply the triangle inequality \\[\\begin{align*}\n  \\NORM{\\mathcal B_{π_k} V^* - V^*} &\\le\n  \\NORM{\\mathcal B_{π_k} V^* - \\mathcal B_{π_k} V_k} +\n  \\NORM{\\mathcal B_{π_k} V_k - \\mathcal B V_k} +\n  \\NORM{\\mathcal B V_k - V^*} \\\\\n  &\\stackrel{(b)}\\le γ \\NORM{V^* - V_k} + ε + γ \\NORM{V_k - V^*} \\\\\n  &\\stackrel{(c)}\\le ε + \\frac{2γδ}{(1-γ)} =: m,\n\\end{align*}\\] where the first term in \\((b)\\) uses the contraction property, the second term uses the fact that \\(π_k\\) is an \\(ε\\)-optimal policy and the third term uses the fact that \\(V^*\\) is the fixed point of \\(\\mathcal B\\) and thus \\(V^* = \\mathcal B V^*\\) and then uses the contraction property. The inequality in \\((c)\\) use the result from the first part.\nNow, from the discounting property of the Bellman operator (Proposition 9.3), \\(\\NORM{\\mathcal B_{π_k} V^* - V^*} \\le m\\) implies \\[ \\NORM{V_{π_k} - V^*} \\le \\frac{m}{(1-γ)}\\] which proves the second part."
  },
  {
    "objectID": "approx-mdps/approx-DP.html#approximate-policy-iteration",
    "href": "approx-mdps/approx-DP.html#approximate-policy-iteration",
    "title": "19  Approximate dynamic programming",
    "section": "19.2 Approximate policy iteration",
    "text": "19.2 Approximate policy iteration\nBefore stating the approximate policy iteration algorithm, we state a preliminary result that serves as the main step in proving the error bounds for approximate policy iteration.\n\nProposition 19.1 Suppose \\(V\\), \\(π\\), and \\(h\\) satisfy \\[ \\NORM{V - V_π} \\le δ\n   \\quad\\text{and}\\quad\n   \\NORM{\\mathcal B_h V - \\mathcal B V} \\le ε.\n\\] Then, \\[ \\NORM{V_h - V^*} \\le γ \\NORM{V_π - V^*} + \\frac{ε + 2γδ}{(1-γ)}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the bounds on \\(V\\), \\(π\\), and \\(h\\) and the discounting property of the Bellman operator (Proposition 9.3), we have that \\[\\begin{equation}\\label{eq:P1}\n  \\mathcal B_h V_π \\le \\mathcal B_h V + γδ \\le \\mathcal B V + ε + γ δ.\n\\end{equation}\\]\nAgain, from the bounds on \\(V\\) and \\(π\\) and the discounting property of the Bellman operator, we have \\(\\mathcal B V \\le \\mathcal B V_π + γδ\\). Thus, \\[\\begin{equation}\\label{eq:P2}\n  \\mathcal B_h V_π \\le \\mathcal B V_π + ε + 2γδ\n\\end{equation}\\] For ease of notation, let \\(m := ε + 2γδ\\).\nMoreover, from the definition of the Bellman operator \\[ \\mathcal B V_π \\le \\mathcal B_π V_π = V_π.\\] Substituting the above in \\eqref{eq:P2}, we get that \\[ \\mathcal B_h V_π \\le V_π + m. \\] Therefore, by the discounting property of Bellman operator, we get \\[\\begin{equation}\\label{eq:P3}\n  V_h \\le V_π + \\frac{m}{(1-γ)}.\n\\end{equation}\\]\nUsing \\eqref{eq:P3} and the discounting property, we get that \\[V_h = \\mathcal B_h V_h = \\mathcal B_h V_π + \\big( \\mathcal B_h V_h -\n\\mathcal B_h V_π \\big) \\le \\mathcal B_h V_π + γ \\frac{m}{(1-γ)}. \\]\nSubtracting \\(V^*\\) from both sides we get \\[\\begin{align*}\nV_h - V^* &\\le \\mathcal B_h V_π - V^* + \\frac{mγ}{(1-γ)} \\\\\n&\\stackrel{(a)}\\le \\mathcal B V_π + m - V^* + \\frac{mγ}{(1-γ)} \\\\\n&\\stackrel{(b)}= \\mathcal B V_π - \\mathcal B V^* + \\frac{m}{(1-γ)} \\\\\n&\\stackrel{(c)}{\\le} γ \\NORM{V_π - V^*} + \\frac{m}{(1-γ)},\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:P1}, \\((b)\\) uses the fact that \\(V^*\\) is the fixed point of \\(\\mathcal B\\) and \\((c)\\) uses the contraction property. Substituting the value of \\(m\\) in the above equation gives the result.\n\n\n\n\nTheorem 19.2 Generate a sequence \\(\\{π_k\\}_{k \\ge 0}\\) and \\(\\{V_k\\}_{k \\ge 0}\\) such that \\[ \\NORM{V_k - V_{π_k}} \\le δ\n\\quad\\text{and}\\quad\n\\NORM{\\mathcal B_{π_k} V_k - \\mathcal B V_k} \\le ε.\\] Then, \\[ \\limsup_{k\\to ∞} \\NORM{V_{π_k} - V^*} \\le\n   \\frac{ε+2γδ}{(1-γ)^2}. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\n\nBoth approximate VI and approximate PI have similar error bounds (proportional to \\(1/(1-γ)^2\\).)\nWhen \\(ε = δ = 0\\), then Proposition 19.1 implies that \\(\\NORM{V_{π_{k+1}} - V^*} \\le γ \\NORM{V_{π_k} - V^*}\\). Thus, standard policy iteration has a geometeric rate of convergence (similar to value iteration), though in practice it converges much faster.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom Proposition 19.1 we have\n\\[ \\NORM{V_{π_{k+1}} - V^*} \\le γ \\NORM{V_{π_k} - V^*} + \\frac{ε + 2γδ}{(1-γ)}.\\]\nThe result follows from taking the limit \\(k \\to ∞\\).\n\n\n\n\nProposition 19.2 If the successive policies in approximate policy iteration converge (in general, it may not), i.e.  \\[ π_{k+1} = π_k = π,\n   \\quad \\text{for some $k$}. \\] Then, \\[ \\NORM{V_π - V^*} \\le \\frac{ε + 2γδ}{(1-γ)}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(V\\) be the approximate value function obtained at iteration \\(k\\), i.e., \\[\\NORM{V - V_π} \\le δ\n  \\quad\\text{and}\\quad\n  \\NORM{\\mathcal B_π V - \\mathcal V} \\le ε.\\]\nThen, from the triangle inequality, we have \\[\\begin{align*}\n  \\NORM{\\mathcal B V_π - V_π } &\\le\n  \\NORM{\\mathcal B V_π - \\mathcal B V} +\n  \\NORM{\\mathcal B V - \\mathcal B_π V} +\n  \\NORM{\\mathcal B_π V - \\mathcal B_π V_π} \\\\\n  &\\stackrel{(a)}\\le\n  γ\\NORM{V_π - V} + ε + γ \\NORM{V - V_π} \\\\\n  &\\stackrel{(b)}\\le\n  ε + 2γδ,\n\\end{align*}\\] where \\((a)\\) follows from the fact that \\(V_π = \\mathcal B_π V_π\\) and the contraction property and \\((b)\\) follows from the assumption on \\(V\\). Now, from the discounting property, we get the result.\n\n\n\n\nProposition 19.3 Suppose the successive value functions obtained by approximate policy iteration are “not too different”, i.e., \\[ \\NORM{V - V_π} \\le δ, \\quad\n   \\NORM{B_h V - \\mathcal B V} \\le ε,\n   \\quad\\text{and}\\quad\n   \\NORM{B_π V - \\mathcal B_h V} \\le ζ.\\] Then, \\[ \\NORM{V_π - V^*} \\le \\frac{ε + ζ + 2γδ}{(1-γ)}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe result follows by replacing \\(ε\\) in \\((a)\\) above by \\(ε+ζ\\)."
  },
  {
    "objectID": "approx-mdps/approx-DP.html#notes",
    "href": "approx-mdps/approx-DP.html#notes",
    "title": "19  Approximate dynamic programming",
    "section": "Notes",
    "text": "Notes\nThe results presented in this section are taken from Bertsekas (2013).\n\n\n\n\nBertsekas, D.P. 2013. Abstract dynamic programming. Athena Scientific Belmont. Available at: https://web.mit.edu/dimitrib/www/abstractdp_MIT.html."
  },
  {
    "objectID": "approx-mdps/policy-loss.html#approximate-bellman-update",
    "href": "approx-mdps/policy-loss.html#approximate-bellman-update",
    "title": "20  Upper bounds on policy loss",
    "section": "20.1 Approximate Bellman update",
    "text": "20.1 Approximate Bellman update\nThe definition of \\(π_{\\hat V}\\) assumes that we can perform a Bellman update exactly. Similar to the setup in approximate DP, suppose all we can guarantee is a policy \\(\\hat π\\) such that \\[\n\\| \\BELLMAN \\hat V - \\BELLMAN_{\\hat π} \\hat V\\| \\le ε\n\\] Then, we have the following.\n\nTheorem 20.2 \\[\n\\| V^* - V_{\\hat π}\\|_{∞}\n\\le\n\\frac{2 γ}{1-γ} \\| V^* - \\hat V\\|_{∞}\n+\n\\frac{ε}{1-γ}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom triangle inequality, we have \\[\\begin{align*}\n  \\| V^* - V_{\\hat π}\\|_{∞}\n  &\\le\n  \\| \\BELLMAN V^* - \\BELLMAN \\hat V \\|_{∞}\n  +\n  \\textcolor{red}{\\| \\BELLMAN \\hat V - \\BELLMAN_{\\hat π} \\hat V\\|_{∞}}\n  \\notag \\\\\n  & \\quad\n  +\n  \\| \\BELLMAN_{\\hat π} \\hat V - \\BELLMAN_{\\hat π} V^* \\|_{∞}\n  +\n  \\| \\BELLMAN_{\\hat π} V^* - \\BELLMAN_{\\hat π} V_{\\hat π} \\|_{∞}\n  \\\\\n  &\\le\n  γ \\| V^* - \\hat V \\|_{∞}\n  +\n  \\textcolor{red}{ε}\n  +\n  γ \\| \\hat V -  V^* \\|_{∞}\n  +\n  γ \\| V^* -  V_{\\hat π} \\|_{∞}\n\\end{align*}\\] The result follows from rearranging the terms."
  },
  {
    "objectID": "approx-mdps/policy-loss.html#policy-loss-for-q-learning",
    "href": "approx-mdps/policy-loss.html#policy-loss-for-q-learning",
    "title": "20  Upper bounds on policy loss",
    "section": "20.2 Policy loss for \\(Q\\)-learning",
    "text": "20.2 Policy loss for \\(Q\\)-learning\nA related setting is what happens in \\(Q\\)-learning. Suppose \\(Q^*\\) is the optimal action-value function and we obtain an approximation \\(\\hat Q\\). Let \\(π_{\\hat Q}\\) be the greedy policy with respect to \\(\\hat Q\\), i.e., \\[\n  π_{\\hat Q}(s) \\in \\arg \\min_{a \\in \\ALPHABET A} \\hat Q(s,a).\n\\] Then, we have the following.\n\nTheorem 20.3 \\[\n\\| V^* - V_{π_{\\hat Q}}\\|_{∞}\n\\le\n\\frac{2}{1-γ} \\| Q - \\hat Q\\|_{∞}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor ease of notation, we use \\(\\hat π\\) to denote \\(π_{\\hat Q}\\). Let \\(α = \\| Q - \\hat Q\\|_{∞}\\). Now consider \\[\\begin{align}\nV_{\\hat π}(s) - V^*(s)\n&=\nQ_{\\hat π}(s, \\hat π(s)) - Q^*(s, π^*(s))\n\\notag \\\\\n&\\le\nQ_{\\hat π}(s, \\hat π(s)) - Q^*(s, \\hat π(s)) + 2 α\n\\label{eq:policy-loss-QL-step1}\n\\end{align}\\] where the last inequality uses the fact that \\[\nQ^*(s, \\hat π(s)) - α\n\\le\n\\hat Q(s, \\hat π(s)) \\le Q^*(s, π^*(s))\n\\le\nQ^*(s, π^*(s)) + α.\n\\] Now observe that \\[\nQ_{\\hat π}(s, \\hat π(s)) - Q^*(s, \\hat π(s))\n=\nγ \\sum_{s' \\in \\ALPHABET S}P(s'|s, \\hat π(s)) [ V_{\\hat π}(s') - V^*(s') ]\n\\le\nγ \\| V_{\\hat π} - V^* \\|_{∞}.\n\\] Substituting this in \\eqref{eq:policy-loss-QL-step1} and rearranging the terms gives us the result."
  },
  {
    "objectID": "approx-mdps/policy-loss.html#notes",
    "href": "approx-mdps/policy-loss.html#notes",
    "title": "20  Upper bounds on policy loss",
    "section": "Notes",
    "text": "Notes\nThe material in this section is adapted from Singh and Yee (1994). The proof of Theorem 20.1 is from Tsitsiklis and Roy (1996).\n\n\n\n\nSingh, S.P. and Yee, R.C. 1994. An upper bound on the loss from approximate optimal-value functions. Machine Learning 16, 3, 227–233. DOI: 10.1007/bf00993308.\n\n\nTsitsiklis, J.N. and Roy, B. van. 1996. Feature-based methods for large scale dynamic programming. Machine Learning 22, 1-3, 59–94. DOI: 10.1007/bf00114724."
  },
  {
    "objectID": "approx-mdps/model-approximation.html#bounds-for-model-approximation",
    "href": "approx-mdps/model-approximation.html#bounds-for-model-approximation",
    "title": "21  Model approximation",
    "section": "21.1 Bounds for model approximation",
    "text": "21.1 Bounds for model approximation\n\n21.1.1 Bounding the error for value function approximation\nLet \\(\\BELLMAN^π\\) and \\(\\BELLMAN^*\\) denote the Bellman operator for policy \\(π\\) and the optimality Bellman operator for model \\(\\ALPHABET M\\). Let \\(\\hat {\\BELLMAN}^π\\) and \\(\\hat {\\BELLMAN}^*\\) denote the corresponding quantities for model \\(\\widehat {\\ALPHABET M}\\). Define the Bellman mismatch functionals \\(\\MISMATCH^π\\) and \\(\\MISMATCH^*\\) as follows: \\[\\begin{align*}\n  \\MISMATCH^π v &= \\| \\BELLMAN^π v - \\hat {\\BELLMAN}^π v \\|_∞,\n  \\\\\n  \\MISMATCH^* v &= \\| \\BELLMAN^* v - \\hat {\\BELLMAN}^* v \\|_∞ .\n\\end{align*}\\]\nAlso define the maximum Bellman mismatch as \\[\\begin{align*}\n  \\MISMATCH^{\\max} v &=\n  \\max_{(s,a) \\in \\ALPHABET S, A} \\biggl\\lvert\n    c(s,a) + γ \\sum_{s' \\in \\ALPHABET S} P(s'|s,a)v(s')  \\notag \\\\\n  & \\hskip 6em\n   -\\hat c(s,a) - γ \\sum_{s' \\in \\ALPHABET S} \\hat P(s'|s,a) v(s')\n   \\biggr\\rvert.\n\\end{align*}\\]\n\nLemma 21.1 The following inequalities hold:\n\n\\(\\sup_{π \\in Π} \\MISMATCH^π v = \\MISMATCH^{\\max} v\\)\n\\(\\MISMATCH^* v \\le \\MISMATCH^{\\max} v\\).\n\n\nThe Bellman mismatch functional can be used to bound the performance difference of a policy between the true and approximate models.\n\nLemma 21.2 For any (possibly randomized) policy \\(π\\), \\[\\begin{equation}\\label{eq:bound-0}\n   \\| V^{π} - \\hat V^{π} \\|_∞ \\le\n   \\frac{1}{1-γ} \\min\\{ \\MISMATCH^π V^{π}, \\MISMATCH^π \\hat V^{π} \\}.\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe bound the left hand side of \\eqref{eq:bound-0} in two ways. The first way is as follows: \\[\\begin{align}\n  \\| V^{π} - \\hat V^{π} \\|_∞\n  &=\n  \\| \\BELLMAN^π V^π - \\hat {\\ALPHABET  B}^π \\hat V^π \\|_∞\n  \\notag \\\\\n  &\\le\n  \\| \\BELLMAN^π V^π - \\hat {\\ALPHABET  B}^π V^π \\|_∞\n  +\n  \\| \\hat {\\BELLMAN}^π V^π - \\hat {\\ALPHABET  B}^π \\hat V^π \\|_∞\n  \\notag \\\\\n  &\\le\n  \\MISMATCH^π V^π + γ \\| V^π - \\hat V^π \\|_∞\n  \\label{eq:ineq-3}\n\\end{align}\\] where the first inequality follows from the triangle inequality, and the second inequality follows from the definition of the Bellman mismatch functional and the contraction property of Bellman operators. Rearranging terms in \\eqref{eq:ineq-3} gives us \\[\\begin{equation}\n\\| V^{π} - \\hat V^{π} \\|_∞ \\le \\frac{ \\MISMATCH^π V^{π}}{1 - γ}.\n\\label{eq:ineq-4}\\end{equation}\\] This gives the first bound.\nThe second bound is symmetric and obtained by interchanging the roles of \\(V^π\\) and \\(\\hat V^π\\). \\[\\begin{align}\n  \\| V^{π} - \\hat V^{π} \\|_∞\n  &=\n  \\| \\BELLMAN^π V^π - \\hat {\\ALPHABET  B}^π \\hat V^π \\|_∞\n  \\notag \\\\\n  &\\le\n  \\| \\BELLMAN^π V^π - \\ALPHABET  B^π \\hat V^π \\|_∞\n  +\n  \\| \\BELLMAN^π \\hat V^π - \\hat {\\ALPHABET  B}^π \\hat V^π \\|_∞\n  \\notag \\\\\n  &\\le\n  γ \\| V^π - \\hat V^π \\|_∞\n  +\n  \\MISMATCH^π \\hat V^π\n  \\label{eq:ineq-13}\n\\end{align}\\] Rearranging terms in \\eqref{eq:ineq-13} gives us \\[\\begin{equation}\n\\| V^{π} - \\hat V^{π} \\|_∞ \\le \\frac{ \\MISMATCH^π \\hat V^{π}}{1 - γ}.\n\\label{eq:ineq-14}\\end{equation}\\] This gives the second bound.\n\n\n\nSimilar to the above, we can also bound the difference between the optimal value function of the true and approximate models.\n\nTheorem 21.1 Let \\(V^*\\) and \\(\\hat V^*\\) denote the optimal value functions for \\(\\ALPHABET M\\) and \\(\\widehat {\\ALPHABET M}\\), respectively. Then, \\[\\begin{equation}\\label{eq:bound-1}\n    \\| V^* - \\hat V^* \\|_∞ \\le\n    \\frac{1}{1-γ} \\min\\{ \\MISMATCH^* V^*, \\MISMATCH^* \\hat V^* \\}\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof argument is almost the same as the proof argument for Lemma 21.2. The first was is as follows: \\[\\begin{align}\n  \\| V^{*} - \\hat V^{*} \\|_∞\n  &=\n  \\| \\BELLMAN^* V^* - \\hat {\\ALPHABET  B}^* \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  \\| \\BELLMAN^* V^* - \\hat {\\ALPHABET  B}^* V^* \\|_∞\n  +\n  \\| \\hat {\\BELLMAN}^* V^* - \\hat {\\ALPHABET  B}^* \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  \\MISMATCH^* V^* + γ \\| V^* - \\hat V^* \\|_∞\n  \\label{eq:ineq-1}\n\\end{align}\\] where the first inequality follows from the triangle inequality, and the second inequality follows from the definition of the Bellman mismatch functional and the contraction property of Bellman operators. Rearranging terms in \\eqref{eq:ineq-3} gives us \\[\\begin{equation}\n\\| V^* - \\hat V^* \\|_∞ \\le \\frac{  \\MISMATCH^* V^*}{1 - γ}.\n\\label{eq:ineq-2}\\end{equation}\\] This gives the first bound.\nThe second bound is symmetric and obtained by interchanging the roles of \\(V^*\\) and \\(\\hat V^*\\). \\[\\begin{align}\n  \\| V^{*} - \\hat V^{*} \\|_∞\n  &=\n  \\| \\BELLMAN^* V^* - \\hat {\\ALPHABET  B}^* \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  \\| \\BELLMAN^* V^* - \\ALPHABET  B^* \\hat V^* \\|_∞\n  +\n  \\| \\BELLMAN^* \\hat V^* - \\hat {\\ALPHABET  B}^* \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  γ \\| V^* - \\hat V^* \\|_∞\n  +\n  \\MISMATCH^* \\hat V^*\n  \\label{eq:ineq-11}\n\\end{align}\\] Rearranging terms in \\eqref{eq:ineq-11} gives us \\[\\begin{equation}\n\\| V^{*} - \\hat V^{*} \\|_∞ \\le \\frac{ \\MISMATCH^* \\hat V^{*}}{1 - γ}.\n\\label{eq:ineq-12}\\end{equation}\\] This gives the second bound.\n\n\n\n\n\n21.1.2 Bounding the error for policy approximation error\n\nTheorem 21.2 The policy \\(\\hat π^*\\) is an \\(α\\)-optimal policy of \\(\\ALPHABET M\\) where \\[\\begin{equation} \\label{eq:bound}\nα := \\| V^* - V^{\\hat π^*} \\|_∞ \\le\n\\frac{1}{1 - γ}\\bigl[ Δ_1 +Δ_2 \\bigr]\n\\end{equation}\\] where \\[\nΔ_1 \\le \\min\\{ \\MISMATCH^* V^*, \\MISMATCH^* \\hat V^* \\}\n\\quad\\text{and}\\quad\nΔ_2 \\le \\min\\{ \\MISMATCH^{\\hat π^*} V^{\\hat π^*}, \\MISMATCH^{\\hat π^*}\\hat V^{\\hat π^*} \\}.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom triangle inequality we have \\[\\begin{equation} \\label{eq:triangle-1}\n  \\| V^* - V^{\\hat π^*} \\|_∞ \\le\n  \\| V^* - \\hat V^{\\hat π^*} \\|_∞\n  +\n  \\| V^{\\hat π^*} - \\hat V^{\\hat π^*} \\|_∞.\n\\end{equation}\\] The result then follows by bounding the first term using Theorem 21.1 and the second term using Lemma 21.2.\n\n\n\nNote that depending on how we pick \\(\\MISMATCH_1\\) and \\(\\MISMATCH_2\\), we get four possible upper bounds and can bound \\(α\\) by the tightest of them, i.e., \\[\\begin{equation}\\label{eq:combined}\n  α \\le \\frac{1}{1 - γ} \\min\\bigl\\{\n  \\MISMATCH^* V^* + \\MISMATCH^{\\hat π^*} V^{\\hat π^*}, \\space\n  \\MISMATCH^* V^* + \\MISMATCH^{\\hat π^*} \\hat V^*, \\space\n  \\MISMATCH^* \\hat V^* + \\MISMATCH^{\\hat π^*} V^{\\hat π^*}, \\space\n  \\MISMATCH^* \\hat V^* + \\MISMATCH^{\\hat π^*} \\hat V^{*} \\bigr\\}\n\\end{equation}\\] where we have used the fact that \\(\\hat V^{\\hat π^*} = \\hat V^*\\) to simplify some of the terms.\nIdeally, we want an upper bound that just depends on the value function of true model or just on the value function of the approximate model. The last bound in \\eqref{eq:combined} depends just on the value function of the approximate model. We state it separately for ease of reference.\n\nCorollary 21.1 The policy \\(\\hat π^*\\) is an \\(α\\)-optimal policy of \\(\\ALPHABET M\\) where \\[\n    α := \\| V^* - V^{\\hat π^*} \\|_∞ \\le\n    \\frac{1}{1-γ} \\bigl[ \\MISMATCH^* \\hat V^* + \\MISMATCH^{\\hat\n    π^*} \\hat V^* \\bigr].\n\\] Moreover, since \\(\\MISMATCH^{\\max} \\hat V^*\\) is an upper bound for both \\(\\MISMATCH^{\\hat π^*} \\hat V^*\\) and \\(\\MISMATCH^* \\hat V^*\\), we have \\[\n    α \\le \\frac{2}{(1-γ)}  \\MISMATCH^{\\max}  \\hat V^*.\n\\]\n\nIn the next lemma, we develop a bound that depends just on the value function true model.\n\nTheorem 21.3 The policy \\(\\hat π^*\\) is an \\(α\\)-optimal policy of \\(\\ALPHABET M\\) where \\[\n    α := \\| V^* - V^{\\hat π^*} \\|_∞ \\le\n    \\frac{1}{1-γ} \\MISMATCH^{\\hat π^*} V^*\n    +\n    \\frac{(1+γ)}{(1-γ)^2} \\MISMATCH^* V^* .\n\\]\nMoreover, since \\(\\MISMATCH^{\\max} V^*\\) is an upper bound for both \\(\\MISMATCH^{\\hat π^*} V^*\\) and \\(\\MISMATCH^* V^*\\), we have \\[\n    α \\le \\frac{2}{(1-γ)^2}  \\MISMATCH^{\\max}  V^*.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs before, we consider the following bound by triangle inequality. \\[\\begin{equation} \\label{eq:triangle-2}\n  \\| V^* - V^{\\hat π^*} \\|_∞ \\le\n  \\| V^* - \\hat V^{\\hat π^*} \\|_∞\n  +\n  \\| V^{\\hat π^*} - \\hat V^{\\hat π^*} \\|_∞.\n\\end{equation}\\] We will bound the first term of \\eqref{eq:triangle-2} by (thm:value-bound?). But instead of bounding the second term of \\eqref{eq:triangle-2} by (lem:bound-policy?), we consider the following: \\[\\begin{align}\n  \\| V^{\\hat π^*} - \\hat V^{\\hat π^*} \\|_∞\n  &=\n  \\| V^{\\hat π^*} - \\hat V^{*} \\|_∞\n  = \\| \\BELLMAN^{\\hat π^*} V^{\\hat π^*} -\n       \\hat {\\BELLMAN}^{\\hat π^*} \\hat V^{*} \\|_∞\n  \\notag \\\\\n  &\\le \\| \\BELLMAN^{\\hat π^*} V^{\\hat π^*} -\n          \\BELLMAN^{\\hat π^*} V^{*} \\|_∞\n    +  \\| \\BELLMAN^{\\hat π^*} V^{*} -\n       \\hat {\\BELLMAN}^{\\hat π^*} V^{*} \\|_∞\n    +\n       \\| \\hat {\\BELLMAN}^{\\hat π^*} V^{*} -\n       \\hat {\\BELLMAN}^{\\hat π^*} \\hat V^{*} \\|_∞\n  \\notag \\\\\n  &\\le γ \\| V^* - V^{\\hat π^*} \\|_∞ + \\MISMATCH^{\\hat π^*} V^*\n  + γ \\| V^* - \\hat V^* \\|_∞\n  \\label{eq:ineq-21}.\n\\end{align}\\] where the first inequality follows from the triangle inequality and the second inequality follows from the definition of Bellman mismatch functional and contraction property of Bellman operator.\nSubstituting \\eqref{eq:ineq-21} in \\eqref{eq:triangle-2} and rearranging terms, we get \\[\\begin{align}\n  \\| V^* - V^{\\hat π^*} \\|_∞\n  &\\le\n  \\frac{1}{1-γ} \\MISMATCH^{\\hat π^*} V^*\n  +\n  \\frac{1+γ}{1-γ} \\| V^* - \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  \\frac{1}{1-γ} \\MISMATCH^{\\hat π^*} V^*\n  +\n  \\frac{(1+γ)}{(1-γ)^2} \\MISMATCH^* V^* .\n\\end{align}\\] where the second inequality follows from Theorem 21.1.\n\n\n\n\n\n\n\n\n\nRemark:\n\n\n\nNote that the bound of Corollary 21.1 is tighter by a factor of \\(1/(1-γ)\\), but this bound is in terms of \\(\\hat V^*\\). In some settings, we prefer a bound in terms of \\(V^*\\). Using Theorem 21.3 in such settings leads to scaling by \\(1/(1-γ)\\)."
  },
  {
    "objectID": "approx-mdps/model-approximation.html#example-state-aggregation-or-discretization-or-quantization",
    "href": "approx-mdps/model-approximation.html#example-state-aggregation-or-discretization-or-quantization",
    "title": "21  Model approximation",
    "section": "21.2 Example: State aggregation (or discretization or quantization)",
    "text": "21.2 Example: State aggregation (or discretization or quantization)\nConsider an MDP \\(\\ALPHABET M = \\langle \\ALPHABET S, \\ALPHABET A, p, c, γ \\rangle\\), with large (or possibly continuous) state space. For simplicity, we assume that \\(\\ALPHABET S\\) is continuous (and compact), and that the \\(p\\) is the density of the transition kernel. Note that we are using the term “probability density” in the engineering sense (so, it may be a combination of a continuous function and delta functions) rather than in the precise measure theoretic sense (where it is the Radon-Nikodym derivative with respect to the Lebesgue measure).\nIf exact computations were possible, we can find an optimal solution by solving a dynamic program. However, since the state space is continuous, we cannot compute the value functions exactly. The simplest way to proceed is to discretize or quantize the state space \\(\\ALPHABET S\\). In particular, let \\(\\{\\ALPHABET S_1, \\dots \\ALPHABET S_n\\}\\) denote a partition of \\(\\ALPHABET S\\) (i.e., \\(\\bigcup_{i=1}^n \\ALPHABET S_i = \\ALPHABET S\\) and for any \\(i \\neq j\\), \\(\\ALPHABET S_i \\cap \\ALPHABET S_j = \\emptyset\\)). Pick a representative point \\(\\hat s_i \\in \\ALPHABET S_i\\). We can think of the “grid points” \\(\\hat {\\ALPHABET S} = \\{\\hat s_1, \\dots, \\hat s_n\\}\\) as quantization of the state space \\(\\ALPHABET S\\). To simplify the notation, we define a quantization function \\(\\phi \\colon \\ALPHABET S \\to \\hat {\\ALPHABET S}\\) which maps all points in \\(\\ALPHABET S_i\\) to the representative element \\(\\hat s_i\\).\nWe consider a finite state MDP \\(\\widehat {\\ALPHABET M} = \\langle \\hat {\\ALPHABET S}, \\ALPHABET A, \\hat P, \\hat c, γ\\rangle\\), where \\(\\hat c\\) is the restriction of \\(c\\) onto \\(\\hat {\\ALPHABET S}\\), and \\(\\hat P\\) is given by\n\\[\\hat P(\\hat s_j | \\hat s_i, a) =\n\\int_{\\ALPHABET S_j} P(s' | \\hat s_i, a) dy = P(\\ALPHABET S_j | \\hat s_i, a).\n\\]\nSince model \\(\\widehat {\\ALPHABET M}\\) is discrete, we can find the optimal value function and policy using dynamic programming. We are interested in the questions of quantifying the errors in value approximation and policy approximation in this setup. Since the quantized model is defined on a state space \\(\\hat {\\ALPHABET S}\\) that is different from the original state space, the approximation bounds derived above are not directly applicable. Below we present two intermediate models, to be able to compare models \\(\\ALPHABET M\\) and \\(\\widehat {\\ALPHABET M}\\)\n\n21.2.1 An intermediate model\nDefine a model \\(\\overline {\\ALPHABET M} = \\langle \\ALPHABET S, \\ALPHABET A, \\bar p, \\bar c, γ \\rangle\\), where \\[\n  \\bar c(s,a) = c(\\phi(s),a)\n\\] and \\[\n  \\bar p(s'|s,a) = p(s'|\\phi(s),a).\n\\]\nWe have the following property for the \\(\\bar p\\) dynamics:\n\nLemma 21.3 For any \\(\\hat V \\colon \\hat {\\ALPHABET S} \\to \\reals\\), let \\(V \\colon \\ALPHABET S \\to \\reals\\) be its piecewise constant extrapolation from \\(\\hat {\\ALPHABET S}\\) to \\(\\ALPHABET S\\) (i.e., \\(V(s) = \\hat V(\\phi(s)))\\). Then, for any \\(\\hat s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), we have \\[\n\\int_{\\ALPHABET S_j} \\bar p(s'|\\hat s, a) V(s') ds'\n=\n\\hat P(\\hat s_j | \\hat s, a) \\hat V(\\hat s_j).\n\\] Therefore, \\[\n\\int_{\\ALPHABET S} \\bar p(s'|\\hat s, a) V(s') ds'\n=\n\\sum_{\\hat s' \\in \\hat {\\ALPHABET S}} \\hat P(\\hat s' | \\hat s, a) \\hat V(\\hat s_j).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\(V\\) is constant over \\(\\ALPHABET S_j\\), we have \\[\\begin{align*}\n  \\int_{\\ALPHABET S_j} \\bar p(s'|\\hat s,a) V(s') ds'\n  &=\n  V(\\hat s_j) \\int_{\\ALPHABET S_j} \\bar p(s'|\\hat s,a) ds'\n  \\notag \\\\\n  &= V(\\hat s_j) \\bar p(\\ALPHABET S_j | \\hat s, a)\n\\end{align*}\\] The first result now follows from observing that \\(\\bar p(\\ALPHABET S_j | \\hat s, a) = p(\\ALPHABET S_j | \\hat s, a) = \\hat P(\\hat s_j | \\hat s, a)\\). The second result follows from the fact that for any \\(f \\colon \\ALPHABET S \\to \\reals\\), \\[\n  \\int_{\\ALPHABET S} f(s')  ds' = \\int_{\\ALPHABET S_1} f(s') ds' + \\cdots +\n  \\int_{\\ALPHABET S_n} f(s') d(s').\n\\]\n\n\n\nLet \\(\\bar V^*\\), and \\(\\hat V^*\\) denote the optimal value functions of models \\(\\overline {\\ALPHABET M}\\) and \\(\\widehat {\\ALPHABET M}\\), respectively. Moreover, let \\(\\bar π^*\\) and \\(\\hat π^*\\) denote optimal policies of models \\(\\widehat {\\ALPHABET M}\\), and \\(\\overline {\\ALPHABET M}\\), respectively. Then, these are related as follows.\n\nLemma 21.4 \\(\\bar V^*\\) is a piecewise constant extrapolation of \\(\\hat V^*\\) from \\(\\hat {\\ALPHABET S}\\) to \\(\\ALPHABET S\\). Moreover, the piecewise constant extrapolation of \\(\\hat π^*\\) is optimal for model \\(\\overline {\\ALPHABET M}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(\\bar V\\) be the piecewise constant extrapolation of \\(\\hat V^*\\) from \\(\\hat {\\ALPHABET S}\\) to \\(\\ALPHABET S\\). Lemma 21.3 implies that for any \\(s \\in \\ALPHABET S\\), the one step Bellman update of \\(\\bar V\\) (in model \\(\\overline {\\ALPHABET M}\\)) is given by \\[\\begin{align*}\n  \\min_{a \\in \\ALPHABET S} \\biggl\\{\n    c(\\phi(s),a) + γ \\int_{\\ALPHABET S} \\bar p(s'|\\phi(s),a) \\bar V(s') ds'\n  \\biggr\\}\n  &=\n  \\min_{a \\in \\ALPHABET S} \\biggl\\{\n    \\hat c(\\phi(s),a) + γ \\sum_{\\hat s' \\in \\hat {\\ALPHABET S}}\n    \\hat P(\\hat s' | \\phi(s), a) \\hat V^*(\\hat s')\n  \\biggr\\}\n  \\\\\n  &=\n  \\hat V^*(\\phi(s))\n  \\\\\n  &= \\bar V(s)\n\\end{align*}\\] where the first equality follows from Lemma 21.3, the second equality follows from the fact that \\(\\hat V^*\\) is optimal for model \\(\\widehat {\\ALPHABET M}\\), and the last equality follows from the definition of \\(\\bar V\\). Thus, \\(\\bar V\\) is the fixed point of the Bellman optimality equation for model \\(\\overline {\\ALPHABET M}\\). Hence, \\(\\bar V = \\bar V^*\\).\nThe above equation also implies that the policy \\(\\hat π^*(\\phi(s))\\), which achieves the arg min in the second equality, is optimal for model \\(\\overline{\\ALPHABET M}\\).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nLemma 21.4 implies that the questions of value and policy approximation for model \\(\\widehat {\\ALPHABET M}\\) is the same as those for model \\(\\overline {\\ALPHABET M}\\). The advantage of working with model \\(\\overline {\\ALPHABET M}\\) is that it is defined on the same state space as model \\(\\ALPHABET M\\), so we can directly use the results of Theorem 21.2 and Theorem 21.3.\n\n\n\n\n21.2.2 Approximation bounds\nNow to bound the value error between models \\(\\ALPHABET M\\) and \\(\\overline {\\ALPHABET M}\\), we need to compute \\(\\MISMATCH^* V^*\\), which is given by \\[\\begin{align*}\n  \\MISMATCH^* V^* &=\n  \\max_{s \\in \\ALPHABET S}\n  \\biggl|\n    \\min_{a \\in \\ALPHABET A} \\bigl\\{\n    c(s,a) + γ \\int_{\\ALPHABET S} p(s'|s,a) V^*(s')ds'\n    \\bigr\\}\n    \\notag \\\\\n  &  \\hskip 4em -\n    \\min_{a \\in \\ALPHABET A} \\bigl\\{\n   c(\\phi(s), a) + γ \\int_{\\ALPHABET S} p(s'|\\phi(s), a) V^*(s') ds'\n    \\bigr\\}\n  \\biggr|\n  \\notag \\\\\n  &= \\max_{s \\in \\ALPHABET S} \\bigl| V^*(s) - V^*(\\phi(s)) \\bigr|\n  =: H_{\\max}.\n\\end{align*}\\]\nTherefore, from Theorem 21.1, we have\n\nProposition 21.1 \\[ \\| V^* - \\bar V^{\\bar π^*} \\|_∞ \\le \\frac{H_{\\max}}{1-γ}. \\]\n\nTo bound the policy error, we also need to compute \\(\\MISMATCH^{\\hat π^*} V^*\\). In order to compute this bound, we assume that the model is \\((L_c, L_p)\\) Lipschitz.\n\nAssumpt. #ass:lip\n\n\nFor every \\(a \\in \\ALPHABET A\\), \\(c(s, a)\\) is \\(L_c\\)-Lipschitz in \\(s\\)\nFor every \\(a \\in \\ALPHABET A\\), \\(p(\\cdot | s, a)\\) is \\(L_p\\)-Lipschitz in \\(s\\) (with respect to the Kantorovich distance on probability measures).\n\n\n\nUnder this assumption, we have\n\\[\\begin{align*}\n  \\MISMATCH^{\\bar π^*} V^* &\\le \\max_{s \\in \\ALPHABET S}\n  \\sum_{a \\in \\ALPHABET A} \\bar π^*(a | s) \\biggl|\n    c(s,a) + γ \\int_{\\ALPHABET S} p(s'|s,a) V^*(s')ds'\n  \\notag \\\\\n  &  \\hskip 4em -\n    c(\\phi(s), a) - γ \\int_{\\ALPHABET S} p(s'|\\phi(s), a) V^*(s') ds'\n  \\biggr|\n  \\notag \\\\\n  &\\le  L_c \\max_{s \\in \\ALPHABET S}d(s, \\phi(s))\n  +\n  γ K(p(\\cdot | s, a), p(\\cdot | \\phi(s), a)) \\NORM{V^*}_L )\n  \\notag \\\\\n  &\\le ( L_c  + γ L_p \\NORM{V^*}_L )\n  \\underbrace{\\max_{s \\in \\ALPHABET S}d(s, \\phi(s)) }_{=: D}\n\\end{align*}\\] where the second inequality uses Proposition 16.2.\nThus, we have the following:\n\n\nProposition #prop:policy-aggregation\n\nUnder (ass:lip?), we have \\[\\NORM{V^* - V^{\\bar π^*}}_∞ \\le\n  \\frac{D}{1-γ} \\biggl[ L_c + γ L_p \\NORM{V^*}_L +\n  \\frac{1 + γ}{1-γ} \\NORM{V^*}_L \\biggr]. \\]\nFurthermore, if \\(γ L_p &lt; 1\\), then from Theorem 1 of Lipschitz MDPs we know that \\(\\NORM{V^*}_L \\le L_c/(1- γ L_p)\\). Thus, \\[\\NORM{V^{μ^*} - V^*}_∞ \\le\n  \\frac{2 D L_c }{ (1-γ)^2 (1-γ L_p) }. \\]\n\n\n\n\n\n\n\nProof\n\n\nThe first bound follows from (thm:policy-bound2?), with the additional observation that \\(H_{\\max} \\le \\NORM{V^*}_L D\\).\nFor the second result follows from simple algebra."
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-utility.html#a-simple-lqg-example",
    "href": "risk-sensitive/risk-sensitive-utility.html#a-simple-lqg-example",
    "title": "22  Risk Sensitive Utility",
    "section": "22.1 A simple LQG example",
    "text": "22.1 A simple LQG example\nSuppose \\(x \\in \\reals\\) is the distance of an object from its desired position and the application of a control \\(u \\in \\reals\\) will bring it to \\(x - u\\). Suppose the cost of this maneuver is \\[\n  C =  \\tfrac{1}{2}[ R u^2 + S (x-u)^2] .\n\\]\nHere, the two terms represent the cost of control and the final displacement from the desired position. Elementary calculus shows that the optimal value of \\(u\\) and the minimum cost are \\[\n  u = \\frac{S x}{S + R },\n  \\qquad\n  V(x) = \\frac{1}{2} \\cdot \\frac{RS x^2}{S + R}.\n\\]\nNow suppose there is noise so that \\(x- u\\) is replaced by \\(x - u + w\\). We’ll assume that \\(w \\sim {\\cal N}(0, Σ)\\). The cost then becomes \\[\n  C =  R u^2 + S (x-u + w)^2 .\n\\]\nIn the risk neutral case, the optimal control is same as earlier and the minimum cost \\(V(x)\\) simply increases by \\(\\frac12 SΣ\\). This a special case of a general phenomenon known as certainty equivalence. See the notes of linear quadratic regulator for details.\nNow consider a risk-sensitive version of the problem, in which \\(u\\) is chosen to minimize \\[\n  C_θ =  \\frac{1}{θ} \\log \\EXP[ \\exp(θ C) ].\n\\]\nIn the risk-averse case (i.e., \\(θ &gt; 0\\)), minimizing \\(C_θ\\) is equivalent to minimizing \\[ \\begin{equation} \\label{eq:cost}\n\\EXP[ \\exp(θ C)] =\n\\int \\exp\\Bigl( \\frac{θ}{2} \\Bigl( Ru^2 + S(x-u+w)^2 - \\frac{w^2}{θΣ}\\Bigr)\\Bigr) dw.\n\\end{equation} \\] Let us write the right hand side as \\(\\int \\exp(\\frac{1}{2} θQ((x,u), w) dw\\). Note that \\[\n  \\frac{∂^2 Q((x,u), w)}{∂w^2} = S - \\frac{1}{θΣ}.\n\\] Therefore, \\(Q\\) is negative definite in \\(w\\) if \\(S - 1/θΣ &lt; 0\\), or equivalently (recall \\(θ &gt; 0\\)), \\[\\begin{equation} \\label{eq:critical}\n  θΣS - 1 &lt; 0\n  \\iff\n   0 &lt; θ &lt; \\frac{1}{SΣ}.\n\\end{equation} \\] For now, we assume that \\(θΣS &lt; 1\\) and we will return to what happens when \\(θΣS = 1\\) later.\nSince \\(Q\\) is negative definite in \\(w\\) (and \\(θ &gt; 0\\)), \\(-\\frac{1}{2}θQ((x,u),w))\\) is positive definite in \\(w\\). Therefore, by using Lemma 22.1 in the appendix, we know that \\[ \\begin{equation} \\label{eq:simplify}\n  \\int\\exp\\Bigl( \\frac{θ}{2} Q((x,u),w) \\Bigr) dw\n  = \\sqrt{\\frac{2π (1 - θΣS)}{Σ}}\n  \\exp\\Bigl( \\frac{θ}{2} \\max_{w}Q((x,u),w) \\Bigr).\n\\end{equation} \\] Now, the maximizing value of \\(w\\) is \\(-\\frac{θΣS}{1 - θΣS}(x-u)\\) and therefore we get \\[\n  \\max_{w} Q((x,u), w) = R u^2 + \\frac{S}{1-θΣS}(x-u)^2\n\\]\nSubstituting this base in \\eqref{eq:simplify} and then in \\eqref{eq:cost}, we get \\[\n  \\EXP[\\exp(θC)]\n  = \\sqrt{\\frac{2π (1 - θΣS)}{Σ}}\n  \\exp\\Bigl(\\frac{θ}{2}\\Bigl(R u^2 + \\frac{S}{1 -\n  θΣS}(x-u)^2\\Bigr).\n\\]\nNow, minimizing \\(\\EXP[\\exp(θC)]\\) is same as minimizing the term in coefficient of \\(θ/2\\) (recall \\(θ\\) is positive), which is minimized by \\[\n  u = \\frac{Sx}{S + R - θΣSR}.\n\\] The corresponding minimum value of effective cost is \\[\n  V_θ(x) =\n  \\frac{1}{2} \\cdot \\frac{RS x^2}{R + S - θΣSR}\n  + \\frac{1}{2θ} \\log\\frac{2π (1 - θΣS)}{Σ}.\n\\]\nNote that both the expression for control action and the value become infinity as \\(θ\\) increases through the critical value: \\[\n  θ_{\\text{crit}} =  \\frac{1}{Σ}\\left( \\frac{1}{S} + \\frac{1}{R} \\right)\n\\] First note that for \\(θ &lt; θ_{\\text{crit}}\\), the constraint \\eqref{eq:critical} is automatically satisfied. The value \\(θ = θ_{\\text{crit}}\\) marks a point at which the decision maker is so pessimistic that his apprehension of uncertainties completely overrides the assurances given by known statistical behavior. This is called neurotic breakdown. There is a corresponding optimistic extreme, euphoria, if the cost function contains quadratic reward terms.\n\n\n\n\n\n\nRemark\n\n\n\nWhittle calls the term \\(Q((x,u),w)\\) as the stress. Note that in the above calculations, we choose \\(u\\) to minimize the stress and choose \\(w\\) to maximize the stress. It is as though there is an another agent, the “phantom other”, who exerts the control \\(w\\) at the same time as the optimizer exerts the control \\(u\\). When \\(θ\\) is negative, then the phantom other is opposing the optimizer and trying to maximize the stress. (Note that the minimizing value of \\(w\\) is \\(-\\frac{θΣS}{1 - θΣS}(x-u)\\), which can also be written as \\(θΣRu\\)). So, what started out as a one-person control problem has turned into a two-person game."
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-utility.html#appendix",
    "href": "risk-sensitive/risk-sensitive-utility.html#appendix",
    "title": "22  Risk Sensitive Utility",
    "section": "Appendix",
    "text": "Appendix\n\nLemma 22.1 Suppose that \\(Q(z,w)\\) is a quadratic function of vectors \\(z\\) and \\(w\\), positive definite in \\(w\\).\nLet \\(Q_{ww} = ∂^2 Q(z,w)/∂w^2\\). Since \\(Q(z,w)\\) is a quadratic function, \\(Q_{ww}\\) does not depend on \\(z\\). Since \\(Q\\) is positive definite in \\(w\\), \\(Q_{ww} &gt; 0\\).\nSuppose \\(w \\in \\reals^r\\). Define \\(q = \\log[ (2π)^{r/2} \\det(Q_{ww})^{-1/2}]\\). Then, for a fixed value of \\(z\\) \\[\n  \\int \\exp\\bigl[ -Q(z,w)\\bigr] dw = \\exp\\bigl[ q - \\inf_{w \\in \\reals^r}\n  Q(z,w) \\bigr].\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThe point of the lemma is that, if one replaces an integration with respect to \\(w\\) by a minimization of \\(Q\\) with respect to \\(w\\), then the result is correct as far as terms dependent on the second argument \\(z\\) are concerned.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor the fixed value of \\(z\\), let \\(\\hat w\\) be the minimizing value of \\(Q(z,w)\\). Then, one can write\n\\[ Q(z,w) = Q(z, \\hat w) + \\tfrac 12 (w-\\hat w)^\\TRANS Q_{ww} (w - \\hat w). \\]\nThe result follows from substituting this in the left hand side of the expression in the Lemma and observing that (e.g., from the form of the density function of a multi-nominal Gaussian),\n\\[\\begin{equation}\n  \\int \\exp[ - \\tfrac 12 (w - \\hat w)^\\TRANS Q_{ww} (w - \\hat w) ] dw\n  = \\exp[-q].\n\\end{equation}\\]"
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-utility.html#notes",
    "href": "risk-sensitive/risk-sensitive-utility.html#notes",
    "title": "22  Risk Sensitive Utility",
    "section": "Notes",
    "text": "Notes\nThe material in this section is taken from Whittle (2002).\n\n\n\n\nWhittle, P. 2002. Risk sensitivity, A strangely pervasive concept. Macroeconomic Dynamics 6, 1, 5–18. DOI: 10.1017/s1365100502027025."
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-mdps.html#finite-horizon-setup",
    "href": "risk-sensitive/risk-sensitive-mdps.html#finite-horizon-setup",
    "title": "23  Risk Sensitive MDPs",
    "section": "23.1 Finite horizon setup",
    "text": "23.1 Finite horizon setup\nConsider an MDP with state space \\(\\ALPHABET X\\), action space \\(\\ALPHABET U\\), per-step cost \\(c \\colon \\ALPHABET X × \\ALPHABET U \\to \\reals\\), and controlled transition matrix \\(P\\). However, instead of the risk neutral optimization criteria that we consider previously, we consider a risk-sensitive objective. In particular, the performance of any (possibly non-Markovian) strategy \\(g = (g_1, \\dots, g_T)\\) is given by \\[\n  \\bar J_θ(g) = \\frac{1}{θ} \\log \\EXP\\Bigl[ \\exp\\Bigl(\n    θ \\sum_{t=1}^T c(X_t, U_t)\n  \\Bigr) \\Bigr].\n\\]\nRecall that this is the effective cost for an exponential disutility function. Note that \\(J_θ(g) = \\exp(θ \\bar J(g))\\) may be viewed as a multiplicative cost. Based on the argument for multiplicative cost, we can write the dynamic program for \\(J_θ(g)\\) as follows.\n\n\n\n\n\n\nDynamic program\n\n\n\nInitialize \\(V_{T+1}(x) = 1\\) and recursively compute\n\\[ \\begin{align}\n    Q_t(x,u) &= \\exp(θ c(x,u)) \\sum_{y \\in \\ALPHABET X} P_{xy}(u) V_{t+1}(y),\n    \\label{eq:DP-1}\\\\\n    V_t(x) &= \\min_{u \\in \\ALPHABET X} Q_t(x,u).\n    \\label{eq:DP-2}\n    \\end{align} \\]\nOr, equivalently, working with the effective cost value function:\n\\[ \\begin{align}\n    Q_t(x,u) &= \\exp(θ c(x,u)) \\sum_{y \\in \\ALPHABET X} P_{xy}(u)\n    \\exp(θ \\bar V_{t+1}(y)),\n    \\\\\n    \\bar V_t(x) &= \\frac{1}{θ} \\log \\bigl( \\min_{u \\in \\ALPHABET X} Q_t(x,u) \\bigr).\n    \\end{align} \\]\n\n\nThe dynamic program of \\eqref{eq:DP-1}–\\eqref{eq:DP-2} can be made to resemble the standard dynamic program by defining the disutility matrix \\[\n    D_{xy}(u) = \\exp(θ c(x,u)) P_{xy}(u).\n\\] Note that the elements of \\(D\\) are non-negative. The expression \\eqref{eq:DP-1} can then be written in “standard” form: \\[\n  Q_{t+1}(x) = \\sum_{y \\in \\ALPHABET X} D_{xy}(u) V_{t+1}(y).\n\\]"
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-mdps.html#infinite-horizon-average-cost-setup",
    "href": "risk-sensitive/risk-sensitive-mdps.html#infinite-horizon-average-cost-setup",
    "title": "23  Risk Sensitive MDPs",
    "section": "23.2 Infinite horizon average cost setup",
    "text": "23.2 Infinite horizon average cost setup\nThe objective for infinite-horizon average cost setup is to minimize: \\[\n  J^* = \\inf_{g} \\limsup_{T \\to ∞}\n  \\frac{1}{θT} \\EXP \\Bigl[ \\exp\\Bigl(\n  θ \\sum_{t=1}^T c(X_t, U_t)\n  \\Bigr) \\Bigr].\n\\]\nWhen the matrix \\(P(u)\\) is irreducible and aperiodic for each \\(u\\), the dynamic program for average cost MDP can be written as follows:\n\nTheorem 23.1 Suppose there exist constant \\(J\\) and a bounded function \\(v \\colon \\ALPHABET X \\to \\reals\\) such that \\[ \\begin{equation} \\label{eq:avg}\n  \\exp(θ (J + v(x))) =\n  \\min_{u \\in \\ALPHABET U}\n   \\sum_{y \\in \\ALPHABET X} P_{xy}(u) \\exp( θ( c(x, u) + v(y)))\n\\end{equation} \\] Furthermore, let \\(g^* \\colon \\ALPHABET X \\to \\ALPHABET U\\) denote the policy that achieves the arg min in the right hand side. Then, \\(J\\) is the optimal performance and the time-homogeneous policy \\(g^*\\) achieves that performance.\n\nThe dynamic program of \\eqref{eq:avg} can be written in an alternative form using a Legendre-type transformation and the duality relationship between relative entropy function and the logarithmic moment generating function.\nLet \\(Δ(\\ALPHABET X)\\) denote the set of probability vectors on \\(\\ALPHABET X\\). Then, for any \\(ν \\in Δ(\\ALPHABET X)\\), the relative entropy \\(I(\\cdot \\| ν) \\colon Δ(\\ALPHABET X) \\to \\reals \\cup \\{+∞\\}\\) is by given by \\[\n  I(μ \\| ν) = \\begin{cases}\n    \\sum_{x \\in \\ALPHABET X} \\log(λ(x)) μ(x),\n    & \\text{if } μ \\ll ν, \\\\\n    +∞, & \\text{otherwise}.\n  \\end{cases} \\] where \\[ λ(x) = \\begin{cases}\n  \\frac{μ(x)}{ν(x)}, & \\text{if } ν(x) \\neq 0, \\\\\n  1, & \\text{otherwise}.\n\\end{cases}\\]\n\nLemma 23.1 Let \\(w \\colon \\ALPHABET X \\to \\reals\\) be a bounded function and \\(ν \\in Δ(\\ALPHABET X)\\). Then, \\[\n  \\log \\sum_{x \\in \\ALPHABET X} ν(x) \\exp( w(x)) =\n  \\sup_{μ \\in Δ(\\ALPHABET X)} \\Bigl\\{\n     \\sum_{x \\in \\ALPHABET X}  μ(x) w(x) -\n    I(μ \\| ν)\n  \\Bigr\\},\n\\] where the supremum is attained at the unique probability measure \\(μ^*\\) given by \\[\n  μ^*(x) = \\frac{e^{θv(x)}}{\\int e^{θv(x)}ν(x) dx} ν(x).\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nSuch a dual-representation is a fundamental property of all coherent risk measures and not just the entropic risk measure that we are working with here. See, for example, Föllmer and Schied (2010).\n\n\nUsing Lemma 23.1, the dynamic program of \\eqref{eq:avg} can be written as \\[ \\begin{equation}\n  J + v(x) = \\min_{u \\in \\ALPHABET U} \\sup_{μ \\in Δ(\\ALPHABET X)}\n  \\Bigl\\{\n    c(x,u) + \\sum_{y \\in \\ALPHABET X} μ(y) v(y) - \\frac{1}{θ}\n    I(μ \\| P(\\cdot | x, u) )\n  \\Bigr\\}.\n\\end{equation} \\] This equation corresponds to the Issacs equation associated with a stochastic dynamic game with average cost-per unit time criterion."
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-mdps.html#infinite-horizon-discounted-cost-setup",
    "href": "risk-sensitive/risk-sensitive-mdps.html#infinite-horizon-discounted-cost-setup",
    "title": "23  Risk Sensitive MDPs",
    "section": "23.3 Infinite horizon discounted cost setup",
    "text": "23.3 Infinite horizon discounted cost setup\nWhittle (2002) has a discussion on why discounted cost setup for risk-sensitive MDP is tricky and the solution depends on the interpretation of discounting."
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-mdps.html#notes",
    "href": "risk-sensitive/risk-sensitive-mdps.html#notes",
    "title": "23  Risk Sensitive MDPs",
    "section": "Notes",
    "text": "Notes\nThe basic risk-sensitive MDP was first considered in Howard and Matheson (1972). See Howard and Matheson (1972) for a policy iteration algorithm. It is also mentioned that a version of this result is presented in Bellman’s book. See Hernandez-Hernández and Marcus (1996) and Hernández-Hernández (1999) for a detailed treatment of the average cost case.\n\n\n\n\n\nFöllmer, H. and Schied, A. 2010. Convex risk measures. In: Encyclopedia of quantitative finance. American Cancer Society. DOI: 10.1002/9780470061602.eqf15003.\n\n\nHernandez-Hernández, D. and Marcus, S.I. 1996. Risk sensitive control of markov processes in countable state space. Systems & Control Letters 29, 3, 147–155. DOI: 10.1016/s0167-6911(96)00051-5.\n\n\nHernández-Hernández, D. 1999. Existence of risk-sensitive optimal stationary policies for controlled markov processes. Applied Mathematics and Optimization 40, 3, 273–285. DOI: 10.1007/s002459900126.\n\n\nHoward, R.A. and Matheson, J.E. 1972. Risk-sensitive markov decision processes. Management Science 18, 7, 356–369. DOI: 10.1287/mnsc.18.7.356.\n\n\nWhittle, P. 2002. Risk sensitivity, A strangely pervasive concept. Macroeconomic Dynamics 6, 1, 5–18. DOI: 10.1017/s1365100502027025."
  },
  {
    "objectID": "linear-systems/lqr.html#completion-of-squares",
    "href": "linear-systems/lqr.html#completion-of-squares",
    "title": "24  Linear quadratic regulation",
    "section": "24.1 Completion of squares",
    "text": "24.1 Completion of squares\nWe start from a simple observation.\n\nLemma 24.1 (Completion of squares) Let \\(x \\in \\reals^n\\), \\(u \\in \\reals^m\\), and \\(w \\in \\reals^n\\) be random variables defined on a common probability space. Suppose \\(w\\) is zero mean with finite covariance and independent of \\((x,u)\\). Let \\(x_{+} = Ax + Bu + w\\), where \\(A\\) and \\(B\\) are matrices of appropriate dimensions. Then, given matrices \\(P\\), \\(Q\\), \\(S\\), and \\(R\\) of appropriate dimensions, \\[\n\\EXP\\left[\n\\MATRIX{x \\\\ u}^\\TRANS \\MATRIX{Q & S \\\\ S^\\TRANS & R } \\MATRIX{ x \\\\ u}\n+ x_{+} P x_{+} \\right]\n=\n\\EXP\\bigl[ x^\\TRANS P_{+} x  \n    +\n    (u + Kx)^\\TRANS Δ (u + Kx)\n    +\n    w^\\TRANS P w\n    \\bigr].\n\\] where\n\n\\(Δ = R + B^\\TRANS P B\\)\n\\(K = Δ^{-1}[ S^\\TRANS + B^\\TRANS P A ]\\)\n\\(P_{+} = Q + A^\\TRANS P A - K^\\TRANS Δ K\\)\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\(w\\) is zero mean and independent of \\((x,u)\\), we have \\[\n  \\EXP[ x_{+}^\\TRANS P x ]\n  =\n  \\EXP\\bigl[ (Ax + Bu)^\\TRANS P (Ax + Bu)\n      + w^\\TRANS P w \\bigr].\n\\]\nThe proof follows immediately by completing the square on the left hand side. In particular \\[\\begin{align*}\n& u^\\TRANS R u + 2 x^\\TRANS S u +  (Ax+Bu)^\\TRANS P (Ax + Bu) \\\\\n& \\quad = u^\\TRANS (R + B^\\TRANS S B) u + 2 u^\\TRANS( S^\\TRANS + B^\\TRANS P A) x\n  + x^\\TRANS A^\\TRANS P A x \\\\\n& \\quad = u^\\TRANS Δ u + 2 u^\\TRANS Δ K x + x^\\TRANS A^\\TRANS P A x \\\\\n& \\quad =\n  (u + K x)^\\TRANS Δ (u + Kx) - x^\\TRANS K^\\TRANS Δ K x\n  +  x^\\TRANS A^\\TRANS P A x \\\\\n\\end{align*}\\]\n\n\n\n\nDefinition 24.1 Given the system model \\((A,B)\\) and per-step cost \\((Q,S,R)\\), we define the Riccati operator \\(\\RICCATI \\colon \\mathbb{S}^{n × n}_{\\ge 0} \\to \\mathbb{S}^{n × n}_{\\ge 0}\\) as follows: \\[ \\RICCATI P = Q + A^\\TRANS P A\n- (S^\\TRANS + B^\\TRANS P A)^\\TRANS (R + B^\\TRANS P B)^{-1}\n  (S^\\TRANS + B^\\TRANS P A).\\] Moreover, define the Gain operator \\(\\GAIN \\colon \\mathbb{S}^{n × n}_{\\ge 0} \\to \\reals^{m × n}\\) as \\[ \\GAIN P = - (R + B^\\TRANS P B)^{-1}(S^\\TRANS + B^\\TRANS P A). \\]\n\nNote that with the above notation, the terms defined in Lemma 24.1 may be written as \\[ P_{+} = \\RICCATI P\n\\quad\\text{and}\\quad\nK = \\GAIN P. \\]\n\n\n\n\n\n\nRiccati equations\n\n\n\nRiccati equations are named after Jacopo Riccati (1670–1754) who studied the differential equations of the form \\[\\dot x = a x^2 + b t + c t^2\\] and its variations. In continuous time, such equations arise in optimal control and filtering. The discrete-time version of these equations are also named after Riccati.\nI am calling the updates in Lemma 24.1 as Riccati operators because they are similar to Bellman operators considered earlier.\n\n\n\n\n\n\n\n\nAlternative forms of the Riccati operator\n\n\n\nFor the special case when \\(S = 0\\) (i.e., no cross terms in the cost), the Riccati operator is given by: \\[\n  \\RICCATI P = Q + A^\\TRANS P A -\n  A^\\TRANS P B (R + B^\\TRANS P B)^{-1} B^\\TRANS P A\n\\] The following are equivalent to the Riccati operator:\n\n\\(A^\\TRANS P(I + B R^{-1} B^\\TRANS P)^{-1}A + Q\\).\n\\(A^\\TRANS(P^{-1} + B R^{-1} B^\\TRANS)^{-1} A + Q\\).\n\nHere the first equality follows from the simplified Sherman-Morrison-Woodbudy formula for \\((I + (B R^{-1})(B^\\TRANS P))^{-1}\\) and the second follows simple algebra.\n\n\n\nProposition 24.1 Recursively define the matrices \\(\\{P_t\\}_{t \\ge 1}\\) in a backwards manners as follows: \\(P_T = Q_T\\) and then for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[\\begin{equation}\\label{eq:riccati}\n  P_t = \\RICCATI P_{t+1}.\n\\end{equation}\\] Moreover, define the gains \\(\\{K_t\\}_{t \\ge 1}\\) as: \\[\\begin{equation}\\label{eq:gain}\n  K_t = \\GAIN P_t.\n\\end{equation}\\]\nThen, for any control policy \\(g\\), the total cost \\(J(g)\\) given by \\(\\eqref{eq:cost}\\) may be written as \\[\\begin{equation}\n  J(g) = \\bar J(g) + \\tilde J\n\\end{equation}\\] where the controlled part of the cost is \\[ \\bar J(g) =\n  \\EXP\\biggl[ \\sum_{t=1}^{T-1}\n  (u_t + K_t x_t)^\\TRANS Δ_t (u_t + K_t x_t) \\biggr]\n\\] with \\(Δ_t = R + B^\\TRANS P_{t+1} B\\) and the control-free part of the cost is \\[\n  \\tilde J =\n  \\EXP\\biggl[ x_1^\\TRANS P_1 x_1 +\n  \\sum_{t=1}^{T-1} w_t^\\TRANS P_{t+1} w_t \\biggr]\n\\]\n\n\n\n\n\n\n\nDiscrete Riccati equation\n\n\n\nWe will use the notation: \\[\n  K_{1:T-1} = \\LQR_T(A,B,Q,R;Q_T)\n\\] to denote the LQR gains \\(K_{1:T-1}\\) computed via \\(\\eqref{eq:riccati}\\) and \\(\\eqref{eq:gain}\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe result follows by repeatedly applying Lemma 24.1 starting at time \\(t = T-1\\) and moving backwards.\n\n\n\nNow, an immediate implication of Proposition 24.1 is the following:\n\nTheorem 24.1 The optimal control policy for the LQR problem \\(\\eqref{eq:cost}\\) is given by \\[\n    u_t = - K_t x_t\n  \\] where the feedback gains \\(\\{K_t\\}_{t \\ge 1}\\) are computed as described in Proposition 24.1. The performance of the optimal strategy is given by: \\[\n      J^* = \\tilde J = \\TR(X_1 P_1) + \\sum_{t=1}^{T-1} \\TR(W_t P_{t+1})\n  \\] where \\(X_1\\) is the covariance of the initial state and \\(\\{W_t\\}_{t \\ge 1}\\) is the covariance of the noise process.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFirst observe that \\(\\tilde J\\) does not depend on the control policy. So, minimizing \\(\\bar J(g)\\) is the same as minimizing \\(J(g)\\).\nNow recall that \\(\\{R_t\\}_{t \\ge 1}\\) are positive definite. Therefore, \\(Δ_t = [R_t + B^\\TRANS P B]\\) are positive definite. Hence, for any policy \\(g\\), \\(\\bar J(g) \\ge 0\\). The proposed policy achieves \\(\\bar J(g) = 0\\) and is, therefore, optimal."
  },
  {
    "objectID": "linear-systems/lqr.html#salient-features-of-the-result",
    "href": "linear-systems/lqr.html#salient-features-of-the-result",
    "title": "24  Linear quadratic regulation",
    "section": "24.2 Salient features of the result",
    "text": "24.2 Salient features of the result\n\nWe derived the results for time-invariant dynamics and cost, but the argument trivially generalizes to time-varying dynamics and cost as well.\nThe optimal gains \\(\\{K_t\\}_{t \\ge 1}\\) do not depend on the distribution of the noise \\(\\{W_t\\}_{t \\ge 1}\\). Thus, the noise in the dynamics does not change the closed loop control policy but changes the optimal cost by a term that depends on the noise covariance (but does not depend on the policy).\nA special case of the above observation is that the optimal control policy of the stochastic LQR problem is the same as the optimal control policy of the deterministic LQR problem (where the noise \\(w_t ≡ 0\\)). This result is sometimes called the certainty equivalence principle.\nSuppose the noise was not white but Gaussian and correlated over time (and still independent of the initial state \\(x_1\\)). Then, the optimal control action at time \\(t\\) will be the same as that of the deterministic system \\[\n  x_{τ + 1} = A x_{τ} + B u_{τ} + w_{τ|t},\n  \\quad τ \\ge t,\n\\] where \\(w_{τ|t}\\) is \\(\\EXP[ w_{\\tau} \\mid w_{1:t} ]\\). That is, at time \\(t\\), one replaces future stochastic noise \\(w_τ\\) (\\(τ \\ge t\\)) by an ‘equivalent’ deterministic noise \\(w_{τ|t}\\) and then applies the method of deterministic LQR to deduce the optimal feedback control in terms of the predicted noise. This is also a special instance of the general certainty equivalence principle, which also extends to the case when the state is not perfectly observed."
  },
  {
    "objectID": "linear-systems/lqr.html#an-example-second-order-integrator",
    "href": "linear-systems/lqr.html#an-example-second-order-integrator",
    "title": "24  Linear quadratic regulation",
    "section": "24.3 An example: second order integrator",
    "text": "24.3 An example: second order integrator\nAs an example, consider a discretized model of a second-order integrator, which models the dynamics of a point-mass in one-dimensional space under time-varying force. The continuous-time dynamics of a second-order integrator are given by \\[\n  \\dot x(t) = \\MATRIX{0 & 1 \\\\ 0 & 0} x(t) + \\MATRIX{0\\\\ \\frac 1m}u(t)\n\\] where \\(x(t) \\in \\reals^2\\) with \\(x_1(t)\\) indicating position and \\(x_2(t)\\) indicating velocity, \\(u(t) \\in \\reals\\) denotes force, and \\(m\\) is a parameter denoting mass. We discretize the dynamics using zero-order hold (Wittenmark et al. 2002) with a sampling time of \\(Δt\\). First observe that the matrix \\(A_c = \\left[ \\begin{smallmatrix}0 & 1 \\\\ 0 & 0\\end{smallmatrix}\\right]\\) is nilpotent because \\(A_c^2 = 0\\). Therefore, the matrix exponential simplifies to \\[\n  e^{A_ct} = I + A_c t =\n  \\MATRIX{1 & t \\\\ 0 & 1}.\n\\] Therefore, the discretized model is \\[\\begin{align*}\n  A &= e^{A_c Δt} = \\MATRIX{1 & Δt \\\\ 0 & 1},\n  &\n  B &= \\int_{0}^{Δt} e^{A_ct} B_c dt = \\MATRIX{\\tfrac12 Δt^2 \\\\ \\frac{Δt}{m}}\n\\end{align*}\\]\nWe further assume that there is a disturbance with variance \\(σ^2\\) at the actuation, so the discretized model is \\[\n  x_{t+1} = A x_t + B (u_t + w_t)\n\\] where \\(w_t \\sim {\\cal N}(0, σ^2)\\)1.1 This model may be viewed as a model of the form \\(\\eqref{eq:dynamics}\\) by considering the noise covariance in \\(\\eqref{eq:dynamics}\\) to be \\(σ^2 B B^\\TRANS\\).\nSuppose \\[\n  Q = Q_T = \\MATRIX{1 & 0 \\\\ 0 & 0}\n  \\quad\\text{and}\\quad\n  R = ρ\n\\] We assume \\(m = 1\\,\\text{kg}\\), \\(Δt = 0.1\\,\\text{s}\\) and noise covariance \\(σ^2 = 0.05\\). The output (i.e., position) and the input (i.e., force) for a \\(T = 10\\,\\text{s}\\) simulation of the system are shown in Figure 24.1. Note that, as expected, as the cost of applying control increases, less force is applied and it takes longer for the position to become close to zero.\n\n\n\n\n\n\nviewof R = Inputs.range([0.1, 2.5], {label: \"R\", step: 0.1, value: 0.1 })\n\n\n\n\n\n\n\nPositionPlot = Plot.plot({\n  grid: true,\n  y: { domain: [-0.25, 1] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(sims.filter(d =&gt; d.R == R), {x: \"time\", y: \"position\", curve:\"step-after\"}),\n  ]}\n)\nForcePlot = Plot.plot({\n  grid: true,\n  y: { domain: [-3, 0.5] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(sims.filter(d =&gt; d.R == R), {x: \"time\", y: \"force\", curve: \"step-before\"}),\n  ]}\n)\n\n\n\n\n\n\n\n\n(a) Position over time\n\n\n\n\n\n\n\n(b) Force over time\n\n\n\nFigure 24.1: Optimal regulation of second order integrator for different choices of control cost"
  },
  {
    "objectID": "linear-systems/lqr.html#exercises",
    "href": "linear-systems/lqr.html#exercises",
    "title": "24  Linear quadratic regulation",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 24.1 What is the optimal solution to the LQR problem when \\(Q = 0\\)?\n\n\nExercise 24.2 What is the optimal solution to the LQR problem when \\(Q \\succ 0\\) and \\(R = 0\\)?\n\n\nExercise 24.3 (Noise coupled subsystems) Consider a system with two subsystems: subsytem 1 with state \\(x^1_t \\in \\reals^{n^1}\\) and control \\(u^1_t \\in \\reals^{m^1}\\) and subsystem 2 with state \\(x^2_t \\in \\reals^{n^2}\\) and control \\(u^2_t \\in \\reals^{m^2}\\). The dynamics are coupled only though the noise, i.e., \\[\\begin{align*}\n  x^1_{t+1} &= A^{11} x^1_t + B^1 u^1_t + w^1_t \\\\\n  x^2_{t+1} &= A^{22} x^2_t + B^2 u^2_t + w^2_t\n\\end{align*}\\] where the noise process \\(\\{ (w^1_t, w^2_t)\\}_{t \\ge 1}\\) is correlated across subsystems but independent across time.\nLet per-step cost is decoupled across the subsystems and of the form: \\[\n  c(x_t, u_t) =\n  (x^1_t)^\\TRANS Q^{11} x^1_t + (u^1_t)^\\TRANS R^{11} u^1_t\n  +\n  (x^2_t)^\\TRANS Q^{22} x^2_t + (u^2_t)^\\TRANS R^{22} u^2_t.\n\\] The terminal cost has a similar structure. Show that the optimal control law is of the form: \\[\nu^1_t = - K^1_t x^1_t\n\\quad\\text{and}\\quad\nu^2_t = - K^2_t x^2_t\n\\] where the gains \\(K^1_t\\) and \\(K^2_t\\) are obtained by solving two separate Riccati equations.\nHint: There are two ways to solve this problem. An algebriac method using where one can argue that the Riccati gain \\(P\\) is diagonal and a simpler method that uses certainty equivalence."
  },
  {
    "objectID": "linear-systems/lqr.html#notes",
    "href": "linear-systems/lqr.html#notes",
    "title": "24  Linear quadratic regulation",
    "section": "Notes",
    "text": "Notes\nSee Athans (1971) for a general discussion of the philosophical approach of approximating general stochastic control problems as linear quadratic models. See Dorato and Levis (1971) for a general overview of discrete-time LQR including a summary of how such models might arise and different (dated) approaches to numerically solve the Riccati equation. Deterministic LQR model for continuous time systems was proposed in Kalman (1960). The proof idea of completion of squares is due to Aström (1970) but we loosely follow the proof outlines adapted from Afshari and Mahajan (2023).\nExercise 24.3 is modified from the proof idea used in Arabneydi and Mahajan (2016) and Gao and Mahajan (2022).\nThe term certainty equivalence is due to Simon (1956), who was looking at a static problem; a similar result had earlier been shown by Theil (1954). A result which is essentially equivalent to the stochastic LQR problem is proved by Theil (1957).\n\n\n\n\nAfshari, M. and Mahajan, A. 2023. Decentralized linear quadratic systems with major and minor agents and non-gaussian noise. IEEE Transactions on Automatic Control 68, 8, 4666–4681. DOI: 10.1109/tac.2022.3210049.\n\n\nArabneydi, J. and Mahajan, A. 2016. Linear quadratic mean field teams: Optimal and approximately optimal decentralized solutions. Available at: https://arxiv.org/abs/1609.00056v2.\n\n\nAström, K.J. 1970. Introduction to stochastic control theory. Dover.\n\n\nAthans, M. 1971. The role and use of the stochastic linear-quadratic-gaussian problem in control system design. IEEE Transactions on Automatic Control 16, 6, 529–552. DOI: 10.1109/tac.1971.1099818.\n\n\nDorato, P. and Levis, A. 1971. Optimal linear regulators: The discrete-time case. IEEE Transactions on Automatic Control 16, 6, 613–620. DOI: 10.1109/tac.1971.1099832.\n\n\nGao, S. and Mahajan, A. 2022. Optimal control of network-coupled subsystems: Spectral decomposition and low-dimensional solutions. IEEE Transactions on Control of Network Systems 9, 2, 657–669. DOI: 10.1109/tcns.2021.3124259.\n\n\nKalman, R.E. 1960. Contributions to the theory of optimal control. Boletin de la Sociedad Matematica Mexicana 5, 102–119.\n\n\nSimon, H.A. 1956. Dynamic programming under uncertainty with a quadratic criterion function. Econometrica 24, 1, 74–81. DOI: 10.2307/1905261.\n\n\nTheil, H. 1954. Econometric models and welfare maximization. Wirtschaftliches Archiv 72, 60–83. DOI: 10.1007/978-94-011-2410-2_1.\n\n\nTheil, H. 1957. A note on certainty equivalence in dynamic planning. Econometrica, 346–349. DOI: 10.1007/978-94-011-2410-2_3.\n\n\nWittenmark, B., Åström, K.J., and Årzén, K.-E. 2002. Computer control: An overview. In: IFAC professional brief. IFAC. Available at: https://www.ifac-control.org/publications/list-of-professional-briefs/pb_wittenmark_etal_final.pdf."
  },
  {
    "objectID": "linear-systems/large-scale-systems.html#mean-field-control",
    "href": "linear-systems/large-scale-systems.html#mean-field-control",
    "title": "25  Large scale systems",
    "section": "25.1 Mean-field control",
    "text": "25.1 Mean-field control\nConsider a system consisting of \\(N\\) subsystems, indexed by the set \\(\\ALPHABET N \\coloneqq \\{1, \\dots, N\\}\\). Each subsystem \\(i \\in \\ALPHABET N\\) has a state \\(x^i_t \\in \\reals^{n}\\) and a control input \\(u^i_t \\in \\reals^m\\). The dynamics of each subsystem are given as \\[\\begin{equation}\\label{eq:dynamics}\n  x^i_{t+1} = A x^i_t + B u^i_t + D \\bar x_t + E \\bar u_t + w^i_t\n\\end{equation}\\] where \\[\\begin{equation}\n  \\bar x_t \\coloneqq \\frac 1N \\sum_{i \\in \\ALPHABET N} x^i_t\n  \\quad\\text{and}\\quad\n  \\bar u_t \\coloneqq \\frac 1N \\sum_{i \\in \\ALPHABET N} u^i_t\n\\end{equation}\\] are the emperical mean-field of the state and control, respectively and \\(A\\), \\(B\\), \\(D\\), \\(E\\) are matrices of appropriate dimensions. The noise processes \\(\\{w^i_t\\}_{t \\ge 1}\\), \\(i \\in \\ALPHABET N\\) are correlated across subsystem but are assumed to be independent across time.\nWe use \\(\\pmb x_t \\coloneqq (x^1_t, \\dots, x^N_t)\\) and \\(\\pmb u_t \\coloneqq(u^1_t, \\dots, u^N_t)\\) to denote the global state and control of the system. The system incurs a per-step cost given by \\[\\begin{equation}\\label{eq:mf-cost}\n  c(\\pmb x_t, \\pmb u_t) =\n  \\bar x_t^\\TRANS \\bar Q \\bar x_t + \\bar u_t^\\TRANS \\bar R \\bar u_t\n  +\n  \\frac 1N \\sum_{i \\in \\ALPHABET N}\n  \\bigl[\n  (x^i_t)^\\TRANS Q x^i_t + (u^i_t)^\\TRANS R u^i_t\n  \\bigr]\n\\end{equation}\\] and a terminal cost \\[\\begin{equation}\\label{eq:mf-cost-T}\n  c_T(\\pmb x_T) =\n  \\bar x_T^\\TRANS \\bar Q_T \\bar x_T\n  +\n  \\frac 1N \\sum_{i \\in \\ALPHABET N}\n  (x^i_T)^\\TRANS Q_T x^i_T\n\\end{equation}\\]\nWe are interested in identifying policies \\(g = (g_1, \\dots, g_{T-1})\\) where \\(\\pmb u_t = g_t(\\pmb x_t)\\) to minimize \\[\\begin{equation}\\label{eq:performance}\n  J(g) = \\EXP^g\\biggl[\\sum_{t=1}^{T-1} c(\\pmb x_t, \\pmb u_t) + c_T(\\pmb x_T)\n  \\biggr]\n\\end{equation}\\]\n\n\n\n\n\n\nWeakly coupled dynamics and cost\n\n\n\nNote that the subsystems are weakly coupled in the dynamics and cost. A naive solution using by solving a single Riccati equation has a complexity of \\(\\mathcal O(n^3 N^3)\\), which scales cubically in the number of agents.\n\n\n\n25.1.1 State space decomposition\nWe now present a decomposition method to simplify the above optimization problem. Note that \\(\\eqref{eq:dynamics}\\) implies that \\[\\begin{equation}\\label{eq:mf-dynamics}\n  \\bar x_t = (A + D) \\bar x_t + (B + E) \\bar u_t + \\bar w_t\n\\end{equation}\\] where \\(\\bar w_t = (\\sum_{i \\in \\ALPHABET N}w^i_t)/N\\). Define \\[\n  \\breve x^i_t = x^i_t - \\bar x_t\n  \\quad\\text{and}\\quad\n  \\breve u^i_t = u^i_t - \\bar u_t.\n\\] Then, subtracting \\(\\eqref{eq:mf-dynamics}\\) from \\(\\eqref{eq:dynamics}\\), we get \\[\\begin{equation}\\label{eq:breve-dynamics}\n  \\breve x^i_t = A \\breve x^i_t + B \\breve u^i_t + \\breve w^i_t\n\\end{equation}\\] where \\(\\breve w^i_t = w^i_t - \\bar w_t\\).\nWe can think of \\(\\bar x_t\\) as the “center of mass” of the system and \\(\\breve x^i_t\\) to be the relative coordinates of subsystem \\(i\\) wrt the center of mass. Building on this intuition, we make the following simple observation, which may be viewed as an analog of the :parallel axis theorem in physics.\n\nLemma 25.1 We have the following:\n\n\\(\\displaystyle \\frac 1N\\sum_{i \\in \\ALPHABET N} (x^i_t)^\\TRANS Q x^i_t = \\bar x_t^\\TRANS Q \\bar x_t + \\frac 1N \\sum_{i \\in \\ALPHABET N} (\\breve x^i_t)^\\TRANS Q \\breve x^i_t\\).\n\\(\\displaystyle \\frac 1N\\sum_{i \\in \\ALPHABET N} (u^i_t)^\\TRANS Q u^i_t = \\bar u_t^\\TRANS Q \\bar u_t + \\frac 1N \\sum_{i \\in \\ALPHABET N} (\\breve u^i_t)^\\TRANS Q \\breve u^i_t\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof follows from the observation that \\(\\sum_{i \\in \\ALPHABET N} \\breve x^i_t = 0\\) and elementary algebra.\n\n\n\nAn immediate implication of Lemma 25.1 is that the per-step cost can be decomposed as follows: \\[\\begin{equation}\n  c(\\pmb x_t, \\pmb u_t) = \\bar c(\\bar x_t, \\bar u_t) +\n  \\frac 1N \\sum_{i \\in \\ALPHABET N} \\breve c^i(\\breve x^i_t, \\breve u^i_t)\n\\end{equation}\\] where \\[\\begin{align*}\n  \\bar c(\\bar x_t, \\bar u_t) &=\n  \\bar x_t^\\TRANS (Q + \\bar Q) \\bar x_t + \\bar u_t^\\TRANS (R + \\bar R) \\bar u_t,\n  \\notag \\\\\n  \\breve c^i(\\breve x^i_t, \\breve u^i_t) &=\n   \\frac 1N \\sum_{i \\in \\ALPHABET N}\n   \\bigl[\n    (\\breve x^i_t)^\\TRANS Q \\breve x^i_t\n    +\n    (\\breve u^i_t)^\\TRANS Q \\breve u^i_t\n  \\bigr].\n\\end{align*}\\] A similar decomposition also holds for the terminal cost \\(c_t(\\pmb x_t, \\pmb u_t)\\).\nThus, the original system is equivalent to \\(N+1\\) coupled subsystems:\n\na mean-field subsystem with state \\(\\bar x_t\\), control input \\(\\bar u_t\\), and per-step cost \\(\\bar c(\\bar x_t, \\bar u_t)\\).\n\\(N\\) auxiliary subsytems, where subsystem \\(i \\in \\ALPHABET N\\) has state \\(\\breve x^i_t\\), control input \\(\\breve u^i_t\\), and per-step cost \\(\\breve c(\\breve x^i_t, \\breve u^i_t)\\).\n\nNote that the only coupling between the subsystems is through the noise. Therefore, by the argument presented in Exercise 24.3, the optimal control strategy is of the following form.\n\nProposition 25.1 The optimal control policy for the mean-field control system described above is given by \\[\n    u_t = - \\bar K_t \\bar x_t + \\breve K_t (x^i_t - \\bar x_t)\n  \\] where\n\n\\(\\bar K_{1:T-1} = \\LQR_T(A+D, B+E, Q + \\bar Q, R + \\bar R; Q_T + \\bar Q_T)\\)\n\\(\\breve K_{1:T-1} = \\LQR_T(A,B, Q, R; Q_T)\\).\n\n\n\n\n\n\n\n\nSignificance of the result\n\n\n\nThe above result is significant, both for synthesis and implementation.\nFor synthesis, rather than solving one Riccati equation with state dimension \\(nN\\), we need to solve two Riccati equations with dimension \\(n\\). Thus, the complexity of computing the optimal controller gains does not depend on the number \\(N\\) of subsystems.\nFor implementation, each subsystem does not need access to the global state \\(\\pmb x_t\\); instead it just needs access to the mean-field \\(\\bar x_t\\) in addition to its local state \\(x^i_t\\)."
  },
  {
    "objectID": "linear-systems/large-scale-systems.html#network-coupled-subsystems",
    "href": "linear-systems/large-scale-systems.html#network-coupled-subsystems",
    "title": "25  Large scale systems",
    "section": "25.2 Network coupled subsystems",
    "text": "25.2 Network coupled subsystems"
  },
  {
    "objectID": "linear-systems/large-scale-systems.html#notes",
    "href": "linear-systems/large-scale-systems.html#notes",
    "title": "25  Large scale systems",
    "section": "Notes",
    "text": "Notes\nThe results for mean-field control are adapted from Arabneydi and Mahajan (2016). The discussion above is restricted to the simplest setting of homogenous subsystems. Generalization to hetrogeneous subsystems and infinite horizon settings are also presented in Arabneydi and Mahajan (2016). A similar result for \\(N \\to \\infty\\) is also presented in Elliott et al. (2013).\nThe results for network coupled subsystems are adapted from Gao and Mahajan (2022).\n\n\n\n\nArabneydi, J. and Mahajan, A. 2016. Linear quadratic mean field teams: Optimal and approximately optimal decentralized solutions. Available at: https://arxiv.org/abs/1609.00056v2.\n\n\nElliott, R., Li, X., and Ni, Y.-H. 2013. Discrete time mean-field stochastic linear-quadratic optimal control problems. Automatica 49, 11, 3222–3233. DOI: 10.1016/j.automatica.2013.08.017.\n\n\nGao, S. and Mahajan, A. 2022. Optimal control of network-coupled subsystems: Spectral decomposition and low-dimensional solutions. IEEE Transactions on Control of Network Systems 9, 2, 657–669. DOI: 10.1109/tcns.2021.3124259."
  },
  {
    "objectID": "rl/stochastic-approximation.html#list-of-assumptions",
    "href": "rl/stochastic-approximation.html#list-of-assumptions",
    "title": "26  Stochastic approximation",
    "section": "26.1 List of assumptions",
    "text": "26.1 List of assumptions\nLet \\(\\mathcal F_t = σ(θ_{1:t}, ξ_{1:t})\\). We state the following set of assumptions but note that not every assumption is needed for every result.\n\n26.1.1 Assumptions on the function \\(f\\)\nF1. \\(θ^*\\) is a solution of the equation \\(f(θ) = 0\\).\nF1’. \\(θ^*\\) is the unique solution of the equation \\(f(θ) = 0\\).\nF2. The function \\(f\\) is globally Lipschitz-continuous with constant \\(L\\), i.e., for any \\(θ_1, θ_2 \\in \\reals^d\\), \\[\n  \\| f(θ_1) - f(θ_2) \\|_{2} \\le L \\| θ_1 - θ_2 \\|_2.\n\\] F2’. The function \\(f\\) is twice differentiable and is globally Lipschitz continuous with constant \\(L\\).\n\n\n\n\n\n\nImplication\n\n\n\nAssumption (F2) implies that for each \\(θ \\in \\reals^d\\), there is a unique function \\(s(\\cdot, θ)\\) that satisfies the ODE \\[\n  \\frac{ds(t,θ)}{dt} = f(s(t,θ)), \\quad\n  s(0,θ) = θ.\n\\]\n\n\nF3. The equilibrium \\(θ^*\\) of the ODE \\(\\dot θ = f(θ)\\) is globally asymptotically stable.\nF3’. The equilibrium \\(θ^*\\) of the ODE \\(\\dot θ = f(θ)\\) is globally exponentially stable. Thus, there exists constants \\(μ \\ge 1\\) and \\(γ &gt; 0\\) such that \\[\n\\| s(t,θ) - θ^*\\|_2 \\le μ\\|θ - θ^*\\|_2 \\exp(-γ t),\n\\quad\n\\forall t \\ge 0, \\forall θ \\in \\reals^d.\n\\]\nF4. There is a finite constant \\(K\\) such that \\[\n\\| \\nabla^2 f_i(θ) \\|_{S} \\cdot \\| θ - θ^*\\|_2 \\le K,\n\\quad\n\\forall i \\in \\{1, \\dots, d\\},\n\\forall θ \\in \\reals^d,\n\\] where \\(\\|\\cdot\\|_S\\) denotes the spectral norm of a matrix (i.e., the largest singular value).\n\n\n\n\n\n\nImplication\n\n\n\nAssumption (F4) implies that \\[\n\\left| \\frac{∂^2 f_i(θ)}{∂θ_j ∂θ_k}\\right| \\cdot \\| θ - θ^*\\|_2 \\le K,\n\\quad\n\\forall i.j,k, \\in \\{1,\\dots, d\\},\n\\forall θ \\in \\reals^d.\n\\]\n\n\n\n\n26.1.2 Conditions on the noise\nN1. \\(\\{ξ_t\\}_{t \\ge 0}\\) is a martingale difference sequence with respect to \\(\\{ \\mathcal F_t\\}_{t \\ge 1}\\), i.e., \\[ \\EXP[ ξ_{t+1} | \\mathcal F_t ] = 0, \\text{ a.s.}, \\quad t \\ge 1. \\]\nN2. The noise \\(\\{ξ_t\\}_{t \\ge 1}\\) satisfies \\[\n\\EXP[ \\| ξ_{t+1}^2 \\|_2^2 \\mid \\mathcal F_t ] \\le\nσ^2( 1 + \\| θ_t - θ^*\\|_{2}^2),\n\\quad \\text{a.s. } \\forall t \\ge 1\n\\] for some finite constant \\(σ^2\\).\n\n\n26.1.3 Conditions on the learning rate\nR1. \\(\\sum_{t \\ge 1} α_t^2 &lt; ∞\\).\nR2. \\(\\sum_{t \\ge 1} α_t = ∞\\).\nR3. There exists constants \\(\\underline α, \\bar α \\in (0,1)\\) such that \\(\\underline α \\le α_t \\le \\bar α\\) for all \\(t \\ge 1\\)."
  },
  {
    "objectID": "rl/stochastic-approximation.html#gladyshevs-result",
    "href": "rl/stochastic-approximation.html#gladyshevs-result",
    "title": "26  Stochastic approximation",
    "section": "26.2 Gladyshev’s result",
    "text": "26.2 Gladyshev’s result\nThe following is a restatement of the result of Gladyshev (1965).\n\nTheorem 26.1 Suppose assumptions (F1’), (N1), and (N2) hold. In addition, the function \\(f(\\cdot)\\) is passive, i.e., for each \\(0 &lt; ε &lt; M &lt; ∞\\), \\[ \\sup_{ε &lt; \\| θ - θ^*\\|_2 &lt; M}\n  \\langle θ - θ^*, f(θ) \\rangle\n&lt; 0\\] and \\[\\|f(θ)\\|_2 \\le K \\|θ - θ^*\\|_2, \\quad K &lt; ∞.\\] Then,\n\nIf (R1) holds, then \\(\\{θ_t\\}\\) is bounded almost surely.\nIn addition, if (R2) holds, then \\(θ_t \\to θ^*\\) almost surely as \\(t \\to ∞\\).\n\n\n\n\n\n\n\n\nRemark\n\n\n\nThe second assumption \\(\\|f(θ)\\|_2 \\le K \\| θ - θ^*\\|_2\\) implies that \\(f(⋅)\\) is continuous as \\(θ^*\\), but it need not be continuous anywhere else."
  },
  {
    "objectID": "rl/stochastic-approximation.html#sec-borkar-meyn",
    "href": "rl/stochastic-approximation.html#sec-borkar-meyn",
    "title": "26  Stochastic approximation",
    "section": "26.3 Borkar-Meyn’s result",
    "text": "26.3 Borkar-Meyn’s result\nThe following is a restatement of the result of Borkar and Meyn (2000).\n\nTheorem 26.2 Suppose assumptions (F2), (N1), and (N2) hold. In addition:\n\nThere exists a limit function \\(f_{∞} \\colon \\reals^d \\to \\reals^d\\) such that \\[\n  \\lim_{r \\to ∞} \\frac{f(r θ)}{r} = f_{∞}(θ), \\quad\n  \\forall θ \\in \\reals^d.\n\\]\nOrigin is asymptotically stable equilibrium of the ODE \\[\n  \\dot θ(t) = f_{∞}(θ(t)).\n\\]\n\nThen,\n\nIf (R1) and (R2) hold, then \\(\\{θ_t\\}_{t \\ge 1}\\) is bounded almost surely.\nIn addition, if (F3) holds, then \\(θ_t \\to θ^*\\) almost surely as \\(t \\to ∞\\).\nIf (F3) and (R3) hold, then:\n\nthere exists a \\(α^* &gt; 0\\) and \\(C_1 &lt; ∞\\) such that if \\(\\bar α \\le α^*\\) then \\[\n   \\limsup_{n \\to ∞} \\EXP[ \\| θ_k\\|^2 ] \\le C_1.\n\\]\nif \\(\\bar α \\le α^*\\) then \\(θ_t \\to θ^*\\) in probability. In particular, for any \\(ε &gt; 0\\), there exists a \\(b_1 = b_1(ε) &lt; ∞\\) such that \\[\n   \\limsup_{n \\to ∞} \\PR( \\| θ_k - θ^* \\| \\ge ε ) \\le b_1 \\bar α.\n\\]\nIn addition, if (F3’) holds, then \\(θ_t \\to θ^*\\) in mean square. In particular, there exists a \\(b_2 &lt; ∞\\) such that for any initial condition \\(θ_0 \\in \\reals^d\\), \\[\n   \\limsup_{n \\to ∞} \\EXP[ \\| θ_k - θ^*\\|^2 ] \\le b_2 \\bar α.\n\\]\n\n\n\n\n\n\n\n\n\nRates of convergence\n\n\n\nBorkar and Meyn (2000) also provides rates of convergence of stochastic approximation under stronger assumptions."
  },
  {
    "objectID": "rl/stochastic-approximation.html#vidyasagars-result",
    "href": "rl/stochastic-approximation.html#vidyasagars-result",
    "title": "26  Stochastic approximation",
    "section": "26.4 Vidyasagar’s result",
    "text": "26.4 Vidyasagar’s result\nThe following is a restatement of the result of Vidyasagar (2023).\n\n\n\n\n\n\nFunction classes\n\n\n\nConsider a continuous function \\(f \\colon \\reals_{\\ge 0} \\to \\reals_{\\ge 0}\\).\n\nThe function \\(f\\) is said to belong to class \\(\\mathcal K\\) if \\(f(0) = 0\\) and \\(f(\\cdot)\\) is strictly increasing.\nThe function \\(f \\in \\ALPHABET K\\) is said to belong to class \\(\\ALPHABET K \\ALPHABET R\\) if, in addition, \\(f(r) \\to ∞\\) as \\(s \\to ∞\\).\nThe function \\(f\\) is said to belong to class \\(\\ALPHABET B\\) if \\(f(0) = 0\\) and, in iaddition for all \\(0 &lt; ε &lt; M &lt; ∞\\), we have \\[\n\\inf_{ε \\le r \\le M} f(r) &gt; 0.\n\\]\n\n\n\n\nExample 26.2 Observe that every function \\(f\\) of class \\(\\ALPHABET K\\) also belongs to class \\(\\ALPHABET B\\) but the converse is not true. For example, let \\[\n  f(r) = \\begin{cases}\n  r, & \\text{if } r \\in [0,1], \\\\\n  e^{-(r-1)}, & \\text{if } r &gt; 1.\n  \\end{cases}\n\\] Then, \\(f\\) belongs to class \\(\\ALPHABET B\\).\nHowever, since \\(f(r) \\to 0\\) as \\(r \\to ∞\\), \\(f\\) cannot be bounded below by any function of class \\(\\ALPHABET K\\).\n\n\nTheorem 26.3 Suppose assumptions (F1), (F2), (N1), and (N2) hold. In addition, suppose that there exists a twice differentiable Lyapunov function \\(V \\colon \\reals^d \\to \\reals_{\\ge 0}\\) that satisfies the following conditions:\n\nthere exist constants \\(a, b &gt; 0\\) such that \\[\\begin{equation}\\label{eq:vidyasagar-cond-1}\n  a \\| θ - θ^*\\|_2^2 \\le V(θ) \\le b \\| θ - θ^* \\|_2^2,\n  \\quad \\forall θ \\in \\reals^d.\n\\end{equation}\\]\nThere is a finite constant \\(M\\) such that \\[\\begin{equation}\\label{eq:vidyasagar-cond-2}\n\\| \\GRAD^2 V(θ) \\|_S \\le 2M,\n  \\quad \\forall θ \\in \\reals^d.\n\\end{equation}\\] Then,\n\n\nIf \\(\\dot V(θ) \\coloneqq \\langle \\GRAD V(θ), f(θ) \\rangle \\le 0\\) for all \\(θ \\in \\reals^d\\) and (R1) holds, then the iterates \\(\\{θ_t\\}_{t \\ge 1}\\) are bounded almost surely.\nIf, in addition, (R2) holds and there exists a function \\(\\phi \\in \\ALPHABET B\\) such that \\[\n\\dot V(θ) \\le - \\phi(\\| θ - θ^*\\|_2),\n\\quad \\forall θ \\in \\reals^d.\n\\] Then, \\(θ_t \\to θ^*\\) almost surely as \\(t \\to ∞\\).\n\n\n\n\n\n\n\n\nDiscussion of the conditions\n\n\n\nIt is worthwhile to compare the conditions of Theorem 26.2 and Theorem 26.3.\n\nIn Theorem 26.2, it is assumed that (F1’) holds while in Theorem 26.3, it is assumed that (F1) holds. That is, there is no assumption that \\(θ^*\\) is the unique solution of \\(f(θ) = 0\\).\nThe assumptions on \\(\\dot V\\) in part 1 of Theorem 26.3 imply only that \\(θ^*\\) is a locally stable equilibrim of the ODE \\eqref{eq:ODE}. This is in contrast to Theorem 26.2 imply that \\(θ^*\\) is globally asymptotically stable.\nThe assumptions in part 2 of Theorem 26.4 ensure that \\(θ^*\\) is globally asymptotically stable equilibrium of the ODE \\eqref{eq:ODE}. Therefore, assumption (F1’) is implicit in the second part of Theorem 26.3.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first start by establishing a bound on \\(\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t]\\). To do so, observe that by Taylor series, we have \\[\n  V(θ + η) = V(θ) + \\langle \\GRAD V(θ), η \\rangle\n  + \\frac 12 \\langle η, \\GRAD^2 V(θ + λη)η \\rangle\n\\] for some \\(λ \\in [0,1]\\). Since \\(\\NORM{\\GRAD^2 V(θ+λη)}_S \\le 2M\\), it follows that \\[\n  V(θ + η) \\le V(θ) + \\langle \\GRAD V(θ), η \\rangle\n  + M \\NORM{η}_2^2.\n\\] Now apply the above bound with \\(θ = θ_t\\) and \\(η = θ_{t+1} - θ_t = α_t f(θ_t) + α_t ξ_{t+1}\\). This gives \\[\\begin{align*}\nV(θ_{t+1}) &\\le V(θ_t)\n+ α_t \\langle \\GRAD V(θ_t), f(θ_t) \\rangle\n+ α_t \\langle \\GRAD V(θ_t), ξ_{t+1} \\rangle\n  \\notag \\\\\n&\\quad  + α_t^2 M \\bigl[ \\NORM{f(θ_t)}_2^2 + \\NORM{ξ_{t+1}}_2^2 +\n2 \\langle f(θ_t), ξ_{t+1} \\rangle\n\\bigr]\n\\end{align*}\\] Recall that \\(\\langle V(θ), f(θ) \\rangle \\eqqcolon \\dot V(θ)\\). Now, we can bound \\(\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t]\\) using assumptions (N1) and (N2). \\[\\begin{equation*}\n\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t] \\le V(θ_t)\n+ α_t \\dot V(θ_t)\n+ α_t^2 M \\bigl[ \\NORM{f(θ_t)}_2^2 + σ^2(1 + \\NORM{θ_t - θ^*}_2^2)\n\\bigr]\n\\end{equation*}\\] Assumption (F1) and (F2) implies that \\[\n\\NORM{f(θ_t)}_2^2 = \\NORM{f(θ_t) - f(θ^*)}_2^2 \\le L^2 \\NORM{θ_t - θ^*}_2^2.\n\\] Substituting in the above bound, we get: \\[\\begin{equation}\\label{eq:vidyasagar-1-pf-step-1}\n\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t] \\le V(θ_t)\n+ α_t \\dot V(θ_t)\n+ α_t^2 M \\bigl[ σ^2 + (σ^2 + L^2)\\NORM{θ_t - θ^*}_2^2 \\bigr].\n\\end{equation}\\]\n\nProof of part 1.\nUnder the stated assumptions, we can simplify \\eqref{eq:vidyasagar-1-pf-step-1} \\[\\begin{align*}\n\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t]\n&\\stackrel{(a)}\\le\nV(θ_t)\n+ α_t^2 M \\bigl[ σ^2 + (σ^2 + L^2)\\NORM{θ_t - θ^*}_2^2 \\bigr]\n  \\notag \\\\\n&\\stackrel{(b)}\\le\n\\biggl[ 1 + \\frac{α_t^2 M}{a} (L^2 + σ^2) \\biggr] V(θ_t)\n+\n  α_t^2 M σ^2\n\\end{align*}\\] where \\((a)\\) follows from the assumption that \\(\\dot V(θ) &lt; 0\\) and \\((b)\\) follows from \\(V(θ) \\ge a \\NORM{θ - θ^*}_2^2\\).\nThus, \\(\\{V(θ_t)\\}_{t \\ge 1}\\) is an “almost” supermartingale. Apply Theorem 30.2 with \\(X_t = V(θ_t)\\), \\(β_t = α_t^2 M(L^2 + σ^2)/a\\), \\(Y_t = α_t^2 M σ^2\\), and \\(Z_t = 0\\). Then, from (R2) it follows that \\(\\lim_{t \\to ∞} V(θ_t)\\) exists almost surely and is finite. Condition \\eqref{eq:vidyasagar-cond-1} implies that \\(\\{θ_t\\}_{t \\ge 1}\\) is almost surely bounded.\n\n\nProof of part 2.\nUnder the stated assumptions, we can simplify \\eqref{eq:vidyasagar-1-pf-step-1} \\[\\begin{align*}\n\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t]\n&\\stackrel{(c)}\\le\n\\biggl[ 1 + \\frac{α_t^2 M}{a} (L^2 + σ^2) \\biggr] V(θ_t)\n+\n  α_t^2 M σ^2\n- α_t \\phi(\\NORM{θ_t - θ^*}_2)\n\\end{align*}\\] where the first two terms are simplified in the same way as above and the last term corresponds to the upper bound on \\(\\dot V(θ_t) \\le -\\phi(\\NORM{θ_t - θ^*}_2^2)\\).\nWe can again apply Theorem 30.2 with \\(X_t = V(θ_t)\\), \\(β_t = α_t^2 M(L^2 + σ^2)/a\\), \\(Y_t = α_t^2 M σ^2\\), and \\(Z_t = α_t \\phi(\\NORM{θ_t - θ^*}_2)\\). Thus, we can conclude that there exists a random variable \\(ζ\\) such that \\(V(θ_t) \\to ζ\\) and \\[\\begin{equation}\\label{eq:vidyasagar-1-pf-step-2}\n\\sum_{t \\ge 1} α_t \\phi(\\NORM{θ_t - θ^*}_2^2) &lt; ∞,\n\\quad \\mathrm{a.s.}\n\\end{equation}\\]\nLet \\(Ω_1 \\subset Ω\\) denote the values of \\(ω\\) for which \\[\n  \\sup_{t \\ge 1} V(θ_t(ω)) &lt; ∞,\n  \\lim_{t \\to ∞} V(θ_t(ω)) = ζ(ω),\n  \\sum_{t \\ge 1} α_t \\phi(\\NORM{θ_t(ω) - θ^*}_2) &lt; ∞.\n\\] From Theorem 30.2, we know that \\(P(Ω_1) = 1\\). We will now show that \\(ζ(ω) = 0\\) for all \\(ω \\in Ω_1\\) by contradiction. Assume that for some \\(ω \\in Ω_1\\), we have \\(ζ(ω) = 2 ε &gt; 0\\). Choose a \\(T\\) such that \\(V(θ_t(ω)) \\ge ε\\) for all \\(t \\ge T\\). Define \\(V_M = \\sup_{t \\ge 1} V(θ_t(ω))\\). Then, we have that \\[\n  \\sqrt{\\frac{ε}{b}} \\le \\NORM{θ_t}_2 \\le \\sqrt{\\frac{V_M}{a}},\n  \\quad \\forall t \\ge T.\n\\]\nDefine \\(δ = \\inf_{\\sqrt{ε/b} \\le r \\le \\sqrt{V_M/a}} \\phi(r)\\) and observe that \\(δ &gt; 0\\) because \\(\\phi\\) belongs to class \\(B\\). Therefore, \\[\n\\sum_{t \\ge T} α_t \\phi(\\NORM{θ_t - θ^*}_2) \\ge\n\\sum_{t \\ge T} α_t δ = ∞,\n\\] due to (R2). But this contradicts \\eqref{eq:vidyasagar-1-pf-step-2}. Hence, there is no \\(ω \\in Ω_1\\) such that \\(ζ(ω) &gt; 0\\). Therefore, \\(ζ = 0\\) almost surely, i.e., \\(V(θ_t) \\to 0\\) almost surely. Finally, it follows from \\eqref{eq:vidyasagar-cond-1} that \\(θ_t \\to θ^*\\) almost surely as \\(t \\to ∞\\).\n\n\n\n\nTheorem 26.3 requires the existence of a suitable Lyapunov function that satisfies various conditions. Verifying whether or not such a function exists can be a bottleneck.\nIf can be shown (see Theorem 4 of Vidyasagar (2023)) that the conditions on \\(V\\) in Theorem 26.3 ensure that the equilibrium \\(θ^*\\) of the ODE \\eqref{eq:ODE} is globally asymptotically stable. By strengthening this assumption to global exponential stability of \\(θ^*\\) and adding a few other conditions, it is possible to establish a “converse” Lyapunov theorem that establishes the existence of such a \\(V\\). This is done below.\n\nTheorem 26.4 Suppose assumptions (F1’), (F2’), (F3) and (F4) hold. Then, there exists a twice differentiable function \\(V \\colon \\reals^d \\to \\reals_{\\ge 0}\\) such that \\(V\\) and its derivative \\(\\dot V \\colon \\reals^d \\to \\reals_{\\ge 0}\\) defined as \\(\\dot V(θ) \\coloneqq \\langle \\langle \\GRAD V(θ), f(θ) \\rangle\\) together satisfy the following conditions: there exist positive constants \\(a\\), \\(b\\), \\(c\\), and a finite constant \\(M\\) such that for all \\(θ \\in \\reals^d\\):\n\n\\(a\\NORM{θ - θ^*}_2^2 \\le V(θ) \\le b\\NORM{θ - θ^*}_2^2\\),\n\\(\\dot V(θ) \\le -c\\NORM{θ - θ^*}_2^2\\),\n\\(\\NORM{\\GRAD^2 V(θ)}_S \\le 2M\\).\n\n\nCombining Theorem 26.3 and Theorem 26.4, we get the following “self-contained” theorem:\n\nTheorem 26.5 Suppose assumptions (F1’), (F2’), (F3), and (F4) as well as assumptions (N1) and (N2) hold. Then,\n\nIf (R1) holds then \\(\\{θ_t\\}_{t \\ge 1}\\) is bounded almost surely.\nIf, in addition, (R2) holds then \\(\\{θ_t\\}_{t \\ge 1}\\) converges almost surely to \\(θ^*\\) as \\(t \\to ∞\\)."
  },
  {
    "objectID": "rl/stochastic-approximation.html#notes",
    "href": "rl/stochastic-approximation.html#notes",
    "title": "26  Stochastic approximation",
    "section": "Notes",
    "text": "Notes\nThe stochastic approximation algorithm was introduced by Robbins and Monro (1951). See Lai (2003) for a historical overview. The classical references on this material is Borkar (2008), Chen and Guo (1991), Kushner and Yin (1997).\nExample 26.1 is borrowed from Borkar (2008), who points out that it was proposed by Arthur (1994) to model the phenomenon of decreasing returns in economics.\nThe material in this section is adapted from Vidyasagar (2023).\n\n\n\n\nArthur, W.B. 1994. Increasing returns and path dependence in the economy. University of Michigan Press. DOI: 10.3998/mpub.10029.\n\n\nBorkar, V.S. 2008. Stochastic approximation. Hindustan Book Agency. DOI: 10.1007/978-93-86279-38-5.\n\n\nBorkar, V.S. and Meyn, S.P. 2000. The o.d.e. Method for convergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization 38, 2, 447–469. DOI: 10.1137/s0363012997331639.\n\n\nChen, H.-F. and Guo, L. 1991. Identification and stochastic adaptive control. Birkhäuser Boston. DOI: 10.1007/978-1-4612-0429-9.\n\n\nGladyshev, E.G. 1965. On stochastic approximation. Theory of Probability and Its Applications 10, 2, 275–278. DOI: 10.1137/1110031.\n\n\nKushner, H.J. and Yin, G.G. 1997. Stochastic approximation algorithms and applications. Springer New York. DOI: 10.1007/978-1-4899-2696-8.\n\n\nLai, T.L. 2003. Stochastic approximation: Invited paper. The Annals of Statistics 31, 2. DOI: 10.1214/aos/1051027873.\n\n\nRobbins, H. and Monro, S. 1951. A stochastic approximation method. The Annals of Mathematical Statistics 22, 3, 400–407. DOI: 10.1214/aoms/1177729586.\n\n\nVidyasagar, M. 2023. Convergence of stochastic approximation via martingale and converse Lyapunov methods. Mathematics of Control, Signals, and Systems 35, 2, 351–374. DOI: 10.1007/s00498-023-00342-9."
  },
  {
    "objectID": "probability/convergence.html",
    "href": "probability/convergence.html",
    "title": "27  Convergence of random variables",
    "section": "",
    "text": "Convergence of expected values\n\n\n\nSuppose \\(X_n \\to X\\) almost surely. Then, each of the following is a sufficient condition for \\(\\EXP[X_n] \\to \\EXP[X]\\):\n\nMonotone Convergence Theorem. \\(0 \\le X_1 \\le X_2 \\cdots\\).\nBounded Convergence Theorem. there exists a constant \\(b\\) such that \\(|X_n| \\le b\\) for all \\(n\\).\nDominated Convergence Theorem. there exists a random variable \\(Y\\) such that \\(|X_n| \\le Y\\) almost surely for all \\(n\\) and \\(\\EXP[Y] &lt; ∞\\).\n\\(\\{X_n\\}_{n \\ge 1}\\) is uniformly integrable."
  },
  {
    "objectID": "probability/sub-gaussian.html#prelim-concentration-inequality-of-sum-of-gaussian-random-variables",
    "href": "probability/sub-gaussian.html#prelim-concentration-inequality-of-sum-of-gaussian-random-variables",
    "title": "28  Sub-Gaussian random variables",
    "section": "28.1 Prelim: Concentration inequality of sum of Gaussian random variables",
    "text": "28.1 Prelim: Concentration inequality of sum of Gaussian random variables\nLet \\(\\phi(\\cdot)\\) denote the density of \\(\\mathcal{N}(0,1)\\) Gaussian random variable: \\[ \\phi(x) = \\frac{1}{\\sqrt{2π}} \\exp\\biggl( - \\frac{x^2}{2} \\biggr). \\]\nNote that if \\(X \\sim \\mathcal{N}(μ,σ^2)\\), then the density of \\(X\\) is \\[\n\\frac{1}{σ}\\phi\\biggl( \\frac{x-μ}{σ} \\biggr)\n= \\frac{1}{\\sqrt{2π}\\,σ} \\exp\\biggl( - \\frac{(x-μ)^2}{2 σ^2} \\biggr). \\]\nThe tails of Gaussian random variables decay fast which can be quantified using the following inequality.\n\nProposition 28.1 (Mills inequality) If \\(X \\sim \\mathcal{N}(0, 1)\\), then for any \\(t &gt; 0\\), \\[ \\PR( |X| &gt; t ) \\le \\frac{2\\phi(t)}{t}  \\]\nMore generally, if \\(X \\sim \\mathcal{N}(0, σ^2)\\), then for any \\(t &gt; 0\\), \\[ \\PR( |X| &gt; t ) \\le 2\\frac{σ}{t} \\phi\\biggl(\\frac{t}{σ}\\biggr) =\n\\sqrt{\\frac{2}{π} } \\frac{σ}{t}\n  \\exp\\biggl( - \\frac{t^2}{2σ^2} \\biggr). \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nIn the communication theory literature, this bound is sometimes known as the bound on the erfc or \\(Q\\) function.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe’ll first prove the result for unit variance random variable. Note that \\(X\\) is symmetric around origin. Therefore, \\[ \\PR(|X| &gt; t) = 2\\PR(X &gt; t). \\]\nNow, by using an idea similar to the proof of Markov’s inequality, we have \\[\\begin{align*}\nt \\cdot \\PR( |X| &gt; t) &= t \\int_{t}^∞ \\phi(x) dx  \\\\\n& \\le \\int_{t}^∞ x \\phi(x) dx \\\\\n& = \\int_{t}^∞ \\frac{1}{\\sqrt{2π}} x \\exp\\biggl( - \\frac{x^2}{2} \\biggr) dx \\\\\n&= \\frac{1}{\\sqrt{2π}} \\int_{t}^∞ - \\frac{∂}{∂x} \\exp\\biggl( -\\frac{x^2}{2}\n\\biggr) dx \\\\\n& = \\frac{1}{\\sqrt{2π}} \\exp\\biggl( - \\frac{t^2}{2} \\biggr)\n\\end{align*}\\]\nThe proof for the general case follows by observing that \\[\n\\PR(|X| &gt; t) = \\PR\\biggl( \\biggl| \\frac{X}{σ} \\biggr| &gt; \\frac{t}{σ} \\biggr)\n\\] where \\(X/σ \\sim \\mathcal{N}(0,1)\\).\n\n\n\nThe fact that a Gaussian random variable has tails that decay to zero exponentially fast can be be seen in the moment generating function: \\[\n  M(s) = \\EXP[ \\exp(sX) ] = \\exp\\bigl( sμ + \\tfrac12 s^2 σ^2\\bigr).\n\\]\nA useful application of Mills inequality is the following concentration inequality.\n\nProposition 28.2 (Concentration inequality.) Let \\(X_i \\sim \\mathcal{N}(0, σ^2)\\) (not necessarily independent). Then, for any \\(t &gt; 0\\), \\[\n  \\PR\\Bigl( \\max_{1 \\le i \\le n} |X_i| &gt; t\\Bigr) \\le\n  2n \\frac{σ}{t} \\phi\\biggl( \\frac{t}{σ} \\biggr).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\nThis follows immediately from Mills inequality and the union bound.\n\n\nAnother useful result is the following:\n\nProposition 28.3 (Max of Gaussian random variables.) Let \\(X_i \\sim \\mathcal{N}(0,σ^2)\\) (not necessarily independent). Then, \\[\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} X_i \\Bigr] \\le σ \\sqrt{2 \\log n}\n\\] and \\[\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} |X_i| \\Bigr] \\le σ \\sqrt{2 \\log 2n}.\n\\]\n\nSee these notes for a lower bound with the same rate!\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the first inequality. The second follows by considering \\(2n\\) random variables \\(X_1, \\dots, X_n\\), \\(-X_1, \\dots, -X_n\\).\nFor any \\(s &gt; 0\\), \\[\\begin{align*}\n\\EXP\\Bigl[ \\max_{1 \\le i \\le n} X_i \\Bigr] &=\n\\frac{1}{s}\n\\EXP\\Bigl[ \\log \\Bigl( \\exp\\Bigl( s \\max_{1 \\le i \\le n} X_i \\Bigr) \\Bigr) \\Bigr]\n\\\\\n&\\stackrel{(a)}\\le\n\\frac{1}{s}\n\\log \\Bigl( \\EXP\\Bigl[ \\exp\\Bigl( s \\max_{1 \\le i \\le n} X_i \\Bigr) \\Bigr] \\Bigr)\n\\\\\n&\\stackrel{(b)}=\n\\frac{1}{s}\n\\log \\Bigl( \\EXP\\Bigl[ \\max_{1 \\le i \\le n} \\exp( s X_i ) \\Bigr] \\Bigr)\n\\\\\n&\\stackrel{(c)}\\le\n\\frac{1}{s}\n\\log \\Bigl(\\sum_{i=1}^n \\EXP\\bigl[ \\exp( s X_i ) \\bigr] \\Bigr)\n\\\\\n&\\stackrel{(d)}=\n\\log \\Bigl( \\sum_{i=1}^n\\exp\\Bigl( \\frac{s^2 σ^2}{2} \\Bigr) \\Bigr)\n\\\\\n&= \\frac{\\log n}{s} + \\frac{s^2 σ^2}{2}\n\\end{align*}\\] where \\((a)\\) follows from Jensen’s inequality, \\((b)\\) follows from monotonicity of \\(\\exp(\\cdot)\\), \\((c)\\) follows from definition of max, \\((d)\\) follows from the definition of moment generating function of Gaussian random variables. We get the result by setting \\(s = \\sqrt{2 \\log n}/σ\\) (which minimizes the upper bound).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nWe have stated and proved these inequalities for real-valued random variables. But a version of them continue to hold for vector valued Gaussian variables as well. For a complete treatment, see Picard (2007)."
  },
  {
    "objectID": "probability/sub-gaussian.html#sub-gaussian-random-variables",
    "href": "probability/sub-gaussian.html#sub-gaussian-random-variables",
    "title": "28  Sub-Gaussian random variables",
    "section": "28.2 Sub-Gaussian random variables",
    "text": "28.2 Sub-Gaussian random variables\nIt turns out that the concentration inequalities of the form above continue to hold for more general distributions than the Gaussian. In particular, consider the bound on the max of Gaussian random variables that we established above. The only step which depends on the assumption that the random variables \\(X_i\\) were Gaussian in step \\((d)\\). Thus, as long as \\(\\EXP[ \\exp(s X_i) ] \\le \\exp(\\tfrac12 s^2 σ^2)\\), the result will continue to hold! This motivates the definition of sub-Gaussian random variables.\n\nDefinition 28.1 (Sub-Gaussian random variable) A random variable \\(X \\in \\reals\\) is said to be sub-Gaussian with variance proxy \\(σ^2\\) if \\(\\EXP[X] = 0\\) and its moment generating function satisfies \\[ \\EXP[ \\exp(sX) ] \\le \\exp( \\tfrac12 s^2 σ^2),\n\\quad \\forall s \\in \\reals. \\]\n\nThe reason the parameter \\(σ^2\\) is called a variance proxy is because by a straight forward application of Taylor series expansion and comparing coefficients, it can be shown that \\(\\text{var}(X) \\le σ^2\\). See Rivasplata (2012) for a proof.\nThis definition can be generalized to random vectors and matrices. A random vector \\(X \\in \\reals^d\\) is said the be sub-Gaussian with variance proxy \\(σ^2\\) if \\(\\EXP[X] = 0\\) and for any unit vector \\(u \\in \\reals^d\\), \\(u^\\TRANS X\\) is sub-Gaussian with variance proxy \\(σ^2\\).\nSimilarly, a random matrix \\(X \\in \\reals^{d_1 × d_2}\\) is said to be sub-Gaussian with variance proxy \\(σ^2\\) if \\(\\EXP[X] = 0\\) and for any unit vectors \\(u \\in \\reals^{d_1}\\) and \\(v \\in \\reals^{d_2}\\), \\(u^\\TRANS X v\\) is sub-Gaussian with variance proxy \\(σ^2\\).\nWe will use the phrase “\\(σ\\)-sub-Gaussian” as a short form of “sub-Gaussian with variance proxy \\(σ^2\\)”. One typically writes \\(X \\sim \\text{subG}(σ^2)\\) to denote a random variable with sub-Gaussian distribution with variance proxy \\(σ^2\\). (Strictly speaking, this notation is a bit ambiguous since \\(\\text{subG}(σ^2)\\) is a class of distributions rather than a single distribution.)"
  },
  {
    "objectID": "probability/sub-gaussian.html#examples-of-sub-gaussian-distributions",
    "href": "probability/sub-gaussian.html#examples-of-sub-gaussian-distributions",
    "title": "28  Sub-Gaussian random variables",
    "section": "28.3 Examples of sub-Gaussian distributions",
    "text": "28.3 Examples of sub-Gaussian distributions\n\nIf \\(X\\) be a Rademacher random variable, i.e., \\(X\\) takes the values \\(\\pm 1\\) with probability \\(1/2\\). Then, \\[ \\EXP[ \\exp(sX) ] = \\frac12 e^{-s} + \\frac12 e^s = \\cosh s \\le\n\\exp(\\tfrac12 s^2), \\] so \\(X\\) is\nIf \\(X\\) is uniformly distributed over \\([-a, a]\\). Then, for any \\(s \\neq 0\\), \\[ \\EXP[ \\exp(s X) ] = \\frac{1}{2as}[ e^{as} - e^{-as} ]\n   = \\sum_{n=0}^∞ \\frac{(as)^{2n}}{(2n+1)!}. \\] Using the inequality \\((2n+1)! \\ge n!2^n\\), we get that \\(X\\) is \\(a\\)-sub-Gaussian.\nIt can be shown that (see Rivasplata (2012) ) if \\(X\\) is a random variable with \\(\\EXP[X] = 0\\) and \\(|X| &lt; 1\\) a.s., then \\[ \\EXP[ \\exp(sX) ] \\le \\cosh s, \\quad \\forall s \\in \\reals. \\] Therefore, \\(X\\) is 1-sub-Gaussian.\nAn immediate corollary of the previous example is that if \\(X\\) is a random variable with \\(\\EXP[X] = 0\\) and \\(|X| \\le b\\) a.s., then \\(X\\) is \\(b\\)-sub-Gaussian.\nBy a similar arguement, we can show that if \\(X\\) is a zero mean random variable supported on some interval \\([a,b]\\), then \\(X\\) is \\((b-a)/2\\) sub-Gaussian.\nIf \\(X\\) is \\(σ^2\\) sub-Gaussian, then for any \\(α \\in \\reals\\), \\(α X\\) is \\(|α|σ\\)-sub-Gaussian.\nIf \\(X_1\\) and \\(X_2\\) are \\(σ_1\\) and \\(σ_2\\)-sub-Gaussian, then \\(X_1 + X_2\\) is \\(\\sqrt{σ_1^2 + σ_2^2}\\)-sub-Gaussian."
  },
  {
    "objectID": "probability/sub-gaussian.html#characterization-of-sub-gaussian-random-variables",
    "href": "probability/sub-gaussian.html#characterization-of-sub-gaussian-random-variables",
    "title": "28  Sub-Gaussian random variables",
    "section": "28.4 Characterization of sub-Gaussian random variables",
    "text": "28.4 Characterization of sub-Gaussian random variables\nSub-Gaussian random variables satisfy a concentration result similar to Mills inequality.\n\nLemma 28.1 Let \\(X \\in \\reals\\) be \\(σ\\)-sub-Gaussian. Then, for any \\(t &gt; 0\\), \\[\\begin{equation}\\label{eq:sG-tail-bounds}\n  \\PR(X &gt; t) \\le \\exp\\biggl( - \\frac{t^2}{2σ^2} \\biggr)\n  \\quad\\text{and}\\quad\n  \\PR(X &lt; t) \\le \\exp\\biggl( - \\frac{t^2}{2σ^2} \\biggr)\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis follows from Chernoff’s bound and the definition of sub-Gaussianity. In particular, for any \\(s &gt; 0\\) \\[\n\\PR(X &gt; t) = \\PR(\\exp(sX) &gt; \\exp(st)) \\le \\frac{ \\EXP[\\exp(sX) ]} { \\exp(st) }\n\\le \\exp\\biggl( \\frac{s^2 σ^2}{2} - st \\biggr).\n\\] Now, to find the tightest possible bound, we minimize the above bound with respect to \\(s\\), which is attained at \\(s = t/σ^2\\). Substituting this in the above bound, we get the first inequality. The second inequality follows from a similar argument.\n\n\n\nRecall that the moments of \\(Z \\sim \\mathcal{N}(0,σ^2)\\) are given by \\[\n  \\EXP[ |Z|^k ] = \\frac{1}{\\sqrt{π}} (2σ^2)^{k/2} Γ\\biggl(\\frac{k+1}{2}\\biggr),\n\\] where \\(Γ(\\cdot)\\) denotes the Gamma function. The next result shows that the tail bounds \\eqref{eq:sG-tail-bounds} are sufficient to show that the absolute moments of \\(X \\sim \\text{subG}(σ^2)\\) can be bounded by those of \\(Z \\sim \\mathcal{N}(0,σ^2)\\) up to multiplicative constants.\n\nLemma 28.2 Let \\(X\\) be a random variable such that \\[ \\PR( |X| &gt; t) \\le 2 \\exp\\biggl(- \\frac{t^2}{2σ^2} \\biggr),\\] then for any positive integer \\(k \\ge 1\\), \\[ \\EXP[ |X|^k ] \\le (2σ^2)^{k/2} k Γ(k/2). \\]\n\nNote that for the special case of \\(k=1\\), the above bound implies \\(\\EXP[ |X| ] \\le σ \\sqrt{2π}\\) and for \\(k=2\\), \\(\\EXP[|X|^2] \\le 4σ^2\\).\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis is a simple application of the tail bound. \\[\\begin{align*}\n\\EXP[ |X|^k ] &= \\int_{0}^∞ \\PR( |X|^k &gt; t ) dt \\\\\n&= \\int_{0}^∞ \\PR( |X| &gt; t^{1/k}) dt \\\\\n&\\le 2 \\int_{0}^∞ \\exp\\biggl( - \\frac{t^{2/k}}{2σ^2} \\biggr) dt \\\\\n&= (2σ^2)^{k/2} k \\int_{0}^∞ e^{-u} u^{k/2 - 1} du,\n\\qquad u = \\frac{t^{2/k}}{2σ^2} \\\\\n&= (2σ^2)^{k/2}k Γ(k/2).\n\\end{align*}\\]\nThe result for \\(k=1\\) follows from \\(Γ(1/2) = \\sqrt{π/2}\\).\n\n\n\nUsing moments, we can bound the moment generating function in terms of the tail bounds.\n\nLemma 28.3 Let \\(X\\) be a random variable such that \\[ \\PR( |X| &gt; t) \\le 2 \\exp\\biggl(- \\frac{t^2}{2σ^2} \\biggr)\\] then, \\[\\EXP[ \\exp(sX) ] \\le \\exp(4 s^2 σ^2). \\]\n\nFor this reason, sometimes it is stated that \\(X \\sim \\text{subG}(s^2)\\) when it satisfies the tail bound \\eqref{eq:sG-tail-bounds}.\nThe proof follows from the following Taylor series bound on the exponential function. \\[\n\\exp(sX) \\le 1 + \\sum_{k=2}^∞ \\frac{s |X|^k}{k!}\n\\] and apply the result of Lemma 28.2. See Rigollet (2015) for details."
  },
  {
    "objectID": "probability/sub-gaussian.html#properties-of-sub-gaussian-random-vectors",
    "href": "probability/sub-gaussian.html#properties-of-sub-gaussian-random-vectors",
    "title": "28  Sub-Gaussian random variables",
    "section": "28.5 Properties of sub-Gaussian random vectors",
    "text": "28.5 Properties of sub-Gaussian random vectors\n\nTheorem 28.1 Let \\(X = (X_1, \\dots, X_n)\\) be a vector of independent \\(σ\\)-sub-Gaussian random variables. Then, the random vector \\(X\\) is \\(σ\\)-sub-Gaussian.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any unit vector \\(u \\in \\reals^n\\), and any \\(s \\in \\reals\\) \\[\\begin{align*}\n\\EXP[ \\exp( s u^\\TRANS X) ] &= \\prod_{i=1}^n \\EXP[ \\exp(s u_i X_i) ] \\\\\n&\\le \\prod_{i=1}^n \\exp\\bigl( \\tfrac{1}{2} s^2 u_i^2 σ^2 \\bigr) \\\\\n&= \\exp\\bigl( \\tfrac{1}{2} s^2 \\| u \\|^2 σ^2 \\bigr) \\\\\n&= \\exp\\bigl( \\tfrac{1}{2} s^2 σ^2 \\bigr).\n\\end{align*}\\]"
  },
  {
    "objectID": "probability/sub-gaussian.html#concentration-inequalities",
    "href": "probability/sub-gaussian.html#concentration-inequalities",
    "title": "28  Sub-Gaussian random variables",
    "section": "28.6 Concentration inequalities",
    "text": "28.6 Concentration inequalities\nRecall that if \\(X_1\\) and \\(X_2\\) and \\(σ_1\\) and \\(σ_2\\)-sub-Gaussian, then \\(X_1 + X_2\\) is sub-Gaussian with variance proxy \\(σ_1^2 + σ_2^2\\). An immediate implication of this property is the following:\n\nProposition 28.4 (Hoeffding inequality) Suppose that variables \\(X_i\\), \\(i \\in \\{1,\\dots,n\\}\\), are independent and \\(X_i\\) has mean \\(μ_i\\) and \\(σ_i\\)-sub-Gaussian. Then, for all \\(t &gt; 0\\), we have\n\\[ \\PR\\biggl( \\sum_{i=1}^n( X_i - μ_i) \\ge t \\biggr)\n   \\le \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n σ_i^2 } \\biggr).\n\\]\n\nThe Hoeffding inequality is often stated for the special case of bounded random variables. In particular, if \\(X_i \\in [a,b]\\), then we know that \\(X_i\\) is sub-Gaussian with parameter \\(σ = (b-a)/2\\), so we obtain the bound \\[ \\PR\\biggl( \\sum_{i=1}^n( X_i - μ_i) \\ge t \\biggr)\n   \\le \\exp\\biggl( - \\frac{2t^2}{\\sum_{i=1}^n n(b-a)^2 } \\biggr).\n\\]\nThe Hoeffding inequality can be generalized to Martingales. Recall that a sequence \\(\\{ (D_i, \\mathcal F_i)\\}_{i \\ge 1}\\) is called a martingale difference sequence is for all \\(i \\ge 1\\), \\(D_i\\) is \\(\\mathcal{F}_i\\) measurable, \\[ \\EXP[ |D_i| ] &lt; ∞ \\quad\\text{and}\\quad\n   \\EXP[ D_{i+1} \\mid \\mathcal{F}_i ] = 0. \\]\n\nProposition 28.5 (Asuma-Hoeffding Inequality.) Let \\(\\{ (D_i, \\mathcal{F}_i)\\}_{i \\ge 1}\\) be a martingale difference sequence and suppose that \\(|D_i| \\le b_i\\) almost surely for all \\(i \\ge 1\\). Then for all \\(t \\ge 0\\) \\[ \\PR\\biggl( \\biggl| \\sum_{i=1}^n D_i \\biggr| \\ge t \\biggr)\n   \\le 2 \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n b_i^2 } \\biggr).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\(|D_i| \\le b_i\\), \\(D_i\\) is \\(b_i\\)-subGaussian. Using the smoothing property of conditional expectation, we have \\[\\begin{align}\n\\EXP\\biggl[ \\exp\\biggl( s \\biggl( \\sum_{i=1}^n D_i \\biggr) \\biggr) \\biggl]\n&=\n\\EXP\\biggl[ \\exp\\biggl( s \\biggl( \\sum_{i=1}^{n-1} D_i \\biggr) \\biggr) \\biggl]\n\\,\n\\EXP\\bigl[ \\exp\\bigl( s D_n \\bigr) \\bigm| \\mathcal{F}_{n-1} \\bigl]\n\\notag \\\\\n&\\le\n\\EXP\\biggl[ \\exp\\biggl( s \\biggl( \\sum_{i=1}^{n-1} D_i \\biggr) \\biggr) \\biggl]\n\\, \\exp\\bigl( \\tfrac12 s^2 b_n^2 \\bigr),\n\\end{align}\\] where the inequality followed from \\(D_n\\) being \\(b_n\\)-subGaussian. Iterating backwards this way, we get \\[ \\PR\\biggl(  \\sum_{i=1}^n D_i  \\ge t \\biggr)\n   \\le  \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n b_i^2 } \\biggr).\n\\] By a symmetric argument, we can show that \\[ \\PR\\biggl(  \\sum_{i=1}^n D_i  \\le -t \\biggr)\n   \\le  \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n b_i^2 } \\biggr).\n\\] Conbining these two, we get the stated result.\n\n\n\nNote that we can easily generalize the above inequality to the case when \\(D_k \\in [a_i, b_i]\\) because in that case \\(D_k\\) will be \\((b_i - a_i)/2\\) sub-Gaussian."
  },
  {
    "objectID": "probability/sub-gaussian.html#maximal-inequalities",
    "href": "probability/sub-gaussian.html#maximal-inequalities",
    "title": "28  Sub-Gaussian random variables",
    "section": "28.7 Maximal inequalities",
    "text": "28.7 Maximal inequalities\nAs we explained in the motivation for the definition of sub-Gaussian random variables, the definition implies that sub-Gaussian random variables will satisfy the concentration and maximal inequalities for Gaussian random variables. In particular, we have the following general result.\n\nTheorem 28.2 Let \\(X_i \\in \\reals\\) be \\(σ\\)-sub-Gaussian random variables (not necessarily independent). Then, \\[\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} X_i \\Bigr] \\le σ \\sqrt{2 \\log n}\n\\quad\\text{and}\\quad\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} |X_i| \\Bigr] \\le σ \\sqrt{2 \\log 2n}.\n\\] Moreover, for any \\(t &gt; 0\\), \\[\n  \\PR\\Bigl( \\max_{1 \\le i \\le n} X_i &gt; t\\Bigr) \\le\n  n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr)\n\\quad\\text{and}\\quad\n  \\PR\\Bigl( \\max_{1 \\le i \\le n} |X_i| &gt; t\\Bigr) \\le\n  2n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr).\n\\]\n\nThe proof is exactly the same as the Gaussian case!\nNow we state two generalizations without proof. See Rigollet (2015) for proof.\n\nMaximum over a convex polytope\n\nTheorem 28.3 Let \\(\\mathsf{P}\\) be a polytope with \\(n\\) vertices \\(v^{(1)}, \\dots, v^{(n)} \\in  \\reals^d\\) and let \\(X \\in \\reals^d\\) be a random variable such that \\([  v^{(i)} ]^\\TRANS X\\), \\(i \\in \\{1, \\dots, n\\}\\) are \\(σ\\)-sub-Gaussian random variables. Then, \\[\n  \\EXP\\Bigl[ \\max_{θ \\in \\mathsf{P}} θ^\\TRANS X \\Bigr] \\le σ \\sqrt{2 \\log n}\n\\quad\\text{and}\\quad\n  \\EXP\\Bigl[ \\max_{θ \\in \\mathsf{P}} | θ^\\TRANS X | \\Bigr] \\le σ \\sqrt{2 \\log 2n}.\n\\] Moreover, for any \\(t &gt; 0\\), \\[\n  \\PR\\Bigl(  \\max_{θ \\in \\mathsf{P}} θ^\\TRANS X &gt; t\\Bigr) \\le\n  n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr)\n\\quad\\text{and}\\quad\n  \\PR\\Bigl(  \\max_{θ \\in \\mathsf{P}} |θ^\\TRANS X| &gt; t\\Bigr) \\le\n  2n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr).\n\\]\n\n\n\nMaximum over the \\(\\ell_2\\) ball\n\nTheorem 28.4 Let \\(X \\in \\reals^d\\) be a \\(σ\\)-sub-Gaussian random variable. Then, \\[ \\EXP[ \\max_{ \\| θ \\| \\le 1 } θ^\\TRANS X ] =\n   \\EXP[ \\max_{ \\| θ \\| \\le 1 } | θ^\\TRANS X | ] \\le 4σ \\sqrt{d}.\n\\] Moreover, for any \\(t &gt; 0\\) \\[ \\PR( \\max_{ \\| θ \\| \\le 1 } θ^\\TRANS X &gt; t) =\n   \\PR( \\max_{ \\| θ \\| \\le 1 } | θ^\\TRANS X | &gt; t ) \\le\n  6^d \\exp\\biggl(- \\frac{t^2}{8σ^2} \\biggr).\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nFor any \\(δ &gt; 0\\), take \\(t = σ\\sqrt{8d \\log 6} + 2σ\\sqrt{2 \\log(1/δ)}\\), we obtain that with probability less than \\(1-δ\\), it holds that \\[\n  \\max_{\\|θ\\| \\le 1} θ^\\TRANS X\n  =\n  \\max_{\\|θ\\| \\le 1} | θ^\\TRANS X |\n  \\le 4σ\\sqrt{d} + 2σ \\sqrt{2\\log(1/δ)}.\n\\]"
  },
  {
    "objectID": "probability/sub-gaussian.html#lipschitz-functions-of-gaussian-variables.",
    "href": "probability/sub-gaussian.html#lipschitz-functions-of-gaussian-variables.",
    "title": "28  Sub-Gaussian random variables",
    "section": "28.8 Lipschitz functions of Gaussian variables.",
    "text": "28.8 Lipschitz functions of Gaussian variables.\nRecall that a function \\(f \\colon \\reals^d \\to \\reals\\) is \\(L\\)-Lipschitz with respect to the Eucledian norm if \\[\n  | f(x) - f(y) | \\le L \\| x - y \\|_2,\n  \\quad \\forall x, y \\in \\reals^d.\n\\]\nThe following results shows that any Lipschitz function of a Gaussian random variable is \\(L\\)-sub-Gaussian.\n\nTheorem 28.5 Let \\(X = (X_1, \\dots, X_n)\\) be a vector of i.i.d. standard Gaussian random variables and let \\(f \\colon \\reals^n \\to \\reals\\) be \\(L\\)-Lipschitz with respect to the Euclidean norm. Then, the variable \\(f(X) - \\EXP[ f(X) ]\\) is \\(L\\)-sub-Gaussian and therefore \\[\n  \\PR\\bigl[ \\bigl| f(X) - \\EXP[f(X)] \\bigr| \\ge t \\bigr]\n  \\le 2 \\exp\\biggl(- \\frac{t^2}{2L^2} \\biggr).\n\\]\n\nThis result is remarkable because it guarantees that any \\(L\\)-Lipschitz function of a standard Gaussian random vector, irrespective of the dimension, exhibits concetration like a scalar Gaussian variable with variance \\(L^2\\).\nFor a proof, see Chapter 2 of Wainwright (2019).\n\n\n\n\nPicard, J. 2007. Concentration inequalities and model selection. Springer Berlin Heidelberg. DOI: 10.1007/978-3-540-48503-2.\n\n\nRigollet, P. 2015. High-dimensional statistics. Available at: https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/.\n\n\nRivasplata, O. 2012. Subgaussian random variables: An expository note. Available at: http://stat.cmu.edu/~arinaldo/36788/subgaussians.pdf.\n\n\nWainwright, M.J. 2019. High-dimensional statistics. Cambridge University Press. DOI: 10.1017/9781108627771."
  },
  {
    "objectID": "probability/change-of-measure.html#change-of-measure-of-a-single-random-variable.",
    "href": "probability/change-of-measure.html#change-of-measure-of-a-single-random-variable.",
    "title": "29  Change of Measure",
    "section": "29.1 Change of measure of a single random variable.",
    "text": "29.1 Change of measure of a single random variable.\n\nTheorem 29.1 Let \\((\\Omega, \\mathcal F, P)\\) be a probability space and \\(\\Lambda\\) be an almost surely non-negative random variable such that \\(\\EXP[\\Lambda] = 1\\). For any \\(A \\in \\mathcal F\\), define \\[ P^\\dagger(A) = \\int_A \\Lambda(\\omega) dP(\\omega). \\] Then,\n\n\\(P^\\dagger\\) is a probability measure.\nFor any random variable \\(X\\), \\[ \\EXP^\\dagger[X] = \\EXP[ \\Lambda X]. \\]\nIf \\(\\Lambda\\) is almost surely positive, then \\[ \\EXP[X] = \\EXP^\\dagger \\left[ \\frac{X}{\\Lambda} \\right]. \\]\n\n\n\n\n\n\n\n\nProof\n\n\n\nBy definition. \\(P^\\dagger(\\emptyset) = 0\\) and \\(P^\\dagger(\\Omega) = \\EXP[ \\Lambda] = 1\\). Since \\(\\Lambda\\) is almost surely non-negative, \\(P^\\dagger(A) \\ge 0\\). Hence, \\(P^\\dagger\\) is a probability measure.\nThe second and the third part follow from observing that \\[ dP^\\dagger(\\omega) = \\Lambda(\\omega) dP(\\omega). \\]\n\n\nGiven two measures \\(\\mu\\) and \\(\\nu\\) on a measurable space \\((\\Omega, \\mathcal F)\\), we say that the measure \\(\\mu\\) is absolutely continuous with respect to \\(\\nu\\) (denoted by \\(\\mu \\ll \\nu\\)) if for any \\(A \\in \\mathcal F\\), \\[\n  \\nu(A) = 0 \\implies \\mu(A) = 0.\n\\]\n\nTheorem 29.2 (Radon-Nikodym) Given two probability measures \\(P\\) and \\(P^\\dagger\\) on a measurable space, if \\(P^\\dagger\\) is absolutely continuous with respect to \\(P\\), then there exists an almost surely positive random variable \\(\\Lambda\\) such that \\(\\EXP[\\Lambda] = 1\\) and for any \\(A \\in \\mathcal F\\), \\[\n  P^\\dagger(A) = \\int_A \\Lambda(\\omega) dP(\\omega).\n\\] Such a \\(\\Lambda\\) is called the Radon-Nikodym derivative of \\(P^\\dagger\\) with respect to \\(P\\), and is written as \\[\n  \\Lambda = \\frac{ dP^\\dagger } {dP}.\n\\]\n\n\nRemark\n\n\nThe Radon-Nikodym theorem provides the reverse property of Theorem 29.1. Given two measures \\(μ \\ll ν\\), \\[\n  \\int_{A} f dν = \\int_A f \\frac{dν}{dμ} dμ.\n\\] Thus, in Theorem 29.1, we are constructing a new probaility measure \\(P^\\dagger\\) such that \\(dP^\\dagger/dP = Λ\\).\nThe Radon-Nikodym Theorem is typically stated for \\(σ\\)-finite measures. The above statement is a specialization of Radon-Nikodym Theorem to probability measures.\nIn statistical signal processing literature, the Radon-Nikodym derivative is sometimes known as the likelihood ratio. In the reinforcement learning literature, it is called importance sampling.\nThe density of a random variable is the Radon-Nikodym derivative with respect to the Lebesgue measure.\nThe Radon-Nikodym derivative satisfies the product rule. If \\(μ \\ll ν \\ll λ\\), then \\[\n  \\frac {dμ}{dλ} = \\frac {dμ}{dν} \\frac {dν}{dλ},\n  \\quad λ~\\text{a.s.}.\n\\]\nThe Kullback-Leibler divergence between two probability measures \\(P\\) and \\(Q\\) defined on \\((\\Omega, \\mathcal F)\\) may be written as \\[\n  D_{\\text{KL}}( P \\| Q) = \\int_\\Omega \\log \\left ( \\frac {dP}{dQ} \\right)\n  dP.\n\\]"
  },
  {
    "objectID": "probability/change-of-measure.html#conditional-expectation-under-change-of-measure",
    "href": "probability/change-of-measure.html#conditional-expectation-under-change-of-measure",
    "title": "29  Change of Measure",
    "section": "29.2 Conditional expectation under change of measure",
    "text": "29.2 Conditional expectation under change of measure\n\nTheorem 29.3 Consider two probability measures \\(P\\) and \\(P^\\dagger\\) on \\((Ω, \\mathcal F)\\) such that \\(P^\\dagger \\ll P\\). Let \\(Λ\\) denote the Radon-Nikodym derivative of \\(P^\\dagger\\) with respect to \\(P\\) and \\(\\mathcal G\\) be any sub sigma-field of \\(\\mathcal F\\). Then, for any random variable \\(X\\) \\[\n  \\EXP^\\dagger[ X | \\mathcal G ] =\n  \\dfrac{ \\EXP[ Λ X | \\mathcal G ] } { \\EXP [ Λ | \\mathcal G ] },\n  \\quad P^\\dagger~\\text{a.s.}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(G \\in \\mathcal G\\). Then:\n\\[\\begin{align*}\n  \\int_G \\EXP[ Λ X | \\mathcal G] dP\n  &\\stackrel{(a)}= \\int_G Λ X dP \\\\\n  &\\stackrel{(b)}= \\int_G X dP^\\dagger \\\\\n  &\\stackrel{(c)}= \\int_G \\EXP^\\dagger[ X | \\mathcal G] dP^\\dagger \\\\\n  &\\stackrel{(d)}= \\int_G \\EXP^\\dagger[ X | \\mathcal G] Λ dP \\\\\n  &\\stackrel{(e)}= \\int_G \\EXP[ \\EXP^\\dagger[ X | \\mathcal G]  Λ | \\mathcal G] dP \\\\\n  &\\stackrel{(f)}= \\int_G \\EXP^\\dagger[ X | \\mathcal G]  \\EXP[ Λ | \\mathcal G] dP \\\\\n\\end{align*}\\] where (a), (c), and (e) follow from the definition of conditional expectation, (b) and (d) follow from change of measures, and (f) follows because \\(\\EXP^\\dagger[ X | \\mathcal G]\\) is \\(\\mathcal G\\)-measurable. Thus,\n\\[ \\EXP[ Λ X | \\mathcal G ] = \\EXP^\\dagger[ X | \\mathcal G ] \\EXP[ Λ | \\mathcal G]. \\]"
  },
  {
    "objectID": "probability/change-of-measure.html#change-of-measure-for-a-process",
    "href": "probability/change-of-measure.html#change-of-measure-for-a-process",
    "title": "29  Change of Measure",
    "section": "29.3 Change of measure for a process",
    "text": "29.3 Change of measure for a process\nConsider a probability space \\((Ω, \\mathcal F)\\) and let \\(P\\) and \\(P^\\dagger\\) be two probability measures on \\((Ω, \\mathcal F)\\) such that \\(P^\\dagger \\ll P\\). Let \\(Λ\\) denote the Radon-Nikodym derivative of \\(P^\\dagger\\) with respect to \\(P\\).\nLet \\(\\{\\mathcal F_t\\}_{t \\ge 0}\\) be a filtration on \\((Ω, \\mathcal F)\\). Then, we can define the Radon-Nikodym derivative process \\[\n  Λ_t = \\EXP[ Λ | \\mathcal F_t ].\n\\]\n\nTheorem 29.4  \n\nThe Radon-Nikodym derivative process \\(\\{Λ_t\\}_{t \\ge 0}\\) is a martingale with respect to \\(\\{\\mathcal F_t\\}_{t \\ge 0}\\), i.e., for any \\(s \\le t\\), \\[ \\EXP[ Λ_t | \\mathcal F_s ] = Λ_s. \\]\nLet \\(X_t\\) be an \\(\\mathcal F_t\\) measurable random variable. Then \\[ \\EXP^\\dagger[X_t] = \\EXP[Λ X_t ] = \\EXP[ Λ_t X_t ]. \\]\nThus, \\(Λ_t\\) may be viewed as \\(\\dfrac {dP^\\dagger}{dP} \\Bigg|_{\\mathcal F_t}\\).\nLet \\(X_t\\) be an \\(\\mathcal F_t\\) measurable random varaible. Then for any \\(s &lt; t\\), \\[ \\EXP^\\dagger[X_t | \\mathcal F_s ] =\n   \\dfrac{1}{Λ_s} \\EXP[ Λ_t X_t | \\mathcal F_s ] .\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe fact Radon-Nikodym derivate process is a martingale immediately follows from the towering property of conidtional expectation:\n\\[\n\\EXP[ Λ_t | \\mathcal F_s ] = \\EXP[ \\EXP[ Λ | \\mathcal F_t ] | \\mathcal F_s ]\n= \\EXP[ Λ | \\mathcal F_s ] = Λ_s.\n\\]\nBy definition of Radon-Nikodym derivative, \\(\\EXP^\\dagger[X_t] = \\EXP[Λ X_t]\\). Now, by the towering property of conditional expectation, we have \\[\n  \\EXP[Λ X_t ] = \\EXP[ \\EXP[ Λ X_t | \\mathcal F_t ] ]\n  = \\EXP[ X_t \\EXP[ Λ | \\mathcal F_t ] ] = \\EXP [Λ_t X_t].\n\\] This proves the second part.\nTo prove the third part, Theorem 29.3 implies that\n\\[\\begin{equation}\n\\EXP^\\dagger[ X_t | \\mathcal F_s ] =\n   \\frac{ \\EXP[ Λ X_t | \\mathcal F_s ]} { \\EXP[ Λ | \\mathcal F_s ] } =\n   \\frac{ \\EXP[ Λ X_t | \\mathcal F_s ]} { Λ_s }.\n   \\label{eq:step-1}\n\\end{equation}\\]\nNow, consider the numerator:\n\\[\n\\EXP[ Λ X_t | F_s ] = \\EXP[ \\EXP [ Λ X_t | \\mathcal F_t ] | \\mathcal F_s ]\n= \\EXP [ X_t \\EXP[ Λ | \\mathcal F_t ] ] = \\EXP [ X_t Λ_t ] .\n\\] Substituting this in \\eqref{eq:step-1} completes the proof of the third part.\n\n\n\nAn immediate implication of Theorem 29.4 is the following.\n\nCorollary 29.1 A process \\(\\{X_t\\}_{t \\ge 0}\\) is a \\(P^\\dagger\\)-martingale with respect to \\(\\{\\mathcal F_t\\}_{t \\ge 0}\\) if and only if the process \\(\\{ Λ_t X_t \\}_{t \\ge 0}\\) is a \\(P\\)-martingale."
  },
  {
    "objectID": "probability/martingales.html#examples-and-immediate-properties",
    "href": "probability/martingales.html#examples-and-immediate-properties",
    "title": "30  Martingales",
    "section": "30.1 Examples and Immediate Properties",
    "text": "30.1 Examples and Immediate Properties\n\nMartingales generalize the theory of sums of independent random variables. Let \\(ξ_1, ξ_2, \\dots\\) be indepdent, integrable random variables. Define \\(X_0 = 0\\) and \\(X_t = ξ_1 + \\dots + ξ_t\\). If \\(\\EXP[X_t] = 0\\) for \\(t \\ge 1\\), then the sequence \\(\\{X_t\\}_{t \\ge 1}\\) is a martingale with respect to the natural filtration: \\[\n\\EXP[X_{t+1} \\mid X_{1:t}] = \\EXP[X_t + ξ_{t+1} \\mid X_{1:t}] = X_t.\n\\]\nSimilarly, if \\(ξ_t\\) has positive mean, \\(\\{X_t\\}_{t \\ge 1}\\) is a submartingale, and if \\(ξ_t\\) has negative mean, \\(\\{X_t\\}_{t \\ge 1}\\) is a supermartingale.\nLet \\(\\{X_t\\}_{t \\ge 1}\\) be martingale and let \\(f\\) be a convex function for which each \\(f(X_t)\\) is integrable. Then, a direct application of Jensen’s inequality implies that \\(\\{f(X_t)\\}_{t \\ge 1}\\) is a submartingale.\nA consequence of the previous result is the following. Let \\(\\{X_t\\}_{t \\ge 1}\\) be a martingale and \\(p \\ge 1\\) be such that \\(|X_t|^p\\) is integrable for all \\(t\\). Then \\(\\{|X_t|^p\\}_{t \\ge 1}\\) is a submartingale.\nLet \\(\\{X_t\\}_{t \\ge 1}\\) be submartingale and let \\(f\\) be a convex and increasing function for which each \\(f(X_t)\\) is integrable. Then, \\(\\{f(X_t)\\}_{t \\ge 1}\\) is a submartingale because \\[\n   \\EXP[ f(X_{t+1}) \\mid \\mathcal F_t]\n   \\ge\n   f( \\EXP[ X_{t+1} \\mid \\mathcal F_t ])\n   \\ge\n   f(X_t)\n\\] where the first inequality follows from Jensen’s inequality and the second from the fact that \\(\\{X_t\\}_{t \\ge 1}\\) is a sub-martingale and \\(f\\) is increasing.\nA consequence of the previous result is the following. Let \\(\\{X_t\\}_{t \\ge 1}\\) be a submartingale. Then, for any constant \\(a\\), \\(\\{(X_t - a)^+\\}_{t \\ge 1}\\) is also a submartingale.\nIf \\(\\{X_t\\}_{t \\ge 1}\\) and \\(\\{Y_t\\}_{t \\ge 1}\\) are martingales defined on the same family of \\(σ\\)-algebras, then \\(\\{a X_t + b Y_t\\}_{t \\ge 1}\\) is a martingale.\nThe previous result holds for supermartingales provided \\(a\\) and \\(b\\) are positive.\nIf \\(\\{X_t\\}_{t \\ge 1}\\) and \\(\\{Y_t\\}_{t \\ge 1}\\) are supermartingales defined on the same family of \\(σ\\)-algebras, then \\(\\{X_t \\wedge Y_t\\}_{t \\ge 1}\\) is a supermartingale.\nSuppose \\(X\\) is an integrable random variable and \\(\\{\\mathcal F_t\\}_{t \\ge 1}\\) is a filtration. Define \\(X_t \\coloneqq \\EXP[X \\mid \\mathcal F_t]\\). Then \\(\\{X_t\\}_{t \\ge 1}\\) is a martingale with respect to the filtration (follows from the smoothing property of conditional expectations).\nLikelihood ratios are martingales. Suppose \\(X_1\\), \\(X_2\\), are independent random variables with density \\(f\\). Imagine we are considering the alternative hypothesis that these random variables are independent with a different probability density \\(g\\) (but they are really distributed according to the density \\(f\\)). We assume that \\(f\\) and \\(g\\) have the same support. Define the likelihood ratio \\[\n  Λ_t = \\frac{g(X_1)}{f(X_1)} \\dots \\frac{g(X_t)}{f(X_t)}.\n   \\] Then, since \\(Λ_{t+1} = Λ_t g(X_{t+1})/f(X_{t+1})\\), we have \\[\\begin{align*}\n   \\EXP[Λ_{t+1} \\mid X_{1:t}]\n   &= Λ_t \\EXP\\biggl[ \\frac{g(X_{t+1})}{f(X_{t+1})} \\biggm| X_{1:t} \\biggr] \\\\\n   &= Λ_t \\EXP\\biggl[ \\frac{g(X_{t+1})}{f(X_{t+1})} \\biggr] \\\\\n   &= Λ_t \\int\\biggl( \\frac{g(x)}{f(x)} \\biggr) f(x)dx \\\\\n   &= Λ_t \\int g(x)dx \\\\\n   &= Λ_t.\n  \\end{align*}\\] Hence, \\(\\{Λ_t\\}_{t \\ge 1}\\) is a martingale.\nA martingale \\(\\{X_t\\}_{t \\ge 1}\\) may be written as a sum of increments: \\(X_t = X_0 + ξ_1 + \\dots + ξ_t\\). The increments \\(\\{ξ_t\\}_{t \\ge 1}\\) are called martingale differences. Each \\(ξ_t\\) is integrable and \\(\\EXP[ξ_t \\mid \\mathcal F_{t-1}] = 0\\) for all \\(t\\)."
  },
  {
    "objectID": "probability/martingales.html#doobs-decomposition",
    "href": "probability/martingales.html#doobs-decomposition",
    "title": "30  Martingales",
    "section": "30.2 Doob’s decomposition",
    "text": "30.2 Doob’s decomposition\n\nTheorem 30.1 (Doob’s decomposition) Every sequence \\(\\{X_t\\}_{t \\ge 1}\\) of integrable random variables adapted to a filtration \\(\\{\\mathcal F_t\\}_{t \\ge 1}\\) can be decomposed into a sum of a martingale and an integrable predictable process, i.e., \\[\n    X_t = M_t + A_t\n  \\] where \\(\\{M_t\\}_{t \\ge 1}\\) is a martingale and \\(\\{A_t\\}_{t \\ge 1}\\) is a predictable process\n\n\nExistence. We can identify the Doob’s decomposition by defining \\[ M_t = X_0 + \\sum_{s=1}^{t}\\bigl( X_s - \\EXP[ X_s \\mid \\mathcal F_{s-1} \\bigr],\\] and \\[A_t = \\sum_{s=1}^t \\bigl( \\EXP[ X_s \\mid \\mathcal F_{s-1} ] - X_{s-1} \\bigr).\\]\nUniqueness. The decomposition is almost surely unique. If \\(X_t = M'_t + A'_t\\) is another decomposition, then \\(Y_t = M_t - M'_t\\) being the difference of martingales is a martingale. But \\(M_t - M'_t = A_t - A'_t\\), which is a predictable process; combining this with the martingale property of \\(M_t - M'_t\\) implies that \\[\nA_t - A'_t = \\EXP[ A_t - A'_t \\mid \\mathcal F_t] =\n\\EXP[ M_t - M'_t \\mid \\mathcal F_t ] = M_{t-1} - M'_{t-1} = A_{t-1} - A'_{t-1}.\n\\] Since \\(A_0 = A'_0 = 0\\), we get that \\(A_t = A'_t\\) for all \\(t \\ge 1\\).\nA stochastic process \\(\\{X_t\\}_{t \\ge 1}\\) is a submartingale if and only if its Doob’s decomposition gives a predictable process that is almost surely increasing.\nA stochastic process \\(\\{X_t\\}_{t \\ge 1}\\) is a supermartingale if and only if its Doob’s decomposition gives a predictable process that is almost surely decreasing"
  },
  {
    "objectID": "probability/martingales.html#convergence-of-nonnegative-supermartingales",
    "href": "probability/martingales.html#convergence-of-nonnegative-supermartingales",
    "title": "30  Martingales",
    "section": "30.3 Convergence of nonnegative supermartingales",
    "text": "30.3 Convergence of nonnegative supermartingales\nWe state certain properties of nonnegative supermartingales without proof. This material is taken from Neveu (1975).\n\nMaximal inequality for nonnegative supermartingales. For every nonnegative supermartingale \\(\\{X_t\\}_{t \\ge 1}\\), the random variable \\(\\sup_{t \\ge 1} X_t\\) is a.s. finite on the set \\(\\{X_0 &lt; ∞\\}\\) and, more precisely, satisfies the following inequality: \\[\n  \\PR\\Bigl( \\sup_{t \\ge 1} X_t \\ge a \\Bigm| \\mathcal F_t \\Bigr)\n  \\le\n  \\max\\biggl( \\frac{X_0}{a}, 1 \\biggr)\n\\]\nDoob’s inequalities. Let \\(\\{X_t\\}_{t \\ge 1}\\) be a generalized martingale or a nonnegative submartingale. Then for all \\(p \\in (1,∞)\\), and all \\(t\\), we have: \\[\n\\NORM{X_t}_p \\le \\biggl\\| \\sup_{s \\le t} |X_s| \\biggr\\|_p\n\\le\n\\frac{p}{p-1} \\NORM{X_t}_p.\n  \\]\nSwitching principle for supermartingales. Given two nonnegative supermartingales \\(\\{X^{(i)}_t\\}_{t \\ge 1}\\), \\(i \\in \\{1, 2\\}\\), and a stopping time \\(τ\\) such that \\(X^{(1)}_{τ} \\ge X^{(2)}_{τ}\\) on \\(\\{τ &lt; ∞\\}\\), the formula \\[\nX_t(ω) = \\begin{cases}\nX^{(1)}_t(ω), & \\text{if $t &lt; τ(ω)$} \\\\\nX^{(2)}_t(ω), & \\text{if $t \\ge τ(ω)$}\n\\end{cases}\n\\] defines a new nonnegative supermartingale.\nSupermartingale convergence theorem. Every nonnegative supermartingale \\(\\{X_t\\}_{t \\ge 1}\\) converges almost surely. Furthermore, the limit \\(X_{∞} = \\lim_{t \\to ∞} X_t\\) satisfies the inequality \\[\n  \\EXP[ X_{∞} \\mid \\mathcal F_t] \\le X_t.\n\\]\nA nonnegative supermartingale converges in \\(\\mathcal L^1\\) to its limit \\(X_{∞}\\) if and only if \\(\\EXP[X_t] \\to \\EXP[X_{∞}]\\).\nFor a nonnegative supermartingale \\(\\{X_t\\}_{t \\ge 1}\\) and a stopping time \\(τ\\), the sequence \\(\\{X_{τ \\wedge t}\\}_{t \\ge 1}\\) “stopped at the stopping time \\(τ\\)” is also a non-negative supermartingale.\nStopping theorem Let \\(\\{X_t\\}_{t \\ge 1}\\) be a nonnegative supermartingale. Then, for any bounded stopping times \\(τ_1\\) and \\(τ_2\\), we have \\[\nX_{τ_1} \\ge \\EXP[ X_{τ_2} \\mid \\mathcal F_{τ_1}],\n\\quad \\text{ on } \\{τ_1 \\le τ_2 \\}.\n\\]\n\n\n\n\n\n\n\nInterpretation of supermartingales\n\n\n\n\n\nThe following interpretation is taken from Dellacherie and Meyer (1982).\nSuppose the random variable \\(X_t\\) represents a gambler’s fortune at time \\(t\\). Then his successive gains are presented by the random variable \\(ξ_t = X_t - X_{t-1}\\). The gambler may be in an arbitrarily complicated casino, where he may choose between all sorts of games, move from one table to another, bet on other player’s fortunes, etc., but it is understood that his decisions are unprophetic, i.e., they can only be taken as functions for the past and not as functions of the future, with the convention that the present, i.e., the game which is in the process of being played, forms part of the future.\nThe supermartingale inequality \\(\\EXP[ξ_t \\mid \\ALPHABET F_t] \\le 0\\) means that, whatever decisions are taken by the gambler just before the \\(t\\)-th game, the average profits from that will be negative. In other words, the game favors the casino—which is what happens in reality! (The martingale equality corresponds to the case of an equitable casino)\nNow imagine that the gambler, fearing that he is under the influence of an evil star, confides his fortune to a “luckier” (but still unprophetic) friend and goes out for fresh air and returns at random times \\(τ_1 &lt; τ_2 &lt; \\dots\\). The stopping theorem says that what he observes at these random instances is also a game favorable to the casino (or merely equitable in case of martingales). In other words, things are no better.\nThe restriction on the length of the stopping time has the following meaning: suppose the gambler tells his friend to “call me at the first moment \\(τ_1\\) when my gain \\(X_{τ_1} - X_0\\) is positive. Then call me again at time \\(τ_2\\) when my gain \\(X_{τ_2} - X_{τ_1}\\) is positive. And so on” At such moments, mean gain is also positive and hence stopping theorem seems to be contradicted; however, the stopping theorem affirms that the stopping times \\(τ_1\\), \\(τ_2\\), etc., are not finite.\n\n\n\nThe converges result continues to hold for “almost” supermartingales.\n\nTheorem 30.2 (Almost supermartingale convergence theorem) Suppose \\(\\{X_t\\}_{t \\ge 1}\\), \\(\\{β_t\\}_{t \\ge 1}\\), \\(\\{Y_t\\}_{t \\ge 1}\\), \\(\\{Z_t\\}_{t \\ge 1}\\) are nonnegative stochastic processes adapted to some filtration \\(\\{\\mathcal F_t\\}_{t \\ge 1}\\) that satisfy \\[\\begin{equation}\\label{eq:almost-supermartingale}\n  \\EXP[X_{t+1} \\mid \\mathcal F_t ] \\le\n  (1 + β_t) X_t + Y_t - Z_t,\n  \\quad t \\ge 1\n\\end{equation}\\] Define the set \\(Ω_0\\) by \\[\n  Ω_0 = \\biggl\\{ ω : \\sum_{t \\ge 1} β_t(ω) &lt; ∞ \\biggr\\}\n  \\cap\n  \\biggl\\{ ω : \\sum_{t \\ge 1} Y_t(ω) &lt; ∞ \\biggr\\}.\n\\] Then, for all \\(ω \\in Ω_0\\), we have that\n\n\\(\\lim_{t \\to ∞} X_t(ω)\\) exists and is finite\n\\(\\sum_{t \\ge 1} Z_t(ω) &lt; ∞\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe key idea is that we can fiddle with \\eqref{eq:almost-supermartingale} to make it a supermartingale. Let \\(b_t = 1/\\prod_{\\tau = 1}^{t-1} ( 1 + β_{τ} )\\). Note that \\(\\{b_t\\}_{t \\ge 1}\\) is \\(\\{\\mathcal F_t\\}_{t \\ge 1}\\) predictable. Define the nonnegative processes \\(\\{X'_t\\}_{t \\ge 1}\\) \\(\\{Y'_t\\}_{t \\ge 1}\\) and \\(\\{Z'_t\\}_{t \\ge 1}\\), where \\[\nX'_t = b_t X_t,\n\\quad\nY'_t = b_{t+1} Y_t,\n\\quad\nZ'_t = b_{t+1} Z_t.\n\\] \\[\\begin{align}\n    \\EXP[X'_{t+1} \\mid \\mathcal F_t] &=\n    b_{t+1} \\EXP[ X_{t+1} \\mid \\mathcal F_t] \\notag \\\\\n    &\\le b_{t+1}(1 + β_t) X_t + b_{t+1} Y_t  - b_{t+1} Z_t \\notag \\\\\n    &= X'_t + Y'_t - Z'_t.\n    \\label{eq:almost-supermartingale-st-1}\n\\end{align}\\]\nDefine \\[\\begin{equation}\\label{eq:almost-supermartingale-st-2}\nW_t = X'_t - \\sum_{s = 1}^{t-1} (Y'_s - Z'_s) .\n\\end{equation}\\] From \\eqref{eq:almost-supermartingale-st-1} we have \\[\n\\EXP[W_{t+1} \\mid \\mathcal F_t] =\n\\EXP\\biggl[ X'_{t+1} - \\sum_{s=1}^t(Y'_s - Z'_s) \\biggm| \\mathcal F_t \\biggr]\n\\le X_t - \\sum_{s=1}^{t-1} (Y'_s - Z'_s)\n= W_t.\n\\] Therefore, \\(\\{W_t\\}_{t \\ge 1}\\) is a supermartingale, but we cannot immediately apply the supermartingale convergence theorem because we don’t know that \\(W_t\\) is bounded from below.\nFor an arbitrary \\(a &gt; 0\\), define the stopping time \\(τ = \\inf \\bigl\\{ t : \\sum_{s=1}^{t} Y'_s &gt; a \\bigr\\}.\\) Then, the stopped sequence \\(\\{W_{τ \\wedge t}\\}_{t \\ge 1}\\) is also a supermartingale because \\[\n\\EXP[ W_{τ \\wedge (t+1)} \\mid \\mathcal F_t ]\n= W_{\\tau} \\IND\\{ τ \\le t \\} + \\EXP[ W_{t+1} \\mid \\mathcal F_t ] \\IND\\{ τ &gt; t\\}\n\\le W_{τ \\wedge t}.\n\\] Moreover, \\(W_{τ \\wedge t}\\) is bounded from below since \\[\nW_{t \\wedge τ} \\ge - \\sum_{s = 1}^{τ \\wedge (t-1)} Y_s\n\\ge -a,\\] So, we can think of \\(W_{t \\wedge τ} + a\\) as a nonnegative supermartingale. Therefore, by the supermartingale convergence theorem, \\(\\lim_{t \\to ∞} W_{τ \\wedge t}\\) exists and is finite a.s.\nTherefore, on the set \\(\\{ τ = ∞ \\}\\), i.e., the set \\(\\{ \\sum_{s = 1}^∞ Y'_s \\le a \\}\\), \\(\\lim_{t \\to ∞} W_t\\) exists and is finite.\nSince \\(a\\) is arbitary, we have that \\(\\lim_{t \\to ∞} W_t\\) exists and is finite on \\(\\{ \\sum_{s=1}^∞ Y'_s &lt; ∞ \\}\\). But, we know that \\(Y'_s \\le Y_s\\). Therefore, for \\(ω \\in Ω_0\\), \\[\\sum_{t \\ge 1} Y'_t(ω) \\le \\sum_{t \\ge 1} Y_t(ω) &lt; ∞ . \\] Consequently, for \\(ω \\in Ω_0\\), \\(\\lim_{t \\to ∞} W_t\\) exists and is finite.\nNow \\eqref{eq:almost-supermartingale-st-2} implies that \\[\n  \\lim_{t \\to ∞} \\biggl \\{ X'_t - \\sum_{s=1}^{t-1} (Y'_s - Z'_s) \\biggr\\} &lt; ∞.\n\\] Hence, on \\(Ω_0\\), \\(\\sum_{s=1}^∞ Z'_s &lt; ∞\\) and \\(\\lim_{t \\to ∞} X'_t\\) exists and is finite.\nFrom Exercise 30.1, it follows that for \\(ω \\in Ω_0\\), \\(b_t(ω)\\) has a limit. \\[\n    X_t = X'_t/b'_t\n\\] has a limit. Moreover, the inequality, \\[\n  Z_t \\le Z'_t \\prod_{s \\ge 1} (1 + β_s)\n\\] implies that \\(\\sum_{t \\ge 1} Z_t &lt; ∞\\) for \\(ω \\in Ω_0\\).\n\n\n\nThe following is a determinisitic version of the above result, taken from Bertsekas and Tsitsiklis (2000).\n\nProposition 30.1 Let \\(\\{X_t\\}_{t \\ge 1}\\), \\(\\{Y_t\\}_{t \\ge 1}\\), and \\(\\{Z_t\\}_{t \\ge 1}\\) be sequences such that \\(\\{Z_t\\}_{t \\ge 1}\\) is nonnegative and the sequences satisfy: \\[\n  X_{t+1} \\le X_t + Y_t - Z_t\n\\] and \\(\\sum_{t \\ge 1} Y_t &lt; ∞\\). Then, either \\(X_t \\to -∞\\) or else \\(X_t\\) converges to a finite value and \\(\\sum_{t \\ge 1} Z_t &lt; ∞\\)."
  },
  {
    "objectID": "probability/martingales.html#convergence-of-submartingales",
    "href": "probability/martingales.html#convergence-of-submartingales",
    "title": "30  Martingales",
    "section": "30.4 Convergence of submartingales",
    "text": "30.4 Convergence of submartingales\n\nTheorem 30.3 (Krickeberg decomposition) Let \\(\\{S_t\\}_{t \\ge 1}\\) be a submartingale for which \\(\\sup_t \\EXP[S_t^{+}] &lt; ∞\\). Then, there exists a positive martingale \\(\\{M_t\\}_{t \\ge 1}\\) and a positive supermartingale \\(\\{X_t\\}_{t \\ge 1}\\) such that \\(S_t = M_t - X_t\\) almost sure for each \\(t\\).\n\n\n\n\n\n\n\nRemark\n\n\n\nThe finiteness of \\(\\sup_{t} \\EXP[ S_{t}^{+}]\\) is equivalent to the finiteness of \\(\\sup_{t} \\EXP[|S_t|]\\) because \\(|S_t| = 2S_t{+} - (S_t^{+} - S_t^{-})\\) and by the submartingale property, \\(\\EXP[S_t^{+} - S_t^{-}] = \\EXP[S_t]\\) increases with \\(t\\).\n\n\n\nCorollary 30.1 A submartingale with \\(\\sup_{t} \\EXP[S_t^{+}] &lt; ∞\\) converges almost surely to an integrable limit."
  },
  {
    "objectID": "probability/martingales.html#square-integrable-martingales",
    "href": "probability/martingales.html#square-integrable-martingales",
    "title": "30  Martingales",
    "section": "30.5 Square integrable martingales",
    "text": "30.5 Square integrable martingales\n\nIf \\(\\{X_t\\}_{t \\ge 1}\\) is an integrable sequence adapted to \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\). Define the compensator \\(\\{ \\langle X_t \\rangle\\}_{t \\ge 1}\\) as: \\(\\langle X_1 \\rangle = 0\\) and for \\(t &gt; 1\\): \\[\n  \\langle X_t \\rangle - \\langle X_{t-1} \\rangle\n  =\n\\EXP[ X_t \\mid \\ALPHABET F_{t-1} ] - X_{t-1}.\n\\] Note that the compensator is predictable and \\(X_t - \\langle X_t \\rangle\\) is a martingale.\nNow consder a square integrable martingle \\(\\{X_t\\}_{t \\ge 1}\\). The compensator is given by: \\[\n\\langle X_t^2 \\rangle - \\langle X_{t-1}^2 \\rangle\n=\n\\EXP[ X_t^2 - X_{t-1}^2 \\mid \\ALPHABET F_{t-1} ]\n=\n\\EXP[ (X_t - X_{t-1})^2 \\mid \\ALPHABET F_{t-1} ].\n  \\] Thus, the compensator \\(\\langle X_t^2 \\rangle\\) is increasing.\nConvergence of square integrable martinales. Let \\(\\langle X_{∞} \\rangle = \\lim_{t \\to ∞} \\langle X_t \\rangle\\). If \\(\\EXP[ \\langle X_{∞} \\rangle ] &lt; ∞\\), then \\(\\{X_t\\}_{t \\ge 1}\\) converges almost surely in the mean-square sense to a random variable \\(X_{∞}\\).\nLet \\(\\{X_t\\}_{t \\ge 1}\\) be a square integrable martingale adapted to \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\). Let \\(τ\\) be a stopping time such that \\(\\EXP[ \\langle X_{τ} \\rangle ] &lt; ∞\\). Then,\n\n\\(\\EXP[X_{τ} \\mid \\ALPHABET F_1] = X_1\\).\n\\(\\EXP[X_{τ}^2 \\mid \\ALPHABET F_1 ] = \\EXP[ \\langle M_{τ} \\rangle \\mid \\ALPHABET F_1]\\)."
  },
  {
    "objectID": "probability/martingales.html#strong-law-of-large-number-for-martingale-difference-sequence",
    "href": "probability/martingales.html#strong-law-of-large-number-for-martingale-difference-sequence",
    "title": "30  Martingales",
    "section": "30.6 Strong law of large number for martingale difference sequence",
    "text": "30.6 Strong law of large number for martingale difference sequence\nThe following is a generalization of the strong law of large numbers for martingale differences. See Stout (1974) (Theorem 3.3.1).\n\nTheorem 30.4 Suppose \\(\\{M_t\\}_{t \\ge 1}\\) is a martingale difference sequence with respect to the filtration \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\). Let \\(\\{a_t\\}_{t \\ge 1}\\) be positive sequence that is \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\) predictable and \\(\\lim_{t \\to ∞} a_t = ∞\\).\nIf for some \\(p \\in (0,2]\\), we have \\[\n  \\sum_{t=1}^∞ \\EXP \\biggl[ \\frac{|M_t|^p}{a_t^p} \\biggm|\n  \\ALPHABET F_{t-1} \\biggr] &lt; ∞,\n\\] then \\[\n  \\sum_{t=1}^∞ \\frac{M_t}{a_t} = 0, \\quad a.s.\n\\]"
  },
  {
    "objectID": "probability/martingales.html#exercise",
    "href": "probability/martingales.html#exercise",
    "title": "30  Martingales",
    "section": "Exercise",
    "text": "Exercise\n\nExercise 30.1 The purpose of this exercise is to establish the following: For a sequence of nonnegative real numbers \\(\\{a_t\\}_{t \\ge 1}\\) \\[\\sum_{t \\ge 1} a_t &lt; ∞  \\iff\n\\prod_{t \\ge 1} (1 + a_t) \\text{ converges to a finite non-zero limit}.\n\\]\n\nShow that \\(\\sum_{t \\ge 1}a_t \\le \\prod_{t \\ge 1}(1 + a_t)\\).\nHint: Expand the right hand side.\nShow that \\(\\prod_{t \\ge 1}(1 + a_t) \\le \\exp\\Bigl(\\sum_{t \\ge 1} a_t\\Bigr)\\).\nHint: Use Taylor series expansion of \\(e^x\\).\n\n\n\nExercise 30.2 (Kronecker’s Lemma) This result is useful intermediate step for showing convergence of various iterative algorithms.\nSuppose \\(\\{a_t\\}_{t \\ge 1}\\) is a strictly positive increasing real sequence which diverges to \\(∞\\) and \\(\\{x_t\\}_{t \\ge 1}\\) is a real sequence such that the series \\(\\sum_{t=1}^∞ x_t/a_t\\) converges. Then, \\[\n  \\lim_{T \\to ∞} \\frac 1{a_T} \\sum_{t=1}^T x_t = 0.\n\\]\n\n\nExercise 30.3 Consider a sequence \\(\\{α_t\\}_{t \\ge 1}\\) adapted to \\(\\{\\mathcal F_t\\}\\) such that \\[\n  \\sum_{t=1}^{∞} α_t = \\infty\n  \\quad\\text{and}\\quad\n  \\sum_{t=1}^{∞} α_t^2 &lt; ∞.\n  \\] Now suppose \\(\\{X_t\\}_{t \\ge 1}\\) is an adapted sequence of square integrable random variables such that \\[\n    \\EXP[ X_{t+1} | \\mathcal F_t ] = 0\n    \\quad\\text{and}\\quad\n    \\EXP[ \\| X_{t+1}\\|_2^2 \\mid \\mathcal F_t ] \\le B\n  \\] where \\(B\\) is a deterministic constant. Then \\[\n    \\sum_{t=1}^T α_t X_{t+1}\n    \\quad\\text{and}\\quad\n    \\sum_{t=1}^T α_t^2 \\| X_t\\|_2^2,\n    \\quad\n    T = 1,2, \\dots\n  \\] converge to finite limits almost surely.\n\n\nExercise 30.4 Let \\(\\{X_t\\}_{t \\ge 1}\\) be square integrable martingale with \\(\\EXP[ X_T^2 ] \\le T\\) for all \\(T\\). Prove that \\(X_T/T \\to 0\\), a.s.\n[This may be viewed as a generalization of a SLLN for i.i.d. sequences.] Hint: Use Abel’s lemma to show \\(\\sum_{t=1}^T \\EXP[ M_t^2 ]/t^2 &lt; ∞\\) and then use Theorem 30.4."
  },
  {
    "objectID": "probability/martingales.html#notes",
    "href": "probability/martingales.html#notes",
    "title": "30  Martingales",
    "section": "Notes",
    "text": "Notes\nThe material in the introduction is taken from Pollard (2002), Chang (2007), and Dellacherie and Meyer (1982). The latter is an excellent reference for this material.\nTheorem 30.2 and its proof is from Robbins and Siegmund (1971). A version of Theorem 30.2 with \\(Z_t = 0\\) is stated as Exercise II-4 of Neveu (1975).\nExercise 30.3 is from Bertsekas and Tsitsiklis (2000).\n\n\n\n\nBertsekas, D.P. and Tsitsiklis, J.N. 2000. Gradient convergence in gradient methods with errors. SIAM Journal on Optimization 10, 3, 627–642. DOI: 10.1137/s1052623497331063.\n\n\nChang, J.T. 2007. Stochastic processes. Available at: http://www.stat.yale.edu/~pollard/Courses/251.spring2013/Handouts/Chang-notes.pdf.\n\n\nDellacherie, C. and Meyer, P.-A. 1982. Probabilities and potential B: Theory of martingales. North-Holland Mathematical Studies.\n\n\nNeveu, J. 1975. Discrete parameter martingales. North Holland.\n\n\nPollard, D. 2002. A user’s guide to measure theoretic probability. Cambridge University Press.\n\n\nRobbins, H. and Siegmund, D. 1971. A convergence theorem for non-negative almost supermartingales and some applications. In: Optimizing methods in statistics. Elsevier, 233–257. DOI: 10.1016/b978-0-12-604550-5.50015-8.\n\n\nStout, W.F. 1974. Almost sure convergence. Academic Press."
  },
  {
    "objectID": "probability/IPM.html#definition-of-integral-probability-metrics",
    "href": "probability/IPM.html#definition-of-integral-probability-metrics",
    "title": "31  Integral Probablity Metrics",
    "section": "31.1 Definition of Integral Probability Metrics",
    "text": "31.1 Definition of Integral Probability Metrics\n\nNotation\n\nLet \\((\\ALPHABET X,d)\\) be a complete separable metric space, i.e., a Polish space.\nLet \\(w \\colon \\ALPHABET X \\to [1,∞)\\) be a measurable function and define the weighted norm of any function \\(f \\colon \\ALPHABET X \\to \\reals\\) as \\[ \\NORM{f}_w = \\sup_{x \\in \\ALPHABET X} \\frac{f(x)}{w(x)}. \\]\nLet \\(\\ALPHABET M(\\ALPHABET X)\\) denote the set of all measurable real-valued functions on \\(\\ALPHABET X\\) and let \\(\\ALPHABET M_w(\\ALPHABET X)\\) denote the subset with bounded \\(w\\)-norm, i.e., \\[ \\ALPHABET M_w(\\ALPHABET X) =\n\\{ f \\in \\ALPHABET M(\\ALPHABET X) : \\NORM{f}_w &lt; ∞ \\} \\]\nLet \\(\\mathfrak X\\) denote the Borel \\(σ\\)-algebra of \\(\\ALPHABET X\\).\nLet \\(\\ALPHABET P(\\ALPHABET X)\\) denote the set of all probaility measures on \\((\\ALPHABET X, \\mathfrak X)\\) and \\(\\ALPHABET P_w(\\ALPHABET X) = \\{ μ \\in \\ALPHABET P(\\ALPHABET X) : \\int w dμ &lt; ∞ \\}\\).\n\n\\(\\def\\F{\\mathfrak F}\\) Integral probability metrics (IPMs) are pseudo-metrics on \\(\\ALPHABET P_w(\\ALPHABET X)\\) defined as follows.\n\nDefinition 31.1 The integral probability metric \\(d_{\\mathfrak F}\\) is a pseudo-metric on \\(\\ALPHABET P_w(\\ALPHABET X)\\) generated by the function class \\(\\F \\subset \\ALPHABET M_w(\\ALPHABET X)\\) is defined as \\[\n    d_{\\F}(μ_1, μ_2) =\n    \\sup_{f \\in \\F}\n    \\biggl|\n    \\int f d μ_1 - \\int f d μ_2\n    \\biggr|,\n    \\quad\n    \\forall μ_1, μ_2 \\in \\ALPHABET P_w(\\ALPHABET X).\n  \\] The function \\(f\\) is called the witness function and the function class \\(\\F\\) is called the generator.\n\nAn IPM is a pseudo-metric, i.e., it satisfies the following properties: for all \\(μ_1, μ_2, μ_3 \\in \\ALPHABET P_w(\\ALPHABET X)\\)\n\n\\(d_{\\F}(μ_1, μ_1) = 0\\)\nSymmetry. \\(d_{\\F}(μ_1, μ_2) = d_{\\F}(μ_2, μ_1)\\)\nTriangle inequality. \\(d_{\\F}(μ_1, μ_3) \\le d_{\\F}(μ_1, μ_2) + d_{\\F}(μ_2, μ_3)\\).\n\nIPM is a metric when \\(d(μ_1, μ_2) = 0 \\implies μ_1 = μ_2\\).\n\n\nExamples\n[TODO]\n\n\nMaximal generators\nSo far, we haven’t imposed any restriction on the generator \\(\\F\\). We now present a canonical representation of a generator.\n\nDefinition 31.2 For a function class \\(\\F \\in \\ALPHABET M_w(\\ALPHABET X)\\), define the maximal generator \\(\\ALPHABET G_{\\F}\\) as \\[\n    \\ALPHABET G_{\\F} =\n    \\biggl\\{\n      f \\in \\ALPHABET M_w(\\ALPHABET X) :\n      \\biggl| \\int f d μ_1 - \\int f d μ_2 \\biggr| \\le d_{\\F}(μ_1, μ_2), \\space\n      \\text{for all } μ_1, μ_2 \\in \\ALPHABET P_w(\\ALPHABET X)\n      \\biggr\\}.\n  \\]\n\nMaximal generators have the following properties.\n\nProposition 31.1 Let \\(\\F \\subset \\ALPHABET M_w(\\ALPHABET X)\\) be an arbitary generator. Then:\n\n\\(\\ALPHABET G_{\\F}\\) contains the convex hull of \\(\\F\\).\nIf \\(f \\in \\ALPHABET G_{\\F}\\), then \\(α f + β \\in \\ALPHABET G_{\\F}\\) for all \\(|α| \\le 1\\) and \\(β \\in \\reals\\).\nIf the sequence \\(\\{f_n\\}_{n \\ge 1} \\in \\ALPHABET G_{\\F}\\) converges uniformly to \\(f\\), then \\(f \\in \\ALPHABET G_{\\F}\\).\n\n\nMaximal generators also allow us to compare IPMs with different generators.\n\n\n\n\n\n\nBalanced set\n\n\n\nA set \\(\\ALPHABET S\\) is said to be balanced if for all scalars \\(a\\) satisfying \\(|a| \\le 1\\), we have \\(a \\ALPHABET S \\subset \\ALPHABET S\\).\n\n\n\nProposition 31.2 Let \\(\\F \\subset \\mathfrak D \\subset \\ALPHABET M_w(\\ALPHABET X)\\) and \\(μ_1, μ_2 \\in \\ALPHABET P_w(\\ALPHABET X)\\). Then\n\n\\(d_{\\F}(μ_1, μ_2) \\le d_{\\mathfrak D}(μ_1, μ_2)\\).\n\\(\\ALPHABET G_{\\F} \\subset \\ALPHABET G_{\\mathfrak D}\\).\nIf \\(\\mathfrak D \\subset \\ALPHABET G_{\\F}\\) then \\(d_{\\F}\\) and \\(d_{\\mathfrak D}\\) are identical.\nIf \\(\\F \\subset \\mathfrak D \\subset \\ALPHABET G_{\\F}\\) and \\(\\mathfrak D\\) is convex and balanced, contains the constant functions, and is closed with respect to pointwise convergence, then \\(\\mathfrak D = \\ALPHABET G_{\\F}\\)."
  },
  {
    "objectID": "probability/IPM.html#properties-of-ipms",
    "href": "probability/IPM.html#properties-of-ipms",
    "title": "31  Integral Probablity Metrics",
    "section": "31.2 Properties of IPMs",
    "text": "31.2 Properties of IPMs\nWe will assume that the generator \\(\\F\\) is the maximal generator. In particular, this means that \\(0 \\in \\F\\) is an interior point. Define \\((0,∞)\\F \\coloneqq \\cup_{0 &lt; r &lt; ∞} r\\F\\), which is a subset of \\(\\ALPHABET M_w(\\ALPHABET X)\\).\nFor any function \\(f \\in \\ALPHABET M_w(\\ALPHABET X)\\) define the Minkowski functional (or gauge) as follows: \\[\n  ρ_{\\F}(f) = \\inf \\big\\{ ρ &gt; 0 : f/ρ \\in \\F \\bigr\\}.\n\\] where the infimum of an empty set is defined as infinity. Therefore, \\(ρ_{\\F}(f) &lt; ∞\\) if \\(f \\in (0,∞)\\F\\); otherwise \\(ρ_{\\F}(f) = ∞\\).\n[TODO: Explain the Minkowski functional for the examples]\nIn all the above examples, the Minkowski functional is a semi-norm, which is a general property.\n\nProposition 31.3 The Minkowski functional is a semi-norm, i.e., it satisfies the following properties:\n\nNon-negativity. \\(ρ(f) \\ge 0\\).\nAbsolute homogeneity \\(ρ_{\\F}(a f) = |a| ρ_{\\F}(f)\\) for all scalars \\(a\\).\nSubadditivity. \\(ρ_{\\F}(f_1 + f_2) \\le ρ_{\\F}(f_1) + ρ_{\\F}(f_2)\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNon-negativity holds by definition. Absolute homogeneity holds because \\(\\F\\) is balanced, in particular \\(\\F = -\\F\\). Note that absolute homogeneity implies \\(ρ_{\\F}(0) = 0\\).\nTo see why subadditivity holds, let \\(ρ_1 = ρ_{\\F}(f_1)\\) and \\(ρ_2 = ρ_{\\F}(f_2)\\). Then, since \\(\\F\\) is convex, we have for any \\(λ \\in [0,1]\\), \\[\n  λ \\frac{f_1}{ρ_1} + (1-λ) \\frac{f_2}{ρ_2} \\in \\F.\n\\] Take \\(λ = ρ_1/(ρ_1 + ρ_2)\\), which implies \\((f_1 + f_2)/(ρ_1 + ρ_2) \\in \\F\\). Thus, \\(ρ_{\\F}(f_1 + f_2) \\le ρ_1 + ρ_2\\).\n\n\n\n\nProposition 31.4 For any \\(f \\in (0,∞)\\F\\) such that \\(ρ_{\\F}(f) \\neq 0\\) and any \\(μ_1, μ_2 \\in \\ALPHABET P_w(\\ALPHABET X)\\), we have \\[\n  \\left|\n  \\int f d μ_1 - \\int f d μ_2\n  \\right|\n  \\le\n  ρ_{\\F}(f) d_{\\F}(μ_1, μ_2).\n  \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy the assumptions imposed on \\(f\\), we know that \\(ρ_{\\F}(ρ) \\in (0,∞)\\). Define \\(f' = f/ρ_{\\F}(f) \\in \\F\\). Then, \\[\n\\left| \\int f d μ_1 - \\int f d μ_2 \\right|\n=\nρ_{\\F}(f)\n\\left| \\int f' d μ_1 - \\int f' d μ_2 \\right|\n\\le\nρ_{\\F}(f) d_{\\F}(μ_1, μ_2)\n\\] where the last inequality uses the fact that \\(f' \\in \\F\\).\n\n\n\n\n\n\n\n\n\nWhy care about maximal generators\n\n\n\nProposition 31.2 implies that the maximal generator of \\(\\F\\) is the smallest superset of \\(\\F\\) that is convex and balanced, contains the constant functions, and is closed with respect to pointwise convergence. The IPM distance wrt a generator and its maximal generator are the same but the Minkowski functionals are not! In particular if \\(\\F \\subset \\mathfrak D\\), then \\(ρ_{\\F}(f) \\ge ρ_{\\mathfrak D}(f)\\). So, working with the maximal generators gives us the tightest version of the bound in Proposition 31.4."
  },
  {
    "objectID": "probability/IPM.html#notes",
    "href": "probability/IPM.html#notes",
    "title": "31  Integral Probablity Metrics",
    "section": "Notes",
    "text": "Notes\nIPMs were first defined by Zolotarev (1984), where they were called probability metrics with \\(ζ\\)-structure. The term IPM was coined by Müller (1997). The discussion here is mostly borrowed from Müller (1997).\n\n\n\n\nMüller, A. 1997. Integral probability metrics and their generating classes of functions. Advances in Applied Probability 29, 2, 429–443. DOI: 10.2307/1428011.\n\n\nZolotarev, V.M. 1984. Probability metrics. Theory of Probability & Its Applications 28, 2, 278–302. DOI: 10.1137/1128025."
  },
  {
    "objectID": "probability/stochastic-stability.html#different-notions-of-stability",
    "href": "probability/stochastic-stability.html#different-notions-of-stability",
    "title": "32  Stochastic stability",
    "section": "32.1 Different notions of stability",
    "text": "32.1 Different notions of stability\nFirst we recall some terminology related to almost sure convergence.\n\nA random sequence \\(\\{X_t\\}_{t \\ge 1}\\) in a sample space \\(Ω\\) converges to a random variable \\(X\\) almost surely if \\[\n\\PR\\Bigl(ω \\in Ω : \\lim_{t \\to ∞} \\| X_t(ω) - X(ω) \\| = 0 \\Bigr) = 1.\n\\]\nThe convergence is said to be exponentially fast with rate no slower than \\(γ^{-1}\\) for some \\(γ &gt; 1\\) (not dependent on \\(ω\\)) if \\(γ^t \\| X_t - X \\|\\) converges almost surely to some \\(Δ \\ge 0\\).\nGiven a set \\(\\mathcal D \\in \\reals^n\\), a random sequence \\(\\{X_t\\}_{t \\ge 1}\\) is said to converge to \\(\\mathcal D\\) almost surely if \\[\n\\PR\\Bigl(ω \\in Ω : \\lim_{t \\to ∞} \\mathrm{dist}(X_t(ω), \\mathcal D) = 0 \\Bigr) = 1,\n\\] where \\(\\mathrm{dist}(x,\\mathcal D) = \\inf_{\\tilde x \\in \\mathcal D} \\| x - \\tilde x\\|\\).\n\n\nDefinition 32.1 The origin of \\eqref{eq:stability-dynamics} is said to be:\n\nStable in probability if \\(\\lim_{x_0 \\to 0} \\PR(\\sup_{t \\ge 1} \\| X_t \\| &gt; ε ) = 0\\) for any \\(ε &gt; 0\\).\nAsymptotically stable in prbability if it is stable in probability and moreover \\(\\lim_{x_0 \\to 0} \\PR(\\lim_{t \\to ∞} \\|X_t\\| = 0) = 1\\).\nExponentially stable in prbability if for some \\(γ &gt; 1\\) (not dependent on \\(ω\\)), \\(\\lim_{x_0 \\to 0} \\PR(\\lim_{t \\to ∞} \\|γ^t X_t\\| = 0) = 1\\)."
  },
  {
    "objectID": "probability/stochastic-stability.html#sufficient-conditions-for-stochastic-stability",
    "href": "probability/stochastic-stability.html#sufficient-conditions-for-stochastic-stability",
    "title": "32  Stochastic stability",
    "section": "32.2 Sufficient conditions for stochastic stability",
    "text": "32.2 Sufficient conditions for stochastic stability\n\nDefinition 32.2 Given a set \\(\\mathcal Q \\in \\reals^n\\) containing the origin, the origin of \\eqref{eq:stability-dynamics} is said to be:\n\nlocally a.s. asymptotically stable in \\(\\mathcal Q\\) if starting from \\(x_0 \\in \\mathcal Q\\) all the sample paths \\(X_t\\) stay in \\(\\mathcal Q\\) for all \\(t \\ge 1\\) and converge to origin almost surely.\nlocally a.s. exponentially stable in \\(\\mathcal Q\\) if it is locally a.s. asymptotically stable and the convergence is exponentially fast.\n\nIf the above properties hold for \\(\\mathcal Q = \\reals^n\\), the system is said to be globally a.s. asymptotically (or exponentially) stable.\n\n\n\n\n\n\n\nFunction class \\(\\mathcal K\\)\n\n\n\nA continuous function \\(h \\colon [0, a) \\to [0, ∞)\\) is said to belong to class \\(\\mathcal K\\) if it is strictly increasing and \\(h(0) = 0\\).\n\n\n\nTheorem 32.1 For the stochastic discrete-time system \\eqref{eq:stability-dynamics}, let \\(\\{X_t\\}_{t \\ge 1}\\) be Markov.\nLet \\(V \\colon \\reals^n \\to \\reals\\) be a continuous positive definite and radially unbounded function. Define the level set \\(\\mathcal Q_{λ} \\coloneqq \\{ x : 0 \\le V(x) &lt; λ \\}\\) for some \\(λ &gt; 0\\).\nLet \\(φ \\colon \\reals^n \\to \\reals\\) be a continious function that satisfies \\(φ(x) \\ge 0\\) for all \\(x \\in \\mathcal Q_{λ}\\).\nSuppose the following property holds: for all \\(x \\in \\reals^n\\), \\[\n  \\EXP[ V(X_{t+1}) \\mid X_t = x ] - V(x) \\le -φ(x), \\quad \\forall t \\ge 1.\n\\] Then:\n\nFor any initial condition \\(x_0 \\in \\mathcal Q_{λ}\\), \\(\\{X_t\\}_{t \\ge 1}\\) converges to \\(\\mathcal D_1 \\coloneqq \\{x \\in \\mathcal Q_{λ} : φ(x) = 0 \\}\\) with probability at least \\(1 - V(x_0)/λ\\).\nif moreover \\(φ(x)\\) is positive definite on \\(\\mathcal Q_{λ}\\) and there exist two calss \\(\\mathcal K\\) functions \\(h_1\\) and \\(h_2\\) such that \\(h_1(\\|x\\|) \\le V(x) \\le h_2(\\|x\\|)\\), then \\(x = 0\\) is asymptotically stable in probability.\n\n\nUnder slightly stronger conditions, it is also possible to characterize the rate of convergence.\n\nTheorem 32.2 For the stochastic discrete-time system \\eqref{eq:stability-dynamics}, let \\(\\{X_t\\}_{t \\ge 1}\\) be Markov.\nLet \\(V \\colon \\reals^n \\to \\reals\\) be a continous nonnegative function.\nSuppose the following condition holds: there exists an \\(α \\in (0,1)\\) such that \\[\n  \\EXP[ V(X_{t+1}) \\mid X_t = x ] - V(x) \\le -α V(x), \\quad \\forall t \\ge 1.\n\\] Then:\n\nFor any initial state \\(x_0\\), \\(V(X_t)\\) almost surely converges to \\(0\\) exponentially fast with a rate no slower than \\(1-α\\).\nIf moreover \\(V\\) satisfies \\(c_1 \\|x\\|^p \\le V(x) \\le c_2 \\|x\\|^p\\) for some \\(c_1, c_2, p &gt; 0\\), then \\(x = 0\\) is globally a.s. exponentially stable."
  },
  {
    "objectID": "probability/stochastic-stability.html#weaker-sufficient-conditions-for-stochastic-stability",
    "href": "probability/stochastic-stability.html#weaker-sufficient-conditions-for-stochastic-stability",
    "title": "32  Stochastic stability",
    "section": "32.3 Weaker sufficient conditions for stochastic stability",
    "text": "32.3 Weaker sufficient conditions for stochastic stability\n\nTheorem 32.3 For the stochastic discrete-time system \\eqref{eq:stability-dynamics}, let \\(V \\colon \\reals^n \\to \\reals\\) be a continuous nonnegative and radially unbounded function. Define the set \\(Q_{λ} \\coloneqq \\{x : V(x) &lt; λ \\}\\) for some \\(λ &gt; 0\\).\nSuppose the following conditions hold:\n\nFor any \\(t\\) such that \\(X_t \\in \\mathcal Q_{λ}\\), we have \\[ \\EXP[ V(X_{t+1}) \\mid \\mathcal F_t ] - V(X_t) \\le 0. \\]\nThere is an integer \\(T \\ge 1\\) (not depent on \\(ω\\)) and a continous function \\(φ \\colon \\reals^n \\to \\reals\\) that satisfies \\(φ(x) \\ge 0\\) for all \\(x \\in \\mathcal Q_{λ}\\) such that for any \\(t\\), \\[ \\EXP[ V(X_{t+\\color{red}{T}}) \\mid \\mathcal F_t ] - V(X_t) \\le -φ(X_t) \\]\n\nThen, the implications of Theorem 32.1 hold.\n\nThe sufficient conditions for exponential stability can be weakened in a similar manner.\n\nTheorem 32.4 Suppose assumptions a) and b) of Theorem 32.3 are satisfied with the inequality of b) strengthened to \\[ \\EXP[ V(X_{t+\\color{red}{T}}) \\mid \\mathcal F_t ] - V(X_t) \\le -αV(X_t) \\] for some \\(α \\in (0,1)\\).\nThen, the implications of Theorem 32.2 hold."
  },
  {
    "objectID": "probability/stochastic-stability.html#notes",
    "href": "probability/stochastic-stability.html#notes",
    "title": "32  Stochastic stability",
    "section": "Notes",
    "text": "Notes\nThe material in this section is adapted from Qin et al. (2020).\n\n\n\n\nQin, Y., Cao, M., and Anderson, B.D.O. 2020. Lyapunov criterion for stochastic systems and its applications in distributed computation. IEEE Transactions on Automatic Control 65, 2, 546–560. DOI: 10.1109/tac.2019.2910948."
  },
  {
    "objectID": "linear-algebra/matrix-relationships.html#matrix-identities",
    "href": "linear-algebra/matrix-relationships.html#matrix-identities",
    "title": "33  Some useful matrix relationships",
    "section": "33.1 Matrix identities",
    "text": "33.1 Matrix identities\n\nIf \\((I + U)\\) is invertible, then \\[ U(I + U)^{-1} = I - (I + U)^{-1}. \\] This can be verified my multiplying both sides with \\((I+U)\\).\nSimplified Sherman-Morrison-Woodbudy formula: If \\((I + UV)\\) or, equivalently, \\((I + VU)\\) is invertible, then \\[ (I + UV)^{-1} = I - U(I + VU)^{-1}V. \\] This can be verified by multiplying both sides with \\((I + UV)\\). This relationship can also be written as: \\[ (I + UV)^{-1} = I - UV^{1/2}(I + V^{1/2}UV^{1/2})^{-1}V^{1/2}. \\]\nA slight generalization of the above is: \\[ (I + U T^{-1} V)^{-1} = I - U(T + VU)^{-1}V. \\]\nIf \\((I + UV)\\) or, equivalently, \\((I + VU)\\) is invertible, then \\[ V(I + UV)^{-1} = (I + VU)^{-1}V. \\] This can be verified by left multiplying by \\((I + VU)\\) and right multiplying by \\((I + UV)\\)."
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#definite-and-basic-properties",
    "href": "linear-algebra/positive-definite-matrix.html#definite-and-basic-properties",
    "title": "34  Positive definite matrices",
    "section": "34.1 Definite and basic properties",
    "text": "34.1 Definite and basic properties\n\nDefinition 34.1 A \\(n \\times n\\) symmetric matrix \\(M\\) is called\n\npositive definite (written as \\(M \\succ 0\\)) if for all \\(x \\in \\reals^n\\), \\(x \\neq 0\\), we have \\[x^\\TRANS M x &gt; 0.\\]\npositive semi definite (written as \\(M \\succeq 0\\)) if for all \\(x \\in \\reals^n\\), \\(x \\neq 0\\), we have \\[x^\\TRANS M x \\ge 0.\\]\n\n\n\nExamples\n\n\\(\\MATRIX{ 3 & 0 \\\\ 0 & 2 } \\succ 0\\) because \\(\\MATRIX{ x_1 & x_2 } \\MATRIX{ 3 & 0 \\\\ 0 & 2 } \\MATRIX{ x_1 \\\\ x_2 } = 3 x_1^2 + 2 x_2^2 &gt; 0.\\)\n\\(\\MATRIX{ 0 & 0 \\\\ 0 & 2 } \\succeq 0\\) because \\(\\MATRIX{ x_1 & x_2 } \\MATRIX{ 0 & 0 \\\\ 0 & 2 } \\MATRIX{ x_1 \\\\ x_2 } = 2 x_2^2 \\ge 0\\)."
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#remarks-on-positive-definite-matrices",
    "href": "linear-algebra/positive-definite-matrix.html#remarks-on-positive-definite-matrices",
    "title": "34  Positive definite matrices",
    "section": "34.2 Remarks on positive definite matrices",
    "text": "34.2 Remarks on positive definite matrices\n\nBy making particular choices of \\(x\\) in the definition of positive definite matrix, we have that for a positive definite matrix \\(M\\),\n\n\\(M_{ii} &gt; 0\\) for all \\(i\\)\n\\(M_{ij} &lt; \\sqrt{M_{ii} M_{jj}}\\) for all \\(i \\neq j\\).\n\nHowever, satisfying these inequalities is not sufficient for positive definiteness.\nA symmetric matrix is positive definite (respt. postive semi-definite) if and only if all of its eigenvalues are positive (respt. non-negative).\nTherefore, a sufficient condition for a symmetric matrix to be positive definite is that all diagonal elements are positive and the matrix is diagonally dominant, i.e., \\(M_{ii} &gt; \\sum_{j \\neq i} | M_{ij}|\\) for all \\(i\\).\nIf \\(M\\) is symmetric positive definite, then so is \\(M^{-1}\\).\nIf \\(M\\) is symmetric positive definite, then \\(M\\) has a unique symmetric positive definite square root \\(R\\) (i.e., \\(RR = M\\)).\nIf \\(M\\) is symmetric positive definite, then \\(M\\) has a unique Cholesky factorization \\(M = T^\\TRANS T\\), where \\(T\\) is upper triangular with positive diagonal elements.\nThe set of positive semi-definite matrices forms a convex cone.\nPositive definiteness introduces a partial order on the convex cone of positive semi-definite matrices. In particular, we say that for two positive semi-definite matrices \\(M\\) and \\(N\\) of the same dimension, \\(M \\succeq N\\) if \\(M - N\\) is positive semi-definite. For this reason, often \\(M \\succ 0\\) and \\(M \\succeq 0\\) is used a short-hand to denote that \\(M\\) is positive definite and positive semi-definite.\nLet \\(M\\) is a symmetric square matrix. Let \\[ λ_1(M) \\ge λ_2(M) \\ge \\dots \\ge λ_n(M) \\] denote the ordered (real) eigenvalues of \\(M\\). Then \\[ λ_1(M)I \\succeq M \\succeq λ_n(M)I. \\]\nIf \\(M \\succeq N\\), then \\[ λ_k(M) \\ge λ_k(N), \\quad k \\in \\{1, \\dots, n\\}. \\]\nIf \\(M \\succeq N \\succ 0\\), then \\[ N^{-1} \\succeq M^{-1} \\succ 0. \\]\nIf \\(M \\succeq N\\) are \\(n × n\\) matrices and \\(T\\) is a \\(m × n\\) matrix, then \\[ T^\\TRANS M T \\succeq T^\\TRANS N T. \\]\nIf \\(M, N\\) are \\(n×\\) positive semi-definite matrices, then \\[ \\sum_{i=1}^k λ_i(M) λ_{n-i+1}(N) \\le\n  \\sum_{i=1}^k λ_i(MN) \\le\n  \\sum_{i=1}^k λ_i(M)λ_i(N),\n  \\quad k \\in \\{1, \\dots, n\\}.\n   \\] Note that this property does not require \\(M - N\\) to be positive or negative semi-definite.\nIf \\(M \\succ 0\\) and \\(T\\) are square matrices of the same size, then \\[ TMT + M^{-1} \\succeq 2T. \\]"
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#a-useful-relationship.",
    "href": "linear-algebra/positive-definite-matrix.html#a-useful-relationship.",
    "title": "34  Positive definite matrices",
    "section": "34.3 A useful relationship.",
    "text": "34.3 A useful relationship.\nSymmetric block matrices of the form\n\\[ C = \\MATRIX{ A & X \\\\ X^\\TRANS & B } \\]\noften appear in applications. If \\(A\\) is non-singular, we can write\n\\[\n\\MATRIX{A & X \\\\ X^\\TRANS & B } =\n\\MATRIX{I & 0 \\\\ X^\\TRANS A^{-1} & I}\n\\MATRIX{A & 0 \\\\ 0 & B - X^\\TRANS A^{-1} X }\n\\MATRIX{I & A^{-1} X \\\\ 0 & I }\n\\] which shows that \\(C\\) is congruent to a block diagonal matrix, which is positive definite when its diagonal blocks are postive definite. Therefore, \\(C\\) is positive definite if and only if both \\(A\\) and \\(B - X^\\TRANS A^{-1} X\\) are positive definite. The matrix \\(B = X^\\TRANS A^{-1} X\\) is called the Shur complement of \\(A\\) in \\(C\\)."
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#determinant-bounds",
    "href": "linear-algebra/positive-definite-matrix.html#determinant-bounds",
    "title": "34  Positive definite matrices",
    "section": "34.4 Determinant bounds",
    "text": "34.4 Determinant bounds\n\nProposition 34.1 (Fischer’s inequality) Suppose \\(A\\) and \\(C\\) are positive semidefinite matrix and \\[ M = \\MATRIX{A & B \\\\ B^\\TRANS & C}. \\] Then \\[ \\det(M) \\le \\det(A) \\det(C). \\]\n\nRecursive application of Fischer’s inequality gives the Hadamard’s inequality for a symmetric positive definite matrix: \\[ \\det(A) \\le A_{11} A_{22} \\cdots A_{nn}, \\] with equality if and only if \\(A\\) is diagonal.\n\nProposition 34.2 If \\(M \\succ N \\succ 0\\) are \\(n × n\\) matrices and \\(T\\) is a \\(m × n\\) matrix, then \\[ \\sup_{ T \\neq 0} \\frac{ \\| T^\\TRANS M T \\| }{ \\| T^\\TRANS N T \\|}\n   \\le \\frac{ \\det(M) }{ \\det(N) }, \\] where for any matrix \\(M\\), \\[\n  \\| M \\| = \\sup_{x \\neq 0} \\frac{ \\| M x \\|_2 }{ \\|x\\|_2 }\n\\] is the \\(2\\)-norm of the matrix.\n\nProposition 34.2 is taken from (Abbasi-Yadkori2011?)."
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#references",
    "href": "linear-algebra/positive-definite-matrix.html#references",
    "title": "34  Positive definite matrices",
    "section": "References",
    "text": "References\nThe properties of positive definite matrices are stated in any book on the theory of matrices. See for example Marshall et al. (2011).\nHistorically, a matrix used as a test matrix for testing positive definiteness was the Wilson matrix \\[ W = \\MATRIX{5 & 7 & 6 & 5 \\\\ 7 & 10 & 8 & 7 \\\\ 6 & 8 & 10 & 9 \\\\ 5 & 7 & 9\n& 10}. \\] For a nice overview of Wilson matrix, see this blog post.\n\n\n\n\n\nMarshall, A.W., Olkin, I., and Arnold, B.C. 2011. Inequalities: Theory of majorization and its applications. Springer New York. DOI: 10.1007/978-0-387-68276-1."
  },
  {
    "objectID": "linear-algebra/svd.html#singular-values",
    "href": "linear-algebra/svd.html#singular-values",
    "title": "35  Singular value decomposition",
    "section": "35.1 Singular values",
    "text": "35.1 Singular values\nLet \\(A\\) be a \\(n × d\\) matrix. Then, the matrix \\(A^\\TRANS A\\) is a symmetric \\(d × d\\) matrix, so its eigenvalues are real. Moreover, \\(A^\\TRANS A\\) is positive semi-definite, so the eigen values are non-negative. Let \\(\\{ λ_1, \\dots, λ_d \\}\\) denote the eigenvalues of \\(A^\\TRANS A\\), with repetitions. Order then so that \\(λ_1 \\ge λ_2 \\ge \\dots \\ge λ_d \\ge 0\\). Let \\(σ_i = \\sqrt{λ_i}\\), so that \\(σ_1 \\ge σ_2 \\ge \\dots σ_d \\ge 0\\). These numbers are called the singular values of \\(A\\).\n\nProperties of singular values\n\nThe number of non-zero singular values of \\(A\\) equals to the rank of \\(A\\). In particular, if \\(A\\) is \\(n × d\\) where \\(n &lt; d\\), then \\(A\\) has at most \\(n\\) nonzero singular values.\nIt can be shown that\n\\[ σ_1 = \\max_{\\|x\\| = 1}  \\| A x \\| . \\]\nLet \\(v_1\\) denote the arg-max of the above optimization. \\(v_1\\) is called the first singular vector of \\(A\\). Then,\n\\[ σ_2 = \\max_{ x \\perp v_1, \\|x \\| = 1}  \\| A x\\|. \\]\nLet \\(v_2\\) denote the arg-max of the above optimization. \\(v_2\\) is called the second singular vector of \\(A\\), and so on.\nLet \\(A\\) be a \\(n × d\\) matrix and \\(v_1, \\dots, v_r\\) be the singular vectors, where \\(r = \\text{rank}(A)\\). Then for any \\(k \\in \\{1, \\dots, r\\}\\), let \\(V_k\\) be the subspace spanned by \\(\\{v_1, \\dots, v_k\\}\\). Then, \\(V_k\\) is the best \\(k\\)-dimensional subspace for \\(A\\).\nFor any matrix \\(A\\), \\[ \\sum_{i =1}^r σ_i^2(A) = \\| A \\|_{F}^2\n   := \\sum_{j,k} a_{jk}^2. \\]\nAny vector \\(v\\) can be written as a linear combination of \\(v_1, \\dots, v_r\\) and a vector perpendicular to \\(V_r\\) (defined above). Now, \\(Av\\) can be written as the same linear combination of \\(Av_1, Av_2, \\dots, Av_r\\). So, \\(Av_1, \\dots, Av_r\\) form a fundamental set of vectors associated with \\(A\\). We normalize them to length one by \\[ u_i = \\frac{1}{σ_i(A)} A v_i. \\] The vectors \\(u_1, \\dots, u_r\\) are called the left singular vectors of \\(A\\). The \\(v_i\\) are called the right singular vectors.\nBoth the left and the right singular vectors are orthogonal.\n\n\n\n\n\n\n\nSingular value decomposition\n\n\n\nFor any matrix \\(A\\), \\[ A = \\sum_{i=1}^r σ_i u_i v_i^\\TRANS \\] where \\(u_i\\) and \\(v_i\\) are the left and right singular vectors, and \\(σ_i\\) are the singular values.\nEquivalently, in matrix notation: \\[ A = U D V^\\TRANS \\] where the columns of \\(U\\) and \\(V\\) consist of the left and right singular vectors, respectively, and \\(D\\) is a diagonal matrix whose diagonal entries are the singular values of \\(A\\).\n\n\nIf \\(A\\) is a positive definite square matrix, then the SVD and the eigen-decomposition coincide."
  },
  {
    "objectID": "linear-algebra/svd.html#best-rank-k-approximations",
    "href": "linear-algebra/svd.html#best-rank-k-approximations",
    "title": "35  Singular value decomposition",
    "section": "35.2 Best rank-\\(k\\) approximations",
    "text": "35.2 Best rank-\\(k\\) approximations\nThere are two important matrix norms, the Frobenius norm which is defined as \\[\n  \\| A \\|_{F} = \\sqrt{ \\sum_{i,j} a_{ij}^2 }\n\\] and the induced norm which is defined as \\[\n  \\| A \\|_2 = \\max_{\\|x \\| = 1} \\| A x \\|.\n\\]\nNote that the Frobenius norm is equal to the square root of the sum of squares of the singular values and the \\(2\\)-norm is the largest singular value.\nLet \\(A\\) be an \\(n × d\\) matrix and think of \\(A\\) as the \\(n\\) points in \\(d\\)-dimensional space. The Frobenius norm of \\(A\\) is the square root of the sum of squared distance of the points to the origin. The induced norm is the square root of the sum of squared distances to the origin along the direction that maximizes this quantity.\n\nProposition 35.1 (Best rank-\\(k\\) approximation) Let \\[ A = \\sum_{i = 1}^r σ_i u_i v_i^\\TRANS \\] be the SVD of \\(A\\). For \\(k \\in \\{1, \\dots, r\\}\\), let \\[ A_k = \\sum_{i=1}^k σ_i u_i v_i^\\TRANS \\] be the sum truncated after \\(k\\) terms.\nThen, \\(A_k\\) is the best rank \\(k\\) approximation to \\(A\\), when the error is measured in either the induced norm or the Frobenius norm.\n\n\n\n\n\n\n\nProof outline\n\n\n\n\n\nThis result is established by showing the following properties:\n\nThe rows of \\(A_k\\) are the projections of the rows of \\(A\\) onto the subspace \\(V_k\\) spanned by the first \\(k\\) right singular vectors of \\(A\\).\nFor any matrix \\(B\\) of rank at most \\(k\\) \\[ \\| A - A_k \\|_{F} \\le \\|A - B \\|_{F}. \\]\n\\(\\| A - A_k\\|_2^2 = σ_{k+1}^2.\\)\nFor any matrix \\(B\\) of rank at most \\(k\\) \\[ \\| A - A_k \\|_{2} \\le \\|A - B \\|_{2}. \\]"
  },
  {
    "objectID": "linear-algebra/svd.html#notes",
    "href": "linear-algebra/svd.html#notes",
    "title": "35  Singular value decomposition",
    "section": "Notes",
    "text": "Notes\nThe chapter on SVD in Hopcroft and Kannan (2012) contains a nice intuitive explanation of SVD.\n\n\n\n\nHopcroft, J. and Kannan, R. 2012. Computer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf."
  },
  {
    "objectID": "linear-algebra/rkhs.html#review-of-linear-operators",
    "href": "linear-algebra/rkhs.html#review-of-linear-operators",
    "title": "36  Reproducing Kernel Hilbert Space",
    "section": "36.1 Review of Linear Operators",
    "text": "36.1 Review of Linear Operators\n\n\n\n\n\n\nLinear Operator\n\n\n\nLet \\(\\mathcal F\\) and \\(\\mathcal G\\) be normed vector spaces over \\(\\reals\\). A function \\(A \\colon \\mathcal F \\to \\mathcal G\\) is called a linear operator if it satisfies the following properties:\n\nHonogeneity: For any \\(α \\in \\reals\\) and \\(f \\in \\mathcal F\\), \\(A(αf) = α (Af)\\).\nAdditivity: For any \\(f,g \\in \\mathcal F\\), \\(A(f + g) = Af + Ag\\).\n\nThe operator norm of a linear operator is defined as \\[ \\NORM{A} = \\sup_{f \\in \\mathcal F} \\frac{ \\NORM{A f}_{\\mathcal G}}\n{\\NORM{f}}_{\\mathcal F}. \\]\nIf \\(\\NORM{A} &lt; ∞\\), then the operator is said to be a bounded operator.\n\n\nAs an example, suppose \\(\\mathcal F\\) is an inner product space. For a \\(g \\in \\mathcal F\\), the operator \\(A_g \\colon \\mathcal F \\to \\reals\\) defined by \\(A_g(f) = \\langle f, g \\rangle\\) is a linear operator. Such scalar valued operators are called functionals on \\(\\mathcal F\\).\nLinear operators satisfy the following property.\n\nTheorem 36.1 If \\(A \\colon \\mathcal F \\to \\mathcal G\\) is a linear operator, then the following three conditions are equivalent:\n\n\\(A\\) is a bounded operator.\n\\(A\\) is continuous on \\(\\mathcal F\\).\n\\(A\\) is continious at one point of \\(\\mathcal F\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "href": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "title": "36  Reproducing Kernel Hilbert Space",
    "section": "36.2 Dual of a linear operator",
    "text": "36.2 Dual of a linear operator\nThere are two notions of dual of a linear operator: algebraic dual and topological dual. If \\(\\mathcal F\\) is a normed space, then the space of all linear functionals \\(A \\colon \\mathcal F \\to \\reals\\) is the algebraic dual space of \\(\\mathcal F\\); the space of all continuous linear functions \\(A \\colon \\mathcal F \\to \\reals\\) is the topological dual space of \\(\\mathcal F\\).\nIn finite-dimensional space, the two notions of dual spaces coincide (every linear operator on a normed, finite dimensional space is bounded). But this is not the case for infinite dimensional spaces.\n\nTheorem 36.2 (Riesz representation) In a Hilbert space \\(\\mathcal F\\), all continuous linear functionals are of the form \\(\\langle\\cdot, g\\rangle\\), for some \\(g \\in \\mathcal F\\).\n\nTwo Hilbert spaces \\(\\mathcal F\\) and \\(\\mathcal G\\) are said to be isometrically isomorphic if there is a linear bijective map \\(U \\colon \\mathcal F \\to \\mathcal G\\) which preserves the inner product, i.e., \\(\\langle f_1, f_2 \\rangle_{\\mathcal F} = \\langle U f_1, U f_2 \\rangle_{\\mathcal G}\\).\nNote that Riesz representation theorem gives a natural isometric isomorphism \\(\\psi \\colon g \\mapsto \\langle \\cdot, g \\rangle_{\\mathcal F}\\) between \\(\\mathcal F\\) and its topological dual \\(\\mathcal F'\\), whereby \\(\\NORM{ψ(g)}_{\\mathcal F'} = \\NORM{g}_{\\mathcal F}\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "href": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "title": "36  Reproducing Kernel Hilbert Space",
    "section": "36.3 Reproducing kernel Hilbert space",
    "text": "36.3 Reproducing kernel Hilbert space\nLet \\(\\mathcal H\\) be a Hilbert space of functions mapping from some non-empty set \\(\\ALPHABET X\\) to \\(\\reals\\). Note that for every \\(x \\in \\ALPHABET X\\), there is a very special functional on \\(\\mathcal H\\): the one that assigns to each \\(f \\in \\mathcal H\\), its value at \\(x\\). This is called the evaluation functional and denoted by \\(δ_x\\). In particular, \\(δ_x \\colon \\mathcal H \\to \\reals\\), where \\(δ_x \\colon f \\mapsto f(x)\\).\n\n\n\n\n\n\nReproducing kernel Hilbert space (RKHS)\n\n\n\nA Hilbert space \\(\\mathcal H\\) of functions \\(f \\colon \\ALPHABET X \\to \\reals\\) defined on a non-empty set \\(\\ALPHABET X\\) is said to be a RKHS if \\(δ_x\\) is continuous for all \\(x \\in \\ALPHABET X\\).\n\n\nIn view of Theorem 36.1, an equivalent definition is that a Hilbert space \\(\\mathcal H\\) is RKHS if the evaluation functionals \\(δ_x\\) are bounded, i.e., for every \\(x \\in \\ALPHABET X\\), there exists a \\(M_x\\) such that \\[ | δ_x | = | f(x) | \\le M_x \\| f \\|_{\\mathcal H}, \\quad \\forall f \\in \\mathcal H\\]\nAn immediate implication of the above property is that two functions which agree in RKHS norm agree at every point: \\[ | f(x) - g(x) | = | δ_x(f - g) | \\le M_x \\| f - g \\|_{\\mathcal H},\n   \\quad \\forall f,g \\in \\mathcal H. \\]\nFor example, the \\(L_2\\) space of square integrable functions i.e., \\(\\int_{\\reals^n} f(x)^2 dx &lt; ∞\\) with inner product \\(\\int_{\\reals^n} f(x) g(x)dx\\) is a Hilbert space, but not an RKHS because the delta function, which has the reproducing property \\[ f(x) = \\int_{\\reals^n} δ(x - y) f(y) dy \\] is not bounded.\nRKHS are particularly well behaved. In particular, if we have a sequence of functions \\(\\{f_n\\}_{n \\ge 1}\\) which converges to a limit \\(f\\) in the Hilbert-space norm, i.e., \\(\\lim_{n \\to ∞} \\NORM{f_n - f}_{\\mathcal H} = 0\\), then they also converge pointwise, i.e., \\(\\lim_{n \\to ∞} f_n(x) = f(x)\\) for all \\(x \\in \\ALPHABET X\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-rhks",
    "href": "linear-algebra/rkhs.html#properties-of-rhks",
    "title": "36  Reproducing Kernel Hilbert Space",
    "section": "36.4 Properties of RHKS",
    "text": "36.4 Properties of RHKS\nRKHS has many useful properties:\n\nFor any RKHS, there exists a unique kernel \\(k \\colon \\ALPHABET X × \\ALPHABET X \\to \\reals\\) such that\n\nfor any \\(x \\in \\ALPHABET X\\), \\(k(\\cdot, x) \\in \\mathcal H\\),\nfor any \\(x \\in \\ALPHABET X\\) and \\(f \\in \\mathcal H\\), \\(\\langle f, k(\\cdot, x) \\rangle = f(x)\\) (the reproducing property).\n\nIn particular, for any \\(x,y \\in \\ALPHABET X\\), \\[ k(x,y) = \\langle k(\\cdot, x), k(\\cdot, y) \\rangle. \\] Thus, the kernel is a symmetric function.\nThe kernel is positive definite, i.e., for any \\(n \\ge 1\\), for all \\((a_1, \\dots, a_n) \\in \\reals^n\\) and \\((x_1, \\dots, x_n) \\in \\ALPHABET X^n\\), \\[ \\sum_{i=1}^n \\sum_{j=1}^n a_i a_i h(x_i, x_j) \\ge 0 \\]\nA conseuqence of positive definiteness is that \\[| k(x, y)|^2 \\le k(x, x) k(y, y). \\]\n(Moore-Aronszajn Theorem) For every positive definite kernel \\(K\\) on \\(\\ALPHABET X × \\ALPHABET X\\), there is a unique RKHS on \\(\\ALPHABET X\\) with \\(K\\) as its reproducing kernel."
  },
  {
    "objectID": "linear-algebra/rkhs.html#examples-of-kernels",
    "href": "linear-algebra/rkhs.html#examples-of-kernels",
    "title": "36  Reproducing Kernel Hilbert Space",
    "section": "36.5 Examples of kernels",
    "text": "36.5 Examples of kernels\nSome common examples of symmetric positive definite kernels for \\(\\ALPHABET X = \\reals^n\\) are as follows:\n\nLinear kernel \\[ k(x,y) = \\langle x, y \\rangle\\]\nGaussian kernel \\[ k(x,y) = \\exp\\biggl( - \\frac{\\| x - y \\|^2}{σ^2} \\biggr),\n   \\quad σ &gt; 0. \\]\nPolynomail kernel \\[ k(x,y) = \\bigl( 1 + \\langle x, y \\rangle \\bigr)^d,\n   \\quad d \\in \\integers_{&gt; 0}. \\]"
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-kernels",
    "href": "linear-algebra/rkhs.html#properties-of-kernels",
    "title": "36  Reproducing Kernel Hilbert Space",
    "section": "36.6 Properties of kernels",
    "text": "36.6 Properties of kernels\n\nSuppose \\(φ \\colon \\ALPHABET X \\to \\reals^n\\) is a feature map, then \\[ k(x,y) := \\langle φ(x), φ(y) \\rangle \\] is a kernel.\nNote that there are no conditions on \\(\\ALPHABET X\\) (e.g., \\(\\ALPHABET X\\) doesn’t need to be an inner product space).\nIf \\(k\\) is a kernel on \\(\\ALPHABET X\\), then for any \\(α &gt; 0\\), \\(αk\\) is also a kernel.\nIf \\(k_1\\) and \\(k_2\\) are kernels on \\(\\ALPHABET X\\), then \\(k_1 + k_2\\) is also a kernel.\nIf \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be arbitrary sets and \\(A \\colon \\ALPHABET X \\to \\ALPHABET Y\\) is a map. Let \\(k\\) be a kernel on \\(\\ALPHABET Y\\). Then, \\(k(A(x_1), A(x_2))\\) is a kernel on \\(\\ALPHABET X\\).\nIf \\(k_1 \\colon \\ALPHABET X_1 × \\ALPHABET X_1 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_1\\) and \\(k_2 \\colon \\ALPHABET X_2 × \\ALPHABET X_2 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_2\\), then \\[ k( (x_1, x_2), (y_1, y_2) ) = k_1(x_1, y_1) k_2(x_2, y_2) \\] is a kernel on \\(\\ALPHABET X_1 × \\ALPHABET X_2\\).\n(Mercer-Hilber-Schmit theorems) If \\(k\\) is positive definite kernel (that is continous with finite trace), then there exists an infinite sequence of eiegenfunctions \\(\\{ φ_i \\colon \\ALPHABET X \\to \\reals \\}_{i \\ge 1}\\) and real eigenvalues \\(\\{λ_i\\}_{i \\ge 1}\\) such that we can write \\(k\\) as: \\[ k(x,y) = \\sum_{i=1}^∞ λ_i φ_i(x) φ_i(y). \\] This is analogous to the expression of a matrix in terms of its eigenvector and eigenvalues, except in this case we have functions and an infinity of them.\nUsing this property, we can define the inner product of RKHS in a simpler form. First, for any \\(f \\in \\mathcal H\\), define \\[ f_i = \\langle f, φ_i \\rangle.\\] Then, for any \\(f, g \\in \\mathcal H\\), \\[ \\langle f, g \\rangle = \\sum_{i=1}^∞ \\frac{ f_i g_i } { λ_i }. \\]"
  },
  {
    "objectID": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "href": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "title": "36  Reproducing Kernel Hilbert Space",
    "section": "36.7 Kernel ridge regression",
    "text": "36.7 Kernel ridge regression\nGiven labelled data \\(\\{ (x_i, y_i) \\}_{i=1}^n\\), and a feature map \\(φ \\colon \\ALPHABET X \\to \\ALPHABET Z\\), define the RKHS \\(\\ALPHABET H\\) of functions from \\(\\ALPHABET Z \\to \\reals\\) with the kernel \\(k(x,y) = \\langle φ(x), φ(y) \\rangle_{\\mathcal H}\\). Now, consider the problem of minimizing\n\\[f^* = \\arg \\min_{f \\in \\ALPHABET H}\n\\biggl(\n  \\sum_{i=1}^n \\bigl( y_i - \\langle f, φ(x_i) \\rangle_{\\mathcal{H}} \\bigr)^2 +\n  λ \\NORM{f}^2_{\\mathcal H}\n\\bigr).\\]\n\nTheorem 36.3 (The representer theoreom (simple version)) Given a loss function \\(\\ell \\colon \\ALPHABET Z^n \\to \\reals\\) and a penalty function \\(Ω \\colon \\reals \\to \\reals\\), there is as a solution of \\[ f^* = \\arg \\min_{f \\in \\mathcal H} \\ell(f(x_1), \\dots, f(x_n))\n        + Ω(\\NORM{f}^2_{\\mathcal H}). \\] that takes the the form \\[ f^* = \\sum_{i=1}^n α_i k(\\cdot, x_i).\\]\nIf \\(Ω\\) is strictly increasing, all solutions have this form.\n\nUsing the representer theorem, we know that the solution is of the form \\[ f = \\sum_{i=1}^n α_i φ(x_i). \\] Then, \\[\n\\sum_{i=1}^n \\bigl( y_i - \\langle f, φ_i(x_i) \\rangle_{\\mathcal H} \\bigr)^2\n  + λ \\NORM{f}_{\\mathcal H}^2\n= \\NORM{ y - K α}^2 + λ α^\\TRANS K α. \\]\nDifferentiating wrt \\(α\\) and setting this to zero, we get \\[\n  α^* = (K + λI_n)^{-1} y.\n\\]"
  },
  {
    "objectID": "convexity/convexity.html#convexity",
    "href": "convexity/convexity.html#convexity",
    "title": "37  Convex sets and convex functions",
    "section": "37.1 Convexity",
    "text": "37.1 Convexity\n\nDefinition 37.1 (Convex sets and convex functions) A subset \\(C\\) of \\(\\reals^n\\) is convex if for every \\(x_0, x_1 \\in C\\), the line segment \\([x_0, x_1] \\in C\\), i.e., \\[ (1-λ) x_0 + λ x_1 \\in C \\quad \\text{for all } λ \\in (0,1). \\]\nA function \\(f\\) on a convex set \\(C\\) is convex relative to \\(C\\) if for every \\(x_0, x_1 \\in C\\), one has \\[\nf( (1-λ)x_0 + λ x_1 ) \\le (1-λ) f(x_0) + λ f(x_1)\n\\quad \\text{for all } λ \\in (0,1).\n\\]\n\\(f\\) is strictly convex if this inequality is strict for points \\(x_0 \\neq x_1\\) with \\(f(x_0)\\) and \\(f(x_1)\\) finite.\n\nA function is said to be concave when \\(-f\\) is convex."
  },
  {
    "objectID": "convexity/convexity.html#convex-combinations",
    "href": "convexity/convexity.html#convex-combinations",
    "title": "37  Convex sets and convex functions",
    "section": "37.2 Convex combinations",
    "text": "37.2 Convex combinations\nA convex combination of elements \\(x_0, x_1, \\dots, x_p \\in \\reals^n\\) is a linear combination \\(\\sum_{i=0}^p λ_i x_i\\) where the coefficients \\(λ_i\\) are non-negative and add to one. In case of two elements, convex combinations can be equivalently expressed as \\((1-λ)x_0 + λx_1\\) with \\(λ \\in [0,1]\\), which we have already seen. The next result shows that the definition of convexity in terms of two elements given earlier generalizes to multiple elements.\n\nProposition 37.1 A set \\(C \\subset \\reals^n\\) is convex if and only if \\(C\\) contains all convex combinations of its elements.\nA function \\(f\\) is convex relative to a convex set \\(C\\) if and only if for every choice of points \\(x_0, \\dots, x_p \\in C\\), we have: \\[\\begin{equation}\\label{eq:Jensen-inequality}\nf\\biggl( \\sum_{i=0}^p λ_i x_i \\biggr)\n\\le\n\\sum_{i=0}^p λ_i f(x_i)\n\\quad \\text{when }\nλ_i \\ge 0,\n\\sum_{i=0}^p λ_i = 1.\n\\end{equation}\\]\n\nInequality \\eqref{eq:Jensen-inequality} is also call Jensen’s inequality.\n\n\n\n\n\n\nExtended real line\n\n\n\n\n\nIn many applications, it is convenient to consider function \\(f\\) that are allowed to be extended-real-valued, i.e., take values in \\(\\bar \\reals = [-∞, ∞]\\) instead of just \\(\\reals = (-∞, ∞)\\). The extended real line has all the properties of a compact interval: every subset \\(A \\subset \\bar \\reals\\) has a supremum and an infimum, either of which could be infinite.\nWhen extending arithmetic operations to the extended real line, most rules extend in a natural manner but we have to be careful with two operations: \\(0 ⋅ ∞\\) and \\(∞ - ∞\\). We will define \\[\n0 ⋅ ∞ = 0 = 0 ⋅ (-∞).\n\\]\nFor convexity, one follows the inf-addition convention: \\[\n∞ + (-∞) = (-∞) + ∞ = ∞.\n\\]\nExtended arithematic then obeys the associative, commutative, and distributive laws of ordinary arithematic with one crucial exception: \\[\n  λ ⋅ (∞ - ∞) \\neq (λ ⋅ ∞ - λ ⋅ ∞)\n  \\quad \\text{when } λ &lt; 0.\n\\]\n\n\n\nThe definition of convex functions can be generalized to function defined over the extended real line as long as we invoke the inf-addition convention.\nAny convex function \\(f\\) on a convex set \\(C \\in \\reals^n\\) can be identified with a convex function on all of \\(\\reals^n\\) by defining \\(f(x) = ∞\\) for all \\(x \\not\\in C\\). Such functions are called proper convex functions (in contrast to improper convex functions which take the value \\(f(x) = -∞\\) for all \\(x \\in \\text{int}(\\text{dom} f)\\)."
  },
  {
    "objectID": "convexity/convexity.html#properties-of-convex-function",
    "href": "convexity/convexity.html#properties-of-convex-function",
    "title": "37  Convex sets and convex functions",
    "section": "37.3 Properties of convex function",
    "text": "37.3 Properties of convex function\n\nProposition 37.2 Let \\(I\\) be an index set. Then:\n\nIntersection of convex functions is convex:  \\(\\bigcap_{i \\in I} C_i\\) is convex if each set \\(C_i\\) is convex.\nSupremum of convex functions is convex:  \\(\\sup_{i \\in I} f_i\\) is convex if each function \\(f_i\\) is convex.\nSupremum of finite number of strictly convex functions is strictly convex:  \\(\\sup_{i \\in I} f_i\\) is strictly convex if each function \\(f_i\\) is strictly convex and \\(I\\) is finite.\nPointwise supremum of a collection of convex functions is convex:  \\(f\\) is convex if \\(f(x) = \\lim\\sup_{i \\in I} f_i(x)\\) for all \\(x\\) and each \\(f_i\\) is convex.\n\n\nThe next result presents equivalent characterizations of convexity.\n\nProposition 37.3 For a differentiable function \\(f\\) on an open convex set \\(O \\subset \\reals^n\\), each of the following is both necessary and sufficient for \\(f\\) to be convex on \\(O\\):\n\n\\(\\langle x_1 - x_0, \\GRAD f(x_1) - \\GRAD f(x_0) \\rangle \\ge 0\\) for all \\(x_0, x_1 \\in O\\).\n\\(f(y) \\ge f(x) + \\langle \\GRAD f(x), y - x \\rangle\\) for all \\(x, y \\in O\\).\n\\(\\GRAD^2 f(x)\\) is pointwise-semidefinite for all \\(x \\in O\\) (\\(f\\) twice differentiable)."
  },
  {
    "objectID": "convexity/convexity.html#notes",
    "href": "convexity/convexity.html#notes",
    "title": "37  Convex sets and convex functions",
    "section": "Notes",
    "text": "Notes\nThe material of these notes is adapted from Rockafellar and Wets (2009).\n\n\n\n\nRockafellar, R.T. and Wets, R.J.-B. 2009. Variational analysis. Springer Science & Business Media."
  },
  {
    "objectID": "convexity/duality.html#basic-intuition",
    "href": "convexity/duality.html#basic-intuition",
    "title": "38  Duality",
    "section": "38.1 Basic Intuition",
    "text": "38.1 Basic Intuition\nWe start by stating a basic fact\n\n\n\n\n\n\nCharacterization of convex functions\n\n\n\nAny convex function can be represented as a pointwise supremum of convex functions. More formally, for any convex \\(f \\colon \\reals^n \\to \\reals\\), there exists a countable index set \\(I\\) and a family of \\(\\{a_i \\in \\reals^n\\}_{i \\in I}\\) and \\(\\{b_i \\in \\reals\\}_{i \\in I}\\) such that \\[\n  f(x) = \\sup_{i \\in I} \\{ a_i^\\TRANS x + b_i \\}\n\\]\n\n\n\nSmax = 2\nn = 100\nnp = 3\n\nf = function(s) { return s*s + 1 }\n// Legendre transform of f\ng = function(s) { return s*s/4 - 1 }\n\npoints = {\n\n  var points = new Array()\n  var idx = 0\n  for( var i = -n ; i &lt;= n; i++) {\n    var s = Smax*i/n\n    points[idx++] = { point: s, value: f(s) }\n  }\n  return points\n  }\n\nlegendre = {\n  var points = new Array()\n  var idx = 0\n\n  for( var i = -np; i &lt;= np; i++) {\n    var p = Smax*i/np\n    var gp = g(p)\n    points[idx++] = { x: -Smax, y: -Smax*p - gp, index: i }\n    points[idx++] = { x:  Smax, y:  Smax*p - gp, index: i }\n  }\n\n  return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naveragePlot = Plot.plot({\n  grid: true,\n  y: { domain: [0, 5] },\n\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(legendre, {x:\"x\", y:\"y\", z:\"index\", stroke:\"gray\"} ),\n\n    Plot.line(points, {x:\"point\", y:\"value\",\n                      stroke:\"red\", strokeWidth: 4}),\n  ]\n})\n\n\n\n\n\n\nFigure 38.1: Convex function as a supremum of affine functions\n\n\nAn illustration of this fact is shown in Figure 38.1.\n\n\n\n\n\n\nWhy countable index set?\n\n\n\n\n\nFrom the figure you can convince yourself of the result for an uncountable index set \\(\\bar I\\). Now consider the set \\(\\bar S = \\{a_i, b_i\\}_{i \\in \\bar I} \\subset \\reals^{n+1}\\) (for uncountable \\(\\bar I\\)). Since \\(\\reals^{n+1}\\) is separable, let \\(S = \\{a_i, b_i\\}_{i \\in I}\\) be a countable dense subset of \\(\\bar S\\) (where \\(I\\) is countable). Since the supremum over any set is the same as the supremum of its dense subset, we can replace \\(\\bar I\\) by \\(I\\).\n\n\n\nThe Lengendre-Fenchel transform is a compact way of representing this basic fact. To fix ideas, consider a twice differentiable and strictly convex function \\(f \\colon \\reals \\to \\reals\\). Arbitrarily fix a point \\(x^∘\\) and consider the tangent \\(τ(x)\\) to the plot of \\(f(⋅)\\) at \\(x^∘\\). This tangent has a slope \\(p = f'(x^∘)\\). Let \\(g\\) denote the negative \\(y\\)-intercept of the tangent (using the negative \\(y\\)-intercept is just a matter of convention). Thus, the equation for the tangent is: \\[\n  τ(x) = p x - g, \\quad \\forall x \\in \\reals\n\\] at at the point \\(x\\), the tangent \\(τ\\) intercepts the function \\(f\\); thus, \\(τ(x^∘) = f(x^∘)\\), or equivalently: \\[\n  f(x^∘) = p x^∘ - g\n  \\quad\\hbox{or, equivalently}\\quad\n  g = p x^∘ - f(x^∘)\n\\]\nNote that the \\(g\\) above depends on \\(x^∘\\). Since \\(f\\) is strictly convex, \\(f'\\) is strictly increasing. Thus, there is a one-to-one relationship between the point \\(x^∘\\) and its slope \\(p = f'(x^∘)\\). The Lengendre-Fenchel transform is a method to define the intercept \\(g\\) as a function of \\(p\\). In particular, it is defined as \\[\n  g(p) = \\sup_{x \\in \\reals} \\{ px - f(x) \\}.\n\\]\nTo understand this definition, suppose \\(x^∘\\) achieves the supremum in the above equation. Then, (i) \\(f(x^∘) = p x^∘ - g(p)\\); that is the line \\(px - g(p)\\) touches the function \\(f\\) at \\(x^∘\\); and (ii) at all other points \\(x \\neq x^∘\\), \\(g(p) &gt; p x - f(x)\\) or \\(f(x) &gt; px - g(p)\\); that is the line \\(px - g(p)\\) lies below the function \\(f\\). Hence, \\(px - g(p)\\) is a :supporting line of \\(f\\) (and since \\(f\\) is differentiable, equal to its tangent).\nAn implication of this definition is that for differentiable and convex \\(f\\), the Legendre transform \\(g\\) of \\(f\\) solves \\[\n  g(p) = p x^∘ - f(x^∘)\n\\] where \\(x^∘\\) solves \\(p = f'(x^∘)\\), which is called the duality condition.\n\nExample 38.1 Let \\(f(x) = x^2\\). For a fixed \\(p\\), the duality condition is \\(p = 2x^∘\\) or \\(x^∘ = p/2\\). Therefore, \\[\n  g(p) = p x^∘ - f(x^∘) = \\frac{p^2}{2} - \\frac{p^2}{4} = \\frac{p^2}{4}.\n\\]\n\n\nExample 38.2 Let \\(f(x) = (x^α)/α\\), where \\(α &gt; 1\\). For a fixed \\(p\\), the duality condition is \\(p = (x^∘)^{α-1}\\) or \\(x^∘ = p^{1/(p-1)}\\). Therefore, \\[\n  g(p) = p x^∘ - f(x^∘) = p^{α/(α-1)} - \\frac{1}{α} p^{α/(α-1)}\n  = \\frac{α-1}{α} p^{α/(α-1)}.\n\\] We can compactly write the above expression is \\(g(p) = p^β/β\\), where \\(1/β = 1 - 1/α\\).\n\n\nExample 38.3 Let \\(f(x) = x \\log x + (1-x) \\log (1-x)\\), where the domain is \\([0,1]\\). This is the negative binary entropy. For a fixed \\(p\\), the duality condition is \\[\np = f'(x^∘) = \\log(x^∘) + 1 - \\log(1-x^∘) - 1\n% = \\log(x^∘) - \\log(1-x^∘)\n= \\log \\frac{x^∘}{1-x^∘}\n\\] Thus, \\(x^∘ = e^p/(1 + e^p)\\). Therefore, \\[\n  g(p) = p x^∘ - f(x^∘) = \\log(1 + e^p),\n\\] where the last step follows after some algebra.\n\n\n\n\n\n\n\nFenchel-Young inequality\n\n\n\nLet \\(g\\) be the Legendre-Fenchel transform of \\(f\\). Then, by definition (changing the variable names for convenience), we have \\[\\begin{equation}\\tag{Fenchel's inequality}\nxy \\le f(x) + g(y).\n\\end{equation}\\] For the special case of Example 38.2, we have \\[\\begin{equation}\n\\tag{Young's inequality}\nxy \\le \\frac{x^p}{p} + \\frac{y^q}{q}\n\\end{equation}\\] where \\(p, q &gt; 1\\) are such that \\(\\frac 1p + \\frac 1q = 1\\).\n\n\nAn interesting example of Fenchel-Young inequality is the following. Consider a real-valued, strictly increasing, continuous function \\(h\\) on \\(\\reals\\) which satisfies \\(h(0) = 0\\), \\(h(x) \\to -∞\\) as \\(x \\to -∞\\), and \\(h(x) \\to ∞\\) as \\(x \\to ∞\\). Since \\(h\\) is continuous and strictly increasing, it has an inverse. Define \\[\\begin{equation}\\label{eq:legendre-example}\n  f(x) = \\int_{0}^x h(s) ds\n  \\quad\\hbox{and}\\quad\n  g(y) = \\int_{0}^y h^{-1}(t) dt.\n\\end{equation}\\]\n\nexampleFunction = function(s) {\n    if (s &lt;= 1) {\n        return (s - s**2/2)\n    } else\n    {\n        return 1/2 + (s - 1)**2/2\n    }\n  }\n\nexamplePoints = {\n  var points = new Array()\n  var idx = 0\n\n  const n = 500;\n  const Smax = 2; \n\n  for(var i = 0; i &lt;= n; i++) {\n      var s = Smax*i/n\n      points[idx++] = { x: s, y: exampleFunction(s) } \n  }\n  return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  x : { label: \"s\"}, \n  y : { label: \"t\"}, \n\n  marks: [\n    // Fill\n    Plot.areaY(examplePoints.filter(pt =&gt; pt.x &lt;= 1), {x:\"x\", y:\"y\",\n               fill:\"lightblue\", fillOpacity: 0.5}),\n\n    Plot.areaX(examplePoints.filter(pt =&gt; pt.y &lt;= 0.7), {x:\"x\", y:\"y\",\n               fill:\"pink\", fillOpacity: 0.5}),\n\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n\n    Plot.line([ {x:1, y:0}, {x:1, y: exampleFunction(1)} ], {x:\"x\", y:\"y\", stroke: \"gray\"} ),\n\n    Plot.line([ {x:1, y: exampleFunction(1)}, {x:1, y: 0.7} ], {x:\"x\", y:\"y\", strokeDasharray: \"4\", stroke: \"gray\"} ),\n\n    Plot.line([ {x:0, y:0.7}, {x:1.62, y:0.7} ], {x:\"x\", y:\"y\", stroke: \"gray\"} ),\n\n    // Line\n    Plot.line(examplePoints, {x:\"x\", y:\"y\",\n              stroke:\"red\", strokeWidth: 4}),\n    \n  ]\n})\nPlot.plot({\n  grid: true,\n  x : { label: \"s\"}, \n  y : { label: \"t\"}, \n\n  marks: [\n    // Fill\n    Plot.areaY(examplePoints.filter(pt =&gt; pt.x &lt;= 1.4), {x:\"x\", y:\"y\",\n               fill:\"lightblue\", fillOpacity: 0.5}),\n\n    Plot.areaX(examplePoints.filter(pt =&gt; pt.y &lt;= 0.4), {x:\"x\", y:\"y\",\n               fill:\"pink\", fillOpacity: 0.5}),\n\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n\n    Plot.line([ {x:1.4, y:0}, {x:1.4, y: exampleFunction(1.4)} ], {x:\"x\", y:\"y\", stroke: \"gray\"} ),\n\n    Plot.line([ {x:0.55, y: 0.4}, {x:1.4, y: 0.4} ], {x:\"x\", y:\"y\", strokeDasharray: \"4\", stroke: \"gray\"} ),\n\n    Plot.line([ {x:0, y:0.4}, {x:0.55, y:0.4} ], {x:\"x\", y:\"y\", stroke: \"gray\"} ),\n\n    // Line\n    Plot.line(examplePoints, {x:\"x\", y:\"y\",\n              stroke:\"red\", strokeWidth: 4}),\n    \n  ]\n})\n\n\n\n\n\n\n\n\n(a) \\(x &lt; y\\), with \\(x = 1\\) and \\(y = 0.7\\)\n\n\n\n\n\n\n\n(b) \\(x &gt; y\\), with \\(x = 1.4\\) and \\(y = 0.4\\)\n\n\n\nFigure 38.2: The functions \\(f(x)\\) and \\(g(y)\\) defined in \\eqref{eq:legendre-example}. The red curve shows the graph of \\(t = h(s)\\). The shaded blue region shows the value of \\(f(x)\\) for a particular value of \\(x\\). The red curve shown the value of \\(g(y)\\) for a particular value of \\(y\\).\n\n\n\nThe graph of \\(t = h(s)\\) is shown in Figure 38.2, where the shaded portions represent \\(f(x)\\) and \\(g(y)\\). From the plots, we can infer Fenchel-Young’s inequality: \\[\n  xy \\le f(x) + g(y)\n\\] with equality if and only if \\(y = h(x) = \\dot f(x)\\). This immediately implies that \\[\n  g(y) = \\sup_{x \\in \\reals} \\bigl\\{ xy - f(x) \\bigr\\}\n\\] and \\[\n  f(x) = \\sup_{y \\in \\reals} \\bigl\\{ xy - g(y) \\bigr\\}.\n\\]"
  },
  {
    "objectID": "convexity/duality.html#general-definition",
    "href": "convexity/duality.html#general-definition",
    "title": "38  Duality",
    "section": "38.2 General definition",
    "text": "38.2 General definition\n\nDefinition 38.1 For any function \\(f \\colon \\reals^n \\to \\bar {\\reals}\\), the function \\(f^* \\colon \\reals^n \\to \\bar \\reals\\) defined by \\[\\begin{equation}\\label{eq:conjugate}\n  f^*(p) \\coloneqq \\sup_{x \\in \\reals^n} \\bigl\\{\n  \\langle p, x \\rangle - f(x) \\bigr\\}\n\\end{equation}\\] is conjugate to \\(f\\), while the function \\(f^{**} = (f^*)^*\\) defined by \\[\\begin{equation}\\label{eq:biconjugate}\n  f^{**}(x) \\coloneqq \\sup_{p \\in \\reals^n} \\bigl\\{\n  \\langle p, x \\rangle - f^*(p) \\bigr\\}\n\\end{equation}\\] is biconjugate to \\(f\\). The mapping \\(f \\mapsto f^*\\) is called Legendre-Fenchel transform.\n\nThe significance of the conjugate can be understood in terms of epigraph relationships. Note that \\eqref{eq:conjugate} implies that \\[\n  (p,β) \\in \\text{epi } f^*\n  \\iff\n  β \\ge \\langle p, x \\rangle - α\n  \\text{ for all } (x,α) \\in \\text{epi } f.\n\\] Let \\(\\ell_{p,β}(x) \\coloneqq \\langle p, x \\rangle - β\\), then we can write the above relationship as \\[\n  (p,β) \\in \\text{epi } f^*\n  \\iff\n  \\ell_{p,β} \\le f,\n\\] that is, \\(f^*\\) describes a family of affine functions majorized by \\(f\\). Similarly, \\[\n  β \\ge f^*(p)\n  \\iff\n  β \\ge \\ell_{x,α}(p)\n  \\text{ for all } (x,α) \\in \\text{epi } f.\n\\]\n\nTheorem 38.1 For any function \\(f \\colon \\reals^n \\to \\bar \\reals\\) with \\(\\text{con } f\\) proper, both \\(f^*\\) and \\(f^{**}\\) are proper, lsc, convex and \\[\n  f^{**} = \\text{cl con } f.\n\\] Thus, \\(f^{**} \\le f\\) and when \\(f\\) is itself proper, lsc, and convex, one has \\(f^{**} = f\\)."
  },
  {
    "objectID": "convexity/duality.html#properties-of-legendre-fenchel-transform",
    "href": "convexity/duality.html#properties-of-legendre-fenchel-transform",
    "title": "38  Duality",
    "section": "38.3 Properties of Legendre-Fenchel transform",
    "text": "38.3 Properties of Legendre-Fenchel transform\nThus, Legendre-Fenchel transform sets up a one-to-one correspondence in the class of proper, lsc, and convex functions: if \\(f\\) is conjugate to \\(g\\), then \\(g\\) is a conjugate to \\(f\\): \\[\nf \\xleftrightarrow{\\hskip0.5em*\\hskip0.5em} g\n\\text { when }\n\\begin{cases}\ng(p) = \\sup_{x \\in \\reals^n} \\bigl\\{ \\langle p, x \\rangle - f(x) \\bigr\\} \\\\\nf(x) = \\sup_{x \\in \\reals^n} \\bigl\\{ \\langle p, x \\rangle - g(p) \\bigr\\}\n\\end{cases}\n\\]\nGiven \\(f \\xleftrightarrow{\\hskip0.5em*\\hskip0.5em} g\\), we have the following properties:\n\nScaling properties. For any \\(λ &gt; 0\\), \\[\\begin{align*}\nλ f(x)\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\nλg(λ^{-1} p),\n\\\\\nf(λx)\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\ng(λ^{-1} p),\n\\end{align*}\\]\nTranslation properties. \\[\\begin{align*}\nf(x) - \\langle a, x \\rangle\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\ng(p + a) ,\n\\\\\nf(x + b)\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\ng(p) - \\langle p, b \\rangle,\n\\\\\nf(x) + c\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\ng(p) - c,\n\\\\\n\\end{align*}\\]\n\n\nProposition 38.1 Let \\(f\\) be a finite, coercive, twice differentiable, and strongly convex function, then the conjugate \\(g = f^*\\) is also finite, coercive, twice differentiable, and strongly convex. Moreover,\n\nthe gradient mapping \\(\\GRAD f\\) is one-to-one from \\(\\reals^n\\) to \\(\\reals^n\\), and its inverse is \\(\\GRAD g\\); one has \\[\\begin{align*}\ng(p) &= \\bigl\\langle (\\GRAD f)^{-1}(p), p \\bigr\\rangle\n- f\\bigl((\\GRAD f)^{-1}(p)\\bigr), \\\\\nf(x) &= \\bigl\\langle (\\GRAD g)^{-1}(x), x \\bigr\\rangle\n- g\\bigl((\\GRAD g)^{-1}(x)\\bigr),\n\\end{align*}\\]\nThe matrices \\(\\GRAD^2 f(x)\\) and \\(\\GRAD^2 g(p)\\) are inverse to one another when \\(p = \\GRAD f(x)\\) or, equivalently, \\(x = \\GRAD g(p)\\).\n\n\nStrongly convex functions on a simplex have the following properties.\n\nProposition 38.2 Let \\(Δ\\) be the simplex in \\(\\reals^n\\) and \\(f \\colon Δ \\to \\reals\\) be twice differentiable and strongly convex. Let \\(g \\colon \\reals^n \\to \\reals\\) be the Legendre-Fenchel conjugate of \\(f\\). Then,\n\nUnique maximizing argument: \\(\\GRAD g\\) is Lipschitz and satisfies \\[\\GRAD g(p) = \\arg\\max_{x \\in Δ}\\bigl\\{ \\langle x, p \\rangle - f(x) \\bigr\\}.\\]\nBoundedness: If there are constants \\(L\\) and \\(U\\) such that for all \\(x \\in Δ\\), we have \\(L \\le f(x) \\le U\\), then \\[\n   \\| p \\|_{∞} - U \\le g(p) \\le \\| p \\|_{∞} - L.\n\\]\nDistributivity: For any \\(c \\in \\reals\\), \\[\n   g(p + c \\ONES) = g(p) + c.\n\\]\nMonotonicity: If \\(p_1 \\le p_2\\), then \\(g(p_1) \\le g(p_2)\\)."
  },
  {
    "objectID": "convexity/duality.html#notes",
    "href": "convexity/duality.html#notes",
    "title": "38  Duality",
    "section": "Notes",
    "text": "Notes\nThe material on the intuition behind Legendre-Fenchel transform is adapted from Kennerly (2011). The example for Young-Fenchel inequality and Figure 38.2 is taken from Ellis (1985). The material on general definition and properties is adapted from Rockafellar and Wets (2009). Proposition 38.1 is stated as Example 11.9 in Rockafellar and Wets (2009). Proposition 38.2 is from Geist et al. (2019).\n\n\n\n\nEllis, R.S. 1985. Entropy, large deviations, and statistical mechanics. Springer New York. DOI: 10.1007/978-1-4613-8533-2.\n\n\nGeist, M., Scherrer, B., and Pietquin, O. 2019. A theory of regularized Markov decision processes. Proceedings of the 36th international conference on machine learning, PMLR, 2160–2169. Available at: https://proceedings.mlr.press/v97/geist19a.html.\n\n\nKennerly, S. 2011. A graphical derivation of the legendre transform. Available at: http://einstein.drexel.edu/~skennerly/maths/Legendre.pdf.\n\n\nRockafellar, R.T. and Wets, R.J.-B. 2009. Variational analysis. Springer Science & Business Media."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Afshari, M. and Mahajan, A. 2023.\nDecentralized linear quadratic systems with major and minor agents and\nnon-gaussian noise. IEEE Transactions on Automatic\nControl 68, 8, 4666–4681. DOI: 10.1109/tac.2022.3210049.\n\n\nAltman, Eitan. 1999. Constrained\nmarkov decision processes. CRC Press. Available at: http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf.\n\n\nArabneydi, J. and Mahajan, A. 2016.\nLinear quadratic mean field teams: Optimal and approximately optimal\ndecentralized solutions. Available at: https://arxiv.org/abs/1609.00056v2.\n\n\nArrow, K.J., Blackwell, D., and Girshick,\nM.A. 1949. Bayes and minimax solutions of sequential decision\nproblems. Econometrica 17, 3/4, 213. DOI: 10.2307/1905525.\n\n\nArrow, K.J., Harris, T., and Marschak, J.\n1952. Optimal inventory policy. Econometrica 20, 1,\n250–272. DOI: 10.2307/1907830.\n\n\nArthur, W.B. 1994. Increasing returns\nand path dependence in the economy. University of Michigan Press.\nDOI: 10.3998/mpub.10029.\n\n\nAström, K.J. 1970. Introduction to\nstochastic control theory. Dover.\n\n\nAthans, M. 1971. The role and use of the\nstochastic linear-quadratic-gaussian problem in control system design.\nIEEE Transactions on Automatic Control\n16, 6, 529–552. DOI: 10.1109/tac.1971.1099818.\n\n\nBai, C.-Z., Katewa, V., Gupta, V., and Huang,\nY.-F. 2015. A stochastic sensor selection scheme for sequential\nhypothesis testing with multiple sensors. IEEE transactions on\nsignal processing 63, 14, 3687–3699.\n\n\nBellman, R., Glicksberg, I., and Gross,\nO. 1955. On the optimal inventory equation. Management\nScience 2, 1, 83–104. DOI: 10.1287/mnsc.2.1.83.\n\n\nBerry, R.A. 2000. Power and delay\ntrade-offs in fading channels. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay\ntradeoffs in fading channels—small-delay asymptotics. IEEE\nTransactions on Information Theory 59, 6, 3939–3952. DOI:\n10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002.\nCommunication over fading channels with delay constraints.\nIEEE Transactions on Information Theory\n48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M.\n2012. Energy-efficient scheduling under delay constraints for wireless\nnetworks. Synthesis Lectures on Communication Networks\n5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nBertsekas, D.P. 2011. Dynamic\nprogramming and optimal control. Athena Scientific. Available at:\nhttp://www.athenasc.com/dpbook.html.\n\n\nBertsekas, D.P. 2013. Abstract\ndynamic programming. Athena Scientific Belmont. Available at: https://web.mit.edu/dimitrib/www/abstractdp_MIT.html.\n\n\nBertsekas, D.P. and Tsitsiklis, J.N.\n2000. Gradient convergence in gradient methods with errors.\nSIAM Journal on Optimization 10, 3,\n627–642. DOI: 10.1137/s1052623497331063.\n\n\nBitar, E., Poolla, K., Khargonekar, P.,\nRajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind.\n2012 45th hawaii international conference on system sciences,\nIEEE, 1931–1937.\n\n\nBlackwell, D. 1964. Memoryless strategies\nin finite-stage dynamic programming. The Annals of Mathematical\nStatistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nBlackwell, D. 1965. Discounted dynamic\nprogramming. The Annals of Mathematical Statistics 36,\n1, 226–235. DOI: 10.1214/aoms/1177700285.\n\n\nBlackwell, D. 1970. On stationary\npolicies. Journal of the Royal Statistical Society. Series A\n(General) 133, 1, 33. DOI: 10.2307/2343810.\n\n\nBohlin, T. 1970. Information pattern for\nlinear discrete-time models with stochastic coefficients. IEEE\nTransactions on Automatic Control (TAC) 15, 1, 104–106.\n\n\nBorkar, V.S. 2008. Stochastic\napproximation. Hindustan Book Agency. DOI: 10.1007/978-93-86279-38-5.\n\n\nBorkar, V.S. and Meyn, S.P. 2000. The\no.d.e. Method for convergence of stochastic approximation and\nreinforcement learning. SIAM Journal on Control and\nOptimization 38, 2, 447–469. DOI: 10.1137/s0363012997331639.\n\n\nCassandra, A., Littman, M.L., and Zhang,\nN.L. 1997. Incremental pruning: A simple, fast, exact method for\npartially observable Markov decision processes.\nProceedings of the thirteenth conference on uncertainty\nin artificial intelligence.\n\n\nCassandra, A.R., Kaelbling, L.P., and Littman,\nM.L. 1994. Acting optimally in partially observable stochastic\ndomains. AAAI, 1023–1028.\n\n\nChakravorty, J. and Mahajan, A. 2018.\nSufficient conditions for the value function and optimal strategy to be\neven and quasi-convex. IEEE Transactions on Automatic Control\n63, 11, 3858–3864. DOI: 10.1109/TAC.2018.2800796.\n\n\nChang, J.T. 2007. Stochastic processes.\nAvailable at: http://www.stat.yale.edu/~pollard/Courses/251.spring2013/Handouts/Chang-notes.pdf.\n\n\nChen, H.-F. and Guo, L. 1991.\nIdentification and stochastic adaptive control. Birkhäuser\nBoston. DOI: 10.1007/978-1-4612-0429-9.\n\n\nCheng, H.-T. 1988. Algorithms for\npartially observable markov decision processes.\n\n\nDaley, D.J. 1968. Stochastically monotone\nmarkov chains. Zeitschrift für\nWahrscheinlichkeitstheorie und verwandte Gebiete 10, 4,\n305–317. DOI: 10.1007/BF00531852.\n\n\nDavis, M.H.A. 1979. Martingale methods in\nstochastic control. In: Stochastic control theory and stochastic\ndifferential systems. Springer-Verlag, 85–117. DOI: 10.1007/bfb0009377.\n\n\nDavis, M.H.A. and Varaiya, P.P. 1972.\nInformation states for linear stochastic systems. Journal of\nMathematical Analysis and Applications 37, 2, 384–402.\n\n\nDeGroot, M. 1970. Optimal statistical\ndecisions. Wiley-Interscience, Hoboken, N.J.\n\n\nDellacherie, C. and Meyer, P.-A. 1982.\nProbabilities and potential B: Theory of\nmartingales. North-Holland Mathematical Studies.\n\n\nDevlin, S. 2014. Potential based reward\nshaping tutorial. Available at: http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf.\n\n\nDevlin, S. and Kudenko, D. 2012. Dynamic\npotential-based reward shaping. Proceedings of the 11th\ninternational conference on autonomous agents and multiagent\nsystems, International Foundation for Autonomous Agents; Multiagent\nSystems, 433–440.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A.\n2016. On monotonicity of the optimal transmission policy in cross-layer\nadaptive m -QAM modulation.\nIEEE Transactions on Communications 64, 9, 3771–3785.\nDOI: 10.1109/TCOMM.2016.2590427.\n\n\nDorato, P. and Levis, A. 1971. Optimal\nlinear regulators: The discrete-time case. IEEE\nTransactions on Automatic Control 16, 6, 613–620. DOI: 10.1109/tac.1971.1099832.\n\n\nDubins, L.E. and Savage, L.J. 2014.\nHow to gamble if you must: Inequalities for stochastic\nprocesses. Dover Publications.\n\n\nEdgeworth, F.Y. 1888. The mathematical\ntheory of banking. Journal of the Royal Statistical Society\n51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nElliott, R., Li, X., and Ni, Y.-H. 2013.\nDiscrete time mean-field stochastic linear-quadratic optimal control\nproblems. Automatica 49, 11, 3222–3233. DOI: 10.1016/j.automatica.2013.08.017.\n\n\nEllis, R.S. 1985. Entropy, large\ndeviations, and statistical mechanics. Springer New York. DOI: 10.1007/978-1-4613-8533-2.\n\n\nFeinberg, E.A. and He, G. 2020.\nComplexity bounds for approximately solving discounted MDPs\nby value iterations. Operations Research Letters. DOI: 10.1016/j.orl.2020.07.001.\n\n\nFerguson, T.S. 2008. Optimal stopping and\napplications. Available at: http://www.math.ucla.edu/~tom/Stopping/Contents.html.\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004.\nOptimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nFöllmer, H. and Schied, A. 2010. Convex\nrisk measures. In: Encyclopedia of quantitative finance.\nAmerican Cancer Society. DOI: 10.1002/9780470061602.eqf15003.\n\n\nGao, S. and Mahajan, A. 2022. Optimal\ncontrol of network-coupled subsystems: Spectral decomposition and\nlow-dimensional solutions. IEEE Transactions on Control\nof Network Systems 9, 2, 657–669. DOI: 10.1109/tcns.2021.3124259.\n\n\nGeist, M., Scherrer, B., and Pietquin, O.\n2019. A theory of regularized Markov decision processes.\nProceedings of the 36th international conference on machine\nlearning, PMLR, 2160–2169. Available at: https://proceedings.mlr.press/v97/geist19a.html.\n\n\nGladyshev, E.G. 1965. On stochastic\napproximation. Theory of Probability and Its Applications\n10, 2, 275–278. DOI: 10.1137/1110031.\n\n\nGrzes, M. and Kudenko, D. 2009.\nTheoretical and empirical analysis of reward shaping in reinforcement\nlearning. International conference on machine learning and\napplications, 337–344. DOI: 10.1109/ICMLA.2009.33.\n\n\nHarris, F.W. 1913. How many parts to make\nat once. The magazine of management 10, 2, 135–152.\nDOI: 10.1287/opre.38.6.947.\n\n\nHay, N., Russell, S., Tolpin, D., and Shimony,\nS.E. 2012. Selecting computations: Theory and applications.\nUAI. Available at: http://www.auai.org/uai2012/papers/123.pdf.\n\n\nHernandez-Hernández, D. and Marcus, S.I.\n1996. Risk sensitive control of markov processes in countable state\nspace. Systems & Control Letters 29,\n3, 147–155. DOI: 10.1016/s0167-6911(96)00051-5.\n\n\nHernández-Hernández, D. 1999. Existence\nof risk-sensitive optimal stationary policies for controlled markov\nprocesses. Applied Mathematics and Optimization 40, 3,\n273–285. DOI: 10.1007/s002459900126.\n\n\nHernández-Lerma, O. and Lasserre, J.B.\n1996. Discrete-time markov control processes. Springer New\nYork. DOI: 10.1007/978-1-4612-0729-0.\n\n\nHernández-Lerma, O. and Lasserre, J.B.\n1999. Further topics on discrete-time markov control processes.\nSpringer New York. DOI: 10.1007/978-1-4612-0561-6.\n\n\nHinderer, K. 2005. Lipschitz continuity\nof value functions in Markovian decision processes.\nMathematical Methods of Operations Research 62, 1,\n3–22. DOI: 10.1007/s00186-005-0438-1.\n\n\nHopcroft, J. and Kannan, R. 2012.\nComputer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf.\n\n\nHoward, R.A. 1960. Dynamic\nprogramming and markov processes. The M.I.T. Press.\n\n\nHoward, R.A. and Matheson, J.E. 1972.\nRisk-sensitive markov decision processes. Management Science\n18, 7, 356–369. DOI: 10.1287/mnsc.18.7.356.\n\n\nJenner, E., Hoof, H. van, and Gleave, A.\n2022. Calculus on MDPs: Potential shaping as a gradient. Available at:\nhttps://arxiv.org/abs/2208.09570v1.\n\n\nKalman, R.E. 1960. Contributions to the\ntheory of optimal control. Boletin de la Sociedad Matematica\nMexicana 5, 102–119.\n\n\nKaratzas, I. and Sudderth, W.D. 2010. Two\ncharacterizations of optimality in dynamic programming. Applied\nMathematics and Optimization 61, 3, 421–434. DOI: 10.1007/s00245-009-9093-x.\n\n\nKeilson, J. and Kester, A. 1977. Monotone\nmatrices and monotone markov processes. Stochastic Processes and\ntheir Applications 5, 3, 231–241.\n\n\nKelly, J.L., Jr. 1956. A new\ninterpretation of information rate. Bell System Technical\nJournal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nKennerly, S. 2011. A graphical derivation\nof the legendre transform. Available at: http://einstein.drexel.edu/~skennerly/maths/Legendre.pdf.\n\n\nKoole, G. 2006. Monotonicity in markov\nreward and decision chains: Theory and applications. Foundations and\nTrends in Stochastic Systems 1, 1, 1–76. DOI:\n10.1561/0900000002.\n\n\nKumar, P.R. and Varaiya, P. 1986.\nStochastic systems: Estimation identification and adaptive\ncontrol. Prentice Hall.\n\n\nKushner, H.J. and Yin, G.G. 1997.\nStochastic approximation algorithms and applications. Springer\nNew York. DOI: 10.1007/978-1-4899-2696-8.\n\n\nKwakernaak, H. 1965. Theory of\nself-adaptive control systems. In: Springer, 14–18.\n\n\nLai, T.L. 2003. Stochastic approximation:\nInvited paper. The Annals of Statistics 31, 2. DOI: 10.1214/aos/1051027873.\n\n\nLevy, H. 1992. Stochastic dominance and\nexpected utility: Survey and analysis. Management Science\n38, 4, 555–593. DOI: 10.1287/mnsc.38.4.555.\n\n\nLevy, H. 2015. Stochastic dominance:\nInvestment decision making under uncertainty. Springer. DOI: 10.1007/978-3-319-21708-6.\n\n\nMarshall, A.W., Olkin, I., and Arnold,\nB.C. 2011. Inequalities: Theory of majorization and its\napplications. Springer New York. DOI: 10.1007/978-0-387-68276-1.\n\n\nMorse, P. and Kimball, G. 1951.\nMethods of operations research. Technology Press of MIT.\n\n\nMüller, A. 1997a. Integral probability\nmetrics and their generating classes of functions. Advances in\nApplied Probability 29, 2, 429–443. DOI: 10.2307/1428011.\n\n\nMüller, A. 1997b. How does the value\nfunction of a markov decision process depend on the transition\nprobabilities? Mathematics of Operations Research 22,\n4, 872–885. DOI: 10.1287/moor.22.4.872.\n\n\nNerode, A. 1958. Linear automaton\ntransformations. Proceedings of American Mathematical\nSociety 9, 541–544.\n\n\nNeveu, J. 1975. Discrete parameter\nmartingales. North Holland.\n\n\nNg, A.Y., Harada, D., and Russell, S.\n1999. Policy invariance under reward transformations: Theory and\napplication to reward shaping. ICML, 278–287. Available at: http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf.\n\n\nPicard, J. 2007. Concentration\ninequalities and model selection. Springer Berlin Heidelberg. DOI:\n10.1007/978-3-540-48503-2.\n\n\nPiunovskiy, A.B. 2011. Examples in\nmarkov decision processes. Imperial College Proess. DOI: 10.1142/p809.\n\n\nPollard, D. 2002. A user’s guide to\nmeasure theoretic probability. Cambridge University Press.\n\n\nPorteus, E.L. 1975. Bounds and\ntransformations for discounted finite markov decision chains.\nOperations Research 23, 4, 761–784. DOI: 10.1287/opre.23.4.761.\n\n\nPorteus, E.L. 2008. Building intuition:\nInsights from basic operations management models and principles. In: D.\nChhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nPuterman, M.L. 2014. Markov decision\nprocesses: Discrete stochastic dynamic programming. John Wiley\n& Sons. DOI: 10.1002/9780470316887.\n\n\nQin, Y., Cao, M., and Anderson, B.D.O.\n2020. Lyapunov criterion for stochastic systems and its applications in\ndistributed computation. IEEE Transactions on Automatic\nControl 65, 2, 546–560. DOI: 10.1109/tac.2019.2910948.\n\n\nRachelson, E. and Lagoudakis, M.G. 2010.\nOn the locality of action domination in sequential decision making.\nProceedings of 11th international symposium on artificial\nintelligence and mathematics. Available at: https://oatao.univ-toulouse.fr/17977/.\n\n\nRigollet, P. 2015. High-dimensional\nstatistics. Available at: https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/.\n\n\nRivasplata, O. 2012. Subgaussian random\nvariables: An expository note. Available at: http://stat.cmu.edu/~arinaldo/36788/subgaussians.pdf.\n\n\nRobbins, H. and Monro, S. 1951. A\nstochastic approximation method. The Annals of Mathematical\nStatistics 22, 3, 400–407. DOI: 10.1214/aoms/1177729586.\n\n\nRobbins, H. and Siegmund, D. 1971. A\nconvergence theorem for non-negative almost supermartingales and some\napplications. In: Optimizing methods in statistics. Elsevier,\n233–257. DOI: 10.1016/b978-0-12-604550-5.50015-8.\n\n\nRockafellar, R.T. and Wets, R.J.-B. 2009.\nVariational analysis. Springer Science & Business Media.\n\n\nRoss, S.M. 1974. Dynamic programming and\ngambling models. Advances in Applied Probability 6, 3,\n593–606. DOI: 10.2307/1426236.\n\n\nSayedana, B. and Mahajan, A. 2020.\nCounterexamples on the monotonicity of delay optimal strategies for\nenergy harvesting transmitters. IEEE Wireless\nCommunications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E.\n2020. Cross-layer communication over fading channels with adaptive\ndecision feedback. International symposium on modeling and\noptimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nSerfozo, R.F. 1976. Monotone optimal\npolicies for markov decision processes. In: Mathematical programming\nstudies. Springer Berlin Heidelberg, 202–215. DOI: 10.1007/bfb0120752.\n\n\nShwartz, A. 2001. Death and discounting.\nIEEE Transactions on Automatic Control\n46, 4, 644–647. DOI: 10.1109/9.917668.\n\n\nSimon, H.A. 1956. Dynamic programming\nunder uncertainty with a quadratic criterion function.\nEconometrica 24, 1, 74–81. DOI: 10.2307/1905261.\n\n\nSingh, S.P. and Yee, R.C. 1994. An upper\nbound on the loss from approximate optimal-value functions. Machine\nLearning 16, 3, 227–233. DOI: 10.1007/bf00993308.\n\n\nSkinner, B.F. 1938. Behavior of\norganisms. Appleton-Century.\n\n\nSmallwood, R.D. and Sondik, E.J. 1973.\nThe optimal control of partially observable markov processes over a\nfinite horizon. Operations Research 21, 5, 1071–1088.\nDOI: 10.1287/opre.21.5.1071.\n\n\nStout, W.F. 1974. Almost sure\nconvergence. Academic Press.\n\n\nStriebel, C. 1965. Sufficient statistics\nin the optimal control of stochastic systems. Journal of\nMathematical Analysis and Applications 12, 576–592.\n\n\nSubramanian, J., Sinha, A., Seraj, R., and\nMahajan, A. 2022. Approximate information state for approximate\nplanning and reinforcement learning in partially observed systems.\nJournal of Machine Learning Research 23, 12, 1–83.\nAvailable at: http://jmlr.org/papers/v23/20-1165.html.\n\n\nTheil, H. 1954. Econometric models and\nwelfare maximization. Wirtschaftliches Archiv 72,\n60–83. DOI: 10.1007/978-94-011-2410-2_1.\n\n\nTheil, H. 1957. A note on certainty\nequivalence in dynamic planning. Econometrica, 346–349. DOI: 10.1007/978-94-011-2410-2_3.\n\n\nTopkis, D.M. 1998. Supermodularity\nand complementarity. Princeton University Press.\n\n\nTsitsiklis, J.N. 1984. Periodic review\ninventory systems with continuous demand and discrete order sizes.\nManagement Science 30, 10, 1250–1254. DOI: 10.1287/mnsc.30.10.1250.\n\n\nTsitsiklis, J.N. and Roy, B. van. 1996.\nFeature-based methods for large scale dynamic programming. Machine\nLearning 22, 1-3, 59–94. DOI: 10.1007/bf00114724.\n\n\nUrgaonkar, R., Wang, S., He, T., Zafer, M.,\nChan, K., and Leung, K.K. 2015. Dynamic service migration and\nworkload scheduling in edge-clouds. Performance Evaluation\n91, 205–228. DOI: 10.1016/j.peva.2015.06.013.\n\n\nVeinott, A.F. 1965. The optimal inventory\npolicy for batch ordering. Operations Research 13, 3,\n424–432. DOI: 10.1287/opre.13.3.424.\n\n\nVidyasagar, M. 2023. Convergence of\nstochastic approximation via martingale and converse\nLyapunov methods. Mathematics of Control, Signals, and\nSystems 35, 2, 351–374. DOI: 10.1007/s00498-023-00342-9.\n\n\nWainwright, M.J. 2019.\nHigh-dimensional statistics. Cambridge University Press. DOI:\n10.1017/9781108627771.\n\n\nWald, A. 1945. Sequential tests of\nstatistical hypotheses. The Annals of Mathematical Statistics\n16, 2, 117–186. DOI: 10.1214/aoms/1177731118.\n\n\nWald, A. and Wolfowitz, J. 1948. Optimum\ncharacter of the sequential probability ratio test. The Annals of\nMathematical Statistics 19, 3, 326–339. DOI: 10.1214/aoms/1177730197.\n\n\nWang, S., Urgaonkar, R., Zafer, M., He, T.,\nChan, K., and Leung, K.K. 2019. Dynamic service migration in\nmobile edge computing based on Markov decision process.\nIEEE/ACM Transactions on Networking\n27, 3, 1272–1288. DOI: 10.1109/tnet.2019.2916577.\n\n\nWhitin, S. 1953. The theory of\ninventory management. Princeton University Press.\n\n\nWhittle, P. 1982. Optimization over\ntime: Dynamic programming and stochastic control. Vol. 1 and 2.\nWiley.\n\n\nWhittle, P. 1996. Optimal control:\nBasics and beyond. Wiley.\n\n\nWhittle, P. 2002. Risk sensitivity,\nA strangely pervasive concept. Macroeconomic\nDynamics 6, 1, 5–18. DOI: 10.1017/s1365100502027025.\n\n\nWhittle, P. and Komarova, N. 1988. Policy\nimprovement and the newton-raphson algorithm. Probability in the\nEngineering and Informational Sciences 2, 2, 249–255. DOI:\n10.1017/s0269964800000760.\n\n\nWiewiora, E. 2003. Potential-based\nshaping and q-value initialization are equivalent. Journal of\nArtificial Intelligence Research 19, 1, 205–208.\n\n\nWitsenhausen, H.S. 1975. On policy\nindependence of conditional expectation. Information and\nControl 28, 65–75.\n\n\nWitsenhausen, H.S. 1976. Some remarks on\nthe concept of state. In: Y.C. Ho and S.K. Mitter, eds., Directions\nin large-scale systems. Plenum, 69–75.\n\n\nWitsenhausen, H.S. 1979. On the structure\nof real-time source coders. Bell System Technical Journal\n58, 6, 1437–1451.\n\n\nWittenmark, B., Åström, K.J., and Årzén,\nK.-E. 2002. Computer control: An overview. In: IFAC\nprofessional brief. IFAC. Available at: https://www.ifac-control.org/publications/list-of-professional-briefs/pb_wittenmark_etal_final.pdf.\n\n\nWoodall, W.H. and Reynolds, M.R. 1983. A\ndiscrete markov chain representation of the sequential probability ratio\ntest. Communications in Statistics. Part C: Sequential Analysis\n2, 1, 27–44. DOI: 10.1080/07474948308836025.\n\n\nYeh, E.M. 2012. Fundamental performance\nlimits in cross-layer wireless optimization: Throughput, delay, and\nenergy. Foundations and Trends in Communications and Information\nTheory 9, 1, 1–112. DOI: 10.1561/0100000014.\n\n\nZhang, H. 2009. Partially observable\nMarkov decision processes: A geometric technique and\nanalysis. Operations Research.\n\n\nZhang, N. and Liu, W. 1996. Planning\nin stochastic domains: Problem characteristics and approximation.\nHong Kong Univeristy of Science; Technology.\n\n\nZolotarev, V.M. 1984. Probability\nmetrics. Theory of Probability & Its Applications\n28, 2, 278–302. DOI: 10.1137/1128025."
  },
  {
    "objectID": "assignments/01.html",
    "href": "assignments/01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Exercise 1.1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 1.2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 2.3 from the notes on the newsvendor problem. Provide an analytic solution to the problem, similar to the derivation of the analytic solution for the case of continuous demand and actions in the notes."
  }
]