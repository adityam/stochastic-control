[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Notes",
    "section": "",
    "text": "About the course\nThese notes started as the lecture notes for ECSE 506 (Stochastic Control and Decision Theory) that I teach in the Winter term of every even year. These notes are not meant to be exhaustive; rather my focus is to convey the key ideas in their simplest form. For a more exhaustive treatment of the subject, please refer to the reference books mentioned below.\nIf you find any typos/mistakes in the notes, please let me know. Pull requests are welcome.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#reference-books",
    "href": "index.html#reference-books",
    "title": "Course Notes",
    "section": "Reference books",
    "text": "Reference books\n\nKumar and Varaiya, Stochastic Systems: Estimation, Identification, and Adaptive Control, Prentice Hall, 1986. Reprinted by SIAM 2015  A gentle introduction which emphaisizes the key conceptual ideas.\nBertsekas, Dynamic programming and optimal control, vol 1 and 2, Athena Publications, 2005.  Perhaps the most comprehensive book of different topics in dynamic programming.\nPuterman, Markov decision processes: discrete time dynamic programming, Wiley 1994.  Excellent source algorithms for perfectly observed systems, in particular, infinite horizon dynamic programs.\nRoss, Introduction to Stochastic Dynamic Programming, Academic Press, 1983.  Excellent introduction to dynamic programming, from the point-of-view of applied mathematics.\nDernardo, Dynamic Programming: Models and Applications, Prentice Hall, 1982.  Excellent introduction to dynamic programming, from the point-of-view of operations research.\nPowell, Approximate Dynamic Programming, John Wiley and Sons, 2011.  Comprehensive overview of approximate dynamic programming\nKrishnamurty, Partially Observable Markov Decision Processes, Cambridge University Press, 2016.  Comprehensive overview of POMDPs\nSargent and Stachurski, Dynamic Programming, 2023.  Nice summary of DP ideas applied to economic models. Good mix of theory and numerical examples.\nKochenderfer, Wheeler, and Wray, Algorithms for decision making, MIT Press, 2022.  Broad introduction to decision making under uncertainty. Lots of nice examples.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#how-to-cite-these-notes",
    "href": "index.html#how-to-cite-these-notes",
    "title": "Course Notes",
    "section": "How to cite these notes",
    "text": "How to cite these notes\nTo cite these lecture notes, please use:\n@misc{506notes,\n  author        = {Aditya Mahajan},\n  title         = {Lecture notes on Stochastic Control and Decision Theory},\n  year          = {2024},\n  howpublished  = \"\\url{https://adityam.github.io/stochastic-control/}\",\n}",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "stochastic-optimization/intro.html",
    "href": "stochastic-optimization/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The stochastic optimization problem\nNow consider the simplest stochastic optimization problem. A decision maker has to choose an action \\(a \\in \\ALPHABET A\\). Upon choosing the action \\(a\\), the decision maker incurs a cost \\(c(a,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable with known probability distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(a, W) ]\\), where the expectation is with respect to the random variable \\(W\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:stochastic}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(a, W) ].\n\\end{equation}\\]\nDefine \\(J(a) = \\EXP[ c(a, W) ]\\). Then Problem \\eqref{eq:stochastic} is conceptually the same as Problem \\eqref{eq:basic} with the cost function \\(J(a)\\). Numerically, Problem \\eqref{eq:stochastic} is more difficult because computing \\(J(a)\\) involves evaluating an expectation, but we ignore the computational complexity for the time being.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/intro.html#key-simplifying-idea",
    "href": "stochastic-optimization/intro.html#key-simplifying-idea",
    "title": "1  Introduction",
    "section": "1.2 Key simplifying idea",
    "text": "1.2 Key simplifying idea\nIn the stochastic optimization problems considered above, the decision maker does not observe any data before making a decision. In many situations, the decision maker does observe some data, which is captured by the following model. Suppose a decision maker observes a random variable \\(S\n\\in \\ALPHABET S\\) and then chooses an action \\(A \\in \\ALPHABET A\\) as a function of his observation according to a decision rule \\(π\\), i.e., \\[ A = π(S). \\]\nUpon choosing the action \\(A\\), the decision maker incurs a cost \\(c(S,A,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable. We assume that the primitive random variables \\((S,W)\\) are defined on a common probability space and have a known joint distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(S, π(S), W)]\\), where the expectation is taken with respect to the joint probability distribution of \\((S,W)\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:obs} \\tag{P1}\n  \\min_{π \\colon \\ALPHABET S \\to \\ALPHABET A} \\EXP[ c(S, π(S), W) ].\n\\end{equation}\\]\nDefine \\(J(π) = \\EXP[ c(S, π(S), W) ]\\). Then, Problem \\eqref{eq:obs} is conceptually the same as Problem \\eqref{eq:basic} with one difference: In Problem \\eqref{eq:basic}, the minimization is over a parameter \\(a\\), while in Problem \\eqref{eq:obs}, the minimization is over a function \\(π\\).\nWhen \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are finite sets, the optimal policy can be obtained by an exhaustive search over all policies as follows: for each policy \\(π\\) compute the performance \\(J(π)\\) and then pick the policy \\(π\\) with the smallest expected cost.\nSuch an exhaustive search is not satisfying for two reasons. First, it has a high computational cost. There are \\(| \\ALPHABET A |^{| \\ALPHABET S |}\\) policies and, for each policy, we have to evaluate an expectation, which can be expensive. Second, the above enumeration procedure does not work when \\(\\ALPHABET S\\) or \\(\\ALPHABET A\\) are continuous sets.\nThere is an alternative way of viewing the problem that simplifies it considerably. Instead of viewing the optimization problem before the system starts running (i.e., the ex ante view), imagine that the decision maker waits until they see the realization \\(s\\) of \\(S\\) (i.e., the interim view). they then asks what action \\(a\\) should they take to minimize the expected conditional cost \\(Q(s,a) := \\EXP[ c(s,a, W) | S = s]\\), i.e., they consider the problem\n\\[\\begin{equation} \\label{eq:cond-1} \\tag{P2}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a,W) | S = s], \\quad\n  \\forall s \\in \\ALPHABET S.\n\\end{equation}\\]\nThus, Problem \\eqref{eq:obs}, which is a functional optimization problem, has been reduced to a collection of parameter optimization problems (Problem \\eqref{eq:cond-1}), one for each possible of \\(s\\).\nNow define \\[ \\begin{equation} \\label{eq:cond} \\tag{P2-policy}\n  π^∘(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]\n\\end{equation} \\] where ties (in the minimization) are broken arbitrarily.\n\nTheorem 1.1 The decision rule \\(π^∘\\) defined in \\eqref{eq:cond} is optimal for Problem \\(\\eqref{eq:obs}\\), i.e., \\[\n  \\min_{π \\colon \\ALPHABET S \\to \\ALPHABET A} \\EXP[ c(S, π(S), W) ].\n  =\n  \\EXP \\biggl[ \\min_{a \\in \\ALPHABET A} \\EXP[ c(S,a,W) | S ] \\biggr]\n  .\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nWe restricted the proof to finite \\(\\ALPHABET S\\), \\(\\ALPHABET A\\), \\(\\ALPHABET W\\). This is to avoid any measurability issues. If \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are continuous sets, we need to restrict to measurable \\(π\\) in Problem \\(\\eqref{eq:obs}\\) (otherwise the expectation is not well defined; of course the cost \\(c\\) also has to be measurable). However, it is not immediately obvious that \\(π^∘\\) defined in \\eqref{eq:cond} is measurable. Conditions that ensure this are known as measurable selection theorems.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(π\\) be any other decision rule. Then, \\[ \\begin{align*}\n  \\EXP[ c(S, π(S), W) ] &\\stackrel{(a)}= \\EXP[ \\EXP[c(S, π(S), W) | S ] ] \\\\\n  &\\stackrel{(b)}\\ge \\EXP[\\EXP[ c(S, π^∘(S), W) | S ] ] \\\\\n  &\\stackrel{(c)}= \\EXP[ c(S, π^∘(S), W) ],\n\\end{align*} \\] where \\((a)\\) and \\((c)\\) follow from the law of iterated expectations and \\((b)\\) follows from the definition of \\(π^∘\\) in \\eqref{eq:cond}.\n\n\n\nWe can also provide a partial converse of Theorem 1.1.\n\nTheorem 1.2 If \\(\\PR(S = s) &gt; 0\\) for all \\(s\\), then any optimal policy \\(π^∘\\) for Problem \\(\\eqref{eq:obs}\\) must satisfy \\(\\eqref{eq:cond}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove this by contradiction. Suppose \\(π^*\\) is an optimal policy that does not satisfy \\eqref{eq:cond}. By definition of \\(π^∘\\), it must be the case that for all states \\[\\begin{equation}\n   \\EXP[ c(s, π^∘(s), W) | S = s ]\n   \\le\n   \\EXP[ c(s, π^*(s), W) | S = s ] .\n   \\label{eq:ineq:1}\n\\end{equation}\\] Now, since \\(π^*\\) does not satisfy \\eqref{eq:cond}, there exists some state \\(s^∘ \\in \\ALPHABET S\\) such that \\[\\begin{equation}\n   \\EXP[ c(s^∘, π^*(s^∘), W) | S = s^∘ ]\n   &gt;\n   \\EXP[ c(s^∘, π^∘(s^∘), W) | S = s^∘ ] .\n   \\label{eq:ineq:2}\n\\end{equation}\\] Therefore, \\[\\begin{align*}\n   \\EXP[ c(S, π^*(S), W) ]\n   &=\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^*(s), W) | S = s ] ]\n   \\\\\n   & \\stackrel{(a)}&gt;\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^∘(s), W) | S = s ] ]\n   \\\\\n   &=\n   \\EXP[ c(S, π^∘(S), W) ]\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:ineq:1} and \\eqref{eq:ineq:2} and the inequality is strict becase \\(\\PR(S = s^∘) &gt; 0\\). Thus, \\(J(π^*) &gt; J(π^∘)\\) and, hence, \\(π^*\\) cannot be an optimal policy.\n\n\n\n\n\n\n\n\n\nSome notation\n\n\n\nTo build up to the notation used in MDPs, define \\[\n    Q(s,a) \\coloneqq E[ c(s,a,W) \\mid S = s],\n\\] which is called the action-value function. For any policy \\(π\\), define \\[\n  V^π(s) = Q(s,π(s)),\n\\] which is called the value function of policy \\(π\\). The value function of the optimal policy is often denoted by \\(V^*\\) and, by definition of optimality, it has the property that for any policy \\(π\\): \\[\n  V^*(s) \\le V^π(s),\n  \\quad \\forall s \\in \\ALPHABET S.\n\\]",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/intro.html#an-example-optimal-policy-in-a-card-game",
    "href": "stochastic-optimization/intro.html#an-example-optimal-policy-in-a-card-game",
    "title": "1  Introduction",
    "section": "1.3 An example: optimal policy in a card game",
    "text": "1.3 An example: optimal policy in a card game\nConsier a gamler playing a stylized version of card game played with a deck of \\(4\\) cards: \\(\\{1,2,3,4\\}\\). A dealer deals two cards: one to himself and one to the gabler. The gambler looks at his card, and can decide to take one of two actions: fold or challenge. If the gambler folds, the game is over; if he challenges and has a higher card than the dealer, he wins \\$1; if he challenges but has a lower than the dealer, he loses \\$1. What is the optimal policy?\nWe can model this as a stochastic optimization probem. Let \\(W\\) denote the cards dealt, \\(S\\) denote gambler’s card, and \\(A=0\\) denote the fold action and \\(A=1\\) denote the challenge action. Then: \\[\n\\ALPHABET W = \\{(1,2), (1,3), (1,4), (2,1), (2,3), (2,4), (3,1), (3,2), (3,4), (4,1), (4,2), (4,3) \\},\n\\] where the first index denotes the card dealt to the dealer and the second denotes the card dealt to the gabmler. We will assume that each of these outcomes is equally likely.\nNote that \\(\\ALPHABET S = \\{1,2,3,4\\}\\) and \\(\\ALPHABET A = \\{0,1\\}\\). So, a policy \\(π\\) is a mapping from \\(\\ALPHABET S\\) to \\(\\ALPHABET A\\), i.e., for each card that could be dealt to the gambler, he needs to decide whether to fold or challenge.\nThere are \\(|\\ALPHABET S|^{|\\ALPHABET A|} = 4^2 = 16\\) policies. The performance of a policy \\(π\\) is: \\[\n  J(π) = \\sum_{(d,s) \\in \\ALPHABET W} \\PR(d,s) c(s, π(s), (d,s))\n\\] where we are using the variable \\(d\\) to denote the card dealt to the dealer.\nNow consider one specific policy: \\(π = [0, 0, 1, 1]\\), which means that the gambler folds if he gets card 1 or 2 and challenges if he gets card 3 and 4. Then, we have \\[\\begin{align*}\n  J(π) &=\n  \\underbrace{\\tfrac 1{12} \\cdot 0}_{(d,s) = (1,2)}\n  +\n  \\underbrace{\\tfrac 1{12} \\cdot 1}_{(d,s) = (1,3)}\n  +\n  \\underbrace{\\tfrac 1{12} \\cdot 1}_{(d,s) = (1,4)}\n  \\\\\n  & \\quad +\n  \\underbrace{\\tfrac 1{12} \\cdot 0}_{(d,s) = (2,1)}\n  +\n  \\underbrace{\\tfrac 1{12} \\cdot 1}_{(d,s) = (2,3)}\n  +\n  \\underbrace{\\tfrac 1{12} \\cdot 1}_{(d,s) = (2,4)}\n  \\\\\n  & \\quad +\n  \\underbrace{\\tfrac 1{12} \\cdot 0}_{(d,s) = (3,1)}\n  +\n  \\underbrace{\\tfrac 1{12} \\cdot 0}_{(d,s) = (3,2)}\n  +\n  \\underbrace{\\tfrac 1{12} \\cdot 1}_{(d,s) = (3,4)}\n  \\\\\n  & \\quad +\n  \\underbrace{\\tfrac 1{12} \\cdot 0}_{(d,s) = (4,1)}\n  +\n  \\underbrace{\\tfrac 1{12} \\cdot 0}_{(d,s) = (4,2)}\n  +\n  \\underbrace{\\tfrac 1{12} \\cdot (-1)}_{(d,s) = (4,3)}\n  \\\\\n  &= \\tfrac 4{12} = \\tfrac 13.\n\\end{align*}\\]\nA brute force search corresponds to computing the performance of all \\(16\\) policies and picking the one with the best performance.\nThe alternative way to compute the optimal policy via \\(\\eqref{eq:cond}\\) proceeds as follows. First observe that \\[\n  Q(s,a) \\coloneqq\n  \\EXP[ c(s,a,W) \\mid S = s] = \\sum_{ (d,s) \\in \\ALPHABET W }\n  \\PR( (d,s) \\mid S = s) c(s,a,(d,s)).\n\\] Moreover, \\(Q(s,0)\\) is always \\(0\\). So, we just need to compute \\(Q(s,1)\\) and check if it is larger or smaller than \\(0\\).\n\n\\(Q(1,1) = \\tfrac 13 \\bigl[ -1 -1 -1 \\bigr] = -1 &lt; 0\\).\n\\(Q(2,1) = \\tfrac 13[1 -1 -1] = -\\tfrac 13 &lt; 0\\).\n\\(Q(3,1) = \\tfrac 13[1 + 1 -1] = \\tfrac 13 &gt; 0\\).\n\\(Q(4,1) = \\tfrac 13 [ 1 + 1 + 1] = 1 &gt; 0\\).\n\nThus, the optimal policy is \\(π = [0, 0, 1, 1]\\). We can also verify that \\[\n  J(π) = \\sum_{s \\in \\ALPHABET S} \\PR(s) Q(s,π(s))\n  = \\tfrac 14 [ 0 + 0 + \\tfrac 13 + 1 ] = \\tfrac 13\n\\] which is the same value that we had obtained by direct computation above.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/intro.html#a-tree-representation-of-the-optimization-problem",
    "href": "stochastic-optimization/intro.html#a-tree-representation-of-the-optimization-problem",
    "title": "1  Introduction",
    "section": "1.4 A tree representation of the optimization problem",
    "text": "1.4 A tree representation of the optimization problem\nWhen all variables \\((S,W,A)\\) are finite valued, the stochastic optimization problem defined above can also be modeled as a tree. Such a tree formulation has two benefits:\n\nit provides a geometric view, which can be useful in visualizing the result;\nfor general, multi-stage problems, it is possible to exploit the tree structure to obtain efficient computation algorithms, see, e.g., Fu (2018) for an historical overview.\n\nThere are two ways to construct the tree representation: the first as an one-stage optimization problem with perfect observation and the second as a one-stage optimization problem with imperfect observation. The first is simpler and the second is more general. We explain both formulations below.\n\n1.4.1 One-step optimization with perfect observation\nWe will view the model according to the following timing diagram:\n\n\n\n\n\n\nFigure 1.1: Timing diagram for modeling as a one-step optimization problem with perfect observation\n\n\n\nThus, nature first generate the “state” variable \\(S\\) according to some distribution \\(P_S\\), agent takes action \\(A\\) according to the policy \\(π \\colon \\ALPHABET S \\to \\ALPHABET A\\), and then nature generates the “disturbance” \\(W\\) according to some distribution \\(P_{W|S}\\).\nWe can view this as a tree, shown in Figure 1.2, where the root note corresponds to moves nature (denoted by \\(c\\) for chance) to generate \\(S\\), nodes at depth 1 correspond to the moves of the agent (denoted by \\(d\\) for decision maker) to generate \\(A\\), and nodes at depth 2 correspond to moves of nature to generate \\(W\\).[^simplify]\n[^simplify] For simplicity, we set \\(W = (D,S)\\) and only show \\(D\\) (the card dealt to the dealer) on the tree.\n\n\n\n\n\n\nFigure 1.2: Tree representation of the one-step optimization problem. Nodes represent the “agent”: nature or decision maker; the edges represent the moves of the agents; and the numbers at the leaf nodes represent the cost \\(c(s,a,w)\\). The shaded edges represent a policy.\n\n\n\nWe can do the calculation to compute the optimal policy on the tree. The calculations proceed by starting at the last level and moving upwards. In particular, at the nodes of depth \\(2\\), we compute \\(Q(s,a)\\) using: \\[\nQ(s,a) = \\EXP[ c(s,a,W) \\mid S = s]\n=\n\\sum_{w \\in \\ALPHABET W}P_{W|S}(w|s) c(s,a,w)\n\\] If we are given a policy \\(π\\) (like the policy shown by yellow shaded edges in Figure 1.2), we can compute \\(V^π(s)\\) using: \\[\n  V^π(s) = Q(s,π(s)).\n\\] We can also compute the optimal value function \\(V^*(s)\\) using \\[\n  V^*(s) = \\min_{a \\in \\ALPHABET A}Q(s,a)\n\\] where the optimal policy is given by \\[\n  π^*(s) \\in \\arg\\min_{a \\in \\ALPHABET A}Q(s,a).\n\\]\nThese calculations are the same as we did in the previous section; the tree simply helps in visualizing what is going on.\n\n\n1.4.2 One-step optimization with imperfect observation\nIn this case, we will view the model according to the following timing diagram:\n\n\n\n\n\n\nFigure 1.3: Timing diagram for modeling as a one-step optimization problem with imperfect observation\n\n\n\nWe can view this as a tree, shown in Figure 1.4, where the root note corresponds to moves nature to generate \\((S,W)\\), nodes at depth 1 correspond to the moves of the agent to generate \\(A\\).\n\n\n\n\n\n\nFigure 1.4: Tree representation of the one-step optimization problem. Nodes represent the “agent”: nature or decision maker; the edges represent the moves of the agents; and the numbers at the leaf nodes represent the cost \\(c(s,a,w)\\). The shaded edges represent a policy.\n\n\n\nIn this case, the decision maker cannot distinguish between multiple moves of nature. For example, for \\(W \\in \\{ (2,1), (3,1), (4,1) \\}\\), the decision maker gets the same observation \\(S=1\\). This is indicated in the tree diagram by drawing an information set around these three nodes, and ensuring that the action taken at these nodes is identical.\nIn this formulation, the decision maker has imperfect observation of nature’s move. After making an observation, it forms a posterior belief on the state of nature. For example, \\[\n  \\PR(W | S = 1) =\n  \\begin{cases}\n    \\tfrac 13, & \\hbox{if } W \\in \\{(2,1), (3,1), (4,1) \\} \\\\\n    0, & \\hbox{otherwise}\n  \\end{cases}\n\\] The agent then computes \\(Q(s,a)\\) by averaging over this belief: \\[\n  Q(s,a) = \\sum_{w \\in \\ALPHABET W} \\PR(w|s) c(s,a,w).\n\\] The value functions \\(V^π(s)\\) and \\(V^*(s)\\) and then computed exactly as before.\nThe difference between the two approaches is conceptual. For the one-step optimization problen, the actual mathematical calculations done in both formulations are the same. When we go to multi-stage optimization problem, we will see that perfect and imperfect observation models behave very differently.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "href": "stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "title": "1  Introduction",
    "section": "1.5 Blackwell’s principle of irrelevant information",
    "text": "1.5 Blackwell’s principle of irrelevant information\nIn many scenarios, the decision maker may observe data which is irrelevant for evaluating performance. In such instances, the decision maker may ignore such information without affecting performance. Formally, we have the following result, which is known as Blackwell’s principle of irrelevant information.\n\nTheorem 1.3 (Blackwell’s principle of irrelevant information) Let \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), and \\(\\ALPHABET A\\) be standard Borel spaces and \\(S \\in \\ALPHABET S\\), \\(Y \\in \\ALPHABET Y\\), \\(W \\in\n\\ALPHABET W\\) be random variables defined on a common probability space.\nA decision maker observes \\((S,Y)\\) and chooses \\(A = π(S,Y)\\) to minimize \\(\\EXP[c(S,A,W)]\\), where \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times\n\\ALPHABET W \\to \\reals\\) is a measurable function.\nThen, if \\(W\\) is conditionally independent of \\(Y\\) given \\(S\\), then there is no loss of optimality in choosing \\(A\\) only as a function of \\(S\\).\nFormally, there exists a \\(π^* \\colon \\ALPHABET S \\to \\ALPHABET A\\) such that for all \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), \\[ \\EXP[c(S, π^*(S), W)] \\le \\EXP[ c(S, π(S,Y), W) ]. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result for the case when \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET\nW\\), \\(\\ALPHABET A\\) are finite.\nDefine \\[π^*(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]. \\] Then, by construction, for any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), we have that \\[ \\EXP[ c(s, π^*(s), W ) | S = s]  \\le \\EXP[ c(s,a,W) | S = s]. \\] Hence, for any \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), and for any \\(s \\in \\ALPHABET S\\) and \\(y \\in \\ALPHABET Y\\), we have \\[ \\begin{equation} \\label{eq:opt}\n  \\EXP[ c(s, π^*(s), W) | S = s] \\le \\EXP[ c(s, π(s,y),W) | S = s].\n\\end{equation} \\] The result follows by taking the expectation of both sides of \\eqref{eq:opt}.\n\n\n\nThe above proof doesn’t work for general Borel spaces because \\(π^*\\) defined above may not exist (inf vs min) or may not be measurable. See Blackwell (1964) for a formal proof.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/intro.html#exercises",
    "href": "stochastic-optimization/intro.html#exercises",
    "title": "1  Introduction",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.1 (Computing optimal policies) Suppose \\(\\ALPHABET S = \\{1, 2 \\}\\), \\(\\ALPHABET A = \\{1, 2, 3\\}\\), and \\(\\ALPHABET\nW = \\{1, 2, 3\\}\\). Let \\((S,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET W\\) with joint distribution \\(P\\) shown below.\n\\[ P = \\MATRIX{ 0.25 & 0.15 & 0.05  \\\\ 0.30 & 0.10 & 0.15 } \\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S=2, W=1) = P_{21} = 0.30\\).\nThe cost function \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET\nW \\to \\reals\\) is shown below\n\\[\nc(\\cdot,\\cdot,1) = \\MATRIX{3 & 5 & 1 \\\\ 2 & 3 & 1 }, \\quad\nc(\\cdot,\\cdot,2) = \\MATRIX{4 & 3 & 1 \\\\ 1 & 2 & 8 }, \\quad\nc(\\cdot,\\cdot,3) = \\MATRIX{1 & 2 & 2 \\\\ 4 & 1 & 3 }.\n\\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(a\\). For example \\(c(s=1,a=2,w=1) = 5\\).\nFind the policy \\(π \\colon \\ALPHABET S \\to \\ALPHABET A\\) that minimizes \\(\\EXP[ c(S, π(S), W) ]\\).\n\n\nExercise 1.2 (Blackwell’s principle) Suppose \\(\\ALPHABET S = \\{1, 2\\}\\), \\(\\ALPHABET Y = \\{1, 2\\}\\), \\(\\ALPHABET A =\n\\{1, 2, 3\\}\\), and \\(\\ALPHABET W = \\{1, 2, 3\\}\\). Let \\((S,Y,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET Y × \\ALPHABET W\\), with joint distribution \\(P\\) shown below. \\[\nP_{Y = 1} = \\MATRIX{0.15 & 0.10 & 0.00 \\\\ 0.15 & 0.05 & 0.10}\n\\qquad\nP_{Y = 2} = \\MATRIX{0.10 & 0.05 & 0.05 \\\\ 0.15 & 0.05 & 0.05}\n\\] For a fixed value of \\(y\\), the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S = 1, Y = 1, W = 3) = 0\\). Note that the joint distribution of \\((S,Y,W)\\) is such that the margin on \\((S,W)\\) is the same as the distribution on \\((S,W)\\) given in the previous exercise (Exercise 1.1).\nThe cost function \\(c \\colon \\ALPHABET S × \\ALPHABET A × \\ALPHABET W \\to\n\\reals\\) is the same as the previous exercise.\n\nFind the policy \\(π \\colon \\ALPHABET S × \\ALPHABET Y \\to \\ALPHABET A\\) that minimizes \\(\\EXP[c(S, π(S,Y), W)]\\).\nCompare the solution with the solution of the previous exercise (Exercise 1.1) in view of Blackwell’s principle of irrelevant information. Clearly explain your observations.\nRepeat the above exercise with the following joint distribution: \\[\nP_{Y = 1} = \\MATRIX{0.20 & 0.12 & 0.04 \\\\ 0.24 & 0.08 & 0.12}\n\\qquad\nP_{Y = 2} = \\MATRIX{0.05 & 0.03 & 0.01 \\\\ 0.06 & 0.02 & 0.03}\n\\]\n\n\n\n\nExercise 1.3 (Pollution monitoring) Consider the problem of monitoring the pollution level of a river. The river can have a high pollution level if there is a catastrophic failure of a factory upstream. There are then two “pollution states” indicating whether such a failure has occured. We denote them by \\(W = 0\\) (indicating no failure) and \\(W = 1\\) (indicating catastrophic failure). Let \\([p, 1-p]\\) denote the prior probability mass function of \\(W\\).\nThe pollution monitoring system has a sensor which takes a measurement \\(s\\) of the pollution level. Let \\(f_w(s)\\) denote the probabiity density of the observation \\(s\\) conditional on the value of \\(w\\), \\(w \\in \\{0, 1\\}\\). Two actions are available at the monitoring system: raise an alarm or not raise an alarm. The cost of raising the alarm is \\(C_0\\) if the state \\(W\\) is \\(0\\) or zero if the state \\(W\\) is \\(1\\); the cost of not raising the alarm is zero if the state \\(W\\) is \\(0\\) or \\(C_1\\) if the state \\(W\\) is \\(1\\).\nShow that it is optimal to raise the alarm if \\[ p f_0(s) C_0 &lt; (1 - p) f_1(s) C_1. \\] That is, it is optimal to raise the alarm if the likelihood ratio \\(f_1(s)/f_0(s)\\) exceeds the threshold value \\(p C_0/(1-p) C_1\\).\n\n\nExercise 1.4 (Pollution monitoring, continued) The decision rule obtain in Exercise 1.3 is of the form: raise the alarm if \\[\n    \\frac{f_1(s)}{f_0(s)} &gt; τ,\n    \\quad \\hbox{where } τ = \\frac{p C_0}{(1-p)C_1}.\n\\] When the observation density belongs to the :exponential family, it is more convenient to work with the log-likelihood ratio \\(\\ALPHABET L(s) \\coloneqq \\log (f_1(s)/f_0(s))\\) and use the test: raise the alarm if \\[\n  \\ALPHABET L(s) &gt; \\log τ.\n\\] This is called the log-likelihood ratio test (LLRT).\nSuppose that the observation density is conditionally Gaussian and is given by: \\[\n  f_w(s) = \\exp\\biggl( - \\frac{ (s-w)^2 }{ 2 σ^2 }\\biggr)\n\\] where \\(σ &gt; 0\\) is known. Simplify the LLRT. Does the decision rule intuitively make sense?\n\n\nExercise 1.5 (Stochastic/Randomized policies) In the discussion above, we have assumed that decision rule \\(π\\) is deterministic. It is also possible to consider stochastic/randomized decision rules: \\(π \\colon \\ALPHABET S \\to Δ(\\ALPHABET A)\\), where the performance of a policy is given by \\[\nJ(π) = \\sum_{s,w \\in \\ALPHABET S × \\ALPHABET W} \\sum_{a \\in \\ALPHABET A}\nP(s,w) π(a \\mid s) c(s,a,w).\n\\] Now consider the problem of \\(\\min_{π : \\ALPHABET S \\to Δ(\\ALPHABET A)} J(π)\\). Show that the result of Theorem 1.1 remains valid even when we allow stochastic/randomization.\nNote: This fact is sometimes stated as “randomization does not improve performance”, but is only true for unconstrained problems. When constraints are involved, randomization may improve performance.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/intro.html#notes",
    "href": "stochastic-optimization/intro.html#notes",
    "title": "1  Introduction",
    "section": "Notes",
    "text": "Notes\nTheorem 1.3 is due to Blackwell (1964) in a short 2.5 page paper. A similar result was used by Witsenhausen (1979) to show the structure of optimal coding strategies in real-time communication. Also see the blog post by Maxim Ragisnsky.\nExercise 1.3 is adaptive from Whittle (1996). It is a special instance of Bayesian hypothesis testing problem. We will study a generalization of this model later in sequential hypothesis testing\nHistorically, the tree representation of optimization problems is due to Kuhn (1950; 1953), who used them to model multi-player multi-stage games.\n\n\n\n\nBlackwell, D. 1964. Memoryless strategies in finite-stage dynamic programming. The Annals of Mathematical Statistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nFu, M.C. 2018. Monte carlo tree search: A tutorial. 2018 winter simulation conference (WSC), IEEE. DOI: 10.1109/wsc.2018.8632344.\n\n\nKuhn, H.W. 1950. Extensive games. Proceedings of the National Academy of Sciences 36, 10, 570–576. DOI: 10.1073/pnas.36.10.570.\n\n\nKuhn, H.W. 1953. Extensive games and the problem of information. In: H.W. Kuhn and A.W. Tucker, eds., Contributions to the theory of games. Princeton University Press, 193–216.\n\n\nWhittle, P. 1996. Optimal control: Basics and beyond. Wiley.\n\n\nWitsenhausen, H.S. 1979. On the structure of real-time source coders. Bell System Technical Journal 58, 6, 1437–1451.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html",
    "href": "stochastic-optimization/newsvendor.html",
    "title": "2  The newsvendor problem",
    "section": "",
    "text": "2.1 Interlude with continuous version\nThe problem above has discrete action and discrete demand. To build intuition, we first consider the case where both the actions and demand are continuous. Let \\(f(w)\\) denote the probability density of the demand and \\(F(w)\\) denote the cumulative probability density. Then, the expected reward is \\[ \\begin{equation} \\label{eq:J}\nJ(a) = \\int_{0}^a [ q w - p a ] f(w) dw + \\int_{a}^\\infty [ q a - p a ] f(w) dw.\n\\end{equation}\\]\nTo fix ideas, we consider an example where \\(p = 0.5\\), \\(q = 1\\), and the demand is a :Kumaraswamy distribution with parameters \\((2,5)\\) and support \\([0,100]\\). The performance of a function of action is shown below.\np = 0.5\nq = 1\nr = function(w,a){ if(w&lt;=a) { return q*w - p*a } else { return q*a - p*a } }\n\na_opt = inverseCDF( (q-p)/q )\n\nconfig = ({\n  // Kumaraswamy Distribution: https://en.wikipedia.org/wiki/Kumaraswamy_distribution\n  a: 2,\n  b: 5,\n  max: 100\n})\n\npdf = {\n  const a = config.a\n  const b = config.b\n\n  return function(x) {\n    var normalized = x/config.max\n    return a*b*normalized**(a-1)*(1 - normalized**a)**(b-1)\n  }\n}\n\ninverseCDF= {\n  const a = config.a\n  const b = config.b\n  return function(y) {\n     // Closed form expression for inverse CDF of Kumaraswamy distribution \n     return config.max * (1 - (1-y)**(1/b))**(1/a)\n  }\n}\n\npoints = { \n  const n = 1000\n  var points = new Array(n)\n  var cdf = 0;\n  for (var i = 0 ; i &lt; n; i++) {\n    var x = config.max * i/n\n    cdf = cdf + pdf(x) * 1/n\n    points[i] = {demand: x, pdf: pdf(x), CDF: cdf, reward: r(x,action) }\n  }\n  return points\n}\n\ncost_values = {\n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i &lt; n; i++) {\n    var x = config.max * i/n\n    points[i] = {action: x, performance: J(x)}\n  }\n  return points\n}\n\nJ = {\n  const a = config.a\n  const b = config.b\n\n  return function(action) {\n    const n = 1000\n    var cost = 0\n    var w = 0\n    for (var i = 0; i &lt; n; i++) {\n      w = config.max*i/n\n      if (w &lt;= action) {\n        cost += (q*w - p*action)*pdf(w)/n\n      } else {\n        cost += (q*action - p*action)*pdf(w)/n\n      }\n    }\n    return cost\n  }\n}\nviewof action = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncost = Math.round(J(action)*100)/100\nIn Figure 2.1(a), the plot of \\(J(a)\\) is concave. We can verify that this is true in general.\nThis suggests that we can use calculus to find the optimal value. In particular, to find the optimal action, we need to compute the \\(a\\) such that \\(dJ(a)/da = 0\\).",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The newsvendor problem</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "href": "stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "title": "2  The newsvendor problem",
    "section": "",
    "text": "plotJ = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [action, J(action)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_opt, J(a_opt)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_values, {x:\"action\", y:\"performance\"})\n  ]\n})\n\nplotPDF = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [action,0], [action, pdf(action)] ], {stroke: \"blue\"}),\n    Plot.line(points,{x:\"demand\", y:\"pdf\"}),\n    Plot.areaY(points.filter(pt =&gt; pt.demand &lt;= action),{x:\"demand\", y:\"pdf\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt =&gt; pt.demand &gt; action),{x:\"demand\", y:\"pdf\", fill: \"pink\"})\n  ]\n})\n\nplotReward = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {x:\"demand\", y:\"reward\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\n\n\n\n\nFigure 2.1: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shaded region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\n\n\n\n\n\n\n\nVerify that \\(J(a)\\) is concave\n\n\n\n\n\nTo verify that the function \\(J(a)\\) is concave, we compute the second derivative: \\[\n  \\frac{d^2 J(a)}{da^2} = - p f(a) - (q - p) f(a) = -q f(a) \\le 0.\n\\]\n\n\n\n\n\nProposition 2.1 For the newsvendor problem with continuous demand, the optimal action is \\[\n    a = F^{-1}\\left( 1 - \\frac{p}{q} \\right).\n  \\] In the literature, the quantity \\(1 - (p/q)\\) is called the critical fractile.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\nLeibniz integral rule\n\n\n\n\n\n\\[ \\dfrac{d}{dx} \\left( \\int_{p(x)}^{q(x)} f(x,t) dt \\right)\n   = f(x, q(x)) \\cdot \\dfrac {d}{dx} q(x)\n   - f(x, p(x)) \\cdot \\dfrac {d}{dx} p(x)\n   + \\int_{p(x)}^{q(x)} \\dfrac{\\partial}{\\partial x} f(x,t) dt.\n\\]\n\n\n\nUsing the Leibniz integral rule, the derivative of the first term of \\(\\eqref{eq:J}\\) is \\[ [q a - p a ] f(a) + \\int_{0}^a [ -p ] f(w) dw\n= [q a - p a ] f(a) - p F(a).\n\\]\nSimilarly, the derivative of the second term of \\(\\eqref{eq:J}\\) is \\[ - [q a - p a] f(a) + \\int_{a}^{\\infty} (q-p)f(w)dw\n= - [q a - p a] f(a) + (q -p)[ 1 - F(a)].\n\\]\nCombining the two, we get that \\[ \\dfrac{dJ(a)}{da} = - p F(a) + (q - p) [ 1 - F(a) ]. \\]\nEquating this to \\(0\\), we get \\[ F(a) = \\dfrac{ q - p }{ q}\n\\quad\\text{or}\\quad\na = F^{-1} \\left( 1 - \\dfrac{  p }{ q } \\right).\n\\]\n\n\n\n\n\n\n\n\n\nGraphical interpretation of the result\n\n\n\nThe result of Proposition 2.1 has a nice graphical interpretation. Draw the CDF of the demand. The optimal action is the point where the CDF intersects the horizontal line \\(1 - p/q\\).\n\nviewof pcts = Inputs.range([0.01,1], {value: 0.5, step: 0.01, label: \"p\"})\nviewof qcts = Inputs.range([0.01,1], {value: 1, step: 0.01, label: \"q\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF_opt = 1 - pcts/qcts\nplotOptCts = Plot.plot({\n  grid: true,\n  y: {domain: [0,1]},\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points,{x:\"demand\", y:\"CDF\"}),\n    Plot.line([ [0,F_opt], [points[points.length-1].demand, F_opt] ], {stroke: \"red\"})\n  ]\n})",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The newsvendor problem</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "href": "stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "title": "2  The newsvendor problem",
    "section": "2.2 Back to discrete version",
    "text": "2.2 Back to discrete version\nNow, we come back to the problem with discrete actions and discrete demand. Suppose \\(W\\) takes the values \\(\\ALPHABET W = \\{ w_1, w_2, \\dots, w_k \\}\\) (where \\(w_1 &lt; w_2 &lt;\n\\cdots &lt; w_k\\)) with probabilities \\(\\{ μ_1, μ_2, \\dots, μ_k \\}\\). It is easy to see that in this case the action \\(a\\) should be in the set \\(\\{ w_1, w_2, \\dots,\nw_k \\}\\).\nTo fix ideas, we repeat the above numerical example when \\(\\ALPHABET W = \\{0, 1, \\dots, 100\\}\\).\n\npointsD = { \n  const n = 100\n  var points = new Array(n)\n  var cdf = 0\n  for (var i = 0 ; i &lt; n; i++) {\n    var x = config.max * i/n\n    cdf = cdf + pdf(x) * 1/n\n    points[i] = {demand: x, pdf: pdf(x) * 1/n, CDF: cdf, reward: r(x,actionD) }\n  }\n  return points\n}\n\ncost_valuesD = {\n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i &lt; n; i++) {\n    var x = config.max * i/n\n    points[i] = {action: x, performance: J(x)}\n  }\n  return points\n}\n\n\na_optD = Math.round(a_opt*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof actionD = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncostD = Math.round(J(actionD)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJD = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [actionD, J(actionD)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_optD, J(a_optD)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_valuesD, {x:\"action\", y:\"performance\", curve: \"step-after\"})\n  ]\n})\n\nplotPDFD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    // Plot.line([ [actionD,0], [actionD, pdf(actionD)] ], {stroke: \"blue\"}),\n    Plot.line(pointsD,{x:\"demand\", y:\"pdf\", curve:\"step-after\"}),\n    Plot.areaY(pointsD.filter(pt =&gt; pt.demand &lt;= actionD),{x:\"demand\", y:\"pdf\", curve: \"step-after\", fill: \"lightblue\"}),\n    Plot.areaY(pointsD.filter(pt =&gt; pt.demand &gt;= actionD),{x:\"demand\", y:\"pdf\", curve: \"step-after\", fill: \"pink\"})\n  ]\n})\n\nplotRewardD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(pointsD, {x:\"demand\", y:\"reward\", curve: \"step-after\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\n\n\n\n\nFigure 2.2: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shaded region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn the discrete case, the brute force search is easier (because there are a finite rather than continuous number of values). We cannot directly use the ideas from calculus because functions over discrete domain are not differentiable. But we can use a very similar idea. Instead of checking if \\(dJ(a)/da = 0\\), we check the sign of \\(J(w_{i+1}) - J(w_i)\\).\n\nProposition 2.2 Let \\(\\{M_i\\}_{i \\ge 1}\\) denote the cumulative mass function of the demand. Then, the optimal action is the largest value of \\(w_i\\) such that \\[\n    M_i \\le 1 - \\frac{p}{q}.\n  \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe expected reward for choice \\(w_i\\) is \\[ \\begin{align*} J(w_i) &=\n\\sum_{j &lt; i} μ_j [ q w_j - p w_i ] + \\sum_{j \\ge i} μ_j [q w_i - p w_i]\n\\\\\n&= -p w_i + q \\Bigl[ \\sum_{j &lt; i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr].\n\\end{align*}\\]\nThus, \\[ \\begin{align*}\n  J(w_{i+1}) - J(w_i) &=\n  -p w_{i+1} + q \\Bigl[ \\sum_{j &lt; i+1}  μ_j w_j + \\sum_{j \\ge i+1} μ_j w_{i+1} \\Bigr]\n  \\\\\n  &\\quad + p w_i - q \\Bigl[ \\sum_{j &lt; i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr]\n  \\\\\n  &= -p (w_{i+1} - w_i) + q \\Bigl[ \\sum_{j \\ge i + 1} μ_j ( w_{i+1} - w_i) \\Bigr]\n  \\\\\n  &= \\big( - p + q [ 1 - M_i ] \\big) (w_{i+1} - w_i).\n\\end{align*}\\] Note that \\[\nM_i \\le \\dfrac{q-p}{q}\n\\iff\n-p + q [ 1 - M_i ] \\ge 0.\n\\] Thus, for all \\(i\\) such that \\(M_i \\le (q-p)/q\\), we have \\(J(w_{i+1}) \\ge J(w_i)\\). On the other hand, for all \\(i\\) such that \\(M_i &gt; (q-p)/q)\\), we have \\(J(w_{i+1}) &lt; J(w_i)\\). Thus, the optimal amount to order is the largest \\(w_i\\) such that \\(M_i \\le\n(q-p)/q\\).\n\n\n\n\n\n\n\n\n\nGraphical interpretation of the result\n\n\n\nThe structure of the optimal solution is the same for continuous and discrete demand distributions. In particular, the result of Proposition 2.2 has the same graphical interpretation as that of ?prp-newscendor-cts:\n\nDraw the CDF of the demand. The optimal action is the point where the CDF intersects the horizontal line \\(1 - p/q\\).\n\n\nviewof pdis = Inputs.range([0.01,1], {value: 0.5, step: 0.01, label: \"p\"})\nviewof qdis = Inputs.range([0.01,1], {value: 1, step: 0.01, label: \"q\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF_opt_dis = 1 - pdis/qdis\nplotOptDts = Plot.plot({\n  grid: true,\n  y: {domain: [0,1]},\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(pointsD,{x:\"demand\", y:\"CDF\", curve:\"step-after\"}),\n    Plot.line([ [0,F_opt_dis], [pointsD[pointsD.length-1].demand, F_opt_dis] ], {stroke: \"red\"})\n  ]\n})",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The newsvendor problem</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#exercises",
    "href": "stochastic-optimization/newsvendor.html#exercises",
    "title": "2  The newsvendor problem",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 2.1 (Qualitative properties of optimal solution) Intuitively, we expect that if the purchase price of the newspaper increases but the selling price remains the same, then the newsvendor should buy less newspapers. Formally prove this statement.\nHint: The CDF of a distribution is a weakly increasing function.\n\n\nExercise 2.2 (Monotonicity of optimal action) Consider two scenarios for the case with continuous demand and actions. In scenario 1, the demand is distributed according to PDF \\(f_1\\). In scenario 2, it is distributed according to PDF \\(f_2\\). Suppose \\(F_1(w) \\le F_2(w)\\) for all \\(w\\). Show that the optimal action \\(a_1\\) for scenario 1 is greater than the optimal action \\(a_2\\) for scenario 2.\nHint: Plot the two CDFs and try to interpret the optimal decision rule graphically.\n\n\nExercise 2.3 (Selling random wind) The amount \\(W\\) of power generated by the wind turbine is a positive real-valued random variable with probability density function \\(f\\). The operator of the wind turbine has to commit to provide a certain amount of power in the day-ahead market. The price of power is \\(\\$p\\) per MW.\nIf the operator commits to provide \\(a\\) MWs of power and the wind generation \\(W\\) is less than \\(a\\), then he has to buy the balance \\(a - W\\) from a reserves market at the cost of \\(\\$ q\\) per unit, where \\(q &gt; p\\). Thus, the reward of the operator is \\(r(a,W)\\) where \\[ r(a, w) = \\begin{cases}\n  p a, & \\text{if } w &gt; a \\\\\n  p a - q (a  - w), & \\text{if } w &lt; a.\n\\end{cases}\\]\nFind the value of commitment \\(a\\) that maximizes the expected reward.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The newsvendor problem</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#notes",
    "href": "stochastic-optimization/newsvendor.html#notes",
    "title": "2  The newsvendor problem",
    "section": "Notes",
    "text": "Notes\nPerhaps the earliest model of the newsvendor problem appeared in Edgeworth (1888) in the context of a bank setting the level of cash reserves to cover demands from its customers. The solution to the basic model presented above and some of its variants was provided in Morse and Kimball (1951); Arrow et al. (1952); Whitin (1953). See Porteus (2008) for an accessible introduction.\nThe property \\(F_1(w) \\le F_2(w)\\) used in Exercise 2.2 is called stochastic dominance. Later in the course, we will study how stochastic dominance is useful to establish monotonicity properties of general MDPs.\nThe example of selling random wind in Exercise 2.3 is taken from Bitar et al. (2012).\n\n\n\n\n\n\n\nArrow, K.J., Harris, T., and Marschak, J. 1952. Optimal inventory policy. Econometrica 20, 1, 250–272. DOI: 10.2307/1907830.\n\n\nBitar, E., Poolla, K., Khargonekar, P., Rajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind. Hawaii international conference on system sciences, IEEE, 1931–1937.\n\n\nEdgeworth, F.Y. 1888. The mathematical theory of banking. Journal of the Royal Statistical Society 51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nMorse, P. and Kimball, G. 1951. Methods of operations research. Technology Press of MIT.\n\n\nPorteus, E.L. 2008. Building intuition: Insights from basic operations management models and principles. In: D. Chhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nWhitin, S. 1953. The theory of inventory management. Princeton University Press.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The newsvendor problem</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/certainty-equivalence.html",
    "href": "stochastic-optimization/certainty-equivalence.html",
    "title": "3  Certainty equivalence",
    "section": "",
    "text": "3.1 The quadratic case\nTo understand the intuition behind certainty equivalence, consider the special case when \\(c(a,w)\\) is quadratic, say \\(\\NORM{ a - w}_2^2\\).",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Certainty equivalence</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/certainty-equivalence.html#the-quadratic-case",
    "href": "stochastic-optimization/certainty-equivalence.html#the-quadratic-case",
    "title": "3  Certainty equivalence",
    "section": "",
    "text": "Proposition 3.1 For quadratic cost, \\[ J(a_0) = J^*. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe have that \\[\\begin{align*}\n\\EXP[ \\NORM{ a - W }_2^2 ]\n&=\n\\EXP[ \\NORM{ a - w_\\circ + w_\\circ - W }_2^2 ]\n\\\\\n&=\n\\NORM{ a - w_\\circ }_2^2 + \\EXP[ \\NORM{ w_\\circ - W }_2^2 ]\n+ 2 \\EXP[ (a - w_\\circ)^\\TRANS (w_\\circ - W) ]\n\\\\\n&=\n\\NORM{ a - w_\\circ }_2^2 + \\EXP[ \\NORM{ w_\\circ - W }_2^2 ]\n\\end{align*}\\] Thus, \\(J(a)\\) and \\(\\NORM{a - w_\\circ}_2^2\\) have the same minimizer.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Certainty equivalence</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/certainty-equivalence.html#bounds",
    "href": "stochastic-optimization/certainty-equivalence.html#bounds",
    "title": "3  Certainty equivalence",
    "section": "3.2 An approximation bound",
    "text": "3.2 An approximation bound\nNow we consider the case when \\(c(a,w) = \\NORM{a - w}\\) for some norm \\(\\NORM{\\cdot}\\).\nWe some state some basic properties of the quantities:\n\n(P1). By definition of \\(a_\\circ\\), we have \\[ \\NORM{ a_\\circ - w_\\circ } \\le \\NORM{a - w_\\circ},\n\\quad \\forall a \\in \\ALPHABET A.\n\\]\n(P2). By triangle inequaity, every norm is convex. Thus, by Jensen’s inequality, we have \\[\n\\NORM{ a - w_\\circ}\n=\n\\NORM{ \\EXP[ a - W] }\n\\le\n\\EXP[ \\NORM { a - W} ]\n=\nJ(a).\n\\]\n\n\nProposition 3.2 \\[ J^\\star \\le J(a_\\circ) \\le 3 J^\\star. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe first inequality is trivial since \\(J^\\star\\) is the optimal value. So, we will prove the second inequality.\nNote that \\[\\begin{align*}\n  \\NORM{ a_\\circ - w}\n  &\\stackrel{(a)}\\le \\NORM{ a_\\circ - w_\\circ }\n  + \\NORM{ w_\\circ - a } + \\NORM { a - w }\n  \\\\\n  &\\stackrel{(b)}\\le  2 \\NORM{ a - w_\\circ } + \\NORM{ a - w }\n  \\\\\n  &\\stackrel{(c)}\\le 2 J(a) + \\NORM{ a - w }\n\\end{align*}\\] where \\((a)\\) follows from triangle inequality, \\((b)\\) follows from (P1) and \\((c)\\) follows from (P2).\nTaking expectations of both sides, we get that \\[\nJ(a_\\circ) \\le 3 J(a).\n\\] Taking infimum over \\(a\\), we get the result.\n\n\n\nThe following two examples show that the bound is sharp.\n\nExample 3.1 Suppose the variables take value in \\(\\reals^2\\) and the norm is \\(\\ell_1\\) norm. Let \\(\\ALPHABET A = \\{ (x,y) : y - x = 1 \\}\\) and \\(W = (-1, 0)\\) with probability \\(1-ε\\) and \\(W = ( (1/ε) - 1, 0)\\) with probability \\(ε\\). We can verify that \\(J^* = 1\\).\nTake \\(w_\\circ = \\EXP[W] = (0,0)\\). Then \\(a_\\circ = (0,1)\\) and \\(J(a_\\circ) = 3 - 2 ε\\).\n\n\nExample 3.2 Suppose the variables take value in \\(\\reals\\). Let \\(\\ALPHABET A = \\{-1, +1 \\}\\). Let \\(W = -1\\) with probability \\(1 - ε\\) and \\(W = (1/ε) -1\\) with probability \\(ε\\). We can verify that \\(J^* = 1\\).\nTake \\(w_\\circ = \\EXP[W] = 0\\). Then \\(a_\\circ = +1\\) and \\(J(a_\\circ) = 3 - 4 ε\\).\n\nAlthough the examples above show that the upper bound is tight, we can derive tighter bounds under stronger assumptions. See Witsenhausen (1969) for details.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Certainty equivalence</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/certainty-equivalence.html#notes",
    "href": "stochastic-optimization/certainty-equivalence.html#notes",
    "title": "3  Certainty equivalence",
    "section": "Notes",
    "text": "Notes\nThe term certainty equivalence is due to Simon (1956). A similar result had earlier been shown by Theil (1954). See notes on LQR for more general instance of certainty equivalence for quadratic cost.\nThe material for the bounds in the general case is based on Witsenhausen (1969). For a significant generalization of these results, see Witsenhausen (1970). For a bound on certainty equivalent decision rules in multi-stage problems, see Bozkurt et al. (2023).\n\n\n\n\nBozkurt, B., Mahajan, A., Nayyar, A., and Ouyang, Y. 2023. Weighted norm bounds in MDPs with unbounded per-step cost.\n\n\nSimon, H.A. 1956. Dynamic programming under uncertainty with a quadratic criterion function. Econometrica 24, 1, 74–81. DOI: 10.2307/1905261.\n\n\nTheil, H. 1954. Econometric models and welfare maximization. Wirtschaftliches Archiv 72, 60–83. DOI: 10.1007/978-94-011-2410-2_1.\n\n\nWitsenhausen, H.S. 1969. Inequalities for the performance of suboptimal uncertain systems. Automatica 5, 4, 507–512. DOI: 10.1016/0005-1098(69)90112-5.\n\n\nWitsenhausen, H.S. 1970. On performance bounds for uncertain systems. SIAM Journal on Control 8, 1, 55–89. DOI: 10.1137/0308004.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Certainty equivalence</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/interchange.html",
    "href": "stochastic-optimization/interchange.html",
    "title": "4  Interchange arguments",
    "section": "",
    "text": "4.1 Optimal scheduling\nSuppose we are interested in solving a problem and there are \\(n\\) possible solution approaches that could be used. The only way to find if a solution approach works or not is to test it; testing solution approach \\(i\\) costs \\(c_i\\) resources and may succeed with probability \\(p_i\\). The probability of success of each alternative is independent of others. Find the search order that finds a working solution at minimum expected cost.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interchange arguments</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/interchange.html#optimal-scheduling",
    "href": "stochastic-optimization/interchange.html#optimal-scheduling",
    "title": "4  Interchange arguments",
    "section": "",
    "text": "Proposition 4.1 The optimal search order \\((i_1, \\dots, i_n)\\) is a permutation of \\((1,2,\\dots,n)\\) such that \\[\\begin{equation*}\n  \\frac{c_{i_1}}{p_{i_1}}\n  \\le\n  \\frac{c_{i_2}}{p_{i_2}}\n  \\le\n  \\cdots\n  \\le\n  \\frac{c_{i_n}}{p_{i_n}} .\n\\end{equation*}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove by contradiction. Consider a search order \\(O = (k_1, \\dots, k_n)\\) which does not satisfy the above property. We will show that the search order \\(O\\) cannot be optimal.\nLet \\(i\\) be the first position that does not satisfy the above order, i.e., \\[\nO = (k_1,k_2, \\dots, k_{\\ell},i,j,k_{\\ell+3}, \\dots, k_n),\n\\] is such that \\(c_i/p_i &gt; c_j/p_j\\). Now, consider a search order \\(\\tilde O\\) where we interchange \\(i\\) and \\(j\\), i.e., \\[\n\\tilde O = (k_1,k_2, \\dots, k_{\\ell},j,i,k_{\\ell+3}, \\dots, k_n).\n\\] The cost of each search order is: \\[\\begin{align*}\nJ(O) &= J(k_1, \\dots, k_{\\ell}) + P_{\\ell} c_i + P_{\\ell} (1-p_i)c_j + P_{\\ell}(1-p_i)(1-p_j)J(k_{\\ell+3}, \\dots, k_n), \\\\\nJ(\\tilde O) &= J(k_1, \\dots, k_{\\ell}) + P_{\\ell} c_j + P_{\\ell} (1-p_j)c_i + P_{\\ell}(1-p_j)(1-p_i)J(k_{\\ell+3}, \\dots, k_n),\n\\end{align*}\\] where \\(J(k_1, \\dots, k_{\\ell})\\) is the expected cost of testing approaches \\((k_1, \\dots, k_{\\ell})\\) and \\(P_{\\ell}\\) is the probability that the problem is not solved after testing \\((k_1, \\dots, k_{\\ell})\\) and \\(J(k_{\\ell+3}, \\dots, k_n)\\) is the expected cost of testing alternatives \\((k_{\\ell+3}, \\dots, k_n)\\). Note that \\[\\begin{align*}\nJ(O) \\le J(\\tilde O) &\\iff\nc_i + (1-p_i) c_j \\le c_j + (1-p_j) c_i,\n\\\\\n&\\iff \\frac {c_i}{p_i} \\le \\frac {c_j}{p_j}.\n\\end{align*}\\] Since we assumed that \\(c_i/p_i &gt; c_j/p_j\\), we have \\(J(O) &gt; J(\\tilde O)\\). Thus, search order \\(O\\) is not optimal.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interchange arguments</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/interchange.html#minimizing-cost-of-a-matching",
    "href": "stochastic-optimization/interchange.html#minimizing-cost-of-a-matching",
    "title": "4  Interchange arguments",
    "section": "4.2 Minimizing cost of a matching",
    "text": "4.2 Minimizing cost of a matching\nConsider a weighted bi-partite graph with two sets of \\(n\\) vertices: \\(\\ALPHABET U\\) and \\(\\ALPHABET V\\). There are weights \\((a_1, \\dots, a_n)\\) and \\((b_1, \\dots, b_n)\\) associated with vertices \\(\\ALPHABET U\\) and \\(\\ALPHABET V\\), respectively. For ease of notation, we assume that the nodes are indexed such that \\[\n  b_1 \\ge b_2 \\ge \\cdots \\ge b_n.\n\\] We want to choose a permutation \\((π_1, \\dots, π_n)\\) of \\((1,\\dots,n)\\) to minimize \\[\n  J(π) = \\sum_{i=1}^n a_{π_i} b_i.\n\\]\n\nProposition 4.2 The optimal permutation \\(π\\) that minimizes \\(J(π)\\) is such that \\[\n  a_{π_1} \\le a_{π_2} \\le \\cdots \\le a_{π_n}.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe result can be proved by an interchange argument via contradiction. Suppose \\(π\\) is an optimal permutation but does not satisfy the prescribed order. Let \\(i\\) be the first index that does not satisfy the above order, i.e., \\[\n  π = (π_1, \\dots, π_{\\ell}, i, j, π_{\\ell+3}, \\dots, π_{n})\n\\] is such that \\(a_i &gt; a_j\\). Now, consider a permutation \\(\\tilde π\\) where we interchange \\(i\\) and \\(j\\), i.e., \\[\n  \\tilde π = (π_1, \\dots, π_{\\ell}, j, i, π_{\\ell+3}, \\dots, π_{n}).\n\\] The cost of the each permutation is: \\[\\begin{align*}\n  J(π) &= \\sum_{k \\not\\in \\{i, j\\}} a_{π_k} b_k + a_i b_i + a_j b_j ,\\\\\n  J(\\tilde π) &= \\sum_{k \\not\\in \\{i, j\\}} a_{π_k} b_k + a_j b_i + a_i b_j.\n\\end{align*}\\] Thus, \\[\\begin{align*}\n  J(π) \\le J(\\tilde π) &\\iff a_i b_i + a_j b_j \\le a_i b_j + a_j b_i \\\\\n  &\\iff a_i(b_i - b_j) \\le a_j(b_i - b_j) \\\\\n  &\\iff a_i \\le a_j\n\\end{align*}\\] where we have used the fact that \\(\\{b_i\\}\\) is non-increasing in the last inequality. Since we assumed that \\(a_i &gt; a_j\\), we have that \\(J(π) &gt; J(\\tilde π)\\); therefore, the permutation \\(π\\) is not optimal.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interchange arguments</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/interchange.html#exercises",
    "href": "stochastic-optimization/interchange.html#exercises",
    "title": "4  Interchange arguments",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 4.1 There are \\(n\\) jobs to be processed by a single machine. Processing job \\(i\\) takes a random time \\(S_i\\). Let \\(μ_i = \\EXP[S_i] &lt; ∞\\). A cost \\(c_i\\) is incurred per unit time until job \\(i\\) is completed.\nFor example, suppose there are two jobs, and completing job \\(1\\) and \\(2\\) took \\(s_i\\) and \\(s_2\\) time units respectively. If the processing order was \\(\\{1,2\\}\\), then the cost incurred is \\(c_1 s_1 + c_2 (s_1 + s_2)\\). If the processing order was \\(\\{2,1\\}\\), then the cost incurred is \\(c_2 s_2 + c_1 (s_1 + s_2)\\).\nIn what order should the jobs be processed so as to minimize the total expected cost?\n\n\nExercise 4.2 A prisoner wishes to escape from a prison. There are \\(n\\) passages that leads out of the prison, some of them are dead-ends, others have guards, and remaining are unguarded are lead outside the prison. More precisely, passage \\(i\\) might be a dead-end with probability \\((1-p_i-q_i)\\), might have a guard with probability \\(q_i\\), and might be unguarded and lead outside the prison with probability \\(p_i\\). Thus, if the prisoner tries passage \\(i\\), he will find that the passage is a dead-end with probability \\((1-p_i-q_i)\\) and he can come back and try other passages; he will be captured by a guard with probability \\(q_i\\), and he will escape with probability \\(p_i\\). In what order should the prison test different passages if he wishes to maximize his probability of eventual escape? Does the optimal search order change if his objective is to minimize the probability of capture?",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interchange arguments</span>"
    ]
  },
  {
    "objectID": "stochastic-optimization/interchange.html#notes",
    "href": "stochastic-optimization/interchange.html#notes",
    "title": "4  Interchange arguments",
    "section": "Notes",
    "text": "Notes\nInterchange arguments are commonly used in queueing networks to establish the structure of optimal policies. See Chapter 8 of Walrand (1988) and Nain et al. (1989) for discussion and overview. They are also commonly used in scheduling problems. See Strusevich and Rustogi (2016) for an overview.\nThe result of Proposition 4.2 is often called an :rearrangement inequality and were first established in Hardy et al. (1952). For a generalization of such results, see Marshall et al. (2011).\nExercise 4.1 is an instance of the celebrated \\(c μ\\)-rule in queueing networks. See Baras et al. (1984) for a special case and Buyukkoc et al. (1985) for the general setup.\n\n\n\n\nBaras, J.S., Dorsey, A.J., and Makowski, A.M. 1984. Two competing queues with linear costs: The μc-rule is often optimal. Advances in Applied Probability 16, 1, 8–8. DOI: 10.1017/s000186780002187x.\n\n\nBuyukkoc, C., Varaiya, P., and Walrand, J. 1985. The cμ rule revisited. Advances in Applied Probability 17, 1, 237–238. DOI: 10.2307/1427064.\n\n\nHardy, G.H., Littlewood, J.E., and Pólya, G. 1952. Inequalities. Cambridge University Press.\n\n\nMarshall, A.W., Olkin, I., and Arnold, B.C. 2011. Inequalities: Theory of majorization and its applications. Springer New York. DOI: 10.1007/978-0-387-68276-1.\n\n\nNain, P., Tsoucas, P., and Walrand, J. 1989. Interchange arguments in stochastic scheduling. Journal of Applied Probability 26, 4, 815–826. DOI: 10.2307/3214386.\n\n\nStrusevich, V.A. and Rustogi, K. 2016. Pairwise interchange argument and priority rules. In: Scheduling with time-changing effects and rate-modifying activities. Springer International Publishing, 19–36. DOI: 10.1007/978-3-319-39574-6_2.\n\n\nWalrand, J. 1988. An introduction to queueing networks. Prentice Hall.",
    "crumbs": [
      "Stochastic Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interchange arguments</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html",
    "href": "mdps/intro.html",
    "title": "5  Finite horizon MDPs",
    "section": "",
    "text": "5.1 An example\nTo fix ideas, we start with an example.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html#an-example",
    "href": "mdps/intro.html#an-example",
    "title": "5  Finite horizon MDPs",
    "section": "",
    "text": "Example 5.1 Consider a controlled Markov chain defined over \\(\\ALPHABET S = \\{-2, -1, 0, 1, 2\\}\\) with two control actions, i.e., \\(\\ALPHABET A = \\{0, 1\\}\\). If action \\(A = 0\\) is chosen, the chain evolves according to its “natural” dynamics, which are shown below:\n\n\n\n\n\n\nFigure 5.1: “Natural” dynamics of the Markov chain\n\n\n\nWhen action \\(A = 1\\) is chosen, the chain evolves according to the “forced” dynamics, which are shown below:\n\n\n\n\n\n\nFigure 5.2: “Forced” dynamics of the Markov chain\n\n\n\nNote that under the natural dynamics, the Markov chain will settle to a uniform steady-state distribution; under the forced dynamics, the Markov chain settle to a distribution which is unimodal with a peak at state \\(0\\).\nSuppose it is desirable to keep the Markov chain close to state \\(0\\). We capture this by the system incurs a running cost equal to \\(s^2\\) in state \\(s\\). In addition, choosing action \\(A=0\\) is free but choosing action \\(A=1\\) has a correction cost \\(p\\).\nThus, the decision maker is in a conundrum. It may allow the system to follow its natural dynamics, which causes the system to reach states with larger absolute value and incur a running cost \\(s^2\\); or it may decide to force the system to drift towards state \\(0\\), which reduces the running cost but, in turn, incurs a correction cost \\(p\\). How should the decision maker choose its actions?",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html#basic-model-and-structure-of-optimal-policies",
    "href": "mdps/intro.html#basic-model-and-structure-of-optimal-policies",
    "title": "5  Finite horizon MDPs",
    "section": "5.2 Basic model and structure of optimal policies",
    "text": "5.2 Basic model and structure of optimal policies\nThe type of model described above is called Markov decision processes (MDP), and they are the simplest model of a stochastic control system. There are two ways to model MDPs, which we describe below.\nThe first method to model MDPs is to think of them as a controlled Markov process; in particular, an MDP is a stochastic process \\(\\{S_t\\}_{t \\ge 1}\\), \\(S_t \\in \\ALPHABET S\\), controlled by the process \\(\\{A_t\\}_{t \\ge 1}\\), \\(A_t \\in \\ALPHABET A\\), which satisfies the controlled Markov property: \\[\n  \\PR(S_{t+1} = s_{t+1} \\mid S_{1:t} = s_{1:t}, A_{1:t} = a_{1:t})\n  =\n  \\PR(S_{t+1} = s_{t+1} \\mid S_t = s_t, A_t = a_t).\n\\] For models with finite state and action spaces, the right-hand side of the above may be viewed as an element of the controlled transition matrix \\(P_t(a_t)\\). For instnace, in the example described at the beginning of this section, we have \\[\n\\def\\1{\\tfrac 12}\nP(0) = \\MATRIX{ \\1& \\1&  0&  0&  0\\\\\n                \\1& 0 & \\1&  0&  0\\\\\n                0&  \\1&  0&  \\1& 0\\\\\n                0&  0&  \\1&   0& \\1 \\\\\n                0&  0&   0&  \\1& \\1}\n\\quad\\text{and}\\quad\n\\def\\1{\\tfrac 14}\n\\def\\2{\\tfrac 34}\n\\def\\3{\\tfrac 12}\nP(1) = \\MATRIX{ \\1& \\2&  0&  0&  0\\\\\n                \\1& 0 & \\2&  0&  0\\\\\n                0&  \\1& \\3&  \\1& 0\\\\\n                0&  0&  \\2&   0& \\1 \\\\\n                0&  0&   0&  \\2& \\1}\n\\] Since the model is time-homogeneous, we have replaced \\(P_t(a)\\) by just \\(P(a)\\).\nThis representation is compact and convenient for computational purposes, but I personally feel that it can be a bit opaque for proving the fundamental results of MDP theory. For that reason, I prefer to start with the second representation, which is desibed below.\nIn the second representation, the dynamic behavior of an MDP is modeled by an equation \\[ \\begin{equation}\n  S_{t+1} = f_t(S_t, A_t, W_t) \\label{eq:state}\n\\end{equation}\\] where \\(S_t \\in \\ALPHABET S\\) is the state, \\(A_t \\in \\ALPHABET A\\) is the control input, and \\(W_t \\in \\ALPHABET W\\) is the noise. An agent/controller observes the state and chooses the control input \\(A_t\\).\nWe call this the functional representation of the MDP. Eq. \\eqref{eq:state} is a non-linear stochastic state-space model—non-linear because \\(f_t\\) can be any nonlinear function; stochastic because the system is driven by stochastic noise \\(\\{W_t\\}_{t \\ge 1}\\).\nNote that the controlled Markov chain representation can be easily translated to a functional representation by taking \\(W_t\\) to be a uniform \\([0,1]\\) random variable and using a differnet :Smirnov transformation for each state action pair \\((S_t, A_t)\\).\nAt each time, the system incurs a cost that may depend on the current state and control action. This cost is denoted by \\(c_t(S_t, A_t)\\). The system operates for a time horizon \\(T\\). During this time, it incurs a total cost \\[ \\sum_{t=1}^T c_t(S_t, A_t). \\]\nThe initial state \\(S_1\\) and the noise process \\(\\{W_t\\}_{t \\ge 1}\\) are random variables defined on a common probability space (these are called primitive random variables) and are mutually independent. This seemingly benign assumption is critical for the theory that we present to go through.\nSuppose we have to design such a controller. We are told the probability distribution of the initial state and the noise. We are also told the system update functions \\((f_1, \\dots, f_T)\\) and the cost functions \\((c_1, \\dots, c_T)\\).\nOur objective to determine a control policy, i.e., a function for choosing the control actions. The control policy can be as sophisticated as we want. In principle, it can analyze the entire history of observations and control actions to choose the current control action. Thus, the control action can be written as \\[ A_t = π_t(S_{1:t}, A_{1:t-1}),\\] where \\(S_{1:t}\\) is a shorthand for \\((S_1, \\dots, S_t)\\) and a similar interpretation holds for \\(A_{1:t-1})\\). The function \\(π_t\\) is called the control law at time \\(t\\).\nWe want to choose a control policy \\(π = (π_1, \\dots, π_T)\\) to minimize the expected total cost \\[ J(π) := \\EXP\\bigg[ \\sum_{t=1}^T c_t(S_t, A_t) \\bigg]. \\] How should we proceed?\nAt first glance, the problem looks intimidating. It appears that we have to design a very sophisticated controller: one that analyzes all past data to choose a control input. However, this is not the case. A remarkable result is that the optimal controller can discard all past data and choose the control input based only on the current state of the system. Formally, we have the following:\n\nTheorem 5.1 (Optimality of Markov policies) For the system model described above, there is no loss of optimality in choosing the control action according to \\[ A_t = π_t(S_t), \\quad t=1, \\dots, T.\\] Such a control policy is called a Markov policy.\n\nIn the context of the example presented at the beginning of this section, Theorem 5.1 says that the decision maker can decide whether to choose action \\(0\\) or \\(1\\) based on the current state, without any loss of optimality.\nIn general, Theorem 5.1 claims that the cost incurred by the best Markov policy is the same as the cost incurred by the best history dependent policy. This appears to be a tall claim, so lets see how we can prove it. The main idea of the proof is to repeatedly apply Blackwell’s principle of irrelevant information (Blackwell 1964)\n\nLemma 5.1 (Two-Step Lemma) Consider an MDP that operates for two steps (\\(T=2\\)). Then there is no loss of optimality in restricting attention to a Markov control policy at time \\(t=2\\).\n\nNote that \\(π_1\\) is Markov because it can only depend \\(S_1\\).\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix \\(π_1\\) and look at the problem of optimizing \\(π_2\\). The total cost is \\[ \\EXP[ c_1(S_1, π_1(S_1)) + c_2(S_2, π_2(S_{1:2}, A_1)) ]\\] The choice of \\(π_2\\) does not influence the first term. So, for a fixed \\(π_1\\), minimizing the total cost is the equivalent to minimizing the second term. Now, from Blackwell’s principle of irrelevant information, there exists a \\(π_2^* \\colon S_2 \\mapsto A_2\\) such that for any \\(π_2\\) \\[\\EXP[c_2(S_2, π_2^*(S_2) ] \\le \\EXP[c_2(S_2, π_2(S_{1:2}, A_2) ].\\]\n\n\n\n\nLemma 5.2 (Three-Step Lemma) Consider an MDP that operates for three steps (\\(T=3\\)). Assume that the control law \\(π_3\\) at time \\(t=3\\) is Markov, i.e., \\(A_3 = π_3(S_3)\\). Then, there is no loss of optimality in restricting attention to Markov control law at time \\(t=2\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix \\(π_1\\) and \\(π_3\\) and look at optimizing \\(π_2\\). The total cost is \\[ \\EXP[ c_1(S_1, π_1(S_1)) + c_2(S_2, π_2(S_{1:2}, A_1)) + c_3(S_3, π_3(S_3)].\\]\nThe choice of \\(π_2\\) does not affect the first term. So, for a fixed \\(π_1\\) and \\(π_3\\), minimizing the total cost is the same as minimizing the last two terms. Let us look at the last term carefully. Bu the law of iterated expectations, we have \\[ \\EXP[ c_3(S_3, π_3(S_3) ] = \\EXP[ \\EXP[ c_3(S_3, π_3(S_3)) | S_2, A_2 ] ]. \\] Now, \\[\\begin{align*}\n  \\EXP[ c_3(S_3, π_3(S_3)) | S_2 = s_2, A_2 = a_2 ] &=\n  \\sum_{s_3 \\in \\ALPHABET S} c_3(s_3, π_3(s_3)) \\\\\n  &= \\PR( w_2 \\in \\ALPHABET W : f_2(s_2, a_2, w_2) = s_3 )\n  \\\\\n  &=: h_2(s_2, a_2).\n\\end{align*}\\] The key point is that \\(h_2(s_2, a_2)\\) does not depend on \\(π_1\\) or \\(π_2\\).\nThus, the total expected cost affected by the choice of \\(π_2\\) can be written as \\[\\begin{align*}\n  \\EXP[ c_2(S_2, A_2) + c_3(S_3, A_3) ] &= \\EXP[ c_2(S_2, A_2) + h_2(S_2, A_2)\n  ] \\\\\n  &=: \\EXP[ \\tilde c_2(S_2, A_2) ].\n\\end{align*}\\] Now, by Blackwell’s principle of irrelevant information, there exists a \\(π_2^* : S_2 \\mapsto A_2\\) such that for any \\(π_2\\), we have \\[ \\EXP[ \\tilde c_2(S_2, π_2^*(S_2))] \\le  \\EXP[ \\tilde c_2(S_2, π_2(S_{1:2},\nA_1) ].\\]\n\n\n\nNow we have enough background to present the proof of optimality of Markov policies.\n\n\n\n\n\n\nProof of Theorem 5.1\n\n\n\n\n\nThe main idea is that any system can be thought of as a two- or three-step system by aggregating time. Suppose that the system operates for \\(T\\) steps. It can be thought of as a two-step system where \\(t \\in \\{1, \\dots, T - 1\\}\\) corresponds to step 1 and \\(t = T\\) corresponds to step 2. From the two-step lemma, there is no loss of optimality in restricting attention to Markov control law at step 2 (i.e., at time \\(t=T\\)), i.e., \\[ A_T = π_T(S_T). \\]\nNow consider a system where we are using a Markov policy at time \\(t=T\\). This system can be thought of as a three-step system where \\(t \\in \\{1, \\dots,\nT-2\\}\\) corresponds to step 1, \\(t = T-1\\) corresponds to step 2, and \\(t=T\\) corresponds to step 3. Since the controller at time \\(T\\) is Markov, the assumption of the three step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at step 2 (i.e., at time \\(t=T-1\\)), i.e., \\[A_{T-1} = π_{T-1}(S_{T-1}).\\]\nNow consider a system where we are using a Markov policy at time \\(t \\in\n\\{T-1, T\\}\\). This can be thought of as a three-step system where \\(t \\in \\{1,\n\\dots, T - 3\\}\\) correspond to step 1, \\(t = T-2\\) correspond to step 2, and \\(t\n\\in \\{T-1, T\\}\\) correspond to step 3. Since the controllers at time \\(t \\in\n\\{T-1, T\\}\\) are Markov, the assumption of the three-step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at step 2 (i.e., at time \\(t=T-2\\)), i.e., \\[A_{T-2} = π_{T-2}(S_{T-2}).\\]\nProceeding this way, we continue to think of the system as a three step system by different relabeling of time. Once we have shown that the controllers at times \\(t \\in \\{s+1, s+2, \\dots, T\\}\\) are Markov, we relabel time as follows: \\(t=\\{1, \\dots, s-1\\}\\) corresponds to step 1, \\(t = s\\) corresponds to step 2, and \\(t \\in \\{s+1, \\dots, T\\}\\) corresponds to step 3. Since the controllers at time \\(t \\in \\{s+1, \\dots, T\\}\\) are Markov, the assumption of the three-step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at stage 2 (i.e. at time \\(s\\)), i.e., \\[A_τ = π_τ(S_τ).\\]\nProceeding until \\(s=2\\), completes the proof.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html#performance",
    "href": "mdps/intro.html#performance",
    "title": "5  Finite horizon MDPs",
    "section": "5.3 Performance of Markov policies",
    "text": "5.3 Performance of Markov policies\nWe have shown that there is no loss of optimality to restrict attention to Markov policies. One of the advantages of Markov policies is that their performance can be computed recursively. In particular, given any Markov policy \\(π = (π_1, \\dots, π_T)\\), define the cost-to-go functions or value function as follows: \\[V^{π}_t(s) = \\EXP^π \\bigg[ \\sum_{τ = t}^{T} c_τ(S_τ, π_τ(S_τ)) \\biggm| S_t =\ns\\bigg]. \\] Note that \\(V^{π}_t(s)\\) only depends on the future policy \\((π_t, \\dots, π_T)\\). These functions can be computed recursively as follows: we start with a terminal value function \\(V^{π}_{T+1}(s) ≡ 0\\) and then for \\(t \\in \\{T, T-1, \\dots, 1\\}\\), recursively compute: \\[\\begin{align}\n  V^{π}_t(s) &= \\EXP^π \\bigg[ \\sum_{τ = t}^{T} c_τ(S_τ, π_τ(S_τ)) \\biggm| S_t =\n  s \\bigg] \\notag \\\\\n  &= \\EXP^π \\bigg[ c_t(s, π_t(s)) + \\EXP^π \\bigg[ \\sum_{τ = t+1}^T\n    c_τ(S_τ, π_τ(S_τ)) \\biggm| S_{t+1} \\bigg] \\biggm| S_t = s \\bigg]\n  \\notag \\\\\n  &= \\EXP^π\\big[ c_t(s, π_t(s)) + V^{π}_{t+1}(S_{t+1}; π) \\big| S_t = s \\big]. \\label{eq:finite-policy-evaluation}\n\\end{align}\\]\nThe formula of Eq. \\(\\eqref{eq:finite-policy-evaluation}\\) is called the policy evaluation formula.\nFor the controlled Markov chain representation, the formula can be written in a vector form. In particular, we will think of \\(V^π_t\\) to be a vector in \\(\\reals^n\\), where \\(n = \\ABS{\\ALPHABET S}\\). For any policy \\(π = (π_1, \\dots, π_T)\\), define the \\(n × n\\) transition matrices \\((P^{π}_1, \\dots, P^{π}_T)\\) as \\[\n  P^{π}_t(s'|s) = P_t(s'|s, π_t(s)),\n  \\quad t \\in \\{1,\\dots, T\\}.\n\\] Furthermore, define \\(n\\) dimensional cost vectors \\((c^{π}_1, \\dots, c^{π}_T)\\) as \\[\n  c^π_t(s) = c_t(s, π_t(s)),\n  \\quad t \\in \\{1,\\dots, T\\}.\n\\] Then, the policy evaluation formula \\(\\eqref{eq:finite-policy-evaluation}\\) is equivalent to the following: start with a terminal value function \\(V^π_{T+1} ≡ 0\\) and then for \\(t \\in \\{T, T-1,\\dots, 1\\}\\), recursively compute: \\[\n  V^{π}_t = c^{π}_t + P^{π}_t V^{π}_{t+1}.\n\\]\n\n5.3.1 Example 5.1 (continued)\nAs an example, consider the following time-homogeneous policy Example 5.1: \\[\n  π_t(s) = \\begin{cases}\n    1 & \\text{if } |s| = 2 \\\\\n    0 & \\text{otherwise}\n  \\end{cases}\n\\] Suppose \\(p = 1\\), i.e., \\(c(s,a) = s^2 + a\\). We now compute the value function \\(\\{V^{π}_t\\}_{t=1}^T\\) for \\(T = 5\\).\nFor this policy \\[\n  c^{π} = \\MATRIX{ 5 \\\\ 1 \\\\ 0 \\\\ 1 \\\\  5 }\n  \\quad\\text{and}\\quad\n  \\def\\1{\\tfrac 14}\n  \\def\\2{\\tfrac 34}\n  \\def\\3{\\tfrac 12}\n  P^π = \\MATRIX{ \\1& \\2&  0&  0&  0\\\\\n                \\3& 0 & \\3&  0&  0\\\\\n                0&  \\3&  0&  \\3& 0\\\\\n                0&  0&  \\3&   0& \\3 \\\\\n                0&  0&   0&  \\2& \\1}\n\\] We start with \\(V^π_{t+1} = \\VEC(0,0,0,0,0)\\) and run the following recursion.\n\ncπ = [5, 1, 0, 1, 5] \nPπ = [1//4  3//4  0     0     0 \n      1//2  0     1//2  0     0\n      0     1//2  0     1//2  0\n      0     0     1//2  0     1//2\n      0     0     0     3//4  1//4]\n\nT = 5\nn = size(cπ,1)\n\nVπ = [ zeros(n) for t ∈ 1:T+1 ]\n\nfor t ∈ T:-1:1\n  Vπ[t] = cπ + Pπ*Vπ[t+1]\nend\n\ndisplay(Vπ)\n\n6-element Vector{Vector{Float64}}:\n [13.3515625, 9.046875, 7.4375, 9.046875, 13.3515625]\n [11.09375, 7.4375, 5.0, 7.4375, 11.09375]\n [9.375, 5.0, 3.5, 5.0, 9.375]\n [7.0, 3.5, 1.0, 3.5, 7.0]\n [5.0, 1.0, 0.0, 1.0, 5.0]\n [0.0, 0.0, 0.0, 0.0, 0.0]\n\n\nFigure 5.3 shows the value functions computed as part of the policy evaluation.\n\n\n\n\n\n\nFigure 5.3: Value function for policy evaluation of Example 5.1. Note that the value functions are computed by proceeding backwards in time.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html#DP",
    "href": "mdps/intro.html#DP",
    "title": "5  Finite horizon MDPs",
    "section": "5.4 Dynamic Programming Decomposition",
    "text": "5.4 Dynamic Programming Decomposition\nNow we are ready to state the main result for MDPs.\n\nTheorem 5.2 (Dynamic program) Recursive define value functions \\(\\{V^*_t\\}_{t = 1}^{T+1} \\colon \\ALPHABET S\n\\to \\reals\\) as follows: \\[ \\begin{equation} \\label{eq:DP-1}\n  V^*_{T+1}(s) = 0\n\\end{equation} \\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n   Q^*_t(s,a) &= c(s,a) + \\EXP[ V^*_{t+1}(S_{t+1}) | S_t = s, A_t = a]\n   \\nonumber \\\\\n   &= c(s,a) + \\EXP[ V^*_{t+1}(f_t(s,a,W_t)) ], \\label{eq:DP-2}\n\\end{align}\\] and define \\[ \\begin{equation} \\label{eq:DP-3}\n  V^*_t(s) = \\min_{a \\in \\ALPHABET A} Q^*_t(s,a).\n\\end{equation} \\] Then, a Markov policy is optimal if and only if it satisfies \\[ \\begin{equation} \\label{eq:verification}\n  π_t^*(s) = \\arg \\min_{a \\in \\ALPHABET A} Q_t(s,a).\n\\end{equation} \\]\n\nFor the controlled Markov chain representation, the dynamic program of Theorem 5.2 can be written more succinctly. We will view \\(\\{V^*_t\\}_{t=1}^T\\) as vectors in \\(\\reals^n\\), where \\(n = \\ABS{\\ALPHABET S}\\). Moreover, we will view \\(\\{Q^*_t\\}_{t=1}^T\\) and \\(\\{c_t\\}_{t=1}^T\\) as matrices in \\(\\reals^{n × m}\\), where \\(m = \\ABS{\\ALPHABET A}\\).\nThen, the dynamic program of Theorem 5.2 may be written as follows: start with a terminal value function \\(V^*_{T+1} ≡ 0\\) and then for \\(t \\in \\{T, T-1, \\dots, 1\\}\\), recursively compute: \\[\\begin{align*}\n  Q^*_t &= c_t + [ P_t(1) V^*_{t+1} \\mid P_t(2) V^*_{t+1} \\mid \\cdots \\mid P_t(m) V^*_{t+1} ], \\\\\n  V^*_t &= \\min(Q^*_t, \\hbox{\\tt dim}=2), \\\\\n  π^*_t &= \\arg\\min(Q^*_t, \\hbox{\\tt dim}=2).\n\\end{align*}\\]\n\n5.4.1 Example 5.1 (continued)\nNow we present the dynamic programming solution for Example 5.1:\n\n# We use the default indexing (1,...,n) for convenience. \n\n(n,m) = (5,2)\nS = 1:n\nA = 1:m\nT = 5\n\nP = [ zeros(n,n) for a ∈ A ]\nP[1] = [1//2  1//2  0     0     0 \n        1//2  0     1//2  0     0\n        0     1//2  0     1//2  0\n        0     0     1//2  0     1//2\n        0     0     0     1//2  1//2]\n\nP[2] = [1//4  3//4  0     0     0 \n        1//4  0     3//4  0     0\n        0     1//4  1//2  1//4  0\n        0     0     3//4  0     1//4\n        0     0     0     3//4  1//4]\n\nc = zeros(n,m)\nfor s ∈ S, a ∈ A\n  # use s-3 and a-1 to convert to \"natural\" indices -2:2 and 0:1\n  c[s,a] = (s-3)^2 + (a-1)\nend\n\nV = [ zeros(n)     for t ∈ 1:T+1 ]\nπ = [ zeros(Int,n) for t ∈ 1:T   ]\n\nfor t ∈ T:-1:1\n  Q = c + hcat(P[1]*V[t+1], P[2]*V[t+1])\n\n  # Could be done more efficiently with a single pass\n  V[t] = vec(minimum(Q, dims=2))\n  # Subtract 1 to get to \"natural\" indices 0:1\n  π[t] = argmin.(eachrow(Q)) .- 1\nend\n\ndisplay(V)\ndisplay(π)\n\n6-element Vector{Vector{Float64}}:\n [12.4453125, 7.8984375, 6.40625, 7.8984375, 12.4453125]\n [10.46875, 6.4375, 4.375, 6.4375, 10.46875]\n [8.75, 4.375, 3.0, 4.375, 8.75]\n [6.5, 3.0, 1.0, 3.0, 6.5]\n [4.0, 1.0, 0.0, 1.0, 4.0]\n [0.0, 0.0, 0.0, 0.0, 0.0]\n\n\n5-element Vector{Vector{Int64}}:\n [1, 1, 1, 1, 1]\n [1, 1, 0, 1, 1]\n [0, 1, 0, 1, 0]\n [0, 0, 0, 0, 0]\n [0, 0, 0, 0, 0]\n\n\nFigure 5.4 shows the value functions and optimal policy computed from the dynamic program\n\n\n\n\n\n\nFigure 5.4: Optimal value function and optimal policy for Example 5.1. The red color indicates that the optimal action is \\(1\\); black indicates that the optimal action is \\(0\\). Note that the computations are done by proceeding backwards in time.\n\n\n\n\n\n5.4.2 Back to the proof\nInstead of proving Theorem 5.2, we prove a related result.\n\nTheorem 5.3 (The comparison principle) For any Markov policy \\(π\\) \\[ V^{π}_t(s) \\ge V_t(s) \\] with equality at \\(t\\) if and only if the future policy \\(π_{t:T}\\) satisfies the verification step \\eqref{eq:verification}.\n\nNote that the comparison principle immediately implies that the policy obtained using dynamic programming is optimal.\nThe comparison principle also allows us to interpret the value functions. The value function at time \\(t\\) is the minimum of all the cost-to-go functions over all future policies. The comparison principle also allows us to interpret the optimal policy (the interpretation is due to Bellman and is colloquially called Bellman’s principle of optimality).\n\n\n\n\n\n\nBellman’s principle of optimality.\n\n\n\nAn optimal policy has the property that whatever the initial state and the initial decisions are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n\n\n\n\n\n\n\n\nProof of the comparison principle\n\n\n\n\n\nThe proof proceeds by backward induction. Consider any Markov policy \\(π =\n(π_1, \\dots, π_T)\\). For \\(t = T\\), \\[ \\begin{align*}\n  V_T(s) &= \\min_{a \\in \\ALPHABET A} Q_T(s,a) \\\\\n  &\\stackrel{(a)}= \\min_{a \\in \\ALPHABET A} c_T(s,a) \\\\\n  &\\stackrel{(b)}\\le c_T(s, π_T(s)) \\\\\n  &\\stackrel{(c)}= V^{π}_T(s),\n\\end{align*} \\] where \\((a)\\) follows from the definition of \\(Q_T\\), \\((b)\\) follows from the definition of minimization, and \\((c)\\) follows from the definition of \\(J_T\\). Equality holds in \\((b)\\) iff the policy \\(π_T\\) is optimal. This result forms the basis of induction.\nNow assume that the statement of the theorem is true for \\(t+1\\). Then, for \\(t\\) \\[ \\begin{align*}\n  V_t(s) &= \\min_{a \\in \\ALPHABET A} Q_t(s,a) \\\\\n  &\\stackrel{(a)}= \\min_{a \\in \\ALPHABET A} \\Big\\{\n  c_t(s,a) + \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a]\n  \\Big\\}\n  \\\\\n  &\\stackrel{(b)}\\le  \\Big\\{\n  c_t(s,π_t(s)) + \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = π_t(s)]\n  \\Big\\} \\\\\n  &\\stackrel{(c)}\\le  \\Big\\{\n  c_t(s,π_t(s)) + \\EXP[ J_{t+1}(S_{t+1}; π) | S_t = s, A_t = π_t(s)]\n  \\Big\\} \\\\\n  &\\stackrel{(d)}= V^{π}_t(s),\n\\end{align*} \\] where \\((a)\\) follows from the definition of \\(Q_t\\), \\((b)\\) follows from the definition of minimization, \\((c)\\) follows from the induction hypothesis, and \\((d)\\) follows from the definition of \\(J_t\\). We have equality in step \\((b)\\) iff \\(π_t\\) satisfies the verification step \\eqref{eq:verification} and have equality in step \\((c)\\) iff \\(π_{t+1:T}\\) is optimal (this is part of the induction hypothesis). Thus, the result is true for time \\(t\\) and, by the principle of induction, is true for all time.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html#more-examples",
    "href": "mdps/intro.html#more-examples",
    "title": "5  Finite horizon MDPs",
    "section": "5.5 More examples",
    "text": "5.5 More examples\n\n5.5.1 Machine replacement\n\nExample 5.2 (Machine replacement) Consider a manufacturing process, where the machine used for manufacturing deteriorates over time. Let \\(\\ALPHABET S = \\{0, 1, \\dots n \\}\\) represent the condition of the machine. The higher the value of \\(s\\), the worse the condition of the equipment.\nA decision maker observes the state of the machine and has two options: continue operating the machine or replace it with a a new and identical piece of equipment. Operating the machine is state \\(s\\) costs \\(h(s)\\), where \\(h(⋅)\\) is a weakly increasing function; replace the machine costs a constant amount \\(K\\).\nWhen the machine is operated, it’s state deteriorates according to \\[\n  S_{t+1} = \\min( S_t + W_t , n)\n\\] where \\(\\{W_t\\}_{t \\ge 1}\\) is an i.i.d. process with PMF \\(μ\\).\n\nThe above system may be modelled as an MDP with state space \\(\\ALPHABET S\\) and action space \\(\\ALPHABET A = \\{0, 1\\}\\) where \\(0\\) means operating the machine and \\(1\\) means replacing the machine.\nFor instance, consider \\(n = 5\\) and \\(W \\sim \\text{Bernoulli}(p)\\). The evolution of the Markov chain under action \\(A_t = 0\\) and \\(A_t = 1\\) are shown in Figure 8.1.\n\n\n\n\n\n\n\n\nDynamics under action \\(A_t = 0\\)\n\n\n\n\n\n\n\nDynamics under action \\(A_t = 1\\)\n\n\n\n\n\n\nFigure 5.5: State dynamics for the MDP of Example 5.2\n\n\n\nThus, \\[\n  P(0) = \\MATRIX{q& p& 0& 0& 0& 0\\\\\n                 0& q& p& 0& 0& 0\\\\\n                 0& 0& q& p& 0& 0\\\\\n                 0& 0& 0& q& p& 0\\\\\n                 0& 0& 0& 0& q& p\\\\\n                 0& 0& 0& 0& 0& 1}\n  \\quad\\hbox{and}\\quad\n  P(1) = \\MATRIX{1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0}\n\\] where we have set \\(q = 1-p\\) for notational convenience.\nThe per-step cost is given by \\[\n  c(s,a) = h(s) + λa.\n\\]\nSolving the DP for \\(p=0.2\\), \\(h(s) = 2s\\), \\(λ=10\\), and \\(T=5\\) gives the following:\n\n\nShow code\nusing SparseArrays\n\n# We use the default indexing (1,...,n) for convenience. \n\n(n,m) = (6,2)\nS = 1:n\nA = 1:m\nT = 5\nλ = 10\n\np = 0.2\nq = 1-p\n\nP = [ spzeros(n,n) for a ∈ A ]\nfor i in S\n  P[1][i,i] = q\n  P[1][i,min(i+1,n)] += p\n\n  P[2][i,1] = 1\nend\n\nc = zeros(n,m)\nfor s ∈ S, a ∈ A\n# use s-1 and a-1 to convert to \"natural\" indices 0:5 and 0:1\n  c[s,a] = 2(s-1) + λ*(a-1)\nend\n\nV = [ zeros(n)     for t ∈ 1:T+1 ]\nπ = [ zeros(Int,n) for t ∈ 1:T   ]\n\nfor t ∈ T:-1:1\n  Q = c + hcat(P[1]*V[t+1], P[2]*V[t+1])\n\n  # Could be done more efficiently with a single pass\n  V[t] = vec(minimum(Q, dims=2))\n  # Subtract 1 to get to \"natural\" indices 0:1\n  π[t] = argmin.(eachrow(Q)) .- 1\nend\n\ndisplay(V)\ndisplay(π)\n\n\n6-element Vector{Vector{Float64}}:\n [4.000000000000001, 13.360000000000003, 16.4, 18.4, 20.4, 22.4]\n [2.4000000000000004, 10.400000000000002, 15.2, 17.2, 19.2, 21.2]\n [1.2000000000000002, 7.200000000000001, 13.200000000000001, 16.4, 18.4, 20.4]\n [0.4, 4.4, 8.4, 12.4, 16.4, 20.0]\n [0.0, 2.0, 4.0, 6.0, 8.0, 10.0]\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n\n5-element Vector{Vector{Int64}}:\n [0, 0, 1, 1, 1, 1]\n [0, 0, 1, 1, 1, 1]\n [0, 0, 0, 1, 1, 1]\n [0, 0, 0, 0, 0, 0]\n [0, 0, 0, 0, 0, 0]\n\n\nFigure 5.6 shows the optimal value function and optimal policy as a function of time.\n\n\n\n\n\n\nFigure 5.6: Optimal value function and optimal policy for Example 5.2. The red color indicates that the optimal action is \\(1\\); black indicates that the optimal action is \\(0\\). Note that the computations are done by proceeding backwards in time.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html#variations-of-a-theme",
    "href": "mdps/intro.html#variations-of-a-theme",
    "title": "5  Finite horizon MDPs",
    "section": "5.6 Variations of a theme",
    "text": "5.6 Variations of a theme\n\n5.6.1 Cost depends on next state\nIn the basic model that we have considered above, we assumed that the per-step cost depends only on the current state and current actions. In some applications, such as the inventory management, it is more natural to have a cost function where the cost depends on the current state, current action, and the next state. Conceptually, such problems can be treated in the same way as the standard model.\nIn particular, suppose we have a per-step cost given by \\(c_t(S_t,A_t,S_{t+1})\\), where the objective is to minimize \\[ J(π) = \\EXP\\Bigl[ \\sum_{t=1}^T c_t(S_t, A_t, S_{t+1}) \\Bigr]. \\]\nDefine \\[ \\tilde c_t(s, a) = \\EXP[ c_t(s, a, S_{t+1}) | S_t = s, A_t = a ]\n= \\EXP[ c_t(s,a, f_t(s,a, W_t) ]. \\] Then, by the towering property of conditional expectation, we can write\n\\[ \\begin{align*}\nJ(π) &= \\EXP\\Bigl[ \\sum_{t=1}^T \\EXP[ c_t(S_t, A_t, S_{t+1}) | S_t, A_t] \\Bigr] \\\\\n&= \\EXP\\Bigl[ \\sum_{t=1}^T \\tilde c_t(S_t, A_t) \\Bigr].\n\\end{align*} \\]\nThus, we can equivalently consider this as our standard model with the per-step cost given by \\(\\tilde c_t(S_t, A_t)\\). We can write the recursive step of the dynamic program as follows: \\[ Q^*_t(s,a) = \\EXP[ c_t(s,a, S_{t+1}) + V^*_{t+1}(S_{t+1}) | S_t = s, A_t = a ].\\]\nFor numerically solving the dynamic program when the cost is time-homogeneous (i.e., does not depend on \\(t\\)), it is more efficient to compute \\(\\tilde c\\) once and recuse that in the dynamic program recursion.\n\n\n5.6.2 Discounted cost\nIn some applications, it is common to consider a discounted expected cost given by \\[ J(π) = \\EXP\\Bigl[ \\sum_{t=1}^T γ^{t-1} c_t(S_t, A_t) \\Bigr] \\] where \\(γ \\in (0,1)\\) is called the discount factor.\n\n\n\n\n\n\nDiscount Factor\n\n\n\nThe idea of using discounting in MDPs is due to Blackwell (1965).\nThere are two interpretations of the discount factor \\(γ\\). The first interpretation is an economic interpretation to determine the present value of a utility that will be received in the future. For example, suppose a decision maker is indifferent between receiving 1 dollar today or \\(s\\) dollars tomorrow. This means that the decision maker discounts the future at a rate \\(1/s\\), so \\(γ = 1/s\\).\nThe second interpretation is that of an absorbing state. Suppose we are operating a machine that generates a value of $1 each day. However, there is a probability \\(p\\) that the machine will break down at the end of the day. Thus, the expected return for today is $1 while the expected return for tomorrow is \\((1-p)\\) (which is the probability that the machine is still working tomorrow). In this case, the discount factor is defined as \\((1-p)\\). See Shwartz (2001) for a detailed discussion of this alternative.\n\n\nThe recursive step of the dynamic program for such models can be written as \\[ Q^*_t(s,a) = c_t(s,a) + γ \\, \\EXP[ V^*_{t+1}( S_{t+1}) | S_t = s, A_t = a ].\\]\n\n\n5.6.3 Multiplicative cost\nSo far, we have assumed that the cost is additive. The dynamic proramming decomposition also works for models with multiplicative cost. In particular, suppose that the performance of any policy is given by \\[ J(π) = \\EXP\\Bigl[ \\prod_{t=1}^T c_t(S_t, A_t) \\Bigr] \\] where the per-step cost function is positive. Then, it can be shown that the optimal policy is given by the following dynamic program.\n\nProposition 5.1 (Dynamic Program for multiplicative cost) Initialize \\(V_{T+1}(s) = 1\\) and recursively compute \\[ \\begin{align*}\nQ^*_t(s,a) &= c_t(s,a) \\EXP[ V^*_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\\\\nV^*_t(s) &= \\min_{a \\in \\ALPHABET A} Q^*_t(s,a).\n\\end{align*} \\]\n\n\n\n5.6.4 Exponential cost function\nA special class of multiplicative cost function is exponential of sum: \\[J(π) = \\EXP\\Bigl[ \\exp\\Bigl( \\theta \\sum_{t=1}^T c_t(S_t, A_t) \\Bigr) \\Bigr]. \\]\nWhen \\(\\theta &gt; 0\\), the above captures risk-averse preferences and when \\(\\theta &lt; 0\\), it corresponds to risk-seeking preferences. This is equivalent to a multiplicative cost \\[J(π) = \\EXP\\Bigl[ \\prod_{t=1}^T \\exp( \\theta c_t(S_t, A_t)) \\Bigr]. \\] Therefore, the dynamic program for multiplicative cost is also applicable for this model.\nSee notes on risk-sensitive MDPs for more details\n\n\n5.6.5 Optimal stopping\nLet \\(\\{S_t\\}_{t \\ge 1}\\) be a Markov chain. At each time \\(t\\), a decision maker observes the state \\(S_t\\) of the Markov chain and decides whether to continue or stop the process. If the decision maker decides to continue, he incurs a continuation cost \\(c_t(S_t)\\) and the state evolves. If the DM decides to stop, he incurs a stopping cost of \\(d_t(S_t)\\) and the problem is terminated. The objective is to determine an optimal stopping time \\(\\tau\\) to minimize \\[J(\\tau) := \\EXP\\bigg[ \\sum_{t=1}^{\\tau-1} c_t(S_t) + d_\\tau(S_\\tau)\n\\bigg].\\]\nSuch problems are called Optimal stopping problems.\nDefine the cost-to-go function of any stopping rule as \\[V^{\\tau}_t(s) = \\EXP\\bigg[ \\sum_{τ = t}^{\\tau - 1} c_{\\tau}(S_t) +\nd_\\tau(S_\\tau) \\,\\bigg|\\, \\tau &gt; t \\bigg]\\] and the value function as \\[V^*_t(s) = \\inf_{\\tau} V^{\\tau}_t(s). \\] Then, it can be shown that the value functions satisfy the following recursion:\n\nProposition 5.2 Dynamic Program for optimal stopping \\[ \\begin{align*}\nV^*_T(s) &= s_T(s) \\\\\nV^*_t(s) &= \\min\\{ s_t(s), c_t(s) + \\EXP[ V^*_{t+1}(S_{t+1}) | S_t = s].\n\\end{align*}\\]\n\nSee the notes on optimal stopping for more details.\n\n\n5.6.6 Minimax setup\nTo be written",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html#sec-mdp-cts-spaces",
    "href": "mdps/intro.html#sec-mdp-cts-spaces",
    "title": "5  Finite horizon MDPs",
    "section": "5.7 Continuous state and action spaces",
    "text": "5.7 Continuous state and action spaces\nThe fundamental ideas discussed above also hold for continuous state and action spaces provided one carefully deals with measurability. We first fix some notation:\n\nFor a set \\(\\ALPHABET S\\), let \\(\\mathscr B(\\ALPHABET S)\\) denote the Borel sigma-algebra on \\(\\ALPHABET S\\).\nWe use \\(\\ALPHABET M(\\ALPHABET X, \\ALPHABET Y)\\) to denote the set of measurable functions from the space \\(\\ALPHABET X\\) to the space \\(\\ALPHABET Y\\) (we implicitly assume that the sigma-algebras on \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) is the respective Borel sigma-algebras). When \\(\\ALPHABET Y\\) is \\(\\reals\\), we will sometimes use the notation \\(\\ALPHABET M(\\ALPHABET X)\\).\n\n\nIn order to talk about expected cost, in continuous state spaces we have to assume that the per-step cost is measurable, i.e., \\(c \\in \\ALPHABET M(\\ALPHABET S × \\ALPHABET A)\\), and the \\(P \\colon \\ALPHABET S × \\ALPHABET A × \\mathscr B(\\ALPHABET S) \\to [0,1]\\) is a stochastic kernel, i.e., for every \\((s,a) \\in \\ALPHABET S × \\ALPHABET A\\), \\(p(s,a,\\cdot)\\) is probability measure on \\(\\ALPHABET S\\), and for every \\(B \\in \\ALPHABET S\\), the function \\(p(\\cdot, \\cdot, B) \\in \\ALPHABET M(\\ALPHABET S × \\ALPHABET A)\\). These assumption imply that for every measurable policy \\(π \\in \\ALPHABET M(\\ALPHABET S, \\ALPHABET A)\\), the performance \\(J(\\pi)\\) is well defined. However, these assumptions are not sufficient to establish the optimality of dynamic programmming.\nTo highlight the technical issues, let us consider an MDP with \\(T = 1\\), i.e., a stochastic optimization problem. Define \\[\n  V^*(s) = \\inf_{a \\in \\ALPHABET A} c(s,a)\n  \\quad\\text{and}\\quad\n  π^*(s) = \\arg\\inf_{a \\in \\ALPHABET A} c(s,a).\n\\]\nWe present a few examples below from Blackwell (1965) to highlight technical issues with non-finite state models.\n\nExample 5.3 (No optimal policy) Let \\(\\ALPHABET S = \\{s_\\circ\\}\\), \\(\\ALPHABET A = \\integers_{\\ge 0}\\) Consider \\(c(s_\\circ,a) = (a+1)/a\\). Here \\(v^*(s_\\circ) = 1\\) but there is no policy which achieves this cost.\n\nIn the above example, there is no optimal policy, but given any \\(ε &gt; 0\\), we can identify an \\(ε\\)-optimal policy. The next example shows that we can have a much severe situation where the value function is not measurable. The example relies on the following fact:\n\nThere exist Borel sets in \\(\\reals^2\\) whose projection on \\(\\reals\\) is not Borel. See :the wikipedia article on projections for a discussion.\n\n\nExample 5.4 (Non-measurable value function) Let \\(\\ALPHABET S = \\ALPHABET A = [0,1]\\) and let \\(B \\subset \\ALPHABET S × \\ALPHABET A\\) such that \\(B\\) is measurable but its projection on \\(D\\) onto \\(\\ALPHABET S\\) is not. Consider \\(c(s,a) = -\\IND\\{ (s,a) \\in B \\}\\). Note that \\[\n  v(s) = \\inf_{a \\in \\ALPHABET A} c(s,a) =\n  \\inf_{a \\in \\ALPHABET A} -\\IND\\{ (s,a) \\in B \\} =\n  -\\IND\\{s \\in D \\}\n\\] which is not Borel measurable.\n\nSee Piunovskiy (2011) for various examples on what can go wrong in MDPs. In particular, Example 1.4.15 of Piunovskiy (2011) extends Example 5.4 to provide an example where there is no \\(ε\\)-optimal policy! (Blackwell (1965) also has such an example, but it is much harder to parse).\n\nThere are two ways to resolve the issue with non-existence of optimal policies: either assume that \\(\\ALPHABET A\\) is compact or that the function that we are minimizing is coercive, so that the \\(\\arg\\inf\\) can be replaced by an \\(\\arg\\min\\). Or work with \\(ε\\)-optimal policies.\nResolving the measurability issue is more complicated. There are various measurable selection theorems in the literature to identify sufficient conditions for measurability of the value function and optimal policy. See Hernández-Lerma and Lasserre (1996) and (1999) for an accessible treatment of continous state MDPs.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html#exercises",
    "href": "mdps/intro.html#exercises",
    "title": "5  Finite horizon MDPs",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 5.1 (Monotonicity in time) Consider an MDP where the dynamics and per-step cost are time-homogeneous, i.e., the function \\(f_t\\) and the per-step cost \\(c_t\\) do not depend on \\(t\\) (except, possibly at the terminal time \\(t=T\\)). Suppose that \\(V^*_{T-1}(s) \\le V^*_T(s)\\) for all \\(s \\in \\ALPHABET S\\). Then, show that \\[ V^*_{t}(s) \\le V^*_{t+1}(s), \\quad\n   \\text{for all $s \\in \\ALPHABET S$ and $t$}.\\]\nSimilarly, if we have that \\(V^*_{T-1}(s) \\ge V^*_T(s)\\) for all \\(s \\in\n\\ALPHABET S\\), then \\[ V^*_{t}(s) \\ge V^*_{t+1}(s), \\quad\n   \\text{for all $s \\in \\ALPHABET S$ and $t$}.\\]\n\n\nExercise 5.2 (Dynamic programming for maximizing tail probabilities) Consider a dynamical system that evolves as follows: \\[\n  S_{t+1} = f_t(S_t, A_t, W_t)\n\\] where \\(\\{S_1, W_1, \\dots, W_T\\}\\) are independent random variables and the control actions \\(A_t\\) are chosen according to a history dependent policy \\(π = (π_1, \\dots, π_T)\\): \\[\n  A_t = π_t(S_{1:t}, A_{1:t-1}).\n\\] Given a sequence of functions \\(h_t \\colon \\ALPHABET S \\mapsto \\reals\\), the cost of a policy \\(π\\) is given by the probability that \\(h_t(S_t)\\) exceeds a given threshold \\(α \\in \\reals\\) at some time, i.e., \\[\n  J(π) = \\PR^{π}\\left( \\max_{0 \\le t \\le T} h_t(S_t) \\ge α \\right).\n\\]\nShow that the above cost can be put in an additive form that would enable us to use the theory developed in the class to tackle this setup.\n\n\nExercise 5.3 (Optimal strategy in internet of things) Consider an IoT (Internet of Things) device which is observing an autoregressive process \\(\\{X_t\\}_{t \\ge 0}\\), \\(X_t \\in \\integers\\), which starts at \\(X_1 = 0\\) and for \\(t &gt; 1\\) evolves as \\[ X_{t+1} = X_t + W_t \\] where \\(\\{W_t\\}_{t \\ge 1}\\) is an i.i.d. process with \\(W_t \\in \\{ -5, \\dots,\n5 \\}\\) with \\[ \\PR(W_t = w) =\n\\begin{cases}\n  \\frac{1}{5} - \\frac{|w|}{25}, & \\text{if } |w| \\le 5 \\\\\n  0, & \\text{otherwise}\n\\end{cases}\\]\nThe IoT device can either transmit its observation (denoted by \\(A_t = 1\\)) or not (denoted by \\(A_t = 0\\)). Transmitting a packet has a cost \\(\\lambda\\) while not transmitting has no cost.\nWhen \\(A_t = 0\\), the receiver estimates the state of the process as the previously transmitted observation \\(Z_t\\) and incurs a cost \\((X_t -\nZ_t)^2\\).\nThe above system can be modeled as an MDP with state \\(\\{S_t \\}_{t \\ge 0}\\), where \\(S_t = X_t - Z_t\\). It can be shown that the dynamics of \\(\\{S_t\\}_{t\n\\ge 1}\\) are as follows: \\[ S_{t+1} = \\begin{cases}\n    S_t + W_t, & \\text{if } A_t = 0 \\\\\n    W_t, & \\text{if } A_t = 1\n  \\end{cases} \\]\nThe per-step cost is given by \\[ c(s,a) = \\lambda a + (1-a)s^2. \\]\nThe objective of this exercise is to find the optimal policy for the above problem using dynamic programming.\nIn this model, the state space is unbounded which makes it difficult to use dynamic programming. So, we construct approximate dynamics as follows. We pick a large number \\(B\\) and assume that the dynamics are: \\[ S_{t+1} = \\begin{cases}\n    [S_t + W_t]_{-B}^B, & \\text{if } A_t = 0 \\\\\n    [W_t]_{-B}^B, & \\text{if } A_t = 1\n  \\end{cases} \\]\nSo, we can effective consider the state space to be \\(\\{-B, \\dots, B\\}\\).\n\nSolve the dynamic program for \\(T = 20\\), \\(λ = 100\\), and \\(B = 100\\).\nPlot the value function for \\(t \\in \\{1, 5, 10, 19 \\}\\) on the same plot.\nPlot the optimal policy for \\(t \\in \\{1, 5, 10, 19 \\}\\).\nChange the value of \\(B\\) in the set \\(\\{50, 60, 70, 80 \\}\\) to make sure that our truncation does not have a significant impact on the value function and the optimal policy.\n\n\n\nExercise 5.4 Consider the model of Example 5.2 where \\(W \\sim \\text{Binon}(n,p)\\).\n\nSolve the dynamic program for \\(T=20\\), \\(n=10\\), \\(p=0.4\\), \\(h(s) = 2s\\), and \\(λ=10\\).\nPlot the value function for \\(t \\in \\{1, 5, 10, 19 \\}\\) on the same plot.\nPlot the optimal policy for \\(t \\in \\{1, 5, 10, 19 \\}\\).\nBased on these results, guess the structure of the optimal policy. Change the values of the parameters to check if the structure is retained. We will learn how to establish that such a structure is optimal in Exercise 8.7.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/intro.html#notes",
    "href": "mdps/intro.html#notes",
    "title": "5  Finite horizon MDPs",
    "section": "Notes",
    "text": "Notes\nThe proof idea for the optimality of Markov policies is based on a proof by Witsenhausen (1979) on the structure of optimal coding policies for real-time communication. Note that the proof does not require us to find a dynamic programming decomposition of the problem. This is in contrast with the standard textbook proof where the optimality of Markov policies is proved as part of the dynamic programming decomposition.\nExercise 5.3 is adapted from Chakravorty and Mahajan (2018).\n\n\n\n\n\nBlackwell, D. 1964. Memoryless strategies in finite-stage dynamic programming. The Annals of Mathematical Statistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nBlackwell, D. 1965. Discounted dynamic programming. The Annals of Mathematical Statistics 36, 1, 226–235. DOI: 10.1214/aoms/1177700285.\n\n\nChakravorty, J. and Mahajan, A. 2018. Sufficient conditions for the value function and optimal strategy to be even and quasi-convex. IEEE Transactions on Automatic Control 63, 11, 3858–3864. DOI: 10.1109/TAC.2018.2800796.\n\n\nHernández-Lerma, O. and Lasserre, J.B. 1996. Discrete-time markov control processes. Springer New York. DOI: 10.1007/978-1-4612-0729-0.\n\n\nHernández-Lerma, O. and Lasserre, J.B. 1999. Further topics on discrete-time markov control processes. Springer New York. DOI: 10.1007/978-1-4612-0561-6.\n\n\nPiunovskiy, A.B. 2011. Examples in markov decision processes. Imperial College Proess. DOI: 10.1142/p809.\n\n\nShwartz, A. 2001. Death and discounting. IEEE Transactions on Automatic Control 46, 4, 644–647. DOI: 10.1109/9.917668.\n\n\nWitsenhausen, H.S. 1979. On the structure of real-time source coders. Bell System Technical Journal 58, 6, 1437–1451.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/gambling.html",
    "href": "mdps/gambling.html",
    "title": "6  Optimal gambling",
    "section": "",
    "text": "6.1 Computational experiment\nTo fix ideas, let’s try to find the optimal policy on our own. An example strategy is given below.\nviewof code = Inputs.textarea({label: \"\", height:800, rows:11, width: 800, submit: true,\n   value: `// function bet(t, states, outcomes) {\n// t: current time\n// states: Array of states\n// outcomes: Array of outcomes\n// \n// modify the (javascript) code between the lines:\n// ===============================\n     // As an illustration, we implement the policy to bet\n     //  half of the wealth as long as one is winning. \n     if(t == 0) { \n        return 0.5*states[t] \n     } else { \n        return outcomes[t-1] == 1 ? 0.5*states[t] : 0\n     }\n// ================================\n//}`\n                              })\nviewof strategy = Inputs.radio([\"user code\", \"optimal\"], {value: \"user code\", label: \"Select strategy\"})\nT = 100\nn = 25\nS1 = 100\n\nBernoulli = function(p) { return Math.random() &lt;= p ? 1 : -1 }\n\nuser_strategy = new Function('t', 'states', 'outcomes', code)\n\noptimal_strategy = function(t,states,outcomes) {\n  return p &lt; 0.5 ? 0 : (2*p - 1)*states[t]\n}\n\nbet = function(t, states, outcomes) {\n  return strategy == \"optimal\" ? optimal_strategy(t, states, outcomes) : user_strategy(t, states, outcomes) \n}\n\ndata = { \n  run;\n  var states = new Array(T+1)\n  var outcomes = new Array(T+1)\n  var trajectory = new Array(T+1)\n  var sum = 0\n\n  const initial = 100\n  var idx = 0\n\n  for (var i = 0; i &lt; n; i++) {\n      // Initialize the array to NaN values.\n      for (var t = 0; t &lt; T+1; t++) {\n        states[t] = NaN\n        outcomes[t] = Bernoulli(p)\n      }\n    \n      states[0] = initial\n      var action = 0\n    \n      for (var t = 0; t &lt; T; t++, idx++) {\n        action = bet(t, states, outcomes)\n        states[t+1] = states[t] + outcomes[t] * action\n        trajectory[idx] = { \n          time: t+1, \n          state: states[t],\n          action: action, \n          outcome: outcomes[t],\n          reward: Math.log10(states[t]),\n          sample: i,\n        }\n      }\n      sum += Math.log10(states[T])\n  }\n  return { trajectories: trajectory, mean: sum/n }\n}\nAssuming that $S_1 = $ , we plot the performance of this policy below. Choosing “optimal” in the radio button above gives the performance of the optimal policy (derived below).\nrewardPlot = Plot.plot({\n  grid: true,\n  marginRight: 40,\n  marks: [\n    // Data\n    Plot.line(data.trajectories, {x: \"time\", y: \"reward\", z: \"sample\", stroke: \"gray\", curve: \"step-after\"}),\n    Plot.line(data.trajectories, Plot.groupX({y: \"mean\"}, {x:\"time\", y: \"reward\", stroke: \"red\", strokeWidth: 2, curve: \"step-after\"})),\n\n    // Final value\n    Plot.dot([ [T,data.mean] ], { fill: \"red\"}),\n    Plot.text([ [T,data.mean] ], { text: Math.round(data.mean*100)/100, dx:18, fill:\"red\", fontWeight:\"bold\" }),\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 6.1: Plot of the performance of the strategy for a horizon of \\(T=\\) . The curves in gray show the performance over $n = $  difference sample paths and the red curve shows its mean. For ease of visualization, we are plotting the utility at each stage (i.e., \\(\\log s_t\\)), even though the reward is only received at the terminal time step. The red line shows the mean performance over the \\(n\\) sample paths. The final mean value of the reward is shown in red. You can toggle the select strategy button to see how the optimal strategy performs (and how close you came to it).\nAs we can see, most intuitive policies do not do so well. We will now see how to compute the optimal policy using dynamic programming.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimal gambling</span>"
    ]
  },
  {
    "objectID": "mdps/gambling.html#computational-experiment",
    "href": "mdps/gambling.html#computational-experiment",
    "title": "6  Optimal gambling",
    "section": "",
    "text": "viewof p = Inputs.range([0, 1], {value: 0.6, label: \"p\", step: 0.01})\n\nviewof run = Inputs.button(\"Re-run simulation\")",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimal gambling</span>"
    ]
  },
  {
    "objectID": "mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "href": "mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "title": "6  Optimal gambling",
    "section": "6.2 Optimal gambling strategy and value functions",
    "text": "6.2 Optimal gambling strategy and value functions\nThe above model of optimal gambling is a Markov decision process. Therefore, the optimal solution is given by dynamic programming.\n\nProposition 6.1 (Dynamic programming decomposition) Define the following value function \\(V^*_t \\colon \\reals_{\\ge 0} \\to \\reals\\) \\[ V^*_T(s) = \\log s \\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[ \\begin{align*}\nQ^*_t(s,a) &= \\EXP[ r_t(s,a) + V^*_{t+1}(S_{t+1}) \\,|\\, S_t = s, A_t = a] \\\\\n&= p V^*_{t+1}(s+a) + (1-p) V^*_{t+1}(s-a),\n\\end{align*}\n\\] and \\[ \\begin{align*}\nV^*_t(s) &=  \\max_{a \\in [0, s]} Q^*_t(s,a), \\\\\nπ^*_t(s) &= \\arg \\max_{a \\in [0, s]} Q^*_t(s,a). \\\\\n\\end{align*}\n\\]\nThen the strategy \\(π^* = (π^*_1, \\dots, π^*_{T-1})\\) is optimal.\n\n\n\n\n\n\n\nRemark\n\n\n\nThe above model is one of the rare instances when the optimal strategy and the optimal strategy and value function of an MDP can be identified in closed form.\n\n\n\nTheorem 6.1 (Optimal gambling strategy) When \\(p \\le 0.5\\):\n\nthe optimal strategy is to not gamble, specifically \\(π^*_t(s) = 0\\);\nthe value function is \\(V^*_t(s) = \\log s\\).\n\nWhen \\(p &gt; 0.5\\):\n\nthe optimal strategy is to bet a fraction of the current fortune, specifically \\(π^*_t(s) = (2p - 1)s\\);\nthe value function is \\(V^*_t(s) = \\log s + (T - t) C\\), where \\[ C = \\log 2 + p \\log p + (1-p) \\log (1-p).\\]\n\n\nThe constant \\(C\\) defined in Theorem 6.1 is equal to the capacity of a binary symmetric channel! In fact, the above model was introduced by Kelly (1956) to show a gambling interpretation of information rates.\nWe prove the two cases separately.\n\n\n\n\n\n\nProof when \\(p \\le 0.5\\)\n\n\n\n\n\nLet \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p \\le 0.5\\) implies that \\(p \\le 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V^*_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V^*_{t+1}(s) =\n\\log s\\). Now consider\n\\[ Q^*_t(s,a) = p V^*_{t+1}(s+a) + qV^*_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q^*_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { - (q - p) s - a } {s^2 - a^2 }\n   \\\\\n   &&lt; 0.\n  \\end{align*}   \n\\]\nThis implies that \\(Q^*_t(s,a)\\) is decreasing in \\(a\\). Therefore,\n\\[ π^*_t(s) = \\arg\\max_{a \\in [0, s]} Q^*_t(s,a) = 0. \\]\nMoreover, \\[ V^*_t(s) = Q^*_t(s, π^*_t(s)) = \\log s.\\]\nThis completes the induction step.\n\n\n\n\n\n\n\n\n\nProof when \\(p &gt; 0.5\\)\n\n\n\n\n\nAs in the previous case, let \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p &gt; 0.5\\) implies that \\(p &gt; 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V^*_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V^*_{t+1}(s) =\n\\log s + (T -t - 1)C\\). Now consider\n\\[ Q^*_t(s,a) = p V^*_{t+1}(s+a) + qV^*_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q^*_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { (p - q) s - a } {s^2 - a^2 }\n  \\end{align*}   \n\\]\nSetting \\(\\partial Q^*_t(s,a)/\\partial a = 0\\), we get that the optimal action is\n\\[ π^*_t(s) = (p-q) s. \\]\nNote that \\((p-q) \\in (0,1]\\)\n\\[\n  \\frac { \\partial^2 Q^*_t(s,a) } {\\partial a^2} =\n   - \\frac p { (s + a)^2 } - \\frac q { (s - a)^2 }\n  &lt; 0;\n\\] hence the above action is indeed the maximizer. Moreover, \\[ \\begin{align*}\n  V^*_t(s) &= Q^*_t(s, π^*_t(s))  \\\\\n  &= p V^*_{t+1}(s + π^*_t(s)) + q V^*_{t+1}( s - π^*_t(s) )\\\\\n  &= \\log s + p \\log (1 + (p-q)) + q \\log (1 - (p-q)) + (T - t -1)C \\\\\n  &= \\log s + p \\log 2p + q \\log 2q + (T - t + 1)C \\\\\n  &= \\log s + (T - t) C\n  \\end{align*}   \n\\]\nThis completes the induction step.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimal gambling</span>"
    ]
  },
  {
    "objectID": "mdps/gambling.html#generalized-model",
    "href": "mdps/gambling.html#generalized-model",
    "title": "6  Optimal gambling",
    "section": "6.3 Generalized model",
    "text": "6.3 Generalized model\nSuppose that the terminal reward \\(r_T(s)\\) is monotone increasing2 in \\(s\\).\n2 I use the convention that increasing means weakly increasing. The alternative term non-decreasing implicitly assumes that we are talking about a totally ordered set.\nTheorem 6.2 For the generalized optimal gambling problem:\n\nFor each \\(t\\), the value function \\(V^*_t(s)\\) is monotone increasing in \\(s\\).\nFor each \\(s\\), the value function \\(V^*_t(s)\\) is monotone decreasing in \\(t\\).\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(s\\)\n\n\n\n\n\nWe proceed by backward induction. \\(V^*_T(s) = r_T(s)\\) which is monotone increasing in \\(s\\). Assume that \\(V^*_{t+1}(s)\\) is increasing in \\(s\\). Now, consider \\(V^*_t(s)\\). Consider \\(s_1, s_2 \\in \\reals_{\\ge 0}\\) such that \\(s_1 \\le\ns_2\\). Then for any \\(a \\le s_1\\), we have that\n\\[ \\begin{align*}\n    Q^*_t(s_1, a) &= p V^*_{t+1}(s_1+a) + q V^*_{t+1}(s_1-a) \\\\\n    & \\stackrel{(a)}{\\le} p V^*_{t+1}(s_2 + a) + q V^*_{t+1}(s_2  - a) \\\\\n    & = Q^*_t(s_2, a),\n  \\end{align*}\n\\] where \\((a)\\) uses the induction hypothesis. Now consider\n\\[ \\begin{align*}\n  V^*_t(s_1) &= \\max_{a \\in [0, s_1]} Q^*_t(s_1, a) \\\\\n  & \\stackrel{(b)}{\\le} \\max_{a \\in [0, s_1]} Q^*_t(s_2, a) \\\\\n  & \\le \\max_{a \\in [0, s_2]} Q^*_t(s_2, a) \\\\\n  &= V^*_t(s_2),\n  \\end{align*}\n\\] where \\((b)\\) uses monotonicity of \\(Q^*_t\\) in \\(s\\). This completes the induction step.\n\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(t\\)\n\n\n\n\n\nThis is a simple consequence of the following:\n\\[V^*_t(s) = \\max_{a \\in [0, s]} Q^*_t(s,a) \\ge Q^*_t(s,0) = V^*_{t+1}(s).\\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimal gambling</span>"
    ]
  },
  {
    "objectID": "mdps/gambling.html#exercises",
    "href": "mdps/gambling.html#exercises",
    "title": "6  Optimal gambling",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nNote\n\n\n\nThe purpose of these series of exercises is to generalize the basic result to a model where the gambler can bet on many mutually exclusive outcomes (think of betting on multiple horses in a horse race).\n\n\n\nExercise 6.1 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log w_i\\] subject to:\n\n\\(w_i \\ge 0\\)\n\\(\\sum_{i=1}^n w_i \\le s\\).\n\nShow that the optimal solution is given by \\[ w_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 6.2 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log (s - a + na_i)\\] subject to:\n\n\\(a_i \\ge 0\\)\n\\(a = \\sum_{i=1}^n a_i \\le s\\).\n\nShow that the optimal solution is given by \\[ a_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 6.3 Consider an alternative of the optimal gambling problem where, at each time, the gambler can place bets on many mutually exclusive outcomes. Suppose there are \\(n\\) outcomes, with success probabilities \\((p_1, \\dots,\np_n)\\). Let \\((A_{1,t}, \\dots, A_{n,t})\\) denote the amount that the gambler bets on each outcome. The total amount \\(A_t := \\sum_{i=1}^n A_{i,t}\\) must be less than the gambler’s fortune \\(S_t\\). If \\(W_t\\) denotes the winning outcome, then the gambler’s wealth evolves according to \\[ S_{t+1} = S_t - A_t + nU_{W_t, t}.\\] For example, if there are three outcomes, gambler’s current wealth is \\(s\\), the gambler bets \\((a_1, a_2,\na_3)\\), and outcome 2 wins, then the gambler wins \\(3 a_2\\) and his fortune at the next time is \\[ s - (a_1 + a_2 + a_3) + 3 a_2. \\]\nThe gambler’s utility is \\(\\log S_T\\), the logarithm of his final wealth. Find the strategy that maximizes the gambler’s expected utility.\nHint: Argue that the value function is of the form \\(V^*_t(s) = \\log s +\n(T -t)C\\), where  \\[C = \\log n - H(p_1, \\dots, p_n)\\] where \\(H(p_1, \\dots, p_n) = - \\sum_{i=1}^n p_i \\log p_i\\) is the entropy of a random variable with pmf \\((p_1, \\dots, p_n)\\).The constant \\(C\\) is the capacity of a symmetric discrete memoryless with \\(n\\) outputs and for every input, the output probabilities are a permutation of \\((p_1, \\dots, p_n)\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimal gambling</span>"
    ]
  },
  {
    "objectID": "mdps/gambling.html#notes",
    "href": "mdps/gambling.html#notes",
    "title": "6  Optimal gambling",
    "section": "Notes",
    "text": "Notes\nThe above model (including the model described in the exercise) was introduced by Kelly (1956). However, Kelly restricted attention to “bet a constant fraction of your fortune” betting strategy and found the optimal fraction. This strategy is sometimes referred to as :Kelly criteria. As far as I know, the dynamic programming treatment of the problem is due to Ross (1974). Ross also considered variations where the objective was to maximize the probability of reaching a preassigned fortune or maximizing the time until becoming broke.\nA generalization of the above model to general logarithmic and exponential utilities is presented in Ferguson and Gilstein (2004).\n\n\n\n\n\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004. Optimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nKelly, J.L., Jr. 1956. A new interpretation of information rate. Bell System Technical Journal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nRoss, S.M. 1974. Dynamic programming and gambling models. Advances in Applied Probability 6, 3, 593–606. DOI: 10.2307/1426236.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Optimal gambling</span>"
    ]
  },
  {
    "objectID": "mdps/inventory-management.html",
    "href": "mdps/inventory-management.html",
    "title": "7  Inventory Management",
    "section": "",
    "text": "7.1 Dynamic programming decomposition\n\\(\\def\\S{\\mathbb{S}}\\)\nThe above model is a Markov decision process.1 Therefore, the optimal solution is given by dynamic programming.\nInstead of \\(\\integers\\), we use \\(\\S\\) to denote the possible values of states. The reason is that we will later consider the case when the state space is the set of reals, and we can still use the same equations.\nIt is possible to simplify the above dynamic program by exploiting a feature of the model. Notice that the dynamics can be split into two parts: \\[ \\begin{align*}\n    Z_t &= S_t + A_t,  \\\\\n    S_{t+1} &= Z_t - W_t.\n   \\end{align*}\n\\] The first part, \\(Z_t\\), depends only on the current state and action. The second part depends only on \\(Z_t\\) and a primitive random variable. In this particular model, \\(Z_t\\) is a deterministic function of \\(S_t\\) and \\(A_t\\); but, in general, it could be stochastic as well; what is important is that the second part should only depend on \\(Z_t\\) and a primitive random variable. The variable \\(Z_t\\) is sometimes called the post-decision state.\nNow write the dynamic program in terms of the post-decision state as follows. Define \\[ H_t(z) = \\EXP[ h(z - W) + V^*_{t+1}(z-W) ].\\] Then the value function and optimal policy at time \\(t\\) can be written as: \\[ \\begin{align*}\n  V^*_t(s) &= \\min_{a \\in \\S_{\\ge 0}} \\bigl\\{ pa + H_t(s + a) \\bigr\\}, \\\\\n  π^*_t(s) &= \\arg \\min_{a \\in \\S_{\\ge 0}} \\bigl\\{ pa + H_t(s + a) \\bigr\\}.\n\\end{align*} \\]\nNote that the problem at each step is similar to the newsvendor problem. So, similar to that model, we try to see if we can establish qualitative properties of the optimal solution.\nTo fix ideas, let’s solve this dynamic program for a specific instance.\nWe assume that the demand is distributed according to a Binomial(50,0.4) distribution, as shown in Figure 7.1. We assume that the model parameters are as given below:\n\\[\nc_h = 2,\\quad c_s = 5,\\quad p = 1.\n\\]\nWe consider a horizon \\(T = 15\\), and solve the dynamic program shown above. The optimal value function and policy are shown below:\nThe plots above suggest that the optimal policy has a structure. Play around with the value of the shortage cost to see if that structure is retained.\nWe will now see how to prove the structure of optimal policy.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inventory Management</span>"
    ]
  },
  {
    "objectID": "mdps/inventory-management.html#dynamic-programming-decomposition",
    "href": "mdps/inventory-management.html#dynamic-programming-decomposition",
    "title": "7  Inventory Management",
    "section": "",
    "text": "1 Part of the per-step cost depends on the future state \\(S_{t+1}\\). It is easy to show that the standard MDP model works even when the per-step cost is a function of \\((S_t, A_t, S_{t+1})\\)\n\nProposition 7.1 (Dynamic programming) Define the following value functions \\(V^*_t \\colon \\S \\to \\reals\\) \\[V^*_{T+1}(s) = 0\\] and for \\(t \\in \\{T, \\dots, 1\\}\\) \\[ Q^*_t(s, a) = p a + \\EXP[ h(s + a - W_t) + V^*_{t+1}( s + a - W_t ) ]\\] and \\[ \\begin{align*}\n  V^*_t(s) &= \\min_{a \\in \\S_{\\ge 0}} Q^*_t(s,a) \\\\\n  π^*_t(s) &= \\arg \\min_{a \\in \\S_{\\ge 0}} Q^*_t(s,a)\n  \\end{align*}\n\\] Then the strategy \\(π^* = (π^*_1, \\dots, π^*_T)\\) is optimal.\n\n\n\n\n\n\n\n\ndemandPlot = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(W, {x:\"demand\", y:\"probability\",curve:\"step-after\"})\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 7.1: Demand Distribution\n\n\n\n\n\n\n\nviewof time = Object.assign(Inputs.range([1, T], {label: \"t\", step: 1, value: 1 }), {style: '--label-width:20px'})\nviewof cs_val = Object.assign(Inputs.range([0.5, 5], {label: \"cs\", step: 0.5, value: cs }), {style: '--label-width:20px'})\n\nvaluePlot = Plot.plot({\n  grid: true,\n  y: { domain: [0, 500] },\n  x: { domain: [-50, 50] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(DP.filter(d =&gt; d.time == time && d.shortage == cs_val), {x:\"state\", y:\"value\", curve:\"step-after\"})\n  ]\n})\n\nactionPlot = Plot.plot({\n  grid: true,\n  y: { domain: [0, 35] },\n  x: { domain: [-10, 30] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(DP.filter(d =&gt; d.time == time && d.shortage == cs_val), {x:\"state\", y:\"policy\", curve:\"step-after\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Value function\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Optimal policy\n\n\n\n\n\n\n\nFigure 7.2: Dynamic programming solution for the example",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inventory Management</span>"
    ]
  },
  {
    "objectID": "mdps/inventory-management.html#structure-of-optimal-solution",
    "href": "mdps/inventory-management.html#structure-of-optimal-solution",
    "title": "7  Inventory Management",
    "section": "7.2 Structure of optimal solution",
    "text": "7.2 Structure of optimal solution\nIn this section, we assume that the state space \\(\\S\\) is equal to \\(\\reals\\) (instead of \\(\\integers\\)). See Exercise 7.1 for the case when \\(\\S\\) is equal to \\(\\integers\\).\nFor this setting, the optimal policy is then characterized as follows.\n\nTheorem 7.1 Define \\[ s^*_t = \\arg \\min_{z \\in \\reals} \\bigl\\{ p z + H_t(z) \\bigr\\} . \\] Then, \\[\\begin{equation} \\label{eq:V}\nV^*_t(s) = \\begin{cases}\n  H_t(s_t) + p (s_t - s), &\\text{if } s \\le s^*_t \\\\\n  H_t(s)   , & \\text{otherwise }\n\\end{cases}\n\\end{equation}\\] and \\[\\begin{equation}\\label{eq:pi}\n  π^*_t(s) = \\begin{cases}\n  s^*_t - s, &\\text{if } s \\le s^*_t \\\\\n  0, & \\text{otherwise }\n\\end{cases}\\end{equation}\\]\nFurthermore, for all \\(t\\), \\(H_t(z)\\) and \\(V^*_t(s)\\) are convex in \\(z\\) and \\(s\\), respectively.\n\n\n\n\n\n\n\nBase-stock policy\n\n\n\n\n\nThe optimal policy given by \\eqref{eq:pi} is called a base-stock policy. It states that there is a base-stock level \\(\\{s^*_t\\}_{t \\ge 1}\\) for every time step. If, at the beginning of time \\(t\\), the value of the current stock is below the base stock level \\(s^*_t\\), then the optimal decision is to order more goods so that the level of the stock equals the base stock level.\n\n\n\n\npoints = {\n  const n = 100\n  const Smax = 10\n\n  const f = function(s) { return (s-5)**2 }\n\n  var points = new Array()\n  var W = [0, 1, 2 ]\n  var Pw = [1/3, 1/3, 1/3]\n\n  var idx = 0\n  for( var i = 0; i &lt; n; i++) {\n    var s = Smax*i/n\n    var sum = 0\n    var min = 10000 // CHECK: Large positive number\n    for(var w = 0; w &lt; W.length; w++) {\n      sum += f(s + W[w])\n      min = Math.min(min, f(s+W[w]))\n      points[idx++] = { state: s, value: f(s + W[w]), noise:W[w], type: \"noise\" }\n    }\n    points[idx++] = { state: s, value: sum/W.length, type: \"average\" }\n    points[idx++] = { state: s, value: min, type: \"minimum\" }\n  }\n  return points\n  }\n\n\n\n\n\n\nWe first establish some preliminary results.\n\n\naveragePlot = Plot.plot({\n  grid: true,\n  y: { domain: [0, 25] },\n  marginRight: 40,\n  marginTop: 40,\n\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {filter: d =&gt;  d.value &lt;= 25 && d.type == \"noise\", \n                       x:\"state\", y:\"value\", z:\"noise\", stroke:\"gray\"}),\n    Plot.text(points, Plot.selectLast({\n      filter: d =&gt; d.value &lt;= 25 && d.type == \"noise\", \n      x: \"state\",\n      y: \"value\",\n      z: \"noise\",\n      text: d =&gt; \"w = \" + d.noise,\n      textAnchor: \"start\",\n      dx: 3\n    })),\n    Plot.line(points, {filter: d =&gt; d.value &lt;= 25 && d.type == \"average\", \n                       x:\"state\", y:\"value\", stroke:\"red\", strokeWidth: 4}),\n    Plot.text(points, Plot.selectLast({\n      filter: d =&gt; d.value &lt;= 25 && d.type == \"average\", \n      x: \"state\",\n      y: \"value\",\n      z: \"noise\",\n      text: \"type\",\n      textAnchor: \"start\",\n      fill:\"red\",\n      dy: -10\n    }))\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 7.3: An example showing that the average of convex functions is convex\n\n\n\n\nFor any convex function \\(f \\colon \\reals \\to \\reals\\), \\(F(s) = \\EXP[ f(s - W) ]\\) is convex.\nProof For any realization of \\(W\\), \\(f(s - w)\\) is convex in \\(s\\). The expectation w.r.t. \\(W\\) is simply the sum of convex functions and is, therefore, convex.\nFor any convex function \\(f \\colon \\reals \\to \\reals\\), let \\(s^* =  \\arg \\min_{s \\in \\reals} f(s)\\).2 Then, \\[\\arg \\min_{a \\in \\reals_{\\ge 0}} f(s + a) = \\begin{cases}\n0, & \\text{if } s &gt; s^*, \\\\\ns^* - s, & \\text{if } s \\le s^*\n\\end{cases}\\] and \\[F(s) = \\min_{a \\in \\reals_{\\ge 0}} f(s+a) = \\begin{cases}\nf(s), & \\text{if } s &gt; s^* \\\\\nf(s^*), & \\text{if } s \\le s^*\n\\end{cases}\\] and \\(F(s)\\) is convex in \\(s\\).\n\n2 Note that \\(s^*\\) is unique if \\(f\\) is strictly convex. If not, we take \\(s^*\\) to be the smallest arg min.\nminimumPlot = Plot.plot({\n  grid: true,\n  y: { domain: [0, 25] },\n  marginRight: 60,\n  marginTop: 40,\n\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {filter: d =&gt;  d.value &lt;= 25 && d.type == \"noise\", \n                       x:\"state\", y:\"value\", z:\"noise\", stroke:\"gray\"}),\n    Plot.text(points, Plot.selectLast({\n      filter: d =&gt; d.value &lt;= 25 && d.type == \"noise\", \n      x: \"state\",\n      y: \"value\",\n      z: \"noise\",\n      text: d =&gt; \"f(s +\" + d.noise + \")\",\n      textAnchor: \"start\",\n      dx: 3\n    })),\n    Plot.line(points, {filter: d =&gt; d.value &lt;= 25 && d.type == \"minimum\", \n                       x:\"state\", y:\"value\", stroke:\"red\", strokeWidth: 4}),\n    Plot.text(points, Plot.selectLast({\n      filter: d =&gt; d.value &lt;= 25 && d.type == \"minimum\", \n      x: \"state\",\n      y: \"value\",\n      text: \"type\",\n      textAnchor: \"start\",\n      fill:\"red\",\n      dy: -10\n    }))\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 7.4: An example showing the minimum of \\(f(s)\\), \\(f(s+1)\\), \\(f(s+2)\\).\n\n\n\n\nWe first see an illustration of \\(F(s) = \\min\\{ f(s), f(s+1), f(s+2) \\}\\) in Figure 7.4. Note that the resulting function is not convex because \\(a\\) takes only discrete values. But the plot shows that the minimum will look like when we allow \\(a\\) to take continuous values.\nIf there were no constraint on \\(a\\), then the minimizer will be \\(a = s^* -\ns\\). If \\(s \\le s^*\\), then \\(a = s^* -s \\in \\reals_{\\ge 0}\\) is the minimizer for the constrained problem as well. On the other hand, if \\(s \\ge s^*\\), then the function \\(f(s + a)\\) is increasing as a function of \\(a\\). Hence, the minimizer for the constrained problem is \\(a = 0\\).\n\n\n\n\n\n\nProof of the structural result\n\n\n\n\n\nNow to prove the result, we define \\[ f_t(z) = py + H_t(z). \\] Then, \\[ V^*_t(s) = \\min_{a \\in \\reals_{\\ge 0}} \\bigl\\{ p(s + a) + H_t(s + a)\n\\bigr\\} - p s\n= \\min_{a \\in \\reals_{\\ge 0}} f_t(s+a) - p s.\n\\] As usual, we prove the result by backward induction. For \\(t=T\\), \\[\\bar Q^*_T(z) = \\EXP[ h(z - W) ] \\] which is convex because \\(h(z)\\) is convex. \\(f_T(z) = p z + Q^*_T(z)\\) is the sum of a linear function and convex function and is, therefore, convex. Then, by fact 2 above, \\[π^*_T(s) = \\arg \\min_{a \\in \\reals_{\\ge 0}} f_T(s+a) = \\max(s^*_T - s, 0)\n\\] and \\[V^*_T(s) = \\min_{a \\in \\reals_{\\ge 0}} f_T(s + a) - px =\n  \\begin{cases}\n    f_T(s) - p s, & \\text{if } s &gt; s^*_T \\\\\n    f_T(s^*_T) - px, & \\text{if } s \\le s^*_T.\n  \\end{cases}\n\\] Substituting \\(f_t(z) = p z + H_t(z)\\), we get that both \\(V^*_T\\) and \\(π^*_T\\) have the desired form and \\(V^*_T\\) is convex. This forms the basis of induction.\nNow assume that \\(V^*_{t+1}(s)\\) is convex and of the form \\eqref{eq:V}. Now note that, by fact 1, \\[ H_t(z) = \\EXP[ h(z - W) + V^*_{t+1}(z - W) ]\\] is convex. Hence, \\(f_t(z)\\) is convex. Therefore, by fact 2 above, \\[ π^*_t(s) = \\max(s^*_t - s, 0)\\] and \\(V^*_t(s)\\) is of the desired form and convex.\nThus, the result is holds by induction.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inventory Management</span>"
    ]
  },
  {
    "objectID": "mdps/inventory-management.html#exercises",
    "href": "mdps/inventory-management.html#exercises",
    "title": "7  Inventory Management",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 7.1 Consider the case when the state space \\(\\S\\) is equal to \\(\\integers\\) (i.e., all \\(S_t, A_t, W_t \\in \\integers\\)). In this case, we need the option of discrete convexity, which we explain below.\n\n\n\n\n\n\nDiscrete convexity or \\(L^{\\#}\\) convexity\n\n\n\n\n\nA function \\(f \\colon \\integers \\to \\reals\\) is called convex (or \\(L^{\\#}\\) convex) if for any \\(x \\in \\integers\\), \\[ f(x+1) + f(x-1) \\ge 2 f(x), \\] or, equivalently, for any \\(x, y \\in \\integers\\) \\[ f(x) + f(y) \\ge\n  f\\Bigl(\\Bigl\\lfloor \\frac{x+y}{2} \\Bigr\\rfloor\\Bigr)\n  +\n  f\\Bigl(\\Bigl\\lceil \\frac{x+y}{2} \\Bigr\\rceil\\Bigr).\\]\nIt can be easily seen that \\(L^{\\#}\\) functions satisfy the following properties:\n\nSum of \\(L^{\\#}\\) convex functions is \\(L^{\\#}\\) convex.\nPointwise limits of \\(L^{\\#}\\) convex functions is \\(L^{\\#}\\) convex.\n\nSee Murota (1998) and Chen (2017) for more details.\n\n\n\n\nArgue that the preliminary properties of convex functions established in the preliminary results also hold for discrete convex functions \\(f \\colon Z \\to \\reals\\).\nArgue that the structure of optimal policies continue to hold, i.e., there exists a sequence \\(\\{s_t^*\\}\\), \\(s_t \\in \\integers\\) such that the policy \\[\nπ^*_t(s) = \\begin{cases}\ns^*_t - s, & \\text{if $s \\le s_t^*$} \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nRemark: Exactly the same argument works if the state space \\(\\{ n \\Delta : n \\in \\integers \\}\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inventory Management</span>"
    ]
  },
  {
    "objectID": "mdps/inventory-management.html#notes",
    "href": "mdps/inventory-management.html#notes",
    "title": "7  Inventory Management",
    "section": "Notes",
    "text": "Notes\nInventory management models with deterministic demand were introduced by Harris (1913). The mathematical model of inventory management considered here was originally proposed by Arrow et al. (1952). The optimality of base-stock policy was established by Bellman et al. (1955). See the notes on infinite horizon version of this model to see how to find the threshold in closed form.\nA model where \\(\\S = \\reals\\) but \\(\\ALPHABET A = \\integers_{\\ge 0}\\) is considered in Veinott (1965). It’s generalization with non-zero ordering cost is considered in Tsitsiklis (1984).\n\n\n\n\nArrow, K.J., Harris, T., and Marschak, J. 1952. Optimal inventory policy. Econometrica 20, 1, 250–272. DOI: 10.2307/1907830.\n\n\nBellman, R., Glicksberg, I., and Gross, O. 1955. On the optimal inventory equation. Management Science 2, 1, 83–104. DOI: 10.1287/mnsc.2.1.83.\n\n\nChen, X. 2017. \\(L^{\\#}\\)-convexity and its applications in operations. Frontiers of Engineering Management 4, 3, 283. DOI: 10.15302/j-fem-2017057.\n\n\nHarris, F.W. 1913. How many parts to make at once. The magazine of management 10, 2, 135–152. DOI: 10.1287/opre.38.6.947.\n\n\nMurota, K. 1998. Discrete convex analysis. Mathematical Programming 83, 1–3, 313–371. DOI: 10.1007/bf02680565.\n\n\nTsitsiklis, J.N. 1984. Periodic review inventory systems with continuous demand and discrete order sizes. Management Science 30, 10, 1250–1254. DOI: 10.1287/mnsc.30.10.1250.\n\n\nVeinott, A.F. 1965. The optimal inventory policy for batch ordering. Operations Research 13, 3, 424–432. DOI: 10.1287/opre.13.3.424.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inventory Management</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html",
    "href": "mdps/monotone-mdps.html",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "",
    "text": "8.1 Stochastic dominance\nLet \\({\\rm M}^1\\) and \\({\\rm M}^2\\) denote the CDF of \\(\\mu^1\\) and \\(\\mu^2\\). Then \\eqref{eq:inc-prob} is equivalent to the following: \\[\\begin{equation}\\label{eq:cdf}\n  {\\rm M}^1_s \\le {\\rm M}^2_s, \\quad \\forall s \\in \\ALPHABET S.\n\\end{equation}\\] Thus, visually, \\(S^1 \\succeq_s S^2\\) means that the CDF of \\(S^1\\) lies below the CDF of \\(S^2\\).\nStochastic dominance is important due to the following property.\nWe now state some properties of stochastic monotonicity. See Pomatto et al. (2020) for details.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#stochastic-dominance",
    "href": "mdps/monotone-mdps.html#stochastic-dominance",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "",
    "text": "Let \\(\\ALPHABET S\\) be a totally ordered finite set, say \\(\\{1, \\dots, n\\}\\).Stochastic dominance is a partial order on random variables defined on totally ordered sets\n\n\n\n\n\n\n(First order) stochastic dominance\n\n\n\nSuppose \\(S^1\\) and \\(S^2\\) are \\(\\ALPHABET S\\) valued random variables where \\(S^1 \\sim \\mu^1\\) and \\(S^2 \\sim \\mu^2\\). We say \\(S^1\\) stochastically dominates \\(S^2\\) if for any \\(s \\in \\ALPHABET S\\), \\[\\begin{equation}\\label{eq:inc-prob}\n  \\PR(S^1 \\ge s) \\ge \\PR(S^2 \\ge s).\n\\end{equation}\\]\nStochastic domination is denoted by \\(S^1 \\succeq_s S^2\\) or \\(\\mu^1 \\succeq_s \\mu^2\\).\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\\([0, 0, \\tfrac 14, \\tfrac 34]\n\\succeq_s\n[0, \\tfrac 14, \\tfrac 34, 0]\n\\succeq_s\n[\\tfrac 14, \\tfrac 34, 0, 0]\\)\n\\([0,0,0,1]\n\\succeq_s\n[0,0, \\tfrac 12, \\tfrac 12]\n\\succeq_s\n[0, \\tfrac 13, \\tfrac 13, \\tfrac 13]\n\\succeq_s\n[\\tfrac 14, \\tfrac 14, \\tfrac 14, \\tfrac 14]\\)\n\n\n\n\n\nTheorem 8.1 Let \\(f \\colon \\ALPHABET S \\to \\reals\\) be a (weakly) increasing function and \\(S^1 \\sim \\mu^1\\) and \\(S^2 \\sim \\mu^2\\) are random variables defined on \\(\\ALPHABET S\\). Then \\(S^1 \\succeq_s S^2\\) if and only if \\[\\begin{equation}\\label{eq:inc-fun}\n  \\EXP[f(S^1)] \\ge \\EXP[f(S^2)].\n\\end{equation}\\]\n\n\n\n\n\n\n\nAbel’s lemma or summation by parts\n\n\n\nFor any two sequences \\(\\{f_k\\}_{k \\ge 1}\\) and \\(\\{g_k\\}_{k \\ge 1}\\), \\[\\sum_{k=m}^n f_k(g_{k+1} - g_{k}) =\n(f_n g_{n+1} - f_m g_m) + \\sum_{k=m+1}^n g_k(f_{k+1} - f_k).\\]\nSummation by parts may be viewed as the discrete analog of integration by parts: \\[\n\\int_a^b f(x) g'(x) dx = f(x) g(x)\\Big|_{a}^{b} - \\int_{a}^{b} f'(x)g(x)dx.\n\\] An alternative form which is sometimes useful is: \\[\nf_n g_n - f_m g_m =\n\\sum_{k=m}^{n-1} f_k \\Delta g_k\n+\n\\sum_{k=m}^{n-1} g_k \\Delta f_k\n+\n\\sum_{k=m}^{n-1} \\Delta f_k \\Delta g_k.\n\\]\n\n\n\n\n\n\n\n\nProof (stochastic dominance implies monotone expectations)\n\n\n\n\n\nFor the ease of notation, let \\(f_i\\) to denote \\(f(i)\\) and define \\({\\rm M}^1_0 = {\\rm M}^2_0 = 0\\). Consider the following: \\[\\begin{align*}\n    \\sum_{i=1}^n f_i \\mu^1_i\n    &= \\sum_{i=1}^n f_i ({\\rm M}^1_i - {\\rm M}^1_{i-1})\n    \\\\\n    &\\stackrel{(a)}= \\sum_{i=1}^n {\\rm M}^1_{i-1} (f_{i-1} - f_{i}) + f_n {\\rm M}^1_n\n    \\\\\n    &\\stackrel{(b)}{\\ge}\n    \\sum_{i=1}^n {\\rm M}^2_{i-1} (f_{i-1} - f_{i}) + f_n {\\rm M}^2_n\n    \\\\\n    &\\stackrel{(a)}= \\sum_{i=1}^n f_i ({\\rm M}^2_i - {\\rm M}^2_{i-1})\n    \\\\\n    &= \\sum_{i=1}^n f_i \\mu_i,\n\\end{align*}\\] which completes the proof. In the above equations, both steps marked \\((a)\\) use summation by parts and \\((b)\\) uses the following facts:\n\nFor any \\(i\\), \\({\\rm M}^1_{i-1} \\le {\\rm M}^2_{i-1}\\) (because of \\eqref{eq:cdf}) and \\(f_{i-1} - f_{i} &lt; 0\\) (because \\(f\\) is increasing function). Thus, \\[{\\rm M}^1_{i-1}(f_{i-1} - f_i) \\ge {\\rm M}^2_{i-1}(f_{i-1} - f_i). \\]\n\\({\\rm M}^1_n = {\\rm M}^2_n = 1\\).\n\n\n\n\n\n\n\n\n\n\nProof (monotone expectations implies stochastic monotonicity)\n\n\n\n\n\nSuppose for any increasing function \\(f\\), \\eqref{eq:inc-fun} holds. Given any \\(i \\in \\{1, \\dots, n\\}\\), define the function \\(f_i(k) = \\IND\\{k &gt; i\\}\\), which is an increasing function of \\(k\\). Then, \\[ \\EXP[f_i(S)] = \\sum_{k=1}^n f_i(k) \\mu^1_k = \\sum_{k &gt; i} \\mu^1_k = 1 - {\\rm M}^1_i.\n\\] By a similar argument, we have \\[ \\EXP[f_i(S^2)] = 1 - {\\rm M}^2_i. \\] Since \\(\\EXP[f_i(S)] \\ge \\EXP[f_i(S^2)]\\), we have that \\({\\rm M}^1_i \\le {\\rm M}^2_i\\).\n\n\n\n\n\nIf \\(X \\succeq_s Y\\) and \\(Z\\) is a random variable independent of \\(X\\) and \\(Y\\), then \\(X + Z \\succeq_s Y + Z\\).\nGiven a random variable \\(Z\\), we say that \\(Z'\\) is noisier than \\(Z\\) if there exists an independent random variable \\(W\\) such that \\(Z' = Z + W\\). If \\(X + Z \\succeq_s Y + Z\\) for some \\(Z\\) independent of \\(X\\) and \\(Y\\), then \\(X + Z' \\succeq_s Y + Z'\\) for any independent \\(Z'\\) noisier than \\(Z\\).\nSuppose \\(X\\) and \\(Y\\) are random variables such that \\(\\EXP[X] &gt; \\EXP[Y]\\). Then, there exists a random variable \\(Z\\), independent of \\(X\\) and \\(Y\\), such that \\[ X + Z \\succeq_s Y + Z. \\]\nA real-valued function \\(C\\) defined on the space of random variables with finite moments that satisfies the following properties:\n\nCertainty: \\(C(1) = 1\\)\nMonotonicity: If \\(X \\succeq_s Y\\) then \\(C(X) \\ge C(Y)\\).\nAdditivity: If \\(X\\) and \\(Y\\) are independent then \\(C(X+Y) = C(X) + C(Y)\\)\n\nif and only if \\(C(X) = \\EXP[X]\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#stochastic-monotonicity",
    "href": "mdps/monotone-mdps.html#stochastic-monotonicity",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "8.2 Stochastic monotonicity",
    "text": "8.2 Stochastic monotonicity\nStochastic monotonicity extends the notion of stochastic dominance to Markov chains. Suppose \\(\\ALPHABET S\\) is a totally ordered set and \\(\\{S_t\\}_{t \\ge 1}\\) is a time-homogeneous Markov chain on \\(\\ALPHABET S\\) with transition probability matrix \\(P\\). Let \\(P_i\\) denote the \\(i\\)-th row of \\(P\\). Note that \\(P_i\\) is a PMF.\n\n\n\n\n\n\nStochastic monotonicity\n\n\n\nA Markov chain with transition matrix \\(P\\) is stochastically monotone if \\[ P_i \\succeq_s P_j, \\quad \\forall i &gt; j. \\]\n\n\nAs an example, a birth death Markov chain (without self loops) is stochastically monotone, e.g., for any \\(p + q = 1\\), the following is stochastic monotone \\[\\MATRIX{ q & p & 0 & 0 & 0 \\\\\n           q & 0 & p & 0 & 0 \\\\\n           0 & q & 0 & p & 0 \\\\\n           0 & 0 & q & 0 & p \\\\\n           0 & 0 & 0 & q & p }\\]\nAn immediate implication of the definition of stochastic monotinicity is the following.\n\nTheorem 8.2 Let \\(\\{S_t\\}_{t \\ge 1}\\) be a Markov chain with transition matrix \\(P\\) and \\(f \\colon \\ALPHABET S \\to \\reals\\) is a weakly increasing function. Then, for any \\(s^1, s^2 \\in \\ALPHABET S\\) such that \\(s^1 &gt; s^2\\), \\[ \\EXP[f(S_{t+1}) | S_t = s^1] \\ge \\EXP[ f(S_{t+1}) | S_t = s^2], \\] if and only if \\(P\\) is stochatically monotone.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotonicity-of-value-functions",
    "href": "mdps/monotone-mdps.html#monotonicity-of-value-functions",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "8.3 Monotonicity of value functions",
    "text": "8.3 Monotonicity of value functions\n\nTheorem 8.3 Consider an MDP where the state space \\(\\ALPHABET S\\) is totally ordered. Suppose the following conditions are satisfied.\nC1. For every \\(a \\in \\ALPHABET A\\), the per-step cost \\(c_t(s,a)\\) is weakly inceasing in \\(s\\).\nC2. For every \\(a \\in \\ALPHABET A\\), the transition matrix \\(P(a)\\) is stochastically monotone.\nThen, the value function \\(V^*_t(s)\\) is weakly increasing in \\(s\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe result above also applies to models with continuous (and totally ordered) state space provided the measurable selection conditions hold so that the arg min at each step of the dynamic program is attained.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe proceed by backward induction. By definition, \\(V^*_{T+1}(s) = 0\\), which is weakly increasing. This forms the basis of induction. Assume that \\(V^*_{t+1}(s)\\) is weakly increasing. Now consider, \\[Q^*_t(s,a) = c_t(s,a) + \\EXP[V^*_{t+1}(S_{t+1}) | S_t = s, A_t = a].\\] For any \\(a \\in \\ALPHABET A\\), \\(Q^*_t(s,a)\\) is a sum of two weakly increasing functions in \\(s\\); hence \\(Q^*_t(s,a)\\) is weakly increasing in \\(s\\).\nNow consider \\(s_1, s_2 \\in \\ALPHABET S\\) such that \\(s_1 &gt; s_2\\). Suppose \\(a_1^*\\) is the optimal action at state \\(s_1\\). Then \\[\n  V^*_t(s^1) = Q^*_t(s^1, a_1^*) \\stackrel{(a)}\\ge Q^*_t(s^2,a_1^*) \\stackrel{(b)}\\ge V^*_t(s_2),\n\\] where \\((a)\\) follows because \\(Q^*_t(\\cdot, u^*)\\) is weakly increasing and \\((b)\\) follows from the definition of the value function.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#submodularity",
    "href": "mdps/monotone-mdps.html#submodularity",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "8.4 Submodularity",
    "text": "8.4 Submodularity\n\n\n\n\n\n\nSubmodularity\n\n\n\nLet \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be partially ordered sets. A function \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) is called submodular if for any \\(x^+ \\ge x^-\\) and \\(y^+ \\ge y^-\\), we have \\[\\begin{equation}\\label{eq:submodular}\n  f(x^+, y^+) + f(x^-, y^-) \\le f(x^+, y^-) + f(x^-, y^+).\n\\end{equation}\\]\nThe function is called supermodular if the inequality in \\eqref{eq:submodular} is reversed.\n\n\nA continuous and differentiable function on \\(\\reals^2\\) is submodular iff \\[ \\frac{ \\partial^2 f(x,y) }{ \\partial x \\partial y } \\le 0,\n  \\quad \\forall x,y.\n\\] If the inequality is reversed, then the function is supermodular.\nSubmodularity is a useful property because it implies monotonicity of the arg min.\n\nTheorem 8.4 Let \\(\\ALPHABET X\\) be a partially ordered set, \\(\\ALPHABET Y\\) be a totally ordered set, and \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) be a submodular function. Suppose that for all \\(x\\), \\(\\arg \\min_{y \\in \\ALPHABET Y} f(x,y)\\) exists. Then, \\[\n  π^*(x) := \\max \\{ y^* \\in \\arg \\min_{y \\in \\ALPHABET Y} f(x,y) \\}\n\\] is weakly increasing in \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(x^+, x^- \\in \\ALPHABET X\\) such that \\(x^+ \\ge x^-\\). Since \\(f\\) is submodular, for any \\(y \\le π^*(x^-)\\), we have \\[\\begin{equation}\\label{eq:1}\n  f(x^+, π^*(x^-)) - f(x^+, y) \\le f(x^-, π^*(x^-)) - f(x^-, y) \\le 0,\n\\end{equation}\\] where the last inequality follows because \\(π^*(x^-)\\) is the arg min of \\(f(x^-, y)\\). Eq. \\eqref{eq:1} implies that for all \\(y \\le π^*(x^-)\\), \\[\n  f(x^+, π^*(x^-)) \\le f(x^+, y).\n\\] Thus, \\(π^*(x^+) \\ge π^*(x^-)\\).\n\n\n\nThe analogue of Theorem 8.4 for supermodular functions is as follows.\n\nTheorem 8.5 Let \\(\\ALPHABET X\\) be a partially ordered set, \\(\\ALPHABET Y\\) be a totally ordered set, and \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) be a supermodular function. Suppose that for all \\(x\\), \\(\\arg \\min_{y \\in \\ALPHABET Y} f(x,y)\\) exists. Then, \\[\n  π^*(x) := \\min \\{ y^* \\in \\arg \\min_{y \\in \\ALPHABET Y} f(x,y) \\}\n\\] is weakly decreasing in \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof is similar to Theorem 8.4.\nConsider \\(x^+, x^- \\in \\ALPHABET X\\) such that \\(x^+ \\ge x^-\\). Since \\(f\\) is supermodular, for any \\(y \\ge π^*(x^-)\\), we have \\[\\begin{equation}\\label{eq:2}\n  f(x^+, y) - f(x^+, π^*(x^-)) \\ge f(x^-, y) - f(x^-, π^*(x^-)) \\ge 0,\n\\end{equation}\\] where the last inequality follows because \\(π^*(x^-)\\) is the arg min of \\(f(x^-,\ny)\\). Eq. \\eqref{eq:2} implies that for all \\(y \\ge π^*(x^-)\\), \\[\n  f(x^+, y) \\ge f(x^+, π^*(x^-)).\n\\] Thus, \\(π^*(x^+) \\le π^*(x^-)\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotonicity-of-optimal-policy",
    "href": "mdps/monotone-mdps.html#monotonicity-of-optimal-policy",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "8.5 Monotonicity of optimal policy",
    "text": "8.5 Monotonicity of optimal policy\n\nTheorem 8.6 Consider an MDP where the state space \\(\\ALPHABET S\\) and the action space \\(\\ALPHABET A\\) are totally ordered. Suppose that, in addition to (C1) and (C2), the following condition is satisfied.\nC3. For any weakly increasing function \\(v\\), \\[ c_t(s,a) + \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\] is submodular in \\((s,a)\\).\nLet \\(π^*_t(s) = \\max\\{ a^* \\in \\arg \\min_{a \\in \\ALPHABET A} Q^*_t(s,a) \\}\\). Then, \\(π^*(s)\\) is weakly increasing in \\(s\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConditions (C1) and (C2) imply that the value function \\(V^*_{t+1}(s)\\) is weakly increasing. Therefore, condition (C3) implies that \\(Q^*_t(s,a)\\) is submodular in \\((s,a)\\). Therefore, the arg min is weakly increasing in \\(x\\)\n\n\n\nIt is difficult to verify condition (C3). The following conditions are sufficient for (C3).\n\nLemma 8.1 Consider an MDP with totally ordered state and action spaces. Suppose\n\n\\(c_t(s,a)\\) is submodular in \\((s,a)\\).\nFor all \\(s' \\in \\ALPHABET S\\), \\(H(s' | s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a)\\) is submodular in \\((s,a)\\).\n\nThe condition (C3) of Theorem 8.6 holds.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s^+, s^- \\in \\ALPHABET S\\) and \\(a^+, a^- \\in \\ALPHABET A\\) such that \\(s^+ &gt; s^-\\) and \\(a^+ &gt; a^-\\). Define\n\\[\\begin{align*}\n  μ_1(s) &= \\tfrac 12 P_{s^- s}(a^-) + \\tfrac 12 P_{s^+ s}(a^+), \\\\\n  μ_2(s) &= \\tfrac 12 P_{s^- s}(a^+) + \\tfrac 12 P_{s^+ s}(a^-).\n\\end{align*}\\] Since \\(H(s' | s,a)\\) is submodular, we have \\[ H(s' | s^+, a^+) + H(s' | s^-, a^-) \\le H(s' | s^+, a^-) + H(s' | s^-, a^+) \\] or equivalently, \\[\\sum_{z \\le s'} \\big[ P_{s^+ z}(a^+) + P_{s^- z}(a^-) \\big]\n  \\ge\n  \\sum_{z \\le s'} \\big[ P_{s^+ z}(a^-) + P_{s^- z}(a^+) \\big]. \\] which implies \\[ M_1(s') \\ge M_2(s')\\] where \\(M_1\\) and \\(M_2\\) are the CDFs of \\(μ_1\\) and \\(μ_2\\). Thus, \\(μ_1 \\preceq_s\nμ_2\\).\nHence, for any weakly increasing function \\(v \\colon \\ALPHABET S \\to \\reals\\), \\[ \\sum_{s' \\in \\ALPHABET S} μ_1(s') v(s') \\le\n   \\sum_{s' \\in \\ALPHABET S} μ_2(s') v(s').\\] Or, equivalently, \\[H(s^+, a^+) + H(s^-, a^-) \\le H(s^-, a^+) + H(s^+, a^-)\\] where \\(H(s,a) = \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\).\nTherefore, \\(c_t(s,a) + H_t(s,a)\\) is submodular in \\((s,a)\\).\n\n\n\nThe analogue of Theorem 8.6 for supermodular functions is as follows.\n\nTheorem 8.7 Consider an MDP where the state space \\(\\ALPHABET S\\) and the action space \\(\\ALPHABET A\\) are totally ordered. Suppose that, in addition to (C1) and (C2), the following condition is satisfied.\nC4. For any weakly increasing function \\(v\\), \\[ c_t(s,a) + \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\] is supermodular in \\((s,a)\\).\nLet \\(π^*_t(s) = \\min\\{ a^* \\in \\arg \\min_{a \\in \\ALPHABET S} Q^*_t(s,a) \\}\\). Then, \\(π^*(s)\\) is weakly decreasing in \\(s\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConditions (C1) and (C2) imply that the value function \\(V^*_{t+1}(s)\\) is weakly increasing. Therefore, condition (C4) implies that \\(Q^*_t(s,a)\\) is supermodular in \\((s,a)\\). Therefore, the arg min is decreasing in \\(s\\)\n\n\n\nIt is difficult to verify condition (C4). The following conditions are sufficient for (C4).\n\nLemma 8.2 Consider an MDP with totally ordered state and action spaces. Suppose\n\n\\(c_t(s,a)\\) is supermodular in \\((s,a)\\).\nFor all \\(s' \\in \\ALPHABET S\\), \\(H(s' | s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a)\\) is supermodular in \\((s,a)\\).\n\nThe condition (C4) of Theorem 8.7 holds.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s^+, s^- \\in \\ALPHABET S\\) and \\(a^+, a^- \\in \\ALPHABET A\\) such that \\(s^+ &gt; s^-\\) and \\(a^+ &gt; a^-\\). Define\n\\[\\begin{align*}\n  μ_1(s) &= \\tfrac 12 P_{s^- s}(a^-) + \\tfrac 12 P_{s^+ s}(a^+), \\\\\n  μ_2(s) &= \\tfrac 12 P_{s^- s}(a^+) + \\tfrac 12 P_{s^+ s}(a^-).\n\\end{align*}\\] Since \\(H(s' | s,a)\\) is supermodular, we have \\[ H(s' | s^+, a^+) + H(s' | s^-, a^-) \\ge H(s' | s^+, a^-) + H(s' | s^-, a^+) \\] or equivalently, \\[\\sum_{s' \\le s'} \\big[ P_{s^+ s'}(a^+) + P_{s^- s'}(a^-) \\big]\n  \\le\n  \\sum_{s' \\le s'} \\big[ P_{s^+ s'}(a^-) + P_{s^- s'}(a^+) \\big]. \\] which implies \\[ M_1(s') \\le M_2(s')\\] where \\(M_1\\) and \\(M_2\\) are the CDFs of \\(μ_1\\) and \\(μ_2\\). Thus, \\(μ_1 \\succeq_s\nμ_2\\).\nHence, for any weakly increasing function \\(v \\colon \\ALPHABET S \\to \\reals\\), \\[ \\sum_{s' \\in \\ALPHABET S} μ_1(s') v(s') \\ge\n   \\sum_{s' \\in \\ALPHABET S} μ_2(s') v(s').\\] Or, equivalently, \\[H(s^+, a^+) + H(s^-, a^-) \\ge H(s^-, a^+) + H(s^+, a^-)\\] where \\(H(s,a) = \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\).\nTherefore, \\(c_t(s,a) + H_t(s,a)\\) is supermodular in \\((s,a)\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#constraints-on-actions",
    "href": "mdps/monotone-mdps.html#constraints-on-actions",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "8.6 Constraints on actions",
    "text": "8.6 Constraints on actions\nIn the results above, we have assumed that the action set \\(\\ALPHABET A\\) is the same for all states. The results also extend to the case when the action at state \\(s\\) must belong to some set \\(\\ALPHABET A(s)\\) provided the following conditions are satisfied:\n\nFor any \\(s \\ge s'\\), \\(\\ALPHABET A(s) \\supseteq \\ALPHABET A(s')\\)\nFor any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A(s)\\), \\(a' &lt; a\\) implies that \\(a' \\in \\ALPHABET A(s)\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotone-dynamic-programming",
    "href": "mdps/monotone-mdps.html#monotone-dynamic-programming",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "8.7 Monotone dynamic programming",
    "text": "8.7 Monotone dynamic programming\nIf we can establish that the optimal policy is monontone, then we can use this structure to implement the dynamic program more efficient. Suppose \\(\\ALPHABET S = \\{1, \\dots, n\\}\\) and \\(\\ALPHABET A = \\{1, \\dots. m\\}\\). The main idea is as follows. Suppose \\(V^*_{t+1}(\\cdot)\\) has been caclulated. Insead of computing \\(Q^*_t(s,a)\\) and \\(V^*_t(s)\\), proceed as follows:\n\nSet \\(s = 1\\) and \\(α = 1\\).\nFor all \\(u \\in \\{α, \\dots, m\\}\\), compute \\(Q^*_t(s,a)\\) as usual.\nCompute\n\\[V^*_t(s) = \\min_{ α \\le a \\le m } Q^*_t(s,a)\\]\nand set\n\\[π^*_t(s) = \\max \\{ a \\in \\{α, \\dots, m\\} : V^*_t(s) = Q^*_t(s,a) \\}.\\]\nIf \\(s = n\\), then stop. Otherwise, set \\(α = π^*_t(s)\\) and \\(s = s+1\\) and go to step 2.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#example-a-machine-replacement-model",
    "href": "mdps/monotone-mdps.html#example-a-machine-replacement-model",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "8.8 Example: A machine replacement model",
    "text": "8.8 Example: A machine replacement model\nLet’s revisit the machine replacement problem of Exercise 5.4. For simplicity, we’ll assume that \\(n = ∞\\), i.e., the state space is countable. In this case, the transition matrices are given by \\[ P_{sz}(0) = \\begin{cases}\n  0, & z &lt; s \\\\\n  μ_{z - s}, & z \\ge s\n\\end{cases}\n\\quad\\text{and}\\quad\nP_sz(1) = μ_z.\n\\] where \\(μ\\) is the PMF of \\(W\\).\n\nProposition 8.1 For the machine replacement problem, there exist a series of thresholds \\(\\{s^*_t\\}_{t = 1}^T\\) such that the optimal policy at time \\(t\\) is a threshold policy with threshold \\(s_t\\), i.e., \\[\n  π^*_t(s) = \\begin{cases}\n  0 & \\text{if $s &lt; s_t^*$} \\\\\n  1 & \\text{otherwise}\n\\end{cases}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by verifying conditions (C1)–(C4) to establish that the optimal policy is monotone.\nC1. For \\(a = 0\\), \\(c(s,0) = h(s)\\), which is weakly increasing by assumption. For \\(a = 1\\), \\(c(s,1) = K\\), which is trivially weakly increasing.\nC2. For \\(a = 0\\), \\(P(0)\\) is stochastically monotone (because the CDF of \\(P(\\cdot | s, 0)\\) lies above the CDF of \\(P(\\cdot | s+1, 0)\\)). For \\(a = 1\\), all rows of \\(P(1)\\) are the same; therefore \\(P(1)\\) is stochastically monotone.\nSince (C1) and (C2) are satisfied, by Theorem 8.3, we can assert that the value function is weakly increasing.\nC3. \\(c(s,1) - c(s,0) = K - h(s)\\), which is weakly decreasing in \\(s\\). Therefore, \\(c(s,a)\\) is submodular in \\((s,a)\\).\nC4. Recall that \\(H(s'|s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a).\\) Therefore,\n\\[H(s'|s,0) = 1 - \\sum_{z = s}^{s'} μ_{z -s} = 1 - \\sum_{k = 0}^{s' - s} μ_k\n= 1 - M_{s' - s},\\] where \\(M\\) is the CMF of \\(μ\\), and \\[H(s'|s,1) = 1 - \\sum_{z \\le s'} μ_z = 1 - M_{s'},\\]\nTherefore, \\(H(s'|s,1) - H(s'|s,0) = M_{s'-s} - M_{s'}\\). For any fixed \\(s'\\), \\(H(s'|s,1) - H(s'|s,0)\\) is weakly decreasing in \\(s\\). There \\(H(s'|s,a)\\) is submodular in \\((s,a)\\).\nSince (C1)–(C4) are satisfied, the optimal policy is weakly increasing in~\\(s\\). Since there are only two actions, it means that for every time, there exists a state \\(s^*_t\\) with the property that if \\(s\\) exceeds \\(s^*_t\\), the optimal decision is to replace the machine; and if \\(s \\le s^*_t\\), then the optimal decision is to operate the machine for another period.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#exercises",
    "href": "mdps/monotone-mdps.html#exercises",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 8.1 Let \\(T\\) denote a upper triangular matrix with 1’s on or below the diagonal and 0’s above the diagonal. Then \\[ T^{-1}_{ij} = \\begin{cases}\n  1, & \\text{if } i = j, \\\\\n-1, & \\text{if } i = j - 1, \\\\\n  0, & \\text{otherwise}.\n\\end{cases}\\]\nFor example, for a \\(4 \\times 4\\) matrix \\[\n  T = \\MATRIX{1 & 1 & 1 & 1 \\\\ 0 & 1 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 1},\n  \\quad\n  T^{-1} = \\MATRIX{1 & -1 & 0 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 1 & -1 \\\\\n  0 & 0 & 0 & 1 }.\n\\]\nShow the following:\n\nFor any two PMFs \\(μ^1\\) and \\(μ^2\\), \\(\\mu^1 \\succeq_s \\mu^2\\) iff \\(\\mu^1 T \\ge \\mu^2 T\\).\nA Markov transition matrix \\(P\\) is stochastic monotone iff \\(T^{-1} P T \\ge 0\\).\n\n\n\nExercise 8.2 Show that the following are equivalent:\n\nA transition matrix \\(P\\) is stochastically monotone\nFor any two PMFs \\(μ^1\\) and \\(μ^2\\), if \\(\\mu^1 \\succeq_s \\mu^2\\) then \\(\\mu^1P \\succeq_s \\mu^2P\\).\n\n\n\nExercise 8.3 Show that if two transition matrices \\(P\\) and \\(Q\\) have the same dimensions and are stochastically monotone, then so are:\n\n\\(\\lambda P + (1 - \\lambda) Q\\), where \\(\\lambda \\in (0,1)\\).\n\\(P Q\\)\n\\(P^k\\), for \\(k \\in \\integers_{&gt; 0}\\).\n\n\n\nExercise 8.4 Let \\(\\mu_t\\) denote the distribution of a Markov chain at time \\(t\\). Suppose \\(\\mu_0 \\succeq_s \\mu_1\\). Then \\(\\mu_t \\succeq_s \\mu_{t+1}\\).\n\n\nExercise 8.5 (Testing submodularity for functions defined on integers) Suppose a function \\(f \\colon \\integers \\times \\integers \\to \\reals\\) satisfies \\[ f(x+1, y+1)  - f(x+1, y) \\le f(x, y+1) - f(x, y) \\] for all \\(x, y \\in \\integers\\). Then, show that \\(f\\) is a submodular function.\n\n\nExercise 8.6 Show that sum of submodular functions is submodular. Is the product of submodular functions submodular?\n\n\nExercise 8.7 Consider the example of machine repair presented in Example 5.2. Prove that the optimal policy for that model is weakly increasing.\n\n\nExercise 8.8 Suppose the state space \\(\\ALPHABET S\\) is a symmetric subset of integers of the form \\(\\{-L, -L + 1, \\dots, L-1, L\\}\\) and the action space \\(\\ALPHABET A\\) is discrete. Let \\(\\ALPHABET S_{\\ge 0}\\) denote the set \\(\\{0, \\dots, L\\}\\).\nLet \\(P(a)\\) denote the controlled transition matrix and \\(c_t(s,a)\\) denote the per-step cost. To avoid ambiguity, we define the optimal policy as \\[\nπ^*_t(s) = \\begin{cases}\n    \\max\\bigl\\{ a' \\in \\arg\\min_{a \\in \\ALPHABET A} Q^*_t(s,a) \\bigr\\},\n    & \\text{if } s \\ge 0 \\\\\n    \\min\\bigl\\{ a' \\in \\arg\\min_{a \\in \\ALPHABET A} Q^*_t(s,a) \\bigr\\},\n    & \\text{if } s &lt; 0\n\\end{cases}\\] The purpose of this exercise is to identify conditions under which the value function and the optimal policy are even and :quasi-convex. We do so using the following steps.\n\nWe say that the transition probability matrix \\(P(a)\\) is even if for all \\(s, s' \\in \\ALPHABET S\\), \\(P(s'|s,a) = P(-s'|-s,a)\\). Prove the following result.\n\n\nProposition 8.2 Suppose the MDP satisfies the following properties:\n(A1) For every \\(t\\) and \\(a \\in \\ALPHABET A\\), \\(c_t(s,a)\\) is even function of \\(s\\).\n(A2) For every \\(a \\in \\ALPHABET A\\), \\(P(a)\\) is even.\nThen, for all \\(t\\), \\(V^*_t\\) and \\(π^*_t\\) are even functions.\n\n\nGiven any probability mass function \\(μ\\) on \\(\\ALPHABET S\\), define the folded probability mass function \\(\\tilde μ\\) on \\(\\ALPHABET S_{\\ge 0}\\) as follows: \\[ \\tilde μ(s) = \\begin{cases}\n   μ(0), & \\text{if } s = 0 \\\\\n   μ(s) + μ(-s), & \\text{if } s &gt; 0.\n\\end{cases} \\]\n\nFor ease of notation, we use \\(\\tilde μ = \\mathcal F μ\\) to denote this folding operation. Note that an immediate consequence of the definition is the following (you don’t have to prove this).\n\nLemma 8.3 If \\(f \\colon \\ALPHABET S \\to \\reals\\) is even, then for any probability mass function \\(μ\\) on \\(\\ALPHABET S\\) and \\(\\tilde μ = \\mathcal F μ\\), we have \\[\n  \\sum_{s \\in \\ALPHABET S} f(s) μ(s) =\n  \\sum_{s \\in \\ALPHABET S_{\\ge 0}} f(s) \\tilde μ(s). \\]\n\nThus, the expectation of the function \\(f \\colon \\ALPHABET S \\to \\reals\\) with respect to the PMF \\(μ\\) is equal to the expectation of the function \\(f \\colon \\ALPHABET S_{\\ge 0} \\to \\reals\\) with respect to the PMF \\(\\tilde μ = \\mathcal F μ\\).\nNow given any probability transition matrix \\(P\\) on \\(\\ALPHABET S\\), we can define a probability transition matrix \\(\\tilde P\\) on \\(\\ALPHABET S_{\\ge 0}\\) as follows: for any \\(s \\in \\ALPHABET S\\), \\(\\tilde P_s = \\mathcal F P_s\\), where \\(P_s\\) denotes the \\(s\\)-th row of \\(P\\). For ease of notation, we use \\(\\tilde P = \\mathcal F P\\) to denote this relationship.\nNow prove the following:\n\nProposition 8.3 Given the MDP \\((\\ALPHABET S, \\ALPHABET A, P, \\{c_t\\})\\), define the folded MDP as \\((\\ALPHABET S_{\\ge 0}, \\ALPHABET A, \\tilde P,\n\\{c_t\\})\\), where \\(\\tilde P(a) = \\mathcal F P(a)\\) for all \\(a \\in \\ALPHABET\nA\\). Let \\(\\tilde Q^*_t \\colon \\ALPHABET S_{\\ge 0} \\times \\ALPHABET A \\to\n\\reals\\), \\(\\tilde V^*_t \\colon \\ALPHABET S_{\\ge 0} \\to \\reals\\) and \\(\\tilde\nπ_t^* \\colon \\ALPHABET S_{\\ge 0} \\to \\ALPHABET A\\) denote the action-value function, value function and the policy of the folded MDP. Then, if the original MDP satisfies conditions (A1) and (A2) then, for any \\(s \\in\n\\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), \\[ Q^*_t(s,a) = \\tilde Q^*_t(|s|, a),\n\\quad\n  V^*_t(s) = \\tilde V^*_t(|s|),\n\\quad\n  π_t^*(s) = \\tilde π_t^*(|s|).\n\\]\n\n\nThe result of the previous part implies that if the value function \\(\\tilde V^*_t\\) and the policy \\(\\tilde π^*_t\\) are monotone increasing, then the value function \\(V^*_t\\) and the policy \\(π^*_t\\) are even and quasi-convex. This gives us a method to verify if the value function and optimal policy are even and quasi-convex.\nNow, recall the model of the Internet of Things presented in Exercise 5.3. The numerical experiments done in that exercise suggest that the value function and the optimal policy are even and quasi-convex. Prove that this is indeed the case.\nNow suppose the distribution of \\(W_t\\) is not Gaussian but is some general probability density \\(\\varphi(\\cdot)\\) and the cost function is \\[ c(e,a) = \\lambda a + (1 - a) d(e). \\] Find conditions on \\(\\varphi\\) and \\(d\\) such that the value function and optimal policy are even and quasi-convex.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-mdps.html#notes",
    "href": "mdps/monotone-mdps.html#notes",
    "title": "8  Monotonicity of value function and optimal policies",
    "section": "Notes",
    "text": "Notes\nStochastic dominance has been employed in various areas of economics, finance, and statistics since the 1930s. See Levy (1992) and Levy (2015) for detailed overviews. The notion of stochastic monotonicity for Markov chains is due to Daley (1968). For a generalization of stochastic monotonicity to continuous state spaces, see Serfozo (1976). The characterization of stochastic monotonicity in Exercise 8.1–Exercise 8.4 are due to Keilson and Kester (1977).\nRoss (1974) has an early treatment of monotonicity of optimal policies. The general theory was developed by Topkis (1998). An alternative treatment for queueing models is presented in Koole (2006). The presentation here follows Puterman (2014).\nThe properties here are derived for finite horizon models. General conditions under which such properties extend to infinite horizon models are presented in Smith and McCardle (2002).\nThere are many recent papers which leverage the structural properties of value functions and optimal policy in reinforcement learning. For example Kunnumkal and Topaloglu (2008) and Fu and Schaar (2012) present variants of Q-learning which exploit properties of the value function and Roy et al. (2022) present a variant of policy-learning which exploits properties of the optimal policy.\nExercise 8.8 is from Chakravorty and Mahajan (2018).\n\n\n\n\nChakravorty, J. and Mahajan, A. 2018. Sufficient conditions for the value function and optimal strategy to be even and quasi-convex. IEEE Transactions on Automatic Control 63, 11, 3858–3864. DOI: 10.1109/TAC.2018.2800796.\n\n\nDaley, D.J. 1968. Stochastically monotone markov chains. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 10, 4, 305–317. DOI: 10.1007/BF00531852.\n\n\nFu, F. and Schaar, M. van der. 2012. Structure-aware stochastic control for transmission scheduling. IEEE Transactions on Vehicular Technology 61, 9, 3931–3945. DOI: 10.1109/tvt.2012.2213850.\n\n\nKeilson, J. and Kester, A. 1977. Monotone matrices and monotone markov processes. Stochastic Processes and their Applications 5, 3, 231–241.\n\n\nKoole, G. 2006. Monotonicity in markov reward and decision chains: Theory and applications. Foundations and Trends in Stochastic Systems 1, 1, 1–76. DOI: 10.1561/0900000002.\n\n\nKunnumkal, S. and Topaloglu, H. 2008. Exploiting the structural properties of the underlying markov decision problem in the q-learning algorithm. INFORMS Journal on Computing 20, 2, 288–301. DOI: 10.1287/ijoc.1070.0240.\n\n\nLevy, H. 1992. Stochastic dominance and expected utility: Survey and analysis. Management Science 38, 4, 555–593. DOI: 10.1287/mnsc.38.4.555.\n\n\nLevy, H. 2015. Stochastic dominance: Investment decision making under uncertainty. Springer. DOI: 10.1007/978-3-319-21708-6.\n\n\nPomatto, L., Strack, P., and Tamuz, O. 2020. Stochastic dominance under independent noise. Journal of Political Economy 128, 5, 1877–1900. DOI: 10.1086/705555.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887.\n\n\nRoss, S.M. 1974. Dynamic programming and gambling models. Advances in Applied Probability 6, 3, 593–606. DOI: 10.2307/1426236.\n\n\nRoy, A., Borkar, V., Karandikar, A., and Chaporkar, P. 2022. Online reinforcement learning of optimal threshold policies for Markov decision processes. IEEE Transactions on Automatic Control 67, 7, 3722–3729. DOI: 10.1109/tac.2021.3108121.\n\n\nSerfozo, R.F. 1976. Monotone optimal policies for markov decision processes. In: Mathematical programming studies. Springer Berlin Heidelberg, 202–215. DOI: 10.1007/bfb0120752.\n\n\nSmith, J.E. and McCardle, K.F. 2002. Structural properties of stochastic dynamic programs. Operations Research 50, 5, 796–809. DOI: 10.1287/opre.50.5.796.365.\n\n\nTopkis, D.M. 1998. Supermodularity and complementarity. Princeton University Press.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Monotonicity of value function and optimal policies</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-examples.html",
    "href": "mdps/monotone-examples.html",
    "title": "9  Examples of monotonicity",
    "section": "",
    "text": "9.1 Power-delay tradeoff in wireless communication\nIn a cell phone, higher layer applications such as voicecall, email, browsers, etc. generate data packets. These packets are buffered in a queue and the transmission protocol decides how many packets to transmit at each time depending the number of packets in the queue and the quality of the wireless channel.\nLet \\(X_t \\in \\integers_{\\ge 0}\\) denote the number of packets buffered at time \\(t\\) and \\(A_t \\in \\integers_{\\ge 0}\\), \\(A_t \\le X_t\\), denote the number of packets transmitted at time \\(t\\). The remaining \\(X_t - A_t\\) packets incur a delay penalty given by \\(d(X_t - A_t)\\), where \\(d(\\cdot)\\) is a strictly increasing and discrete-convex function where \\(d(0) = 0\\).\nDuring time \\(t\\), \\(W_t \\in \\integers_{\\ge 0}\\) additional packets arrive and \\[ X_{t+1} = X_t - A_t + W_t.\\] We assume that \\(\\{W_t\\}_{t \\ge 1}\\) is an i.i.d. process.\nThe packets are transmitted over a wireless fading channel. Let \\(S_t \\in \\ALPHABET S\\) denote the state of the fading channel. We assume that the states are ordered such that a lower value of state denotes a better channel quality.\nIf the channel has two states, say GOOD and BAD, we typically expect that \\[ \\PR(\\text{GOOD} \\mid \\text{GOOD}) \\ge \\PR(\\text{GOOD} \\mid \\text{BAD}). \\] This means that the two state transition matrix is stochastically monotone. So, in general (i.e., when the channel has more than two states), we assume that \\(\\{S_t\\}_{t \\ge 1}\\) is a stochastically monotone Markov process that is independent of \\(\\{W_t\\}_{t \\ge 1}\\).\nThe transmission protocol sets the transmit power such that the signal to noise ratio (SNR) at the receiver is above a desired threshold. It can be shown that for additive white Gaussian channels (AWGN), the transmitted power is of the form \\[p(A_t) q(S_t),\\] where\nThe objective is to choose a transmission policy \\(A_t = π^*_t(X_t, S_t)\\) to minimize the weighted sum of transmitted power and delay \\[ \\EXP\\bigg[ \\sum_{t=1}^T \\big[ p(A_t) q(S_t) + \\lambda d(X_t - A_t) \\big]\n\\bigg],\\] where \\(\\lambda\\) may be viewed as a Lagrange multiplier corresponding to a constrained optimization problem.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Examples of monotonicity</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-examples.html#power-delay-tradeoff",
    "href": "mdps/monotone-examples.html#power-delay-tradeoff",
    "title": "9  Examples of monotonicity",
    "section": "",
    "text": "Discrete convexity or \\(L^{\\#}\\) convexity\n\n\n\n\n\nA function \\(f \\colon \\integers \\to \\reals\\) is called convex (or \\(L^{\\#}\\) convex) if for any \\(x \\in \\integers\\), \\[ f(x+1) + f(x-1) \\ge 2 f(x), \\] or, equivalently, for any \\(x, y \\in \\integers\\) \\[ f(x) + f(y) \\ge\n  f\\Bigl(\\Bigl\\lfloor \\frac{x+y}{2} \\Bigr\\rfloor\\Bigr)\n  +\n  f\\Bigl(\\Bigl\\lceil \\frac{x+y}{2} \\Bigr\\rceil\\Bigr).\\]\nIt can be easily seen that \\(L^{\\#}\\) functions satisfy the following properties:\n\nSum of \\(L^{\\#}\\) convex functions is \\(L^{\\#}\\) convex.\nPointwise limits of \\(L^{\\#}\\) convex functions is \\(L^{\\#}\\) convex.\n\nSee Murota (1998) and Chen (2017) for more details.\n\n\n\n\n\n\n\n\n\\(p(\\cdot)\\) is a strictly increasing and convex function where \\(p(0) = 0\\);\n\\(q(\\cdot)\\) is a strictly increasing function.\n\n\n\n9.1.1 Dynamic program\nWe can assume \\(Y_t = X_t - A_t\\) as a post-decision state in the above model and write the dynamic program as follows:\n\\[ V^*_{T+1}(x,s) = 0 \\] and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\\begin{align*}\n  H_t(y,s) &= \\lambda d(y) + \\EXP[ V^*_{t+1}(y + W_t, S_{t+1}) | S_t = s ], \\\\\n  V^*_t(x,s) &= \\min_{0 \\le a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\big\\}\n\\end{align*}\\]\n\n\n9.1.2 Monotonicity of value functions\n\nLemma 9.1 For all \\(t\\), \\(V^*_t(x,s)\\) and \\(H_t(y,s)\\) are increasing in both variables.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that the constraint set \\(\\ALPHABET A(x) = \\{0, \\dots, x\\}\\) satisfies the conditions that generalize the result of monotonicity to constrained actions.\nWe prove the two monotonicity properties by backward induction. First note that \\(V^*_{T+1}(x,s)\\) is trivially monotone. This forms the basis of induction. Now suppose \\(V^*_{t+1}(x,s)\\) is increasing in \\(x\\) and \\(s\\). Since \\(\\{S_t\\}_{t \\ge 1}\\) is stochastically monotone, \\[H_t(y,s) = \\lambda d(y) + \\EXP[ V^*_{t+1}(y + W_t, S_{t+1}) | S_t = s ]\\] is increasing in \\(s\\). Moreover, since both \\(d(y)\\) and \\(V^*_{t+1}(y + w, s)\\) are increasing in \\(y\\), so is \\(H_t(y,s)\\).\nNow, for every \\(a\\), \\(p(a) q(s)\\) and \\(H_t(x-a, s)\\) is increasing in \\(x\\) and \\(s\\). So, the pointwise minima over \\(a\\) is also increasing in \\(x\\) and \\(s\\).\n\n\n\n\n\n9.1.3 Convexity of value functions\n\nLemma 9.2 For all time \\(t\\) and channel state \\(s\\), \\(V^*_t(x,s)\\) and \\(H_t(y,s)\\) are convex in the first variable.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe proceed by backward induction. First note that \\(V^*_{T+1}(x,s)\\) is trivially convex in \\(x\\). Now assume that \\(V^*_{t+1}(x,s)\\) is convex in \\(x\\). Then, \\(\\EXP[V^*_{t+1}(y + W_t, S_{t+1}) | S_t = s]\\) is weighted sum of convex functions and is, therefore, convex in \\(y\\). Therefore, \\(H_t(y,s)\\) is a sum of two convex functions and, therefore, convex in \\(y\\).\nWe cannot directly show the convexity of \\(V^*_t(x,s)\\) because the pointwise minimum of convex functions is not convex. So, we consider the following argument. Fix \\(s\\) and pick \\(x &gt; 1\\). Let \\(\\underline a = π^*_t(x-1,s)\\) and \\(\\bar a = π^*_t(x+1,s)\\). Let \\(\\underline v = \\lfloor (\\underline a + \\bar a)/2\n\\rfloor\\) and \\(\\bar v = \\lceil (\\underline a + \\bar a)/2\n\\rceil\\). Note that both \\(\\underline v\\) and \\(\\bar v\\) are feasible at \\(x\\). Then, \\[ \\begin{align*}\n  \\hskip 2em & \\hskip -2em\n  V^*_t(x-1, s) + V^*_t(x+1, s)\n  \\\\\n  &=\n  [ p(\\underline a) + p(\\bar a) ] q(s) + H_t(x - 1 - \\underline a, s)\n  + H_t(x + 1 - \\bar a, s)\n  \\\\\n  &\\stackrel{(a)}\\ge [ p(\\underline v) + p(\\bar v)] q(s) +\n    H_t(x - \\underline v, s) + H_t(x - \\bar v, s) \\\\\n  &\\ge 2 \\min_{a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\\\\n  &= 2 V^*_t(x,s),\n\\end{align*} \\] where \\((a)\\) follows from convexity of \\(p(\\cdot)\\) and \\(H_t(\\cdot, s)\\). Thus, \\(V^*_t(x,s)\\) is convex in \\(x\\). This completes the induction step.\n\n\n\n\n\n9.1.4 Monotonicity of optimal policy in queue length\n\nTheorem 9.1 For all time \\(t\\) and channel state \\(s\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is increasing in the queue length \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn Lemma 9.2, we have shown that \\(H_t(y,s)\\) is convex in \\(y\\). Therefore, \\(H_t(x-a, s)\\) is submodular in \\((x,a)\\).\n\nThus, for a fixed \\(s\\), \\(p(a)q(s) + H_t(x-a, s)\\) is submodular in \\((x,a)\\). Therefore, the optimal policy is increasing in \\(x\\).\n\n\n\nOne can show submodularity by finite difference, but for simplicity, we assume that \\(H_t(y,s)\\) is twice differentiable. Then, \\(\\partial^2 H_t(x - a, s)/ \\partial\nx \\partial a \\le 0\\) (by convexity of \\(H_t\\)).\n\n9.1.5 Lack of monotonicity of optimal policy in channel state\nIt is natural to expect that for a fixed \\(x\\) the optimal policy is decreasing in \\(s\\). However, it is not possible to obtain the monotonicity of optimal policy in channel state in general. To see why this is difficult, let us impose a mild assumption on the arrival distribution.\n\n\n\n(asm-power-delay-density?)\nThe packet arrival distribution is weakly decreasing, i.e., for any \\(v,w\n\\in \\integers_{\\ge 0}\\) such that \\(v \\le w\\), we have that \\(P_W(v) \\ge\nP_W(w)\\).\n\n\nWe first start with a slight generalization of stochastic monotonicity result.\n\nLemma 9.3 Let \\(\\{p_i\\}_{i \\ge 0}\\) and \\(\\{q_i\\}_{i \\ge 0}\\) be real-valued non-negative sequences satisfying \\[ \\sum_{i \\le j} p_i \\le \\sum_{i \\le j} q_i, \\quad \\forall j.\\] (Note that the sequences do not need to add to 1). Then, for any increasing sequence \\(\\{v_i\\}_{i \\ge 0}\\), we have \\[ \\sum_{i = 0}^\\infty p_i v_i \\ge \\sum_{i=0}^\\infty q_i v_i. \\]\n\nThe proof is similar to the proof for stochastic monotonicity.\n\nLemma 9.4 Under (asm-power-delay-density?), for all \\(t\\), \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe idea of the proof is similar to Lemma 8.1.\nFix \\(y^+, y^- \\in \\integers_{\\ge 0}\\) and \\(s^+, s^- \\in \\ALPHABET S\\) such that \\(y^+ &gt; y^-\\) and \\(s^+ &gt; s^-\\). Now, for any \\(y' \\in \\integers_{\\ge 0}\\) and \\(s'\n\\in \\ALPHABET S\\) define \\[\\begin{align*}\n  π(y',s') = P_W(y' - y^+)P_S(s'|s^+) +\n             P_W(y' - y^-)P_S(s'|s^-),\n             \\\\\n  μ(y',s') = P_W(y' - y^-)P_S(s'|s^+) +\n             P_W(y' - y^+)P_S(s'|s^-).\n\\end{align*}\\]\nSince \\(P_S\\) is stochastically monotone, we have that for any \\(σ \\in \\ALPHABET\nS\\), \\[ \\sum_{s'=1}^{σ} P_S(s'|s^+) \\le \\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Moreover, due to (asm-power-delay-density?), we have that \\(P_W(y' - y^-)\n\\le P_W(y' - y^+)\\). Thus, \\[ [P_W(y' - y^+) - P_W(y' - y^-)] \\sum_{s'=1}^{σ} P_S(s'|s^+)\n\\le [P_W(y' - y^+) - P_W(y' - y^-)]\\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Rearranging terms, we get \\[ \\sum_{s'=1}^σ π(y',s') \\le \\sum_{s'=1}^σ μ(y',s'). \\] Thus, for any \\(y'\\), the sequence \\(π(y',s')\\) and \\(ν(y',s')\\) satisfy the condition of Lemma 9.3.\nNow, in Lemma 9.1, we have established that for any \\(y'\\), \\(V_{t+1}(y',s')\\) is increasing in \\(s'\\). Thus, from Lemma 9.3, we have \\[  \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Summing up over \\(y'\\), we get \\[  \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Or equivalently, \\[\\begin{align*}\n\\hskip 2em & \\hskip -2em\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^-) ]\n\\\\\n& \\ge\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^-) ] .\n\\end{align*}\\] Thus, \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\n\n\nEven under (asm-power-delay-density?), we cannot establish the monotonicity of \\(π^*_t(x,s)\\) is \\(s\\).\n\n\n\nNote that we have established that \\(H_t(y,s)\\) is supermodular in \\((y,s)\\). Thus, for any fixed \\(x\\), \\(H_t(x-a,s)\\) is submodular in \\((a,s)\\). Furthermore the function \\(p(a)q(s)\\) is increasing in both variables and therefore supermodular in \\((a,s)\\). Therefore, we cannot say anything specific about \\(p(a)q(s) + H_t(x-a, s)\\) which is a sum of submodular and supermodular functions.\nWe need to impose a much stronger assumption to establish monotonicity in channel state. See Exercise 9.1.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Examples of monotonicity</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-examples.html#exercises",
    "href": "mdps/monotone-examples.html#exercises",
    "title": "9  Examples of monotonicity",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 9.1 Suppose that the channel state \\(\\{S_t\\}_{t \\ge 1}\\) is an i.i.d. process. Then prove that for all time \\(t\\) and queue state \\(x\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is decreasing in channel state \\(s\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Examples of monotonicity</span>"
    ]
  },
  {
    "objectID": "mdps/monotone-examples.html#notes",
    "href": "mdps/monotone-examples.html#notes",
    "title": "9  Examples of monotonicity",
    "section": "Notes",
    "text": "Notes\nThe mathematical model of power-delay trade-off is taken from Berry (2000), where the monotonicty results were proved using first principles. More detailed characterization of the optimal transmission strategy when the average power or the average delay goes to zero are provided in Berry and Gallager (2002) and Berry (2013). A related model is presented in Ding et al. (2016).\nFor a broader overview of power-delay trade offs in wireless communication, see Berry et al. (2012) and Yeh (2012).\nThe remark after Lemma 9.4 shows the difficulty in establishing monotonicity of optimal policies for a multi-dimensional state space. In fact, sometimes even when monotonicity appears to be intuitively obvious, it may not hold. See Sayedana and Mahajan (2020) for an example. For general discussions on monotonicity for multi-dimensional state spaces, see Topkis (1998) and Koole (2006). As an example of using such general conditions to establish monotonicity, see Sayedana et al. (2020).\n\n\n\n\nBerry, R.A. 2000. Power and delay trade-offs in fading channels. PhD thesis, Massachusetts Institute of Technology. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay tradeoffs in fading channels—small-delay asymptotics. IEEE Transactions on Information Theory 59, 6, 3939–3952. DOI: 10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002. Communication over fading channels with delay constraints. IEEE Transactions on Information Theory 48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M. 2012. Energy-efficient scheduling under delay constraints for wireless networks. Synthesis Lectures on Communication Networks 5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nChen, X. 2017. \\(L^{\\#}\\)-convexity and its applications in operations. Frontiers of Engineering Management 4, 3, 283. DOI: 10.15302/j-fem-2017057.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A. 2016. On monotonicity of the optimal transmission policy in cross-layer adaptive \\(m\\) -QAM modulation. IEEE Transactions on Communications 64, 9, 3771–3785. DOI: 10.1109/TCOMM.2016.2590427.\n\n\nKoole, G. 2006. Monotonicity in markov reward and decision chains: Theory and applications. Foundations and Trends in Stochastic Systems 1, 1, 1–76. DOI: 10.1561/0900000002.\n\n\nMurota, K. 1998. Discrete convex analysis. Mathematical Programming 83, 1–3, 313–371. DOI: 10.1007/bf02680565.\n\n\nSayedana, B. and Mahajan, A. 2020. Counterexamples on the monotonicity of delay optimal strategies for energy harvesting transmitters. IEEE Wireless Communications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E. 2020. Cross-layer communication over fading channels with adaptive decision feedback. International symposium on modeling and optimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nTopkis, D.M. 1998. Supermodularity and complementarity. Princeton University Press.\n\n\nYeh, E.M. 2012. Fundamental performance limits in cross-layer wireless optimization: Throughput, delay, and energy. Foundations and Trends in Communications and Information Theory 9, 1, 1–112. DOI: 10.1561/0100000014.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Examples of monotonicity</span>"
    ]
  },
  {
    "objectID": "mdps/reward-shaping.html",
    "href": "mdps/reward-shaping.html",
    "title": "10  Reward Shaping",
    "section": "",
    "text": "10.1 Generalization to discounted models\nNow consider a finite horizon discounted cost problem, where the performance of a policy \\(π\\) is given by \\[\nJ(π) = \\EXP\\Bigl[ \\sum_{t=1}^{T-1} γ^{t-1} c_t(S_t, A_t) + γ^T c_T(S_T)\n       \\Bigr].\n\\] As argued in the introduction to discounted models, the dynamic prgram for this case is given by\n\\[ V^*_{T}(s) = c_T(s) \\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[ \\begin{align*}\n  Q^*_t(s,a) &= c(s,a) + γ \\EXP[ V^*_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\\\\n  V^*_t(s) &= \\min_{a \\in \\ALPHABET A} Q^*_t(s,a).\n\\end{align*} \\]\nFor such models, we have the following.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reward Shaping</span>"
    ]
  },
  {
    "objectID": "mdps/reward-shaping.html#generalization-to-discounted-models",
    "href": "mdps/reward-shaping.html#generalization-to-discounted-models",
    "title": "10  Reward Shaping",
    "section": "",
    "text": "Corollary 10.2 For discounted cost models, the results of Theorem 10.1 and Corollary 10.1 continue to hold if condition 2 is replaced by\n\nFor \\(t \\in \\{1, \\dots, T-1\\}\\),\n\\[ c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) + γ Φ_{t+1}(s_{+}) - Φ_t(s). \\]\n\n\n\n\n\n\n\n\nInfinite horizon models\n\n\n\nIf the cost function is time homogeneous, Corollary 10.2 extends naturally to infinite horizon models with a time-homogeneous potential function. A remarkable feature is that if the potential function is chosen as the value function, i.e., \\(Φ(s) = V^*(s)\\), then the value function of the modified cost \\(\\tilde c(s,a,s_{+})\\) is zero!",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reward Shaping</span>"
    ]
  },
  {
    "objectID": "mdps/reward-shaping.html#examples",
    "href": "mdps/reward-shaping.html#examples",
    "title": "10  Reward Shaping",
    "section": "10.2 Examples",
    "text": "10.2 Examples\nAs an example of reward shaping, see the notes on inventory management. Also see the notes on martingale approach to stochastic control for an iteresting relationship between reward shaping and martingales.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reward Shaping</span>"
    ]
  },
  {
    "objectID": "mdps/reward-shaping.html#notes",
    "href": "mdps/reward-shaping.html#notes",
    "title": "10  Reward Shaping",
    "section": "Notes",
    "text": "Notes\nThe idea of reward shaping was proposed by Skinner (1938) to synthesize complex behavior by guiding animals to perform simple functions (see :Skinner’s Box Experiment). The formal description of reward shaping comes from Porteus (1975), who established a result similar to Ng et al. (1999), and called it the transformation method. Porteus (1975) also describes transformations of the dynamics which preserve the optimal policy.\nCorollary 10.2 was also re-established by Ng et al. (1999), who aslo provided a partial converse. The results of Porteus (1975) and Ng et al. (1999) were restricted to time-homogeneous potential functions. The generalization to time-varying potential functions was presented in Devlin and Kudenko (2012).\nThe partial converse of Corollary 10.1 established by Ng et al. (1999) states that the shaping presented in Theorem 10.1 is the only additive cost transformation that that preserves the set of optimal policy. However, this converse was derived under the assumption that the transition dynamics are complete (see Ng et al. (1999)). A similar converse under a weaker set of assumptions on the transition dynamics is established in Jenner et al. (2022).\nFor a discussion on practical considerations in using reward shaping in reinforcement learning, see Grzes and Kudenko (2009) and Devlin (2014). As a counter-point, Wiewiora (2003) shows that the advantages of reward shaping can also be achieved by simply adding the potential function to the \\(Q\\)-function initialization.\n\n\n\n\n\nDevlin, S. 2014. Potential based reward shaping tutorial. Available at: http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf.\n\n\nDevlin, S. and Kudenko, D. 2012. Dynamic potential-based reward shaping. Proceedings of the 11th international conference on autonomous agents and multiagent systems, International Foundation for Autonomous Agents; Multiagent Systems, 433–440.\n\n\nGrzes, M. and Kudenko, D. 2009. Theoretical and empirical analysis of reward shaping in reinforcement learning. International conference on machine learning and applications, 337–344. DOI: 10.1109/ICMLA.2009.33.\n\n\nJenner, E., Hoof, H. van, and Gleave, A. 2022. Calculus on MDPs: Potential shaping as a gradient. Available at: https://arxiv.org/abs/2208.09570v1.\n\n\nNg, A.Y., Harada, D., and Russell, S. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. ICML, 278–287. Available at: http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf.\n\n\nPorteus, E.L. 1975. Bounds and transformations for discounted finite markov decision chains. Operations Research 23, 4, 761–784. DOI: 10.1287/opre.23.4.761.\n\n\nSkinner, B.F. 1938. Behavior of organisms. Appleton-Century.\n\n\nWiewiora, E. 2003. Potential-based shaping and q-value initialization are equivalent. Journal of Artificial Intelligence Research 19, 1, 205–208.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reward Shaping</span>"
    ]
  },
  {
    "objectID": "mdps/optimal-stopping.html",
    "href": "mdps/optimal-stopping.html",
    "title": "11  Optimal stopping",
    "section": "",
    "text": "11.1 Example: Time-to-Market Model\nConsider a firm that decides when to introduce a new product. When the firm introduces the product earlier than the competition, it captures a larger market share. However, an early introduction results in high production costs and low profit margins due to low manufacturing yields. Hence, the firm needs to determine the optimal time to enter the market. Suppose that the total market demand \\(D\\) is deterministic. Let \\(\\{S_t\\}_{t \\ge 1}\\) denote the number of competitors at time \\(t\\). It is assumed that \\[ S_{t+1} = S_t + W_t, \\] where \\(\\{W_t\\}_{t \\ge 1}\\) is an independent process independent of \\(S_1\\).\nLet \\(r(s)\\) denote the market share of the firm when it enters the market after the \\(s\\)-th competitor. It is assumed that \\(r(s)\\) is decreasing and concave in \\(s\\).\nLet \\(p_*\\) denote the sale price of the product and \\(p_t\\) denote the production cost at time \\(t\\). It is assumed that \\(p_t\\) decreases with \\(t\\).\nThe continuation reward is zero and the stopping reward at time \\(t\\) is \\[ d_t(s) = r(s)(p_* - p_t) D. \\] When should the firm enter the market?\nFirst observe that \\(\\{S_t\\}_{t \\ge 1}\\) is a monotone process. Now consider the one step look-ahead function \\[ \\begin{align*}\nM_t(s) &= \\EXP[ d_{t+1}(s + W) ] - d_t(s) \\\\\n&= \\EXP[ r(s + W) (p_* - p_{t+1}) D ] - r(s)(p_* - p_t) D \\\\\n&= \\EXP[ r(s + W) - r(s) ] (p_* - p_{t+1}) D\n    + r(s)( p_* - p_{t+1})D  - r(s)(p_* - p_t ) D \\\\\n&= \\EXP[ r(s + W) - r(s) ] (p_* - p_{t+1}) D\n    + r(s) (p_t - p_{t+1}) D.\n\\end{align*} \\] Since \\(r(s)\\) is concave, the first term is decreasing in \\(s\\). The second term is also decreasing in \\(s\\) because \\(r(s)\\) is decreasing in \\(s\\) and \\(p_t \\ge p_{t+1}\\). Therefore, \\(M_t(s)\\) is decreasing in \\(s\\). Hence, by Theorem 11.1, there exist a sequence of thresholds \\(\\{\\lambda_t\\}_{t \\ge 1}\\) such that the firm should enter the market at time \\(t\\) iff \\(S_t \\ge \\lambda_t\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal stopping</span>"
    ]
  },
  {
    "objectID": "mdps/optimal-stopping.html#example-optimal-choice-of-the-best-alternative",
    "href": "mdps/optimal-stopping.html#example-optimal-choice-of-the-best-alternative",
    "title": "11  Optimal stopping",
    "section": "11.2 Example: Optimal choice of the best alternative",
    "text": "11.2 Example: Optimal choice of the best alternative\nA decision maker (DM) wants to select the best alternative from a set of \\(T\\) alternatives. The DM evaluates the alternatives sequentially. After evaluating alternative \\(t\\), the DM knows whether alternative \\(t\\) was the best alternative so far or not. Based on this information, the DM must decide whether to choose alternative \\(t\\) and stop the search, or to permanently reject alternative \\(t\\) and evaluate remaining alternatives. The DM may reject the last alternative and not make a choice at all. All alternatives are equally likely to be the best. Find the optimal choice strategy that maximize the probability of picking the best alternative.\nThis optimization problem is known by different names including secretary problem (in which the alternatives correspond to finding the best candidate as a secretary), marriage problem (in which the alternatives correspond of find the best spouse), Googol (in which the alternatives consist of finding the biggest number), parking problem (in which the alternatives correspond to finding the nearest parking spot) and so on\nWe can view this an optimal stopping problem with binary state \\(S_t\\). \\(S_t = 1\\) means that alternative \\(t\\) is the best alternative so far. Thus, \\(\\{S_t\\}_{t \\ge 1}\\) is an independent process with \\(\\PR(S_t = 1) = 1/t\\). The continuation reward is zero. The DM receives a stopping reward only if the current alternative is best, i.e., the current alternative is best so far (\\(S_t = 1\\)) and better than all future alternative (\\(S_\\tau = 0, \\tau &gt; t\\)). Thus, the expected stopping reward conditioned on \\(S_t\\) is \\[ d_t(s) = \\IND\\{ s = 1 \\} \\cdot \\PR( S_{t+1:T} = 0 | S_t = s ) = s \\cdot \\frac tT. \\] Thus, the optimal strategy is given by the solution of the following dynamic program.\n\n\n\n\n\n\nDynamic program\n\n\n\n\\[ \\begin{align*}\n  V^*_{T+1}(s) &= 0 \\\\\n  V^*_t(s) &= \\max\\bigg\\{ s \\cdot \\frac tT,\n  \\EXP[ V^*_{t+1}(S_{t+1}) ] \\bigg\\}\n\\end{align*}\\]\n\n\n\nLemma 11.1 Define \\[ L_t = V^*_t(0) =\n\\frac t{t+1} V^*_{t+1}(0) + \\frac 1{t+1}V^*_{t+1}(1). \\] Then, \\[V^*_t(1) = \\max\\bigg\\{ \\frac tT, L_t \\bigg\\}\\] and, therefore, \\[\n  L_t - L_{t+1} = \\bigg[ \\frac 1T - \\frac {L_{t+1}}{t+1} \\bigg]^+\n  \\quad \\text{with } L_T = 0.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe result follows immediately from the definition of \\(L_t\\).\n\n\n\nNote that it is never optimal to select an alternative if it is not the best so far (i.e., \\(S_t = 0\\)). Thus, we can completely characterize an optimal strategy by solving for \\(\\{L_t\\}_{t=1}^T\\) in a backward manner.\n\nTheorem 11.2  \n\nThere exists a critical time \\(t_0\\), \\(t_0 &lt; T\\), such that it is optimal to reject all alternatives until \\(t_0 - 1\\) and then select the first alternative that is superior to all previous ones, if it occurs.\nThe critical time is the smallest integer \\(t_0\\) such that \\[\n  \\sum_{k=t_0}^{T-1} \\frac 1k &lt; 1.\n\\]\nThe value function are given by \\[\n  L_t = \\begin{cases}\n  \\displaystyle \\frac tT \\sum_{k=t}^{T-1} \\frac 1k,\n  & \\text{for } t \\ge t_0, \\\\\n  L_{t_0}, & \\text{for } t &lt; t_0.\n  \\end{cases}\n\\]\nFor large \\(T\\), \\(t_0 \\approx T/e\\) and the probability of selecting the best candidate is approximately \\(1/e\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNote that \\(L_t - L_{t+1} \\ge 0\\). Thus, \\(L_t\\) is decreasing with time.\n\\(V^*_t(1) = \\max\\{t/T, L_t\\}\\) where the first term is increasing with time and the second term is decreasing with time. Thus, the critical time \\(t_0\\) is the first time when \\(t/T \\ge L_t\\). Since \\(L_T = 0\\) and \\(T/T = 1\\), such a \\(t_0 &lt; T\\).\nFor any \\(t &lt; t_0\\) (i.e., \\(t/T &lt; L_t\\)), \\[\n  L_{t-1} = L_t + \\bigg[ \\frac 1T - \\frac{L_t}{t} \\bigg]^+ = L_t.\n\\]\nFor any \\(t \\ge t_0\\) (i.e., \\(t/T \\ge L_t\\)), we have \\((t+1)/T \\ge L_{t+1}\\). Therefore, \\[\n  L_{t} = L_{t+1} + \\bigg[ \\frac 1T - \\frac{L_{t+1}}{t+1} \\bigg]^+\n  = L_{t+1} + \\frac 1T - \\frac{L_{t+1}}{t+1}\n  = \\frac tT \\bigg[ \\frac 1t + \\frac{T}{t+1} L_{t+1} \\bigg].\n\\] The above \\(L_t\\) can be shown to be equal to the form given above in point 3 by induction.\nFor large \\(T\\), \\[\n  \\sum_{k=t}^{T-1} \\frac 1k \\approx\n  \\int_{t}^T \\frac 1k dk = \\log \\frac Tt.\n\\] Thus, \\(t_0 \\approx T/e\\). Moreover \\[\nV^*_1(0) = V^*_1(1) = L_1 = L_{t_0} \\approx \\frac{t_0}{T} = \\frac 1e.\n\\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal stopping</span>"
    ]
  },
  {
    "objectID": "mdps/optimal-stopping.html#example-call-options",
    "href": "mdps/optimal-stopping.html#example-call-options",
    "title": "11  Optimal stopping",
    "section": "11.3 Example: Call options",
    "text": "11.3 Example: Call options\nAn investor has a :call option to buy one share of a stock at a fixed price \\(p\\) dollars and has \\(T\\) days to exercise this option. For simplicity, we assume that the investor makes a decision at the beginning of each day.\nThe investor may decide not to exercise the option but if he does exercise the option when the stock price is \\(s\\), he effectively gets \\((s-p)\\).\nAssume that the price of the stock varies with independent increments, i.e., the price on day \\(t+1\\) is \\[S_{t+1} = S_t + W_t\\] where \\(\\{W_t\\}_{t \\ge 1}\\) is an i.i.d. process with mean \\(\\mu\\).\nLet \\(\\tau\\) denote the stopping time when the investor exercises his option. Then the optimization problem is to maximize \\[ \\EXP\\big[ (S_{\\tau} - p )\\cdot \\IND\\{\\tau \\le T \\} \\big].\\]\nSince this is an optimal stopping problem with perfect state observation, the optimal strategy is given by the solution of the following dynamic program\n\n\n\n\n\n\nDynamic program\n\n\n\n\\[\\begin{align*}\nV^*_{T}(s) &= \\max\\{ s-p, 0 \\} \\\\\nV^*_{t}(s) &= \\max\\{ s-p, \\EXP[ V^*_{t+1}(s + W) \\}.\n\\end{align*}\\]\n\n\n\nLemma 11.2  \n\nFor all \\(t\\), \\(V^*_t(s)\\) is increasing in \\(s\\).\nFor all \\(t\\), \\(V^*_t(s) - s\\) is decreasing in \\(s\\).\nFor all \\(s\\), \\(V^*_t(s) \\ge V^*_{t+1}(s)\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe first property follows immediately from monotonicity of terminal reward and the monotonicity of the dynamics. From Exercise 5.1, to show the third property, we need to show that \\(V^*_{T-1}(s) \\ge V^*_T(s)\\). Observe that \\[ V^*_{T-1}(s) =\n\\max\\{s - p, \\EXP[V^*_{T}(s + W) \\} \\ge \\max\\{ s - p, 0 \\} = V^*_T(s).\n\\]\nNow we prove the second property using backward induction. At \\(t=T\\), \\[ V^*_T(s) - s = \\max\\{ -p, -s \\}\\] which is decreasing in \\(s\\). This forms the basis of induction. Now assume that \\(V^*_{t+1}(s) - s\\) is decreasing in \\(s\\). Then, \\[ \\begin{align*}\n  V^*_t(s) - s &= \\max\\{ -p, \\EXP[ V^*_{t+1}(s+W) ] - s  \\} \\\\\n  &= \\max\\{ -p, \\EXP[ V^*_{t+1}(s+W) - (s + W) ] + \\EXP[W] \\}.\n\\end{align*} \\] By the induction hypothesis the second term is decreasing in \\(s\\). The minimum of a constant and a decreasing function is decreasing in \\(s\\). Thus, \\(V^*_t(s) - s\\) is decreasing in \\(s\\). This completes the induction step.\n\n\n\n\nLemma 11.3 At any time \\(t\\), if it is optimal to sell when the stock price is \\(s^\\circ\\), then it is optimal to sell at all \\(s \\ge s^\\circ\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince it is optimal to sell at \\(s^\\circ\\), we must have \\[\\begin{equation} \\label{eq:p1}\n  s^\\circ - p \\ge \\EXP[V^*_{t+1}(s^\\circ + W) ]\n\\end{equation}\\] Since \\(V^*_{t}(s) - s\\) is decreasing in \\(s\\), we have that for any \\(s \\ge\ns^\\circ\\), \\[\\begin{equation} \\label{eq:p2}\n\\EXP[ V^*_{t+1}(s + W) - s ] \\le \\EXP[ V^*_{t+1}(s^\\circ + W) - s^\\circ ]\n\\le -p\n\\end{equation}\\] where the last inequality follows from \\eqref{eq:p1}. Eq \\eqref{eq:p2} implies that \\[ \\EXP[ V^*_{t+1}(s+W) ] \\le s - p. \\] Thus, the stopping action is also optimal at \\(s\\).\n\n\n\n\nTheorem 11.3 The optimal strategy is of the threshold type. In particular, there exist numbers \\(α_1 \\ge α_2 \\ge \\cdots \\ge α_T\\) such that it is optimal to exercise the option at time \\(t\\) if and only if \\(s_t \\ge α_t\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(D_t = \\{s : π^*_t(s) = 1\\}\\). The previous Lemma shows that \\(D_t\\) is of the form \\([α_t, \\infty)\\), where \\(α_t = \\min \\{ s : π^*_t(s) = 1\\}\\), where we assume that \\(α_t = \\infty\\) is it is not optimal to stop in any state. Thus proves the threshold property.\nTo show that the thresholds are decreasing with time, it suffices to show that \\(D_t \\subseteq D_{t+1}\\). Suppose \\(s \\in D_t\\). Then, \\[\\begin{equation} \\label{eq:p3}\ns - p \\ge \\EXP[ V^*_{t+1}(s + W) ] \\ge \\EXP[ V^*_{t+2}(s + W) ],\n\\end{equation}\\] where the first inequality follows because \\(s \\in D_t\\) and the second inequality follows because \\(V^*_{t+1}(s) \\ge V^*_{t+2}(s)\\). Eq \\eqref{eq:p3} implies that \\(s \\in D_{t+1}\\). Hence, \\(D_t \\subseteq D_{t+1}\\) and, therefore, the optimal thresholds are decreasing.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal stopping</span>"
    ]
  },
  {
    "objectID": "mdps/optimal-stopping.html#exercises",
    "href": "mdps/optimal-stopping.html#exercises",
    "title": "11  Optimal stopping",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 11.1 Derive a version of Theorem 11.1 where \\(M_t(s)\\) is weakly decreasing in \\(s\\).\n\n\nExercise 11.2 Derive a version of Theorem 11.1 for an optimal stopping problem where the objective is reward maximization instead of cost minimization. In particular, assume that \\(c_t\\) denotes the continuation reward and \\(d_t\\) denote the stopping reward at time \\(t\\). Define the benefit function \\(B_t(s)\\) and the one-step look-ahead function \\(M_t(s)\\) as above.\n\nWrite the benefit function in terms of the one-step look-ahead function.\nDerive a version similar to Theorem 11.1 assuming \\(M_t(s)\\) is increasing in \\(s\\).\n\n\n\nExercise 11.3 (Selling an asset) Consider the decision problem by a person selling an asset. Let \\(W_t\\) denote the offer received by the person at time \\(t\\). We assume that \\(\\{W_t\\}_{t \\ge 1}\\) is an i.i.d. process. If the person sells the asset at time \\(t\\), then he receives a reward equal to the best offer received so far, i.e., \\(\\max\\{W_1, \\dots, W_t\\}\\). If he decides to continue, then he has to pay a continuation cost of \\(c\\). Show that there exist a sequence of thresholds \\(\\{λ_t\\}_{t \\ge 1}\\) such that the optimal strategy is to sell the asset when \\(\\max\\{W_1, \\dots, W_t\\} \\ge λ_t\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal stopping</span>"
    ]
  },
  {
    "objectID": "mdps/optimal-stopping.html#notes",
    "href": "mdps/optimal-stopping.html#notes",
    "title": "11  Optimal stopping",
    "section": "Notes",
    "text": "Notes\nTheorem 11.1 is taken from Oh and Özer (2016).\nFor a history of the secretary problem, see Ferguson (1989)\nThe above model for pricing options was introduced by Taylor (1967).\n\n\n\n\nFerguson, T.S. 1989. Who solved the secretary problem? Statistical science, 282–289.\n\n\nOh, S. and Özer, Ö. 2016. Characterizing the structure of optimal stopping policies. Production and Operations Management 25, 11, 1820–1838. DOI: 10.1111/poms.12579.\n\n\nTaylor, H.M. 1967. Evaluating a call option and optimal timing strategy in the stock market. Management Science 14, 1, 111–120. Available at: http://www.jstor.org/stable/2628546.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Optimal stopping</span>"
    ]
  },
  {
    "objectID": "mdps/inf-horizon.html",
    "href": "mdps/inf-horizon.html",
    "title": "12  Infinite horizon MDPs",
    "section": "",
    "text": "12.1 Performance of a time-homogeneous Markov policy\nAn infinite horizon Markov policy \\((π_1, π_2, \\ldots)\\) is called time-homongeous1 if \\(π_1 = π_2 = π_3 = \\cdots\\). With a slight abuse of notation, we will denote a time-homogenous policy \\((π,π,\\ldots)\\) simply by \\(π\\).\nThe expected discounted cost of a time-homogeneous Markov policy is given by \\[ V^π(s) = \\EXP^π\\bigg[ \\sum_{t=1}^∞ γ^{t-1} c(S_t, A_t) \\biggm| S_1 = s\n\\bigg].\\]\nWe now introduce a vector notation. Let \\(V^π\\) denote the \\(n × 1\\) vector given by \\([V^π]_s = V^π(s)\\). Furthermore, define Define the \\(n × 1\\) vector \\(c_π\\) and the \\(n × n\\) matrix \\(P_π\\) as follows: \\[ [c_π]_s = c(s, π(s))\n   \\quad\\text{and}\\quad\n   [P_π]_{ss'} = P_{ss'}(π(s)).\n\\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infinite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/inf-horizon.html#performance-of-a-time-homogeneous-markov-policy",
    "href": "mdps/inf-horizon.html#performance-of-a-time-homogeneous-markov-policy",
    "title": "12  Infinite horizon MDPs",
    "section": "",
    "text": "1 Time-homogenous policies are sometimes also called stationary policies in the literature. I personally feel that the term stationary can be a bit confusing because stationary processes has a precise but different meaning in probability theory.\n\n\nProposition 12.1 The performance of a time-homogeneous Markov policy may be computed by solving a system of linear equations given by: \\[ V^{π} = (1 - γ P_{π})^{-1} c_{π}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first observe that the dynamics under policy \\(π\\) are Markovian with transition probability matrix \\(P_π\\) and a cost function \\(c_π\\). Therefore, \\[ \\begin{align*}\n\\EXP^π\\big[ c(S_t, π(S_t)) \\bigm| S_1 = s \\big]\n  &= \\sum_{s' \\in \\ALPHABET S} \\PR^π(S_t = s' | S_1 = s) c(s', π(s'))\n  \\\\\n  &= \\sum_{s' \\in \\ALPHABET S} [P_π^{t-1}]_{ss'} [c_π]_y\n  \\\\\n  &= δ_s P_π^{t-1} c_π.\n\\end{align*} \\]\nNow consider the performance of policy \\(π\\), which is given by \\[ \\begin{align*}\nV^π &= c_π + γ P_π c_π + γ^2 P_π^2 c_π + \\cdots \\\\\n    &= c_π + γ P_π \\big( c_π + γ P_π c_π + \\cdots \\big) \\\\\n    &= c_π + γ P_π V^π.\n\\end{align*} \\] Rearranging terms, we have \\[ (I - γ P_π) V^π = c_π. \\]\nThe :spectral radius \\(ρ(γ P_d)\\) of a matrix is upper bounded by its :spectral norm \\(\\lVert γ P_d \\rVert = γ &lt; 1\\). Therefore, the matrix \\((I - γ P_π)\\) has an inverse and by left multiplying both sides by \\((I - γ P_π)^{-1}\\), we get \\[ V^π = (I - γP_π)^{-1} c_π. \\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infinite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/inf-horizon.html#bellman-operators",
    "href": "mdps/inf-horizon.html#bellman-operators",
    "title": "12  Infinite horizon MDPs",
    "section": "12.2 Bellman operators",
    "text": "12.2 Bellman operators\nTo succinctly describe the analysys, we define two operaors:\n\nThe Bellman operator for policy \\(π\\) denoted by \\(\\BELLMAN^π\\): For any \\(v \\in \\reals^n\\), \\[ \\BELLMAN^π v = c_{π} + γ P_{π} v. \\]\nThe Bellman optimality operator denoted by \\(\\BELLMAN^*\\): For any \\(v \\in \\reals^n\\): \\[ [\\BELLMAN^* v]_s = \\min_{a \\in \\ALPHABET A}\n  \\Big\\{ c(s,a) + γ \\sum_{s' \\in \\ALPHABET S} P_{ss'}(a) v_y \\Big\\}.\\] We say that a policy \\(π\\) is a greedy policy with respect to \\(v\\), written as \\(π = \\GREEDY(v)\\), if \\[ B^* v = B^π v. \\]\n\nNote that the Bellman optimality operator may also be written as2 \\[ \\BELLMAN^* v = \\min_{π \\in \\Pi} \\BELLMAN^π v, \\] where \\(\\Pi\\) denotes the set of all deterministic Markov policies.\n2 This is true for general models only when the arg min at each state exists.For any \\(v \\in \\reals^n\\), let \\(\\NORM{v}_{∞}\\) denote the sup-norm of \\(v\\), i.e., \\(\\NORM{v}_{∞} := \\sup_{s \\in \\ALPHABET S} \\ABS{v(s)}\\). Recall that \\((\\reals^n, \\NORM{\\cdot}_{∞})\\) is a :Banach space\n\nProposition 12.2 The operators \\(\\BELLMAN^π\\) and \\(\\BELLMAN^*\\) are contractions under the sup-norm, i.e., for any \\(v, w \\in \\reals^n\\), \\[ \\NORM{\\BELLMAN v - \\BELLMAN w} \\le γ \\NORM{v - w}, \\] where \\(\\BELLMAN \\in \\{\\BELLMAN^π, \\BELLMAN^*\\}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst consider the case of \\(\\BELLMAN^π\\). Note that \\[\n  \\BELLMAN^π v - \\BELLMAN^π w =\n  (c_{π} + γ P_{π} v) - (c_{π} + γ P_{π} w)\n  =\n  γ P_{π} (v - w).\n\\] Therefore, \\[\n  \\NORM{\\BELLMAN^π v - \\BELLMAN^π w}_{∞}\n  \\le γ \\NORM{v - w}_{∞} P_{π} \\mathbb{1}\n  = γ \\NORM{v - w}_{∞}.\n\\]\nNow consider the case of \\(\\BELLMAN^*\\). Fix a state \\(s \\in \\ALPHABET S\\) and consider \\([\\BELLMAN^* v](s) - [\\BELLMAN^* w](s)\\). In particular, let \\(a^*\\) be the optimal action in the right hand side of \\([\\BELLMAN^* w](s)\\). Then, \\[\\begin{align*}\n  [\\BELLMAN^* v - \\BELLMAN^* w](s) &=\n  \\min_{a \\in \\ALPHABET A}\\bigl\\{ c(s,a) + γ \\sum_{s' \\in \\ALPHABET S}\n  P_{ss'}(a) v(s') \\bigr\\} -\n  \\min_{a \\in \\ALPHABET A}\\bigl\\{ c(s,a) + γ \\sum_{s' \\in \\ALPHABET S}\n  P_{ss'}(a) w(s') \\bigr\\}\n  \\\\\n  &\\le c(s,a^*) + γ \\sum_{s'\\in \\ALPHABET S} P_{ss'}(a^*) v(s') -\n       c(s,a^*) - γ \\sum_{s'\\in \\ALPHABET S} P_{ss'}(a^*) w(s')\n  \\\\\n  &\\le γ \\sum_{s' \\in \\ALPHABET S} P_{ss'}(a^*) \\| v - w \\|\n  \\\\\n  &= γ \\| v - w \\|.\n\\end{align*} \\]\nBy a similar argument, we can show that \\([\\BELLMAN^* w - \\BELLMAN^* v](s) \\le\nγ \\| v - w \\|\\), which proves the other side of the inequality.\n\n\n\nLet’s revisit the result of Proposition 12.1, which may be written as \\[ V^π = c_π + γ P_π V^π \\] We can write the above using Bellman operators as: \\[\\begin{equation}\\label{eq:policy-evaluation}\nV^π = \\BELLMAN^π V^π.\n\\end{equation}\\]\nThus, we can equivalently state the result of Proposition 12.1 by saying that \\(V^π\\) is the unique bounded fixed point of \\(\\eqref{eq:policy-evaluation}\\).\nWhat about the Bellman optimality operator? Does the equation \\(V^* = \\BELLMAN V^*\\) have a solution? The contraction property (Proposition 12.2) and the :Banach fixed point theorem imply the following.\n\nTheorem 12.1 There is a unique bounded \\(V^* \\in \\reals^n\\) that satisfies the Bellman equation \\[ V^* = \\BELLMAN^* V^* \\]\nMoreover, if we start from any \\(V^0 \\in \\reals^n\\) and recursively define \\(V^n = \\BELLMAN^* V^{n-1}\\) then \\[ \\lim_{n \\to ∞} V^n = V^*. \\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infinite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/inf-horizon.html#optimal-time-homogeneous-policy",
    "href": "mdps/inf-horizon.html#optimal-time-homogeneous-policy",
    "title": "12  Infinite horizon MDPs",
    "section": "12.3 Optimal time-homogeneous policy",
    "text": "12.3 Optimal time-homogeneous policy\nWhat does \\(V^*\\) signify? What about a policy \\(π^* \\in \\GREEDY(V^*)\\)? We answer these questions below.\n\n\nProposition 12.3 Define \\[ V^{\\text{opt}}_∞(s) := \\min_{π} \\EXP^π \\bigg[ \\sum_{t=1}^∞ γ^{t-1} c(S_t, A_t)\n\\biggm| S_1 = s \\bigg], \\] where the minimum is over all (possibly randomized) history dependent policies. Then, \\[ V^{\\text{opt}}_∞ = V^*, \\] where \\(V^*\\) is the solution of the Bellman equation.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the state and action space are finite, without loss of optimality, we can assume that \\(0 \\le c(s,a) \\le M\\).\nConsider the finite horizon truncation \\[ V^{\\text{opt}}_T(s) =  \\min_{π} \\EXP^π\\bigg[ \\sum_{t=1}^T γ^{t-1} c(S_t, A_t) | S_1 = s \\bigg].\n\\] From the results for finite horizon MDP, we have that \\[ V^{\\text{opt}}_T = (\\BELLMAN^*)^T V^0 \\] where \\(V^0\\) is the all zeros vector.\nNow by construction, \\[V^{\\text{opt}}_∞(s) \\ge V^{\\text{opt}}_T(s) = [(\\BELLMAN^*)^T V^0](s). \\] Taking limit as \\(T \\to ∞\\), we get that \\[\\begin{equation}\\label{eq:1}\n  V^{\\text{opt}}_∞(s) \\ge \\lim_{T \\to ∞} [(\\BELLMAN^*)^T V^0](s) = V^*(s).\n\\end{equation}\\]\nSince \\(0 \\le c(s,a) \\le M\\), for any \\(T\\), \\[ \\begin{align*}\nV^{\\text{opt}}_∞(s) &\\le \\min_π \\EXP^π \\bigg[ \\sum_{t=1}^T γ^{t-1} c(S_t, A_t)\n\\biggm| S_1 = s \\bigg] + \\sum_{t=T+1}^∞ γ^{t-1} M \\\\\n&= V^{\\text{opt}}_T(s) + γ^T M / (1 - γ) \\\\\n&= [(\\BELLMAN^*)^T V^0](s) + γ^T M / (1-γ).\n\\end{align*} \\] Taking limit as \\(T \\to ∞\\), we get that \\[\\begin{equation}\\label{eq:2}\n  V^{\\text{opt}}_∞(s) \\le \\lim_{T \\to ∞}\n  \\big\\{ [(\\BELLMAN^*)^T V^0](s) + γ^T M / (1-γ) \\big\\} = V^*(s).\n\\end{equation}\\]\nFrom \\eqref{eq:1} and \\eqref{eq:2}, we get that \\(V^{\\text{opt}}_∞ = V^*\\).\n\n\n\nThus, \\(V^*\\) is the optimal performance. Now we can formally state the main result.\n\nTheorem 12.2 A time-homogeneous Markov policy \\(π^*\\) is optimal if and only if it satisfies \\(π^* \\in \\GREEDY(V^*)\\), where \\(V^*\\) is the unique uniformly bounded solution of the following fixed point equation: \\[\n  V^* = \\BELLMAN^* V^*.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom Theorem 12.1, we know that the equation \\(V^* = \\BELLMAN^* V^*\\) has a unique bounded solution \\(V^*\\). From Proposition 12.3, we know that \\(V^*\\) is the optimal policy.\nTake a policy \\(π^*\\) such that \\(π^* \\in \\GREEDY(V^*)\\). Note that \\[ V^* = \\BELLMAN^* V^* = \\BELLMAN^{π^*} V^*. \\] Thus, \\(V^*\\) is a fixed point of \\(\\BELLMAN^{π^*}\\) and therefore equal to \\(V^{π^*}\\). Thus, any policy that is greedy wrt \\(V^*\\) has performance \\(V^*\\) and (by Proposition 12.3) is optimal.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infinite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/inf-horizon.html#properties-of-bellman-operator",
    "href": "mdps/inf-horizon.html#properties-of-bellman-operator",
    "title": "12  Infinite horizon MDPs",
    "section": "12.4 Properties of Bellman operator",
    "text": "12.4 Properties of Bellman operator\n\nProposition 12.4 The Bellman operator satisfies the following properties\n\nMonotonicity. For any \\(v, w \\in \\reals^n\\), if \\(v \\le w\\), then \\(\\BELLMAN^π v \\le \\BELLMAN^π w\\) and \\(\\BELLMAN^* v \\le \\BELLMAN^* w\\).\nDiscounting. For any \\(v \\in \\reals^n\\) and \\(m \\in \\reals\\), \\(\\BELLMAN^π (v + m \\ONES) = \\BELLMAN^π v + γ m \\ONES\\) and \\(\\BELLMAN^* (v + m \\ONES) = \\BELLMAN^* v + γ m \\ONES\\).\n\n\n\n\n\n\n\n\nProof of monotonicity property\n\n\n\n\n\nRecall that \\[ \\BELLMAN^π v = c_π + γ P_π v. \\] So, monotonicity of \\(\\BELLMAN^π\\) follows immediately from monotonicity of matrix multiplication for positive matrices.\nLet \\(μ\\) be such that \\(\\BELLMAN^* w = \\BELLMAN^μ w\\). Then, \\[ \\BELLMAN^* v \\le \\BELLMAN^μ v\n\\stackrel{(a)} \\le \\BELLMAN^μ w = \\BELLMAN^* w,\n\\] where \\((a)\\) uses the monotonicity of \\(\\BELLMAN^μ\\).\n\n\n\n\n\n\n\n\n\nProof of discounting property\n\n\n\n\n\nRecall that \\[ \\BELLMAN^π v = c_π + γ P_π v. \\] Thus, \\[ \\BELLMAN^π(v+m \\ONES) = c_π + γ P_π (v+m \\ONES) = c_π + γ P_π v + γ m\n\\ONES = \\BELLMAN^π\nv + γ m \\ONES.\\] Thus, \\(\\BELLMAN^π\\) is discounting. Now consider \\[ \\BELLMAN^* (v + m \\ONES ) = \\min_{π} \\BELLMAN^π (v+m \\ONES)\n= \\min_π (\\BELLMAN^π v + γ m \\ONES) = \\BELLMAN^* v + γ m \\ONES.\\] Thus, \\(\\BELLMAN^*\\) is discounting.\n\n\n\n\nProposition 12.5 For any \\(V \\in \\reals^n\\),\n\nIf \\(V \\ge \\BELLMAN^* V\\), then \\(V \\ge V^*\\);\nIf \\(V \\le \\BELLMAN^* V\\), then \\(V \\le V^*\\);\nIf \\(V = \\BELLMAN^* V\\), then \\(V\\) is the only vector with this property and \\(V = V^*\\).\n\nThe same bounds are true when \\((\\BELLMAN^*, V^*)\\) is replaced with \\((\\BELLMAN^π, V^π)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the first part. The proof of the other parts is similar.\nWe are given that \\[V \\ge \\BELLMAN^* V.\\] Then, by monotonicity of the Bellman operator, \\[ \\BELLMAN^* V \\ge \\BELLMAN^*^2 V.\\] Continuing this way, we get \\[ \\BELLMAN^*^k V \\ge \\BELLMAN^*^{k+1} V.\\] Adding the above equations, we get \\[ V \\ge \\BELLMAN^*^{k+1} V.\\] Taking limit as \\(k \\to ∞\\), we get \\[V \\ge V^*.\\]\n\n\n\n\nProposition 12.6 For any \\(V \\in \\reals^n\\) and \\(m \\in \\reals\\),\n\nIf \\(V + m \\ONES \\ge \\BELLMAN^* V\\), then \\(V  + m \\ONES/(1-γ) \\ge V^*\\);\nIf \\(V + m \\ONES \\le \\BELLMAN^* V\\), then \\(V  + m \\ONES/(1-γ) \\le V^*\\);\n\nThe same bounds are true when \\((\\BELLMAN^*, V^*)\\) is replaced with \\((\\BELLMAN^π, V^π)\\).\n\n\n\n\n\n\n\nRemark\n\n\n\nThe above result can also be stated as follows:\n\n\\(\\displaystyle \\| V^π - V \\| \\le \\frac{1}{1-γ}\\| \\BELLMAN^π V - V \\|\\).\n\\(\\displaystyle \\| V^* - V \\| \\le \\frac{1}{1-γ}\\| \\BELLMAN^* V - V \\|\\).\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAgain, we only prove the first part. The proof of the second part is the same. We have that \\[ V + m \\ONES \\ge \\BELLMAN^* V. \\] From discounting and monotonicity properties, we get \\[ \\BELLMAN^* V + γ m \\ONES \\ge \\BELLMAN^*^2 V. \\] Again, from discounting and monotonitiy properties, we get \\[ \\BELLMAN^*^2 V + γ^2 m \\ONES \\ge \\BELLMAN^*^3 V. \\] Continuing this way, we get \\[ \\BELLMAN^*^k V + γ^k m \\ONES \\ge \\BELLMAN^*^{k+1} V. \\] Adding all the above equations, we get \\[ V + \\sum_{\\ell = 0}^k γ^\\ell m \\ONES \\ge \\BELLMAN^*^{k+1} V. \\] Taking the limit as \\(k \\to ∞\\), we get \\[ V + m \\ONES/(1-γ) \\ge V^*. \\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infinite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/inf-horizon.html#countable-or-continuous-state-and-action-spaces",
    "href": "mdps/inf-horizon.html#countable-or-continuous-state-and-action-spaces",
    "title": "12  Infinite horizon MDPs",
    "section": "12.5 Countable or continuous state and action spaces",
    "text": "12.5 Countable or continuous state and action spaces\nThe discussion above was restricted to finite state and action spaces. In general, when working with non-finite state and action spaces, we need to be careful about existence of solutions. See Section 5.7 for a discussion.\nFor infinite horizon problems, there are additional technical challenges. We first point out via examples that the Bellman equation may not have a unique fixed point equation.\n\nExample 12.1 (Phantom solutions of DP (Bertsekas 2011)) Consider an MDP with \\(\\ALPHABET S = [0,∞)\\), \\(\\ALPHABET A = \\{a_\\circ\\}\\) (dummy action), \\(p(ds'\\mid s,a) = δ_{s' - s/γ}\\) (so the state deterministically transitions to \\(s/γ\\)) and \\(c(s,a) ≡ 0\\). It is clear that the optimal value function \\(V^*(s) ≡ 0\\). However, the dynamic programming equation is \\[\n  V^*(s) = γ V^*(s/γ)\n\\] and is satisfied by any linear function \\(V^*(s) = α s\\)!\n\n\n\n\n\n\n\nDid we just violate the Banach fixed point theorem?\n\n\n\nTo apply the Banach fixed point theorem, we need to be in a Banach space, i.e., a complete normed metric space. So, what was the Banach space in Theorem 12.1? Implicitly, we had used the space \\[\n  \\ALPHABET V^M \\coloneqq \\{ v \\colon \\ALPHABET S \\to \\reals \\text{ such that } \\| v \\|_{∞} \\le M \\}\n\\] where \\(M\\) is any constant greater than \\(\\|c\\|_{∞}/(1-γ)\\). Banach fixed point theorem says that DP has a unique fixed point in \\(\\ALPHABET V^M\\). It does not say anything about solutions outside \\(\\ALPHABET V^M\\)!\nAmong all the solutions, only the one corresponding to \\(α = 0\\) is bounded, which is also the optimal value function.\n\n\n\nExample 12.2 Consider a birth-death Markov chain with \\(0\\) as an aborbing state. In particular, \\(\\ALPHABET S = \\integers_{\\ge 0}\\), \\(\\ALPHABET A = \\{a_\\circ\\}\\) (dummy action), and \\(c(s,a) = \\IND\\{s &gt; 0\\}\\). The transition dynamics are given by: for \\(s = 0\\), \\(P(s'|0,a) = \\IND\\{s' = 0\\}\\) and for \\(s &gt; 0\\), we have \\[\nP(s'|s,a) = p \\IND\\{ s' = s+1\\} + (1-p) \\IND\\{s' = s-1\\}.\n\\] So, the DP is given by \\(v(0) = 0\\) and for \\(s &gt; 0\\) \\[\nV^*(s) = 1 + γ p  V^*(s+1) + γ(1-p) V^*(s-1),\n\\] which is a harmonic equation and has a generic solution of the form: \\[\n  V^*(s) = \\frac{1}{1-γ} -\n  \\biggl[ \\frac{1}{1-γ} + C \\biggr] q_1^s +\n  C q_2^s\n\\] where \\(C\\) is an arbitrary constant and \\[\n  q_{1,2} = \\frac{1 \\pm \\sqrt{1 - 4 γ^2 p (1-p)}}{2 γp}.\n\\] It can be verified that \\(q_1 \\in (0,1)\\) and \\(q_2 \\in (1,∞)\\). So, every solution except \\(C = 0\\) is unbounded.\nAgain we have multiple solutions of the DP and the correct solution is the only bounded solution (which corresponds to \\(C = 0\\)).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infinite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/inf-horizon.html#exercises",
    "href": "mdps/inf-horizon.html#exercises",
    "title": "12  Infinite horizon MDPs",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 12.1 (One-step look-ahead error bounds.) Given any \\(V \\in \\reals^n\\), let \\(π\\) be such that \\(\\BELLMAN^* V = \\BELLMAN^π V\\). Moreover, let \\(V^*\\) denote the unique fixed point of \\(\\BELLMAN^*\\) and \\(V^π\\) denote the unique fixed point of \\(\\BELLMAN^π\\). Then, show that\n\n\\[ \\| V^* - V \\| \\le \\frac{1}{1-γ} \\| \\BELLMAN^* V - V \\|. \\]\n\\[ \\| V^* - \\BELLMAN^* V \\| \\le \\frac{γ}{1-γ} \\| \\BELLMAN^* V - V \\|. \\]\n\\[ \\| V^π - V \\| \\le \\frac{1}{1-γ} \\| \\BELLMAN^π V - V \\|. \\]\n\\[ \\| V^π - \\BELLMAN^π V \\| \\le \\frac{γ}{1-γ} \\| \\BELLMAN^π V - V \\|. \\]\n\\[ \\| V^π - V^* \\| \\le \\frac{2}{1-γ} \\| \\BELLMAN^* V - V \\|. \\]\n\\[ \\| V^π - V^* \\| \\le \\frac{2γ}{1 - γ} \\| V - V^* \\|. \\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infinite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/inf-horizon.html#notes",
    "href": "mdps/inf-horizon.html#notes",
    "title": "12  Infinite horizon MDPs",
    "section": "Notes",
    "text": "Notes\nThe material included here is referenced from different sources. Perhaps the best sources to study this material are the books by Puterman (2014), Whittle (1982), and Bertsekas (2011).\n\n\n\n\nBertsekas, D.P. 2011. Dynamic programming and optimal control. Athena Scientific. Available at: http://www.athenasc.com/dpbook.html.\n\n\nBlackwell, D. 1965. Discounted dynamic programming. The Annals of Mathematical Statistics 36, 1, 226–235. DOI: 10.1214/aoms/1177700285.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887.\n\n\nShwartz, A. 2001. Death and discounting. IEEE Transactions on Automatic Control 46, 4, 644–647. DOI: 10.1109/9.917668.\n\n\nWhittle, P. 1982. Optimization over time: Dynamic programming and stochastic control. Vol. 1 and 2. Wiley.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infinite horizon MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/mdp-algorithms.html",
    "href": "mdps/mdp-algorithms.html",
    "title": "13  MDP algorithms",
    "section": "",
    "text": "13.1 Value Iteration Algorithm",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>MDP algorithms</span>"
    ]
  },
  {
    "objectID": "mdps/mdp-algorithms.html#value-iteration-algorithm",
    "href": "mdps/mdp-algorithms.html#value-iteration-algorithm",
    "title": "13  MDP algorithms",
    "section": "",
    "text": "Value Iteration Algorithm\n\n\n\n\nStart with any \\(V_0 \\in \\reals^n\\).\nRecursively compute \\(π_k = \\GREEDY(V_k)\\) and \\(V_{k+1} = \\BELLMAN^{π_k} V_k = \\BELLMAN^* V_k\\).\nDefine \\[ \\begin{align*}\n   \\underline δ_k &= \\frac{γ}{1-γ} \\min_s \\{ V_k(s) - V_{k-1}(s) \\}, \\\\\n   \\bar δ_k &=       \\frac{γ}{1-γ} \\max_s \\{ V_k(s) - V_{k-1}(s) \\}.\n\\end{align*} \\]\n\nThen, for all \\(k\\)\n\n\\(V_k + \\underline δ_k \\ONES \\le V^* \\le V_k + \\bar δ_k \\ONES\\).\n\\((\\underline δ_k - \\bar δ_k) \\ONES  \\le V^{π_k} - V^* \\le (\\bar δ_k -\n\\underline δ_k) \\ONES.\\)\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy construction, \\[ \\begin{align*}\n   \\BELLMAN^* V_k - V_k &= \\BELLMAN^* V_k - \\BELLMAN^* V_{k-1} \\\\\n   & \\le \\BELLMAN^{π_{k-1}} V_k - \\BELLMAN^{π_{k-1}} V_{k-1}\\\\\n   & \\le γ P_{π_{k-1}}[ V_k - V_{k-1} ] \\\\\n   &= (1-γ) \\bar δ_k \\ONES.\n\\end{align*} \\] Thus, by Proposition 12.6, we have \\[\\begin{equation} \\label{eq:VI-1}\n  V^* \\le V_k + \\bar δ_k \\ONES.\n\\end{equation}\\] Note that \\(\\BELLMAN^* V_k = \\BELLMAN^{π_k} V_k\\). So, we have also show that \\(\\BELLMAN^{π_k} V_k - V_k \\le (1-γ) \\bar δ_k \\ONES\\). Thus, again by Proposition 12.6, we have \\[\\begin{equation}\\label{eq:VI-2}\n  V^{π_k} \\le V_k + \\bar δ_k \\ONES.\n\\end{equation}\\]\nBy a similar argument, we can show \\[\\begin{equation}\\label{eq:VI-3}\n  V^* \\ge V_k + \\underline δ_k \\ONES\n\\quad\\text{and}\\quad\nV^{π_k} \\ge V_k + \\underline δ_k \\ONES.\n\\end{equation}\\]\nEq. \\eqref{eq:VI-1} and \\eqref{eq:VI-3} imply the first relationship of the result. To establish the second relationship, note that the triangle inequality \\[ \\max\\{ V^{π_k} - V^* \\} \\le\n   \\max\\{ V^{π_k} - V_k \\} + \\max\\{ V_{k} - V^* \\}\n   \\le (\\bar δ_k - \\underline δ_k) \\ONES.\n\\] Similarly, \\[\n  \\max\\{ V^* - V^{π_k} \\} \\le\n   \\max \\{ V^* - V_k \\} + \\max\\{ V_k - V^{π_k} \\}\n   \\le (\\bar δ_k - \\underline δ_k) \\ONES.\n\\] Combining the above two equation, we get the second relationship of the result.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>MDP algorithms</span>"
    ]
  },
  {
    "objectID": "mdps/mdp-algorithms.html#policy-iteration-algorithm",
    "href": "mdps/mdp-algorithms.html#policy-iteration-algorithm",
    "title": "13  MDP algorithms",
    "section": "13.2 Policy Iteration Algorithm",
    "text": "13.2 Policy Iteration Algorithm\n\nLemma 13.1 (Policy improvement) Suppose \\(V^π\\) is the fixed point of \\(\\BELLMAN^π\\) and \\(μ = \\GREEDY(V^{π})\\). Then, \\[V^{μ}(s) \\le V^π(s), \\quad \\forall s \\in \\ALPHABET S. \\] Moreover, if \\(π\\) is not optimal, then at least one inequality is strict.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[ V^π = \\BELLMAN^π V^π \\ge \\BELLMAN^* V^π = \\BELLMAN^{μ} V^π.\\] Thus, \\[ V^π \\ge V^{μ}. \\]\nFinally, suppose \\(V^μ = V^π\\). Then, \\[ V^μ = \\BELLMAN^μ V^μ = \\BELLMAN^μ V^π = \\BELLMAN^* V^π = \\BELLMAN\nV^μ. \\] Thus, \\(V^μ\\) (and \\(V^π\\)) is the unique fixed point of \\(\\BELLMAN^*\\). Hence \\(μ\\) and \\(π\\) are optimal.\n\n\n\n\n\n\n\n\n\n Policy Iteration Algorithm\n\n\n\n\nStart with an arbitrary policy \\(π_0\\). Compute \\(V_0 = \\BELLMAN^{π_0} V_0\\).\nRecursively compute a policy \\(π_k\\) such that \\[ π_k \\in \\GREEDY(V_{k-1})\\] and compute the performance of the policy using \\[ V_k = \\BELLMAN^{π_k} V_k.\\]\nStop if \\(V_k = V_{k-1}\\) (or \\(π_k = π_{k-1}\\)).\n\n\n\nThe policy improvement lemma (Lemma 13.1) implies that \\(V_{k-1} \\ge\nV_k\\). Since the state and action spaces are finite, there are only a finite number of policies. The value function improves at each step. So the process must converge in finite number of iterations. At convergence, \\(V_k = V_{k-1}\\) and the policy improvement lemma implies that the corresponding policies \\(π_k\\) or \\(π_{k-1}\\) are optimal.\n\n13.2.1 Policy iteration as Newton-Raphson algoritm\n\nRecall that the main idea behind Newton-Raphson is as follows. Suppose we want to solve a fixed point equation \\(V = \\BELLMAN^* V\\) and we have an approximate solution \\(V_k\\). Then we can search for an improved soluiton \\(V_{k+1} = V_k + Δ_k\\) by setting \\[\\begin{equation} \\label{eq:NR}\nV_k + Δ_k = \\mathcal{B}( V_k + Δ_k ),\n\\end{equation} \\] expanding the right-hand side as far as first-order terms in \\(Δ_k\\) and solving the consequent linear equation for \\(Δ_k\\).\nNow, let’s try to apply this idea to find the fixed point of the Bellman equation. Suppose we have identified a guess \\(V_k\\) and \\(\\BELLMAN^* V_k =\n\\BELLMAN^{π_{k+1}} V_k\\). Because the choice of control action \\(a\\) is optimization out in \\(\\BELLMAN^*\\), the varation of \\(a\\) induced by the variation \\(Δ_k\\) of \\(V_k\\) has no first-order effect on the value of \\(\\BELLMAN^*(V_k +\nΔ_k)\\). Therefore, \\[\n  \\mathcal{B}(V_k + Δ_k) = \\BELLMAN^{π_{k+1}}(V_k + Δ_k) + o(Δ_k).\n\\] It follows that the linearized version of \\eqref{eq:NR} is just \\[\n  V_{k+1} = \\BELLMAN^{π_{k+1}} V_{k+1}.\n\\] That is, \\(V_{k+1}\\) is just the value function for the policy \\(π_{k+1}\\), where \\(π_{k+1}\\) was deduced from the value function \\(V_k\\) exactly by the policy improvement procedure. Therefore, we can conclude the following.\n\nTheorem 13.1 The policy improvement algorithm is equivalent to the application of Newton-Raphson algorithm to the fixed point equation \\(V = \\BELLMAN^* V\\) of dynamic programming.\n\nThe equivalence between policy iteration and Newton-Raphson partily explains why policy iteration approaches converge in few iterations.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>MDP algorithms</span>"
    ]
  },
  {
    "objectID": "mdps/mdp-algorithms.html#optimistic-policy-iteration",
    "href": "mdps/mdp-algorithms.html#optimistic-policy-iteration",
    "title": "13  MDP algorithms",
    "section": "13.3 Optimistic Policy Iteration",
    "text": "13.3 Optimistic Policy Iteration\n\n\n\n\n\n\n Optimistic Policy Iteration Algorithm\n\n\n\n\nFix a sequence of integers \\(\\{\\ell_k\\}_{k \\in \\integers_{\\ge 0}}\\).\nStart with an initial guess \\(V_0 \\in \\reals^n\\).\nFor \\(k=0, 1, 2, \\dots\\), recursively compute a policy \\(π_k\\) such that \\[ π_k \\in \\GREEDY(V_k) \\] and then update the value function \\[ V_{k+1} = (\\BELLMAN^{π_k})^{\\ell_k} V_k. \\]\n\n\n\nNote that if \\(\\ell_k = 1\\), the optimistic policy iteration is equivalent to value iteration and if \\(\\ell_k = \\infty\\), then optimistic policy iteration is equal to policy iteration.\nIn the remainder of this section, we state the modifications of the main results to establish the convergence bounds for optimistic policy iteration.\n\nProposition 13.1 For any \\(V \\in \\reals^n\\) and \\(m \\ONES \\in \\reals_{\\ge 0}\\)\n\nIf \\(V + m \\ONES \\ge \\BELLMAN^* V = \\BELLMAN^π V\\), then for any \\(\\ell \\in \\integers_{&gt; 0}\\), \\[ \\BELLMAN^* V + \\frac{γ}{1 - γ} m \\ONES \\ge (\\BELLMAN^π)^\\ell V \\] and \\[ (\\BELLMAN^π)^\\ell V + γ^\\ell m \\ONES \\ge \\BELLMAN^*( (\\BELLMAN^π)^\\ell V). \\]\n\n\nThe proof is left as an exercise.\n\nProposition 13.2 Let \\(\\{(V_k, π_k)\\}_{k \\ge 0}\\) be generated as per the optimistic policy iteration algorithm. Define \\[ \\alpha_k = \\begin{cases}\n  1, & \\text{if } k = 0 \\\\\n  γ^{\\ell_0 + \\ell_1 + \\dots + \\ell_{k-1}}, & \\text{if } k &gt; 0.\n\\end{cases}\\] Suppose there exists an \\(m \\in \\reals\\) such that \\[ \\| V_0 - \\BELLMAN^* V_0 \\| \\le m. \\] Then, for all \\(k \\ge 0\\), \\[ \\BELLMAN^* V_{k+1} - \\alpha_{k+1} m \\le V_{k+1} \\le\n\\BELLMAN^* V_k + \\frac{γ}{1-γ} \\alpha_k m.\n\\] Moreover, \\[ V_{k} - \\frac{(k+1) γ^k}{1-γ} m \\le\n    V^* \\le\n    V_k + \\frac{\\alpha_k}{1 - γ} m \\le\n    V_k + \\frac{γ^k}{1 - γ} m. \\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>MDP algorithms</span>"
    ]
  },
  {
    "objectID": "mdps/mdp-algorithms.html#exercises",
    "href": "mdps/mdp-algorithms.html#exercises",
    "title": "13  MDP algorithms",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 13.1 Show that the error bound for value iteration is monotone with the number of iterations, i.e, \\[ V_k + \\underline δ_k \\ONES \\le V_{k+1} + \\underline δ_{k+1} \\ONES\n\\le V^*\n\\le V_{k+1} + \\bar δ_{k+1} \\ONES \\le V_k + \\bar δ_k \\ONES. \\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>MDP algorithms</span>"
    ]
  },
  {
    "objectID": "mdps/mdp-algorithms.html#notes",
    "href": "mdps/mdp-algorithms.html#notes",
    "title": "13  MDP algorithms",
    "section": "Notes",
    "text": "Notes\nThe techniques for value iteration and policy improvement were formalized by Howard (1960). The equivalence of policy improvement and the Newton-Raphson algorithm was demonstrated in the LQ case by Whittle and Komarova (1988), for which it holds in a tighter sense.\n\n\n\n\nHoward, R.A. 1960. Dynamic programming and markov processes. The M.I.T. Press.\n\n\nWhittle, P. and Komarova, N. 1988. Policy improvement and the newton-raphson algorithm. Probability in the Engineering and Informational Sciences 2, 2, 249–255. DOI: 10.1017/s0269964800000760.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>MDP algorithms</span>"
    ]
  },
  {
    "objectID": "mdps/inventory-management-revisited.html",
    "href": "mdps/inventory-management-revisited.html",
    "title": "14  Inventory management (revisted)",
    "section": "",
    "text": "Exercises\nThe plots below verify this result numerically. We simulate the system with parameters \\(μ = 5\\), \\(p = 1\\), \\(γ = 0.9\\).\nValuePlotExp = Plot.plot({\n  grid: true,\n  x: { domain: [-11, 11] },\n  y: { domain: [0, 200] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    Plot.ruleX(threshold_exp.filter(d =&gt; d.shortage == cs_exp && d.holding == ch_exp), \n               {x: \"threshold\", stroke: \"red\"} ),\n    // Data\n    Plot.line(exponential.filter(d =&gt; d.shortage == cs_exp && d.holding == ch_exp),\n              {x: \"state\", y: \"policy\", stroke: \"black\"}),\n  ]}\n)\n\n\n\n\n\n\n\n\nFigure 14.3: Optimal policy for model with exponential demand. The black curve shows the optimal policy computed via value iteration and the red line shows the threshold computed via the formula of Exercise 14.1.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Inventory management (revisted)</span>"
    ]
  },
  {
    "objectID": "mdps/inventory-management-revisited.html#exercises",
    "href": "mdps/inventory-management-revisited.html#exercises",
    "title": "14  Inventory management (revisted)",
    "section": "",
    "text": "Exercise 14.1 Suppose that the arrival process is exponential with rate \\(1/\\mu\\), i.e., the density of \\(W\\) is given by \\(e^{-s/\\mu}/\\mu\\). Show that the optimal threshold is given by \\[ s^* = \\mu \\log \\left[ \\frac{ c_h + c_s} { c_h + p (1-γ)/γ} \\right]. \\]\nHint: Recall that the CDF the exponential distribution is \\(F(s) = 1 - e^{-s/μ}\\).\n\n\nviewof ch_exp = Object.assign(Inputs.range([0.5, 5], {label: \"ch\", step: 0.5, value: 2.0 }), {style: '--label-width:20px'})\nviewof cs_exp = Object.assign(Inputs.range([0.5, 5], {label: \"cs\", step: 0.5, value: 5.0 }), {style: '--label-width:20px'})",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Inventory management (revisted)</span>"
    ]
  },
  {
    "objectID": "mdps/inventory-management-revisited.html#notes",
    "href": "mdps/inventory-management-revisited.html#notes",
    "title": "14  Inventory management (revisted)",
    "section": "Notes",
    "text": "Notes\nThe idea of using reward shaping to derive a closed form expression for inventory management is taken from Whittle (1982). It is interesting to note that Whittle (1982) uses the idea of reward shaping more than 17 years before the paper by Ng et al. (1999) on reward shaping. It is possible that Whittle was using the results of Porteus (1975). As far as I know, the explicit formula of the threshold presented in Theorem 14.1 has not appeared in the literature before.\nAs established in the notes on Lipschitz MDPs, it can be shown that the optimal value function for the inventory management model above is Lipschitz continuous.\nIn the analysis above, we did not formally verify that all the functions exist and infinimizations were well defined. For a detailed discussion of such technical details, see Feinberg (2016).\n\n\n\n\nFeinberg, E.A. 2016. Optimality conditions for inventory control. In: Optimization challenges in complex, networked and risky systems. INFORMS, 14–45. DOI: 10.1287/educ.2016.0145.\n\n\nNg, A.Y., Harada, D., and Russell, S. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. ICML, 278–287. Available at: http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf.\n\n\nPorteus, E.L. 1975. Bounds and transformations for discounted finite markov decision chains. Operations Research 23, 4, 761–784. DOI: 10.1287/opre.23.4.761.\n\n\nWhittle, P. 1982. Optimization over time: Dynamic programming and stochastic control. Vol. 1 and 2. Wiley.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Inventory management (revisted)</span>"
    ]
  },
  {
    "objectID": "mdps/mobile-edge-computing.html",
    "href": "mdps/mobile-edge-computing.html",
    "title": "15  Service Migration in Mobile edge computing",
    "section": "",
    "text": "15.1 Structure of the optimal policy\nWe provide a basic characterization of the optimal policy.\nProposition 15.1 states that the optimal policy always migrates the user to a server which is closer than the one already serving the user.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Service Migration in Mobile edge computing</span>"
    ]
  },
  {
    "objectID": "mdps/mobile-edge-computing.html#structure-of-the-optimal-policy",
    "href": "mdps/mobile-edge-computing.html#structure-of-the-optimal-policy",
    "title": "15  Service Migration in Mobile edge computing",
    "section": "",
    "text": "Proposition 15.1 Let \\(π^*\\) denote the optimal policy. Then for any \\((x,s) \\in \\ALPHABET\nX × \\ALPHABET S\\), we have \\[ \\| x - π^*(x,s) \\| \\le \\| x - s \\|. \\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result using an interchange argument. Suppose we are given a service migration policy \\(π\\) such that the service is migrated to a location farther away from the user, i.e., \\(\\|x - a\\| &gt; \\| x - s \\|\\). We will show that for an arbitrary sample path of the user locations \\(\\{ x_t\\}_{t \\ge 1}\\), we can find a (possibly history dependent) policy \\(μ\\) that does not migrate to locations further away from the user in any time slot and performs no worse than policy \\(π\\).\nGiven a arbitrary sample path of user locations \\(\\{x_t\\}_{t \\ge 1}\\) let \\(t_0\\) denote the first timeslot in which the service is migrated to somewhere farther away from the user when following policy \\(π\\). The state of \\(t_0\\) is \\((x_{t_0}, s_{t_0})\\) and the policy \\(π\\) moves the service to server \\(a_{t_0} =\nπ(x_{t_0}, s_{t_0})\\), where \\(\\|x_{t_0} - a_{t_0}\\| &gt; \\| x_{t_0} - s_{t_0} \\|\\). Let \\(\\{a^π_t\\}_{t \\ge t_0}\\) denote the subsequent locations of the server (after migration) under policy \\(π\\).\nNow, we define a policy \\(μ\\) such that the following conditions are satisfied for the given sample path \\(\\{x_t\\}_{t \\ge 1}\\) of the user locations as follows. The policy \\(μ\\) chooses the same migration actions as policy \\(π\\) in timeslots \\(t &lt; t_0\\).\nNow, suppose \\[\\begin{equation} \\label{eq:1}\n  \\| x_{t} - s^π_{t_0} \\| \\le \\| x_{t} - a^π_{t} \\|, \\quad \\forall t &gt; t_0.\n\\end{equation}\\] Then, the policy \\(μ\\) does not choose any migrations from time \\(t_0\\) onwards. Hence, \\(a^h_t = s^π_{t_0}\\) for all \\(t \\ge t_0\\). Note that from time \\(t_0\\) onwards, policy \\(μ\\) doesn’t incur any migration cost and always incurs a transmission cost which is less than \\(π\\). Hence, policy \\(μ\\) performs at least as well as policy \\(π\\).\nNow suppose \\eqref{eq:1} does not hold. Then define \\(t_m\\) to be the first timeslot after \\(t_0\\) such that \\[\n  \\| x_{t_m} - s^π_{t_0} \\| &gt; \\| x_{t_m} - a^π_{t_m} \\|.\n\\]\nNow, we define policy \\(μ\\) as a policy which does not specify any migrations for time \\(t \\in [t_0, t_m - 1]\\), migrates to location \\(a^π_{t_m}\\) at timeslot \\(t_m\\), and follows policy \\(π\\) from \\(t_m\\) onwards.\nNote that policies \\(π\\) and \\(μ\\) agree on \\([1, t_0 -1]\\) and \\([t_m + 1, ∞)\\). In the interval \\([t_0, t_m]\\), \\[\n  \\| x_{t} - a^h_t \\| \\le \\| x_t - a^π_t \\|.\n\\] Thus, the transmission cost of policy \\(μ\\) is no more than the transmission cost of policy \\(π\\).\nNow, the migration cost incurred by policy \\(π\\) in the interval \\([t_0, t_m]\\) can be lower bounded by the migration cost incurred by policy \\(μ\\) as follows: \\[\\begin{align*}\n  \\hskip 2em & \\hskip -2em\n  γ^{t_0 - 1}  b(\\| s^π_{t_0} - a^π_{t_0} \\|) +\n  γ^{t_0 } b(\\| a^π_{t_0} - a^π_{t_0 + 1} \\|) + \\cdots  +\n  γ^{t_m - 1} b(\\| a^π_{t_m -1} - a^π_{t_m} \\|) \\\\\n  &\\ge\n  γ^{t_m - 1}\\bigl[\n    b(\\| s^π_{t_0} - a^π_{t_0} \\|) +\n   b(\\| a^π_{t_0} - a^π_{t_0 + 1} \\|) + \\cdots  +\n   b(\\| a^π_{t_m -1} - a^π_{t_m} \\|)  \\bigr]\n  \\\\\n  &\\ge\n  γ^{t_m - 1} b(\\| s^π_{t_0} - a^π_{t_m} \\|),\n\\end{align*}\\] where the first inequality follows because \\(γ &lt; 1\\) and the second follows from the triangle inequality.\nHence, policy \\(μ\\) performs at least as well as policy \\(π\\). The above procedure can be repeated so that all the mitigation actions to a location farther away from the user can be removed without increasing the overall cost. \nNote that the policy \\(μ\\) constructed above is a history dependent policy. From the result for infinite horizon MDP, we know that a history dependent policy cannot outperform Markovian policies. Therefore, there exists a Markovian policy that does not migrate to a location farther away from the user, which does not perform worse than \\(π\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Service Migration in Mobile edge computing</span>"
    ]
  },
  {
    "objectID": "mdps/mobile-edge-computing.html#notes",
    "href": "mdps/mobile-edge-computing.html#notes",
    "title": "15  Service Migration in Mobile edge computing",
    "section": "Notes",
    "text": "Notes\nThe model and results presented here are taken from Wang et al. (2019). See Urgaonkar et al. (2015) for a variation of this model.\n\n\n\n\nUrgaonkar, R., Wang, S., He, T., Zafer, M., Chan, K., and Leung, K.K. 2015. Dynamic service migration and workload scheduling in edge-clouds. Performance Evaluation 91, 205–228. DOI: 10.1016/j.peva.2015.06.013.\n\n\nWang, S., Urgaonkar, R., Zafer, M., He, T., Chan, K., and Leung, K.K. 2019. Dynamic service migration in mobile edge computing based on Markov decision process. IEEE/ACM Transactions on Networking 27, 3, 1272–1288. DOI: 10.1109/tnet.2019.2916577.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Service Migration in Mobile edge computing</span>"
    ]
  },
  {
    "objectID": "mdps/computational-complexity-vi.html",
    "href": "mdps/computational-complexity-vi.html",
    "title": "16  Computational complexity of value interation",
    "section": "",
    "text": "16.1 Span norm contraction\nLet \\(\\SPAN(v) = \\max(v) - \\min(v)\\) denotes the span semi-norm. We start by stating some basic properties of span semi-norm.\nProperties 1–3 imply that \\(\\SPAN(v)\\) is a semi-norm. However, it is not a norm because of property 4; that is, \\(\\SPAN(v) = 0\\) does not imply that \\(v =\n0\\). If \\(\\SPAN(v) = 0\\), then \\(v = m \\mathbf{1}\\) for some scalar \\(m\\).\nA basic result for our analysis is the following:\nProposition 16.1 illustrates the “averaging” property of a transition matrix. By multiplying a vector by a transition matrix, the resulting vector has components which are more nearly equal. When \\(P\\) is a square matrix, the quantity \\(β_P\\) is called the ergodicity coefficient, which is often written in an alternative form by using the relation \\(|a - b| = (a + b) - 2 \\min(a,b)\\): \\[\n  β_P = \\frac12\n  \\max_{s,s' \\in \\ALPHABET S} \\sum_{z \\in \\ALPHABET S}\n  \\bigl| P_{sz} - P_{s'z} \\bigr|. \\] The ergodicity coefficient is an upper bound on the second largest eigenvalue of \\(P\\). \\(β_P\\) equals \\(0\\) if all rows of \\(P\\) are equal and equals \\(1\\) if at least two rows of \\(P_d\\) are orthogonal. From a different perspective, \\(β_P &lt;\n1\\) if for each pair of states there exists at least one state which they both can reach with positive probability in one step.\nDefine the contraction factor \\[\\begin{equation}\\label{eq:contraction}\n  β = \\max_{\\substack{ s,s' \\in \\ALPHABET S \\\\ a \\in \\ALPHABET A(s), w \\in\n  \\ALPHABET A(s')}}\n  \\biggl[ 1 - \\sum_{z \\in \\ALPHABET S}\n    \\min\\{ P(z | s,a), P(z | s', w) \\biggr].\n\\end{equation}\\] Note that \\(β \\in [0, 1]\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Computational complexity of value interation</span>"
    ]
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#span-norm-contraction",
    "href": "mdps/computational-complexity-vi.html#span-norm-contraction",
    "title": "16  Computational complexity of value interation",
    "section": "",
    "text": "\\(\\SPAN(v) \\ge 0\\) for all \\(v \\in \\reals^n\\)\n\\(\\SPAN(v + w) \\le \\SPAN(v) + \\SPAN(w)\\) for all \\(v, w \\in \\reals^n\\).\n\\(\\SPAN(m v) \\le |m| \\SPAN(v)\\) for all \\(v \\in \\reals^n\\) and \\(m \\in \\reals\\).\n\\(\\SPAN(v + m \\mathbf{1}) = \\SPAN(v)\\) for all \\(m \\in \\reals\\).\n\\(\\SPAN(v) = \\SPAN(-v)\\).\n\\(\\SPAN(v) \\le 2\\|v\\|\\).\n\n\n\n\nProposition 16.1 Let \\(v \\in \\reals^n\\) and \\(P\\) be any matrix of compatible dimensions. Then, \\[ \\SPAN(P v) \\le β_P \\SPAN(v), \\] where \\[\\begin{equation} \\label{eq:span-matrix}\n  β_P = 1 - \\min_{s, s' \\in \\ALPHABET S}\n  \\sum_{z \\in \\ALPHABET S} \\min\\{ P_{sz}, P_{s'z} \\}.\n\\end{equation}\\] Furhermore, \\(β_P \\in [0, 1]\\) and there exists a \\(v \\in \\reals^n\\) such that \\(\\SPAN(Pv) = β_P \\SPAN(v)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNote that for any \\(v \\in \\reals^n\\) \\[\\begin{align*}\n\\SPAN(Pv) &= \\max_{s \\in \\ALPHABET S} \\sum_{z \\in \\ALPHABET S} P_{sz} v_z\n- \\min_{s' \\in \\ALPHABET S} \\sum_{z \\in \\ALPHABET S} P_{s'z} v_z \\\\\n&= \\max_{s, s' \\in \\ALPHABET S}\n  \\sum_{z \\in \\ALPHABET S} P_{sz} v_z - \\sum_{z \\in \\ALPHABET S} P_{s'z} v_z\n\\end{align*}\\] Let \\(B(z; s,s') = \\min\\{ P_{sz}, P_{s'z} \\}\\). Then consider \\[\\begin{align*}\n  \\sum_{z \\in \\ALPHABET S} P_{sz} v_z - \\sum_{z \\in \\ALPHABET S} P_{s'z} v_z\n  &=\n  \\sum_{z \\in \\ALPHABET S} [ P_{sz} - B(z; s,s') ] v_z -\n  \\sum_{z \\in \\ALPHABET S} [ P_{s'z} - B(z; s,s') ] v_z \\\\\n  &\\le\n  \\sum_{z \\in \\ALPHABET S} [ P_{sz} - B(z; s,s') ] \\max(v) -\n  \\sum_{z \\in \\ALPHABET S} [ P_{s'z} - B(z; s,s') ] \\min(v)\n  \\\\\n  &= \\biggl[ 1 - \\sum_{z \\in \\ALPHABET S} B(z; s, s') \\biggr] \\SPAN(v).\n\\end{align*}\\] Hence, \\[ \\SPAN(Pv) \\le \\max_{s, s' \\in \\ALPHABET S} \\biggl[\n  1 - \\sum_{z \\in \\ALPHABET S} B(z; s, ) \\biggr] \\SPAN(v). \\]\nNow we show that there exists a \\(v\\) such that \\eqref{eq:span-matrix} holds with equality. If \\(β_P = 0\\), then \\(P\\) has equal rows, so that \\(\\SPAN(Pv) = 0 = 0 \\cdot \\SPAN(v)\\) for all \\(v \\in \\reals^n\\). Suppose \\(β_P\n&gt; 0\\). Using the identity \\([ a - b]^{+} = a - \\min(a,b)\\), we can write \\[\nβ_P = \\max_{s,s' \\in \\ALPHABET S}\n\\sum_{z \\in \\ALPHABET  S} \\bigl[ P_{sz} - P_{s'z} \\bigr]^{+}.\n\\] Let \\(s^*\\) and \\(s'^*\\) be such that \\[\n  β_P = \\sum_{z \\in \\ALPHABET S}  \n\\sum_{z \\in \\ALPHABET  S} \\bigl[ P_{s^*z} - P_{s'^*z} \\bigr]^{+}.\n\\] Define \\(v\\) by \\[\n  v_z = \\IND\\{ P_{s^*z} &gt; P_{s'^*z} \\}.\n\\] Then, note that \\(\\SPAN(v) = 1\\) and \\[\\begin{align*}\n  \\SPAN(Pv) &\\ge\n  \\sum_{z \\in \\ALPHABET S} P_{s^* z} v_z -\n  \\sum_{z \\in \\ALPHABET S} P_{s'^* z} v_z \\\\\n  &=\n  \\sum_{z \\in \\ALPHABET  S} \\bigl[ P_{s^*z} - P_{s'^*z} \\bigr]^{+}\n  \\\\\n  &= β_P \\SPAN(v).\n\\end{align*}\\] Combining with \\eqref{eq:span-matrix}, we get \\(\\SPAN(Pv) = β_P\n\\SPAN(v)\\).\n\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that \\[\\begin{equation}\\label{eq:ergodicity-bound}\nβ_P \\le 1 - \\sum_{z \\in \\ALPHABET S} \\min_{s \\in \\ALPHABET S} P_{sz}\n=: β_P'\n\\end{equation}\\] which is easier to compute.\n\n\n\n\nTheorem 16.1 For any \\(V_1, V_2 \\in \\reals^n\\), \\[ \\SPAN(\\mathcal B V_1 - \\mathcal B V_2) \\le γ β\\, \\SPAN(V_1 - V_2). \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(π_i\\) be such that \\(\\mathcal B V_i = \\mathcal B_{π_i}V_i\\). Let \\[\\begin{align*}\n  s^* &= \\arg \\max_{s \\in \\ALPHABET S}(\\mathcal B V_1(s) - \\mathcal B V_2(s)),\n  \\\\\n  s_* &= \\arg \\min_{s \\in \\ALPHABET S}(\\mathcal B V_1(s) - \\mathcal B V_2(s)),\n\\end{align*}\\] Then, \\[\n  \\mathcal B V_1(s^*) - \\mathcal B V_2(s^*) \\le\n  \\mathcal B_{π_2} V_1(s^*) - \\mathcal B_{π_2} V_2(s^*)\n  = γ P_{π_2}(V_1 - V_2)(s^*)\n\\] and \\[\n  \\mathcal B V_1(s_*) - \\mathcal B V_2(s_*) \\ge\n  \\mathcal B_{π_1} V_1(s^*) - \\mathcal B_{π_1} V_2(s^*)\n  = γ P_{π_1}(V_1 - V_2)(s^*).\n\\] Therefore, \\[\\begin{align*}\n  \\SPAN(\\mathcal B V_1 - \\mathcal B V_2) &\\le\n  γ P_{π_2}(V_1 - V_2)(s^*) - γ P_{π_1}(V_1 - V_2)(s_*) \\\\\n  &\\le \\max_{s \\in \\ALPHABET S} γ P_{π_2} (V_1 - V_2)(s) -\n  \\min_{s \\in \\ALPHABET S} γ P_{π_1}(V_1 - V_2)(s)\n  \\\\\n  &\\le \\SPAN(γ\\, \\ROWS(P_{π_2}, P_{π_1})(V_1 - V_2).\n\\end{align*}\\] By applying Proposition 16.1, we get \\[ \\SPAN(\\mathcal B V_1 - \\mathcal B V_2) \\le γβ_{\\bar P} \\SPAN(V_1 - V_2), \\] where \\(β_{\\bar P}\\) is given by \\eqref{eq:span-matrix} with \\(\\bar P =\n\\ROWS(P_{π_2}, P_{π_1})\\). The result follows by noting that \\(β_{\\bar P}\\) is at most \\(β\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Computational complexity of value interation</span>"
    ]
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#computational-complexity-of-value-iteration",
    "href": "mdps/computational-complexity-vi.html#computational-complexity-of-value-iteration",
    "title": "16  Computational complexity of value interation",
    "section": "16.2 Computational complexity of value iteration",
    "text": "16.2 Computational complexity of value iteration\nNote that \\(β \\in [0, 1]\\). We will first rule out the case \\(β = 0\\). If \\(β = 0\\), then \\(P(z | s, a) = P(z | s', w)\\) for all \\(s, s', z \\in \\ALPHABET S\\) and \\(a \\in\n\\ALPHABET A(s)\\) and \\(w \\in \\ALPHABET A(s')\\), which implies that all deterministic policies have the same transition probabilities. Therefore, a deterministic policy \\(π\\) is optimal if and only if it minimize the one-step cost. Thus, the case with \\(β = 0\\) is trivial. So, in the rest of the analysis, we assume that \\(β \\in (0, 1)\\).\n\nTheorem 16.2 Start with an abritrary \\(V_0\\). If \\(Δ_1 = 0\\), then we obtain an optimal policy in iteration 1. Otherwise, for any \\(ε &gt; 0\\), value iteration finds an \\(ε\\)-optimal policy in no more than \\(K^*(γ)\\) iterations, where \\[\\begin{equation}\\label{eq:K*}\n  K^*(γ) =  \\left\\lceil\n  \\frac{ \\log \\frac{(1-γ) ε β}{Δ_1} } {\\log(γβ)}\n  \\right\\rceil.\n\\end{equation}\\] In addition, each iteration uses at most \\(O(nM)\\) operations.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf follows from the definition of Bellman operator that each iteration uses at most \\(O(nM)\\) iterations (to compute the \\(Q\\) function, for each state-action pair, we need to compute a sum over \\(n\\) terms).\nFrom Theorem 16.1, we get that \\(Δ_k \\le (γβ)^{k-1} Δ_1\\). Therefore, the minimum number of iterations required to achieve \\(Δ_k \\le\n\\frac{1-γ}{γ}ε\\) is given \\(K^*(γ)\\). \\(\\Box\\)\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that finding the value of \\(β\\) requires computing the sum in \\eqref{eq:contraction} for all couples \\(\\{ (s,a), (s',b) \\}\\) of state action pairs such that \\((s,a) \\neq (s',b)\\). The total number of such pairs are \\(M(M-1)/2 = O(M^2)\\). Therefore, the number of arithematic operators in \\eqref{eq:contraction}, which are additions, is \\(n\\) for each couple. Therefore, computation of \\(β\\) requires \\(O(nM^2)\\) operations, which can be significantly larger than the complexity of computing an \\(ε\\)-optimal policy which is given by Theorem 16.2! Based on \\eqref{eq:ergodicity-bound} we can replace \\(β\\) by in \\eqref{eq:K*} by \\[\n  β' = 1 - \\sum_{z \\in \\ALPHABET S} \\min_{s \\in \\ALPHABET S, a \\in \\ALPHABET A}\n       P(z | s,a)\n\\] which requies \\(O(nM)\\) operations.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Computational complexity of value interation</span>"
    ]
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#the-bound-may-be-exact",
    "href": "mdps/computational-complexity-vi.html#the-bound-may-be-exact",
    "title": "16  Computational complexity of value interation",
    "section": "16.3 The bound may be exact",
    "text": "16.3 The bound may be exact\nWe now present an example (due to Feinberg and He (2020)) to show that the bound in Theorem 16.2 may be exact. Consider an MDP with \\(\\ALPHABET S = \\{1, 2,\n3\\}\\), \\(\\ALPHABET A = \\{1,2 \\}\\), with \\(\\ALPHABET A(1) = \\{1, 2\\}\\) and \\(\\ALPHABET A(2) = \\ALPHABET A(3) = \\{1\\}\\). The per-step transitions are \\[\n  P(1) = \\MATRIX{0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 },\n  \\quad\\text{and}\\quad\n  P(2) = \\MATRIX{0 & 1 & 0 \\\\ * & * & * \\\\ * & * & *},\n\\] where \\(*\\) indicates that the corresponding action is infeasible. The reward matrix is \\[\n  r = \\MATRIX{1 & 0 \\\\ 1 & * \\\\ -1 & *}.\n\\]\nSuppose we start with an initial \\(V_0 = \\VEC(1, 2, -2)\\). Then elementary calculations show that \\[\n  V_k = \\MATRIX{ γ^k \\\\ γ^k \\\\ -γ^k} +\n  \\MATRIX{ \\sum_{\\ell = {\\color{red} 1}}^{k} γ^\\ell \\\\\n           \\sum_{\\ell = 0}^{k} γ^\\ell \\\\\n           \\sum_{\\ell = 0}^{k} γ^\\ell }.\n\\] Thus, \\[\n  V_k - V_{k-1} = \\MATRIX{\n  2 γ^k - γ^{k-1} \\\\\n  2 γ^k - γ^{k-1} \\\\\n  - 2 γ^k + γ^{k-1} }.\n\\] Hence, \\[\n\\SPAN(V_k - V_{k-1}) = 2γ^{k-1} |2γ - 1| = γ^{k-1} \\SPAN(V_1 - V_0).\n\\]\nThus, for this model, the expression \\eqref{eq:K*} is exact.\n\n\n\n\n\n\nRemark\n\n\n\nThe exact number of iterations need not be monotone in \\(γ\\)! In the above example, let \\(ε = 0.02\\), then \\[\n  K^*(0.24) = 3, \\quad\n  K^*(0.47) = 4, \\quad\n  K^*(0.48) = 3.\n\\]\nThus, the number of iterations is not monotone in \\(γ\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Computational complexity of value interation</span>"
    ]
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#notes",
    "href": "mdps/computational-complexity-vi.html#notes",
    "title": "16  Computational complexity of value interation",
    "section": "Notes",
    "text": "Notes\nThe discussion on span semi-norm and Theorem 16.1 is from Puterman (2014). Theorem 16.2 is from Feinberg and He (2020).\n\n\n\n\nFeinberg, E.A. and He, G. 2020. Complexity bounds for approximately solving discounted MDPs by value iterations. Operations Research Letters. DOI: 10.1016/j.orl.2020.07.001.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Computational complexity of value interation</span>"
    ]
  },
  {
    "objectID": "mdps/martingale.html",
    "href": "mdps/martingale.html",
    "title": "17  Thirfty and equalizing policies",
    "section": "",
    "text": "Notes\nSee Davis (1979) for a historical review of martingale methods for stochastic control.\nThe idea of thirfty and equalizing policies is due to Dubins and Savage (2014). It was used by Blackwell (1970) for characterizing optimal solutions of total reward problems and is a commonly used technique in that literature. The “translation” to discounted cost problems is taken from Karatzas and Sudderth (2010).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Thirfty and equalizing policies</span>"
    ]
  },
  {
    "objectID": "mdps/martingale.html#notes",
    "href": "mdps/martingale.html#notes",
    "title": "17  Thirfty and equalizing policies",
    "section": "",
    "text": "Blackwell, D. 1970. On stationary policies. Journal of the Royal Statistical Society. Series A (General) 133, 1, 33. DOI: 10.2307/2343810.\n\n\nDavis, M.H.A. 1979. Martingale methods in stochastic control. In: Stochastic control theory and stochastic differential systems. Springer-Verlag, 85–117. DOI: 10.1007/bfb0009377.\n\n\nDubins, L.E. and Savage, L.J. 2014. How to gamble if you must: Inequalities for stochastic processes. Dover Publications.\n\n\nKaratzas, I. and Sudderth, W.D. 2010. Two characterizations of optimality in dynamic programming. Applied Mathematics and Optimization 61, 3, 421–434. DOI: 10.1007/s00245-009-9093-x.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Thirfty and equalizing policies</span>"
    ]
  },
  {
    "objectID": "mdps/linear-programming.html",
    "href": "mdps/linear-programming.html",
    "title": "18  Linear programming formulation",
    "section": "",
    "text": "Primal Linear Program\nChoose a weight function \\(p \\colon \\ALPHABET S \\to \\reals_{\\ge 0}\\) such that \\(\\sum_{s \\in \\ALPHABET S} p(s) = 1\\). This can be thought of as the distribution of the initial state. Then the primal LP corresponding to \\eqref{eq:constaints} is \\[\\begin{gather}\n  \\max \\sum_{s \\in \\ALPHABET S} p(s) V(s) \\notag \\\\\n  \\text{subject to}\\quad\n  V(s) - γ \\sum_{z \\in \\ALPHABET S} P(z|s,a)V(z) \\le c(s,a),\n  \\quad \\forall s \\in \\ALPHABET S, a \\in \\ALPHABET A.\n  \\label{eq:primal}\n\\end{gather}\\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Linear programming formulation</span>"
    ]
  },
  {
    "objectID": "mdps/linear-programming.html#constrained-mdps",
    "href": "mdps/linear-programming.html#constrained-mdps",
    "title": "18  Linear programming formulation",
    "section": "18.1 Constrained MDPs",
    "text": "18.1 Constrained MDPs\nSuppose, in addition to the per-step cost function \\(c \\colon \\ALPHABET S\n\\times \\ALPHABET A \\to \\reals\\), we have a per-step constraint function \\(d\n\\colon \\ALPHABET S \\times \\ALPHABET A \\to \\reals\\) and we are interested in the following constrained optimization problem:\n\\[\\begin{gather*}\n\\min \\EXP\\Bigl[ \\sum_{t=1}^\\infty γ^{t-1} c(S_t, A_t) \\Bigr] \\\\\n\\text{subject to}\\quad\n\\EXP\\Bigl[ \\sum_{t=1}^\\infty γ^{t-1} d(S_t, A_t) \\Bigr] \\le D.\n\\end{gather*}\\]\nThe dual LP in this case is given by \\[\\begin{gather}\n  \\min \\sum_{s \\in \\ALPHABET S} \\sum_{a \\in \\ALPHABET A} \\mu(s,a) c(s,a)\n  \\notag \\\\\n  \\text{subject to}\\quad\n  \\mu(s,a) - γ\\sum_{s \\in \\ALPHABET S} \\sum_{a \\in \\ALPHABET A}\n  P(s | z, a) \\mu(s,a) = p(s), \\quad \\forall s \\in \\ALPHABET S, \\notag \\\\\n  \\sum_{s \\in \\ALPHABET S} \\sum_{a \\in \\ALPHABET A} \\mu(s,a) d(s,a) \\le D \\notag \\\\\n  \\mu(s,a) \\ge 0, \\quad \\forall s \\in \\ALPHABET S, a \\in \\ALPHABET A.\n\\end{gather}\\]\nIf we interpret \\(\\mu(s,a)\\) as the occupation measure of any policy, then this formulation follows immediately.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Linear programming formulation</span>"
    ]
  },
  {
    "objectID": "mdps/linear-programming.html#notes",
    "href": "mdps/linear-programming.html#notes",
    "title": "18  Linear programming formulation",
    "section": "Notes",
    "text": "Notes\nThe material in this section is taken from Puterman (2014). See Altman (1999) for a detailed treatment of constrained MDPs.\n\n\n\n\nAltman, Eitan. 1999. Constrained markov decision processes. CRC Press. Available at: http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Linear programming formulation</span>"
    ]
  },
  {
    "objectID": "mdps/lipschitz-mdps.html",
    "href": "mdps/lipschitz-mdps.html",
    "title": "19  Lipschitz MDPs",
    "section": "",
    "text": "19.1 Preliminaries",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Lipschitz MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#preliminaries",
    "href": "mdps/lipschitz-mdps.html#preliminaries",
    "title": "19  Lipschitz MDPs",
    "section": "",
    "text": "Lipschitz continuous functions\nGiven two metric spaces \\((\\ALPHABET X, d_X)\\) and \\((\\ALPHABET Y, d_Y)\\), the Lipschitz constant of function \\(f \\colon \\ALPHABET X \\to \\ALPHABET Y\\) is defined by \\[ \\| f\\|_{L} = \\sup_{x_1 \\neq x_2}\n    \\left\\{ \\frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } :\n    x_1, x_2 \\in \\ALPHABET X \\right\\} \\in [0, ∞]. \\] The function is called Lipschitz continuous if its Lipschitz constant is finite.\nIntuitively, a Lipschitz continuous function is limited by how fast it can change. For example, the following image from Wikipedia shows that for a Lipschitz continuous function, there exists a double cone (white) whose origin can be moved along the graph so that the whole graph always stays outside the double cone.\n\n\n\nImage credit: https://en.wikipedia.org/wiki/File:Lipschitz_Visualisierung.gif\n\n\nLet \\(\\ALPHABET Z\\) be an arbitrary set. A function \\(f \\colon \\ALPHABET X ×\n\\ALPHABET Z \\to \\ALPHABET Y\\) is said to be uniformly Lipschitz in \\(u\\) if \\[ \\sup_{z \\in \\ALPHABET Z} \\| f(\\cdot, z) \\|_L  =\n  \\sup_{z \\in \\ALPHABET Z} \\sup_{x_1 \\neq x_2}\n  \\dfrac{ d_Y(f(x_1,z), f(x_2, z)) }{ d_X(x_1, x_2) } &lt; ∞. \\]\n\n\nSome examples\nA function \\(f \\colon \\reals \\to \\reals\\) is Lipschitz continuous if and only if it has bounded first derivative. The Lipschitz constant of such a function is equal to the maximum absolute value of the derivative.\nHere are some examples of Lipschitz continuous functions:\n\nThe function \\(f(x) = \\sqrt{x^2 + 1}\\) defined over \\(\\reals\\) is Lipschitz continuous because it is everywhere differentiable and the maximum value of the derivative is \\(L = 1\\).\nThe function \\(f(x) = |x|\\) defined over \\(\\reals\\) is Lipschitz continuous with Lipschitz constant equal to \\(1\\). Note that this function is continuous but not differentiable.\nThe function \\(f(x) = x + \\sin x\\) defined over \\(\\reals\\) is Lipschitz continuous with a Lipschitz constant equal to \\(1\\).\nThe function \\(f(x) = \\sqrt{x}\\) defined over \\([0,1]\\) is not Lipschitz continuous because the function becomes infinitely steep as \\(x\\) approaches \\(0\\).\nThe function \\(f(x) = x^2\\) defined over \\(\\reals\\) is not Lipschitz continuous because it becomes arbitrarily steep as \\(x\\) approaches infinity.\nThe function \\(f(x) = \\sin(1/x)\\) is bounded but not Lipschitz because becomes infinitely steep as \\(x\\) approaches \\(0\\).\n\n\n\nProperties of Lipschitz functions\n\nProposition 19.1 Lipschitz continuous functions have the following properties:\n\nIf a function \\(f \\colon (\\ALPHABET X, d_X) \\to (\\ALPHABET Y, d_Y)\\) is Lipschitz continuous, then \\(f\\) is uniformly continuous and measurable.\n\\(\\| f\\|_L = 0\\) if and only if \\(f\\) is a constant.\nIf \\(f \\colon (\\ALPHABET X, d_X) \\to  (\\ALPHABET Y, d_Y)\\) and \\(g \\colon\n(\\ALPHABET Y, d_Y) \\to (\\ALPHABET Z, d_Z)\\) are Lipschitz continuous, then \\[ \\| f \\circ g \\|_L \\le \\| f \\|_L \\cdot \\| g \\|_L. \\]\nThe \\(\\| \\cdot \\|_{L}\\) is a seminorm on the vector space of Lipschitz functions from a metric space \\((\\ALPHABET X, d_X)\\) to \\((\\ALPHABET Y, d_Y)\\). In particular, \\(\\| \\cdot \\|_L\\) has the following properties: \\(\\| f \\|_L \\in [0,\n∞]\\), \\(\\| α f\\|_L = |α| \\cdot \\|f\\|_L\\) for any \\(α \\in \\reals\\), and \\(\\| f_1 +\nf_2 \\|_L \\le \\|f_1 \\|_L + \\|f_2 \\|_L\\).\nGiven a family of functions \\(f_i\\), \\(i \\in I\\), on the same metric space such that \\(\\sup_{i \\in I} f_i &lt; ∞\\), \\[ \\| \\sup_{i \\in I} f_i \\|_{L} \\le \\sup_{i \\in I} \\| f_i \\|_{L}. \\]\nLet \\(f_n\\), \\(n \\in \\integers_{\\ge 1}\\), and \\(f\\) be functions from \\((\\ALPHABET\nX, d_X)\\) to \\((\\ALPHABET Y, d_Y)\\). If \\(f_n\\) converges pointwise to \\(f\\) for \\(n\n\\to ∞\\), then \\[ \\| f \\|_{L} \\le \\lim\\inf_{n \\to ∞} \\| f_i \\|_{L}. \\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Lipschitz MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#kantorovich-distance",
    "href": "mdps/lipschitz-mdps.html#kantorovich-distance",
    "title": "19  Lipschitz MDPs",
    "section": "19.2 Kantorovich distance",
    "text": "19.2 Kantorovich distance\nLet \\(\\mu\\) and \\(\\nu\\) be probability measures on \\((\\ALPHABET X, d_X)\\). The Kantorovich distance between distributions \\(\\mu\\) and \\(\\nu\\) is defined as: \\[ K(\\mu,\\nu) = \\sup_{f : \\| f\\|_L \\le 1 }\n   \\left| \\int_{\\ALPHABET X} f\\, d\\mu - \\int_{\\ALPHABET X} f\\, d\\nu \\right|. \\]\nThe next results follow immediately from the definition of Kantorovich distance.\n\nProposition 19.2 For any Lipschitz function \\(f \\colon (\\ALPHABET X, d_X) \\to (\\reals,\n\\lvert \\cdot \\rvert)\\), and \\(μ,ν\\) are probability measures on \\((\\ALPHABET X,\nd_X)\\), \\[ \\left|\n  \\int_{\\ALPHABET X} f\\, dμ - \\int_{\\ALPHABET X} f\\, dν \\right| \\le\n  \\| f \\|_L \\cdot K(μ,ν). \\]\n\nThe Kantorivich distance is a special class metrics on probability spaces known as integral probability meterics (IPMs). Proposition 19.2 is a special case of a similar general result for IPMs (Proposition 35.4).\n\nSome examples\n\nLet \\((\\ALPHABET X, d_X)\\) be a metric space and for any \\(x,y \\in \\ALPHABET X\\), let \\(δ_x\\) and \\(δ_y\\) denote the Dirac delta distributions centered at \\(x\\) and \\(y\\). Then, \\[ K(δ_x, δ_y) = d_X(x,y). \\]\nLet \\((\\ALPHABET X, d_X)\\) be a Euclidean space with Euclidean norm. Let \\(μ\n\\sim \\mathcal{N}(m_1, \\Sigma_1)\\) and \\(ν \\sim \\mathcal{N}(m_2, \\Sigma_2)\\) be two Gaussian distributions on \\(\\ALPHABET X\\). Then, \\[K(μ,ν) = \\sqrt{ \\| m_1 - m_2 \\|_2^2\n+ \\text{Tr}( \\Sigma_1 + \\Sigma_2 - 2(\\Sigma_2^{1/2} \\Sigma_1 \\Sigma_2^{1/2})^{1/2} ) }. \\] If the two covariances commute, i.e. \\(\\Sigma_1\\Sigma_2 = \\Sigma_2\n\\Sigma_1\\), then, \\[K(μ,ν) = \\sqrt{ \\| m_1 - m_2 \\|_2^2\n+ \\| \\Sigma_1^{1/2} - \\Sigma_2^{1/2} \\|^2_F},\\] where \\(\\| ⋅ \\|_{F}\\) denotes the Frobeinus norm of a matrix.\nWhen \\(\\Sigma_1 = \\Sigma_2\\), we have \\[K(μ,ν) = \\| m_1 - m_2 \\|_2. \\]\nIf \\(\\ALPHABET X = \\reals\\) and \\(d_X = | \\cdot |\\), then for any two distributions \\(μ\\) and \\(ν\\), \\[\\begin{equation}\\label{eq:Kantorovich-CDF}\nK(μ,ν) = \\int_{-∞}^∞ \\left| F_μ(x) - F_ν(x) \\right| dx,\n\\end{equation}\\] where \\(F_μ\\) and \\(F_ν\\) denote the CDF of \\(μ\\) and \\(ν\\).\nFurthermore, if \\(μ\\) is stochastically dominated by \\(ν\\), then \\(F_μ(x) \\ge\nF_ν(x)\\). Thus, \\[\\begin{equation}\\label{eq:Kantorovich-stochastic-dominance}\nK(μ, ν) = \\bar μ - \\bar ν\n\\end{equation}\\] where \\(\\bar μ\\) and \\(\\bar ν\\) are the means of \\(μ\\) and \\(ν\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Lipschitz MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#lipschitz-mdps",
    "href": "mdps/lipschitz-mdps.html#lipschitz-mdps",
    "title": "19  Lipschitz MDPs",
    "section": "19.3 Lipschitz MDPs",
    "text": "19.3 Lipschitz MDPs\nConsider an MDP where the state and action spaces are metric spaces. We use \\(d_S\\) and \\(d_A\\) to denote the corresponding metric. For ease of exposition, we define a metric \\(d\\) on \\(\\ALPHABET S × \\ALPHABET A\\) by \\[ d( (s_1, a_1), (s_2, a_2) ) = d_S(s_1, s_2) + d_A(a_1, a_2). \\]\nWe allow for randomized policies. Thus, given any state \\(s \\in \\ALPHABET S\\), \\(π(\\cdot | s)\\) is a probability distribution on \\(\\ALPHABET A\\). We say that a (possibly) randomized policy \\(π\\) has a Lipschitz constant of \\(L_π\\) if for any \\(s_1, s_2 \\in \\ALPHABET S\\), \\[ K(π(\\cdot| s_1), π(\\cdot | s_2)) \\le L_π d_S(s_1, s_2). \\]\nNote that if \\(π\\) is deterministic, then due to property of Kantorovich distance between delta distributions, the above relationship simplifies to \\[ d_A(π(s_1), π(s_2)) \\le L_π d_S(s_1, s_2). \\]\n\nDefinition 19.1 An MDP is \\((L_c, L_p)\\)-Lipschitz if for all \\(s_1, s_2 \\in \\ALPHABET S\\) and \\(a_1, a_2 \\in \\ALPHABET A\\),\n\n\\(| c(s_1, a_1) - c(s_2, a_2) | \\le L_c\\bigl( d_S(s_1, s_2) + d_A(a_1,\na_2) \\bigr)\\).\n\\(K(p(\\cdot | s_1, a_1), p(\\cdot | s_2, a_2)) \\le L_p\\bigl( d_S(s_1, s_2)\n+ d_A(a_1, a_2) \\bigr)\\).\n\n\n\nExample 19.1 As an example, consider the inventory management problem considered earlier. We assume that \\(\\ALPHABET S = \\reals\\) and \\(\\ALPHABET A = \\reals_{\\ge 0}\\); the cost function and the dynamics are the same as before. We will show that this model is \\((L_c, L_p)\\) Lipschitz with \\[\n  L_c = p + \\max\\{ c_h, c_s \\}\n  \\quad\\text{and}\\quad\n  L_p = 1.\n\\]\n\n\n\n\n\n\n\nProof of Lipschitz continuity of the inventory model\n\n\n\n\n\nNote that in this model, the per-step cost depends on the next stage, so we need to make the appropriate changes to compute \\(L_c\\).\nWe first consider \\(L_p\\). For random variables \\(X \\sim μ\\) and \\(Y \\sim ν\\), we will use the notation \\(K(X,Y)\\) to denote \\(K(μ,ν)\\). Let \\(y_1 = s_1 +a_1\\) and \\(y_2 = s_2 + a_2\\). Then, \\[\n  K(p(\\cdot | s_1, a_1), p( \\cdot | s_2, a_2))\n  =\n  K( y_1 - W, y_2 - W )\n  =\n  K( W - y_1, W - y_2)\n\\] where we have used the following fact that \\(K(X,Y) = K(-X,-Y)\\). Now observe that if \\(y_1 &gt; y_2\\), the CDF of the RV \\(W - y_1\\) lies above the CDF of the RV \\(W - y_2\\); thus \\(W - y_2\\) [stochastically dominates] \\(W - y_1\\), hence from \\(\\eqref{eq:Kantorovich-stochastic-dominance}\\), \\(K(W - y_1, W - y_2) = y_1 - y_2\\). By symmetry, if \\(y_1 &lt; y_2\\), \\(K(W - y_1, W - y_2) = y_2 - y_1\\). Thus, \\[\n  K( W - y_1, W - y_2) = | y_1 - y_2 |\n  \\le | s_1 - s_2 | + | a_1 - a_2|\n\\] The above relationship implies \\(L_p = 1\\).\nNow consider \\[\n  \\bar c(s,a) = \\EXP[ c(s,a,S_{+}) \\mid S = s, A = a]\n  = pa + \\EXP[ h(s+a - W) ]\n\\] Then \\[\\begin{align*}\n  | \\bar c(s_1, a_1) - \\bar c(s_2, a_2) |\n  &\\le\n  p| a_1 - a_2 | + \\| h \\|_L K(s_1 + a_1 - W, s_2 + a_2 - W)\n  \\\\\n  &\\stackrel{(a)}\\le\n  p| a_1 - a_2 | + \\| h \\|_L | s_1 + a_1 - s_2 - a_2 |\n  \\\\\n  &\\le\n  (p + \\| h\\|_L)[ |s_1 - s_2| + |a_1 - a_2| ]\n\\end{align*}\\] where \\((a)\\) follows from Proposition 19.2. Thus, \\(L_c = p + \\|h\\|_L\\).\n\n\n\n\nLipschitz continuity of Bellman updates\nWe now prove a series of results for the Lipschitz continuity of Bellman updates.\n\nLemma 19.1 Let \\(V \\colon \\ALPHABET S \\to \\reals\\) be \\(L_V\\)-Lipschitz continuity. Define \\[ Q(s,a) = c(s,a) + γ \\int V(y) p(y|s,a)dy. \\] Then \\(Q\\) is \\((L_c + γ L_p L_V)\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider, \\[\\begin{align*}\n| Q(s_1, a_1) - Q(s_2, a_2) | &\\stackrel{(a)}\\le\n| c(s_1, a_1) - c(s_2, a_2) | \\\\\n& \\quad +\nγ \\left|\\int V(y) p(y|s_1, a_1) dy -\n             \\int V(y) p(y|s_2, a_2) dy \\right|\n  \\\\\n  &\\stackrel{(b)}\\le  L_c d( (s_1, a_1), (s_2, a_2) ) +\n  γ L_V L_p d( (s_1, a_1), (s_2, a_2) ),\n\\end{align*}\\] where \\((a)\\) follows from the triangle inequality and \\((b)\\) follows from Proposition 19.2. Thus, \\(L_Q = L_c + γ L_p L_V\\).\n\n\n\n\nLemma 19.2 Let \\(Q \\colon \\ALPHABET S × \\ALPHABET A \\to \\reals\\) be \\(L_Q\\)-Lipschitz continuous. Define \\[V(s) = \\min_{a \\in \\ALPHABET A} Q(s,a).\\] Then \\(V\\) is \\(L_Q\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s_1, s_2 \\in \\ALPHABET S\\) and let \\(a_1\\) and \\(a_2\\) denote the corresponding optimal action. Then, \\[ \\begin{align*}\nV(s_1) - V(s_2) &= Q(s_1, a_1) - Q(s_2, a_2) \\\\\n&\\stackrel{(a)}\\le Q(s_1, a_2) - Q(s_2, a_2) \\\\\n&\\stackrel{(b)}\\le L_Q( d_S(s_1, s_2) + d_A(a_2, a_2) )\\\\\n&= L_Q d_S(s_1, s_2).\n\\end{align*} \\]\nBy symmetry, \\[ V(s_2) - V(s_1) \\le L_Q d_S(s_2, s_1). \\] Thus, \\[ | V(s_1) - V(s_2) | \\le L_Q d_S(s_1, s_2). \\] Thus, \\(V\\) is \\(L_Q\\)-Lipschitz continuous.\n\n\n\n\nLemma 19.3 Let \\(Q \\colon \\ALPHABET S × \\ALPHABET A \\to \\reals\\) be \\(L_Q\\)-Lipschitz continuous and \\(π\\) be a (possibly randomized) \\(L_π\\)-Lipschitz policy. Define \\[V_π(s) = \\int Q(s, a) π(a | s) du.\\] Then, \\(V_π\\) is \\(L_Q( 1 +\nL_π)\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(s_1, s_2 \\in \\ALPHABET S\\), consider \\[ \\begin{align}\n| V_π(s_1) - V_π(s_2) | &=\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_2) du \\right|\n\\notag \\\\\n&\\stackrel{(a)}\\le\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_1) du \\right|\n\\notag \\\\\n& \\quad +\n\\left| \\int Q(s_2, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_2) du \\right|\n\\label{eq:split}\n\\end{align} \\] where \\((a)\\) follows from the triangle inequality. Now we consider both terms separately.\nThe first term of \\eqref{eq:split} simplifies as follows: \\[\\begin{align}\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_1) du \\right|\n&\\stackrel{(b)}\\le\n\\int \\left|Q(s_1, a) - Q(s_2, a)\\right| π(a | s_1) du \\notag \\\\\n&\\stackrel{(c)}\\le\n\\int L_Q d_S(s_1, s_2) π(a | s_1) du \\notag \\\\\n&= L_Q d_S(s_1, s_2), \\label{eq:first}\n\\end{align} \\] where \\((b)\\) follows from the triangle inequality and \\((c)\\) follows from Lipschitz continuity of \\(Q\\).\nThe second term of \\eqref{eq:split} simplifies as follows: \\[ \\begin{align}\n  \\left| \\int Q(s, a) π(a | s_1) du - \\int Q(s,a) π(a | s_2) du \\right|\n  &\\stackrel{(d)}\\le L_Q K (π(\\cdot | s_1), π(\\cdot | s_2))\n  \\notag \\\\\n  &\\stackrel{(e)}\\le L_Q L_π d_S(s_1, s_2),\n  \\label{eq:second}\n  \\end{align}\n\\] where the \\((d)\\) inequality follows from Proposition 19.2 and \\((e)\\) follows from the definition of Lipschitz continuous policy.\nSubstituting \\eqref{eq:first} and \\eqref{eq:second} in \\eqref{eq:split}, we get \\[ \\begin{align*}\n| V_π(s_1) - V_π(s_2) | &\\le L_Q d_S(s_1, s_2) + L_Q L_π d_S(s_1, s_2)\n\\\\\n&= L_Q(1 + L_π) d_S(s_1, s_2).\n\\end{align*} \\] Thus, \\(V\\) is Lipschitz continuous with Lipschitz constant \\(L_Q(1 +\nL_π)\\).",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Lipschitz MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#lipschitz-continuity-of-value-iteration",
    "href": "mdps/lipschitz-mdps.html#lipschitz-continuity-of-value-iteration",
    "title": "19  Lipschitz MDPs",
    "section": "19.4 Lipschitz continuity of value iteration",
    "text": "19.4 Lipschitz continuity of value iteration\n\nLemma 19.4 Consider a discounted infinite horizon MDP which is \\((L_c, L_p)\\)-Lipschitz. Start with \\(V^{(0)} = 0\\) and recursively define\n\n\\(\\displaystyle Q^{(n+1)}(s,a) = c(s,a) + γ \\int V^{(n)}(y) p(y|s,a) dy.\\)\n\\(\\displaystyle V^{(n+1)}(s) = \\min_{a \\in \\ALPHABET A} Q^{(n+1)}(s,a).\\)\n\nThen, \\(V^{(n)}\\) is Lipschitz continuous and its Lipschitz constant \\(L_{V^{(n)}}\\) satisfies the following recursion: \\[L_{V^{(n+1)}} = L_c + γ L_p L_{V^{(n)}}.\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by induction. For \\(n=1\\), \\(Q^{(1)}(s,a) = c(s,a)\\), which is Lipschitz with Lipschitz constant \\(L_{Q^{(1)}} = L_c\\). Then, by Lemma 19.2, \\(V^{(1)}\\) is Lipschitz with Lipschitz constant \\(L_{V^{(1)}} = L_{Q^{(1)}} = L_c\\). This forms the basis of induction. Now assume that \\(V^{(n)}\\) is \\(L_{V^{(n)}}\\)-Lipschitz. Then, by Lemma 19.1, \\(Q^{(n+1)}\\) is \\((L_c + γL_p L_{V^{(n)}})\\)-Lipschitz. Therefore, by Lemma 19.2, \\(V^{(n+1)}\\) is Lipschitz with constant \\[ L_{V^{(n+1)}} = L_c + γ L_p L_{V^{(n)}}.\\]\n\n\n\n\nLemma 19.5 Consider a discounted infinite horizon MDP which is \\((L_c, L_p)\\)-Lipschitz and let \\(π\\) be any randomized time-homogeneous policy which is \\(L_π\\)-Lipschitz. Start with \\(V^{(0)} = 0\\) and then recursively define\n\n\\(V^{(n)}_π(s) = \\int Q^{(n)}_π(s,a)π(a|s) du.\\)\n\\(\\displaystyle Q^{(n+1)}_π(s,a) = c(s,a) + γ \\int V^{(n)}_π(y) p(y|s,a) dy.\\)\n\nThen, then \\(Q^{(n)}_π\\) is Lipschitz continuous and its Lipschitz constant \\(L_{Q^{(n)}_π}\\) satisfies the follwoing recursion: \\[ L_{Q^{(n+1)}_π} + L_c + γ(1 + L_π)L_p L_{Q^{(n)}_π}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by induction. For \\(n=1\\), \\(Q^{(1)}_π(s,a) = c(s,a)\\), which is Lipschitz with Lipschitz constant \\(L_{Q^{(1)}_π} = L_c\\). This forms the basis of induction. Now assume that \\(Q^{(n)}_π\\) is \\(L_{Q^{(n)}_π}\\)-Lipschitz. Then, by Lemma 19.3, \\(V^{(n)}_π\\) is Lipschitz with Lipschitz constant \\(L_{V^{(n)}_π} = L_{Q^{(n)}_π}(1 + L_π)\\) and by Lemma 19.1, \\(Q^{(n+1)}_π\\) is Lipschitz with Lipschitz constant \\(L_{Q^{(n+1)}_π} = L_c + γL_p L_{V^{(n)}_π}.\\) Combining these two we get \\[ L_{Q^{(n+1)}_π} + L_c + γ(1 + L_π)L_p L_{Q^{(n)}_π}. \\]\n\n\n\n\nTheorem 19.1 Given any \\((L_c, L_p)\\)-Lipschitz MDP, if \\(γ L_p &lt; 1\\), then the infinite horizon \\(γ\\)-discounted value function \\(V\\) is Lipschitz continuous with Lipschitz constant \\[ L_{V} = \\frac{L_c}{1 - γ L_p} \\] and the action-value function \\(Q\\) is Lipschitz with Lipschitz constant \\[ L_Q = L_V = \\frac{L_c}{1 - γ L_p}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the sequence of \\(L_n = L_{V^{(n)}}\\) values. For simplicity write \\(α =\nγ L_p\\). Then the sequence \\(\\{L_n\\}_{n \\ge 1}\\) is given by: \\(L_1 = L_c\\) and for \\(n \\ge 1\\), \\[ L_{n+1} = L_c + α L_n. \\] Hence, \\[ L_n = L_c + α L_c + \\dots + α^{n-1} L_c = \\frac{1 - α^n}{1 - α} L_c. \\] This sequence converges if \\(|α| &lt; 1\\). Since \\(α\\) is non-negative, this is equivalent to \\(α &lt; 1\\), which is true by hypothesis. Hence \\(L_n\\) is a convergent sequence. At convergence, the limit \\(L_V\\) must satisfy the fixed point of the recursion relationship introduced in Lemma 19.4, hence \\[ L_V = L_c + γ L_p L_V. \\] Consequently, the limit is equal to \\[ L_V = \\frac{L_c}{1 - γ L_p}. \\] The Lipschitz constant of \\(Q\\) follows from Lemma 19.1.\n\n\n\n\n\n\n\n\n\nExample 19.1 (continued)\n\n\n\nAs discussed in Example 19.1, the inventory management example is \\((p + \\max\\{c_h,c_s\\}, 1)\\)-Lipschitz. Therefore, Theorem 19.1 implies that the value function of the inventory management problem is \\(L_V\\)-Lipschitz with \\[\n  L_V = \\frac{p + \\max\\{ c_h + c_s \\}}{1 - γ}.\n\\]\nLater, in the notes on model approximation, we show that the bound on the Lipschitz constant is useful to understand the approximation error if we use a policy designed for a model with a slightly different demand distribution.\nTo understand the tightness of this bound, we consider a specific instance of inventory management problem where the demand is \\(\\text{Exp}(1)\\), \\(c_h = 2\\), \\(c_s = 5\\), and \\(p = 1\\). The theoretical maximum value of the Lipschitz constant (for \\(γ = 0.9\\)) is \\(L_V = 60\\). In Figure 19.1, we show the animation of this upper bound, in the style of the wikipedia animation shown at the beginning of this lecture.\n\n\n\n\n\n\n\n\nFigure 19.1: Animation showing the upper bound on the Lipschitz constant of the value function, computed via Theorem 19.1.\n\n\n\n\n\nNote that since the demand is \\(\\text{Exp}(1)\\), most of the mass of the demand is in the range \\([0,10]\\). So, the region of the value function of interest is perhaps \\([-20,20]\\) or so. We plot a larger region to highlight the fact that the bound on the Lipschitz constant has to capture the Lipschitz constant of the value function over the entire real line.\n\n\n\nTheorem 19.2 Given any \\((L_c, L_p)\\)-Lipschitz MDP and an \\(L_π\\)-Lipschitz (possibly randomized) time-homogeneous policy \\(π\\), if \\(γ (1 + L_π) L_p &lt; 1\\), then the infinite horizon \\(γ\\)-discounted value-action function \\(Q_π\\) is Lipschitz continuous with Lipschitz constant \\[ L_{Q_π} = \\frac{L_c}{1 - γ(1 + L_π) L_p} \\] and the value function \\(V_π\\) is Lipschitz with Lipschitz constant \\[ L_{V_π} = L_{Q_π}(1 + L_π) =\n   \\frac{L_c(1 + L_π)}{1 - γ(1 + L_π) L_p}. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThe restrictive assumption in the result is that \\(γ(1 + L_π)L_p &lt; 1\\). For a specific model, even when this assumption does not hold, it may be possible to directly check if the \\(Q\\)-function is Lipschitz continuous. Such a direct check often gives a better Lipschitz constant.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the sequence of \\(L_n = L_{Q^{(n)}_π}\\) values. For simplicity, write \\(α = γ(1 + L_π)L_p\\). Then, the sequence \\(\\{L_n\\}_{n \\ge 1}\\) is given by: \\(L_1 = L_c\\) and for \\(n \\ge 1\\), \\[L_{n+1} = L_c + α L_n. \\] Hence, \\[ L_n = L_c + α L_c + \\dots + α^{n-1} L_c = \\frac{1 - α^n}{1 - α} L_c. \\] This sequence converges if \\(|α| &lt; 1\\). Since \\(α\\) is non-negative, this is equivalent to \\(α &lt; 1\\), which is true by hypothesis. Hence \\(L_n\\) is a convergent sequence. At convergence, the limit \\(L_{Q_π}\\) must satisfy the fixed point of the recursion relationship introduced in Lemma 19.5, hence \\[ L_{Q_π} = L_c + γ(1 + L_π)L_p L_{Q_π}. \\] Consequently, the limit is equal to \\[ L_{Q_π} = \\frac{L_c}{1 - γ(1 + L_π) L_p}. \\]\nThe Lipschitz constant of \\(V_π\\) follows from Lemma 19.3.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Lipschitz MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#influence-radius",
    "href": "mdps/lipschitz-mdps.html#influence-radius",
    "title": "19  Lipschitz MDPs",
    "section": "19.5 Influence Radius",
    "text": "19.5 Influence Radius\nWhen the \\(Q\\)-function of an MDP is Lipschitz continuous, then the optimal action does not change too abruptly. More precisely, suppose an action \\(a\\) is optimal at state \\(s\\). Then, we can identify a hyperball \\(B(s, ρ(s))\\) of radius \\(ρ(s)\\) centered around \\(s\\) such that \\(a\\) is guaranteed to be the dominating action in \\(ρ(s)\\). This radius \\(ρ(s)\\) is called the influence radius.\nLet \\(π\\) denote the optimal policy, i.e., \\[ π(s) = \\arg \\min_{a \\in \\ALPHABET A} Q(s,a) \\] and \\(h\\) denote the second best action, i.e., \\[ h(s) = \\arg \\min_{a \\in \\ALPHABET A \\setminus \\{π(s)\\}} Q(s,a). \\] Define the domination value of state \\(s\\) to be \\[ Δ(s) = Q(s, h(s)) - Q(s, π(s)). \\]\n\nTheorem 19.3 For a Lipschitz continuous \\(Q\\)-function, the influence radius at state \\(s\\) is given by \\[ ρ(s) = \\frac{ Δ(s) }{ 2 L_Q }. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nCombining Theorem 19.2 and Theorem 19.3 implies that under the condition of Theorem 19.2, the influence radius at state \\(s\\) is at least \\[ ρ(s) = Δ(s)(1 - γ(1 + L_π)L_p)/2L_c. \\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe intuition behind the proof is the following. The value of the action \\(π(s)\\) can only decrease by \\(L_Q ρ(s)\\) in \\(B(s, ρ(s))\\), while the value of the second best action \\(h(s)\\) can only increase by \\(L_Q ρ(s)\\). So, the shortest distance \\(ρ(s)\\) from \\(s\\) needed for an action \\(h(s)\\) to “catch-up” with action \\(π(s)\\) should satisfy \\(2 L_Q ρ(s) = Δ(s)\\) or \\(ρ(s) = Δ(s)/2L_Q\\).\nFormally, for any \\(s' \\in B(s,ρ(s))\\), \\(d_S(s,s') \\le ρ(s)\\). Thus, for any action \\(a \\in \\ALPHABET A\\), \\[ | Q(s,a) - Q(s',a)| \\le L_Q d_S(s,s') \\le L_Q ρ(s). \\] Equivalently, \\[ Q(s,a) - L_Q ρ(s) \\le Q(s',a) \\le Q(s,a) + L_Q ρ(s) \\] which states that as \\(s'\\) moves away from \\(s\\), the value of \\(Q(s',a)\\) remains within a symmetric bound that depends on the radius \\(ρ(s)\\). Since this bound holds for all \\(a\\), they also hold for \\(a = π(s)\\). Thus, \\[ Q(s, π(s)) - L_Q ρ(s) \\le Q(s', π(s)) \\le Q(s, π(s)) + L_Q ρ(s). \\]\nSince \\(π(s)\\) is the optimal action, for any other action \\(a \\neq π(s)\\), \\[ Q(s,π(s)) \\le Q(s,a). \\] Thus, the action \\(π(s)\\) is optimal as long as the upper bound on \\(Q(s', π(s))\\) is lower than the lower bound on \\(Q(s',a)\\), i.e., \\[ Q(s, π(s)) + L_Q ρ(s) \\le Q(s,a) - L_Q ρ(s).  \\] Thus, the maximum value of \\(ρ(s)\\) is when the relationship holds with equality, i.e., \\[ ρ(s) = \\frac{Q(s,a) - Q(s,π(s))}{2 L_Q} \\ge \\frac{Δ(s)}{2 L_Q}. \\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Lipschitz MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#exercises",
    "href": "mdps/lipschitz-mdps.html#exercises",
    "title": "19  Lipschitz MDPs",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 19.1 Let \\((\\ALPHABET S, d_S)\\) be a metric space and \\(s, s' \\in \\ALPHABET S\\). Consider two Bernoulli measures \\[ μ = a δ_s + (1-a) δ_{s'}, \\qquad\n      ν = b δ_s + (1-b) δ_{s'}. \\]\nShow that \\[ K(μ,ν) = |a - b| d(s,s'). \\]",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Lipschitz MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#notes",
    "href": "mdps/lipschitz-mdps.html#notes",
    "title": "19  Lipschitz MDPs",
    "section": "Notes",
    "text": "Notes\nThe material in this section is taken from Rachelson and Lagoudakis (2010) and Hinderer (2005).\nThe proof of Lipschitz continuity for the inventory management problem in Example 19.1 is adapted from Müller (1997). Later, in the notes on model approximation, we show that the bound on the Lipschitz constant is useful to understand the approximation error if we use a policy designed for a model with a slightly different demand distribution.\n\n\n\n\nHinderer, K. 2005. Lipschitz continuity of value functions in Markovian decision processes. Mathematical Methods of Operations Research 62, 1, 3–22. DOI: 10.1007/s00186-005-0438-1.\n\n\nMüller, A. 1997. How does the value function of a markov decision process depend on the transition probabilities? Mathematics of Operations Research 22, 4, 872–885. DOI: 10.1287/moor.22.4.872.\n\n\nRachelson, E. and Lagoudakis, M.G. 2010. On the locality of action domination in sequential decision making. Proceedings of 11th international symposium on artificial intelligence and mathematics. Available at: https://oatao.univ-toulouse.fr/17977/.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Lipschitz MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/periodic-mdps.html",
    "href": "mdps/periodic-mdps.html",
    "title": "20  Periodic MDPs",
    "section": "",
    "text": "Policy evaluation\nLet \\(π = (π_0, \\dots, π_{L-1})\\) be any periodic policy and let \\(V^π_t\\) denote its performance starting at time \\(t\\). Then, \\[\n  V^π_t = v^π_{[t]}\n\\] where \\((v^π_0, v^π_1, \\dots, v^π_{L-1})\\) satisfy \\[\n  v^π_\\ell =\n  \\BELLMAN^{π_\\ell}_{\\ell} v^π_{[\\ell + 1]},\n  \\quad \\ell \\in \\ALPHABET L.\n\\]\nAn alterative method is to evaluate the policy every \\(L\\) time steps. For ease of notation, we assume that the system starts at time \\(0\\) (rather than at time \\(1\\) as we have been assuming so far). In particular, define: \\[\n\\bar c^π(s) = \\EXP^{π}\\biggl[ \\sum_{\\ell \\in \\ALPHABET L} γ^{\\ell-1}\nc(S_{\\ell}, A_{\\ell}) \\biggm| S_0 = s \\biggr],\n\\] to be the performance of policy \\(π\\) over one period. Note that this can be computed via dynamic programming as: \\[\n  \\bar c^π =\n  \\BELLMAN^{π_0}_{0}\n  \\BELLMAN^{π_1}_{1}\n  \\cdots\n  \\BELLMAN^{π_{L-1}}_{L-1}\n  \\mathbf{0}\n\\] where \\(\\mathbf{0}\\) is the all zeros vector. Now define \\[\n  \\bar P^π = P^{π_0}_0 P^{π_1}_1 \\cdots P^{π_{L-1}}_{L-1}\n\\] to be the \\(L\\) step transition matrix. Then, the value of policy \\(π\\) (evaluated every \\(L\\) steps) is just like a regular MDP and therefore satisfies: \\[\n  v^π_0 = \\bar c^π + γ^L \\bar P^π v^π_0\n\\] or, equivalently, \\[\n  v^π_0 = (I - γ^L \\bar P^π)^{-1} \\bar c^π\n\\] which may be thought of as the equivalent of Proposition 12.1 for periodic MDPs.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Periodic MDPs</span>"
    ]
  },
  {
    "objectID": "mdps/periodic-mdps.html#notes",
    "href": "mdps/periodic-mdps.html#notes",
    "title": "20  Periodic MDPs",
    "section": "Notes",
    "text": "Notes\nOur presentation borrows heavily from the tutorial slides of Scherrer (2016).\nPeriodic MDPs were first investigated in Riis (1965), who proposed the policy evaluation formula and presented a variation of policy iteration for periodic MDPs.\n\n\n\n\nRiis, J.O. 1965. Discounted Markov programming in a periodic process. Operations Research 13, 6, 920–929. DOI: 10.1287/opre.13.6.920.\n\n\nScherrer, B. 2016. On periodic markov decision processes. Available at: https://ewrl.files.wordpress.com/2016/12/scherrer.pdf.",
    "crumbs": [
      "MDPs",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Periodic MDPs</span>"
    ]
  },
  {
    "objectID": "pomdps/intro.html",
    "href": "pomdps/intro.html",
    "title": "21  Introduction",
    "section": "",
    "text": "21.1 History dependent dynamic program\nOur first step to develop an efficient dynamic programming decomposition is to simply ignore efficiency and develop a dynamic programming decomposition. We start by deriving a recursive formula to compute the performance of a generic history dependent strategy \\(π = (π_1, \\dots, π_T)\\).",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pomdps/intro.html#history-dependent-dynamic-program",
    "href": "pomdps/intro.html#history-dependent-dynamic-program",
    "title": "21  Introduction",
    "section": "",
    "text": "Performance of history-dependent strategies\nLet \\(H_t = (Y_{1:t}, A_{1:t-1})\\) denote all the information available to the decision maker at time \\(t\\). Thus, given any history dependent strategy \\(π\\), we can write \\(A_t = π_t(H_t)\\). Define the cost-to-go functions as follows: \\[\n  J_t(h_t; π) = \\EXP^π\\biggl[ \\sum_{s=t}^T c_s(S_s, A_s) \\biggm| H_t = h_t\n  \\biggr].\n\\] Note that \\(J_t(h_t; π)\\) only depends on the future strategy \\((π_t, \\dots,\nπ_T)\\). These functions can be computed recursively as follows: \\[\\begin{align*}\n  J_t(h_t; π) &= \\EXP^π\\biggl[ \\sum_{s=t}^T c_s(H_s, π_s(H_s)) \\biggm|\n    H_t = h_t \\biggr] \\\\\n    &\\stackrel{(a)}= \\EXP^π \\biggl[ c_t(h_t, π_t(h_t)) + \\EXP^π\\biggl[\n    \\sum_{s=t+1}^T c_s(S_s, π_s(S_s)) \\biggm| H_{t+1} \\biggr] \\biggm|\n    H_t = h_t \\biggr]  \\\\\n    &= \\EXP^π[ c_t(h_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid H_t = h_t ],\n\\end{align*}\\] where \\((a)\\) follows from the towering property of conditional expectation and the fact that \\(H_t \\subseteq H_{t+1}\\).\nThus, we can use the following dynamic program to recursively compute the performance of a history-dependent strategy: \\(J_{T+1}(h_{T+1}) = 0\\) and for \\(t\n\\in \\{T, \\dots, 1\\}\\), \\[\nJ_t(h_t; π) = \\EXP^π [ c_t(h_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid\n  H_t = h_t ].\n\\]\n\n\nHistory-dependent dynamic programming decomposition\nWe can use the above recursive formulation for performance evaluation to derive a history-dependent dynamic program.\n\nTheorem 21.1 Recursively define value functions \\(\\{V_t\\}_{t = 1}^{T+1}\\), where \\(V_t\n\\colon \\ALPHABET H_t \\to \\reals\\) as follows: \\[\\begin{equation}\n  V_{T+1}(h_{T+1}) = 0\n\\end{equation}\\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n  Q_t(h_t, a_t) &= \\EXP[ c_t(S_t, a_t) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = a_t ] \\\\\n  V_t(h_t) &= \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t)\n\\end{align}\\] Then, a history-dependent policy \\(π\\) is optimal if and only if it satisfies \\[\\begin{equation} \\label{eq:history-verification}\n  π_t(h_t) \\in \\arg \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t).\n\\end{equation}\\]\n\nThe proof idea is similar to the proof for MDPs. Instead of proving the above result, we prove a related result.\n\nTheorem 21.2 (The comparison principle) For any history-dependent strategy \\(π\\) \\[ J_t(h_t; π) \\ge V_t(h_t) \\] with equality at \\(t\\) if and only if the future straegy \\(π_{t:T}\\) satisfies the verification step \\eqref{eq:history-verification}.\n\nNote that the comparison principle immediately implies that the strategy obtained using dynamic program of Theorem 21.1 is optimal. The proof of the comparison principle is almost identical to the proof for MDPs.\n\n\n\n\n\n\nProof of the comparison principle\n\n\n\n\n\nThe proof proceeds by backward induction. Consider any history dependent policy \\(π = (π_1, \\dots, π_T)\\). For \\(t = T+1\\), the comparison principle is satisfied by definition and this forms the basis of induction. We assume that the result holds for time \\(t+1\\), which is the induction hypothesis. Then for time \\(t\\), we have \\[\\begin{align*}\n  V_t(h_t) &= \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t) \\\\\n  &\\stackrel{(a)}= \\min_{a_t \\in \\ALPHABET A}\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = π_t(h_t) ]\n  \\\\\n  &\\stackrel{(b)}\\le\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = π_t(h_t)]\n  \\\\\n  &\\stackrel{(c)}\\le\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid\n  H_t = h_t, A_t = π_t(h_t)]\n  \\\\\n  &= J_t(h_t, π).\n\\end{align*}\\] where \\((a)\\) follows from the definition of the \\(Q\\)-function; \\((b)\\) follows from the definition of minimization; and \\((c)\\) follows from the induction hyothesis. We have the equality at step \\((b)\\) iff \\(π_t\\) satisfies the verification step \\eqref{eq:history-verification} and have the equality in step \\((c)\\) iff \\(π_{t+1:T}\\) is optimal (this is part of the induction hypothesis). Thus, the result is true for time \\(t\\) and, by the principle of induction, is true for all time.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pomdps/intro.html#info-state",
    "href": "pomdps/intro.html#info-state",
    "title": "21  Introduction",
    "section": "21.2 The notion of an information state",
    "text": "21.2 The notion of an information state\nNow that we have obtained a dynamic programming decomposition, let’s try to simplify it. To do so, we define the notion of an information state.\n\n\n\n\n\n\nInformation state\n\n\n\nA stochastic process \\(\\{Z_t\\}_{t = 1}^T\\), \\(Z_t \\in \\ALPHABET Z\\), is called an information state if \\(Z_t\\) be a function of \\(H_t\\) (which we denote by \\(Z_t =\nφ_t(H_t)\\)) and satisfies the following two properties:\nP1. Sufficient for performance evaluation, i.e., \\[ \\EXP^π[ c_t(S_t, A_t) \\mid H_t = h_t, A_t = a_t]\n    =  \\EXP[ c_t(S_t, A_t) \\mid Z_t = φ_t(h_t), A_t = a_t ] \\]\nP2. Sufficient to predict itself, i.e., for any Borel measurable subset \\(B\\) of \\(\\ALPHABET Z\\), we have \\[ \\PR^π(Z_{t+1} \\in B \\mid H_t = h_t, A_t = a_t) =\n       \\PR(Z_{t+1} \\in B \\mid Z_t = φ_t(h_t), A_t = a_t).\n    \\]\n\n\nInstead of (P2), the following sufficient conditions are easier to verify in some models:\n\n\n\n\n\n\nAn equivalent characterization\n\n\n\nP2a. Evolves in a state-like manner, i.e., there exist measurable functions \\(\\{ψ_t\\}_{t=1}^T\\) such that \\[ Z_{t+1} = ψ_t(Z_t, Y_{t+1}, A_t). \\]\nP2b. Is sufficient for predicting future observations, i.e., for any Borel subset \\(B\\) of \\(\\ALPHABET Y\\), \\[ \\PR^π(Y_{t+1} \\in B | H_t = h_t, A_t = a_t) =\n        \\PR(Y_{t+1} \\in B | Z_t = φ_t(h_t), A_t = a_t).\n     \\]\n\n\n\n\n\n\n\n\nRemark\n\n\n\nThe right hand sides of (P1) and (P2) as well as (P2a) and (P2b) do not depend on the choice of the policy \\(π\\).\n\n\n\nProposition 21.1 : (P2a) and (P2b) imply (P2).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any Borel measurable subset \\(B\\) of \\(\\ALPHABET Z\\), we have \\[\\begin{align*}\n  \\hskip 1em & \\hskip -1em\n  \\PR(Z_{t+1} \\in B \\mid H_t = h_t, A_t = a_t)  \n  \\stackrel{(a)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(Y_{t+1} = y_{t+1}, Z_{t+1} \\in B\n  \\mid H_t = h_t, A_t = a_t ]\n  \\\\\n  &\\stackrel{(b)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\IND\\{ ψ_t(φ_t(h_t), y_{t+1}, a_t) \\}\n  \\PR(Y_{t+1} = y_{t+1} \\mid H_t = h_t, A_t = a_t)\n  \\\\\n  &\\stackrel{(c)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\IND\\{ ψ_t(φ_t(h_t), y_{t+1}, a_t) \\}\n  \\PR(Y_{t+1} = y_{t+1} \\mid Z_t = φ_t(h_t), A_t = a_t)\n  \\\\\n  &\\stackrel{(d)}=\n  \\PR(Z_{t+1} \\in B \\mid Z_t = φ_t(h_t), A_t = a_t)  \n\\end{align*}\\] where \\((a)\\) follows from the law of total probability, \\((b)\\) follows from (P2a), \\((c)\\) follows from (P2b), and \\((d)\\) from the law of total probability.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pomdps/intro.html#examples-of-an-information-state",
    "href": "pomdps/intro.html#examples-of-an-information-state",
    "title": "21  Introduction",
    "section": "21.3 Examples of an information state",
    "text": "21.3 Examples of an information state\nWe start by define the belief state \\(b_t \\in Δ(\\ALPHABET\nS)\\) as follows: for any \\(s \\in \\ALPHABET S\\) \\[ b_t(s) = \\PR^π(S_t = s \\mid H_t = h_t). \\] The belief state is a function of the history \\(h_t\\). When we want to explicitly show the dependence of \\(b_t\\) on \\(h_t\\), we write it as \\(b_t[h_t]\\).\n\nLemma 21.1 The belief state \\(b_t\\) does not depend on the policy \\(π\\).\n\n\n\n\n\n\n\nSignificance of policy indepdendence of conditional independence\n\n\n\nThis is an extremely important result which has wide-ranging implications in stochastic control. For a general discussion of this point, see Witsenhausen (1975).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the law of total probability and Bayes rule, we have \\[\\begin{equation} \\label{eq:belief}\n  \\PR(s_t | y_{1:t}, a_{1:t-1})\n  = \\sum_{s_{1:t-1}} \\PR(s_{1:t} | y_{1:t}, a_{1:t-1})\n  = \\sum_{s_{1:t-1}}\n   \\frac{\\PR(s_{1:t}, y_{1:t}, a_{1:t-1})}\n   {\\sum_{s'_{1:t}} \\PR(s'_{1:t}, y_{1:t}, a_{1:t-1})}\n\\end{equation}\\]\nNow consider \\[\\begin{align*}\n  \\PR(s_{1:t}, y_{1:t}, a_{1:t-1}) &=\n  \\PR(s_1) \\PR(y_1 | s_1) \\IND\\{ a_1 = π_1(y_1) \\} \\\\\n  & \\times\n  \\PR(s_2 | s_1, a_1) \\PR(y_2 | s_2) \\IND \\{ a_2 = π_2(y_{1:2}, a_1)\\} \\\\\n  & \\times \\cdots \\\\\n  & \\times\n  \\PR(s_{t-1} | s_{t-2}, a_{t-2}) \\PR(y_{t-1} | s_{t-1}) \\IND \\{ a_{t-1} =\n  π_{t-1}(y_{1:t-1}, a_{1:t-2}) \\} \\\\\n  & \\times\n  \\PR(s_{t} | s_{t-1}, a_{t-1}) \\PR(y_{t} | s_{t}).\n\\end{align*}\\] Substitute the above expression in both the numerator and the denominator of \\eqref{eq:belief}. Observe that the terms of the form \\(\\IND\\{ a_s =\nπ_s(y_{1:s}, a_{1:s-1})\\) are common to both the numerator and the denominator and cancel each other. Thus, \\[\\begin{equation} \\label{eq:belief-fn}\n  \\PR(s_t | y_{1:t}, a_{1:t-1}) = \\sum_{s_{1:t-1}}\n  \\frac{ \\prod_{s=1}^t \\PR(s_s \\mid s_{s-1}, a_{s-1}) \\PR(y_s \\mid s_s) }\n  { \\sum_{s'_{1:t}} \\prod_{s=1}^t \\PR(s'_s \\mid s'_{s-1}, a_{s-1}) \\PR(y_s \\mid s'_s) }.\n\\end{equation}\\] None of the terms here depend on the policy \\(π\\). Hence, the belief state does not depend on the policy \\(π\\).\n\n\n\n\nLemma 21.2 The belief state \\(b_t\\) updates in a state like manner. In particular, for any \\(s_{t+1} \\in \\ALPHABET S\\), we have \\[\n  b_{t+1}(s_{t+1}) = \\sum_{s_t \\in \\ALPHABET S}\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n   { \\sum_{s'_{t:t+1}} \\PR(y_{t+1} | s'_{t+1}) \\PR(s'_{t+1} | s'_t, a_t) b_t(s'_t) }.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(s_{t+1} \\in \\ALPHABET S\\), consider\n\\[\\begin{align}\nb_{t+1}(s_{t+1}) &= \\PR(s_{t+1} | y_{1:t+1}, a_{1:t}) \\notag \\\\\n&= \\sum_{s_t} \\PR(s_{t:t+1} | y_{1:t+1}, a_{1:t}) \\notag \\\\\n&= \\sum_{s_t} \\frac{ \\PR(s_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) }\n  {\\sum_{s'_{t:t+1}}\\PR(s'_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) }.\n\\label{eq:update-1}\n\\end{align}\\]\nNow consider \\[\\begin{align}\n\\hskip 1em & \\hskip -1em\n\\PR(s_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) \\notag \\\\\n&= \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t)\n   \\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\n   \\PR(s_t | y_{1:t}, a_{1_t-1}) \\notag \\\\\n&= \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t)\n   \\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\n   b_t(s_t). \\label{eq:belief-2}\n\\end{align}\\] Substitute the above expression in both the numerator and the denominator of \\eqref{eq:update-1}. Observe that \\(\\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\\) is common to both the numerator and the denominator and cancels out. Thus, we get the result of the lemma.\n\n\n\nNow, we present three examples of information state here. See the Exercises for more examples.\n\nExample 21.1 The complete history \\(H_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove that \\(Z_t = H_t\\) satisfies properties (P1), (P2a), and (P2b).\nP1. \\(\\displaystyle \\EXP^π[ c_t(S_t, A_t) | H_t = h_t, A_t = a_t ]\n= \\sum_{s_t \\in \\ALPHABET S} c_t(s_t, a_t) b_t[h_t](s_t)\\).\nP2a. \\(H_{t+1} = (H_t, Y_{t+1}, A_t)\\)\nP2b. \\(\\displaystyle \\PR^π(y_{t+1} | y_{1:t}, a_{1:t})\n= \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR( s_{t+1} | s_t, a_t) \\PR(s_t |\ny_{1:t}, a_{1:t})\\). Note that in the last term \\(\\PR^π(s_t | y_{1:t}, a_{1:t})\\) we can drop \\(a_t\\) from the conditioning because it is a function of \\((y_{1:t},\na_{1:t-1})\\). Thus, \\[ \\PR^π(s_t | y_{1:t}, a_{1:t}) = \\PR^π(s_t | y_{1:t}, a_{1:t-1}) =\nb_t[h_t](s_t).\\] Note that in the last step, we have used Lemma 21.1. Thus, \\(\\displaystyle \\PR^π(y_{t+1} | y_{1:t}, a_{1:t})\n= \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR( s_{t+1} | s_t, a_t)\nb_t[h_t](s_t)\\).\n\n\n\n\nExample 21.2 The belief state \\(b_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe belief state \\(b_t\\) is a function of the history \\(h_t\\). (The exact form of this function is given by \\eqref{eq:belief-fn}). In the proof of Example 21.1, we have already shown that \\(b_t\\) satisfies (P1) and (P2b). Moreover Lemma 21.2 implies that the belief update satisfies (P2a).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nBoth the above information states are generic information states which work for all models. For specific models, it is possible to identify other information states as well. We present some examples of such an information state below.\n\n\n\nExample 21.3 An MDP is a special case of a POMDP where \\(Y_t = S_t\\). For an MDP \\(Z_t = S_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will show that \\(Z_t = S_t\\) satisfies (P1) and (P2).\n(P1) is satisfied because the per-step cost is a function of the \\((S_t, A_t)\\). (P2) is equivalent to the control Markov property.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pomdps/intro.html#information-state-based-dynamic-program",
    "href": "pomdps/intro.html#information-state-based-dynamic-program",
    "title": "21  Introduction",
    "section": "21.4 Information state based dynamic program",
    "text": "21.4 Information state based dynamic program\nThe main feature of an information state is that one can always write a dynamic program based on an information state.\n\nTheorem 21.3 Let \\(\\{Z_t\\}_{t=1}^T\\) be any information state, where \\(Z_t = φ_t(H_t)\\). Recursively define value functions \\(\\{ \\hat V_t \\}_{t=1}^T\\), where \\(\\hat V_t\n\\colon \\ALPHABET Z \\to \\reals\\), as follows: \\[ \\hat V_{T+1}(z_{T+1}) = 0 \\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n  \\hat Q_t(z_t, a_t) &= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}(Z_{t+1}) \\mid\n  Z_t = z_t, A_t = a_t] \\\\\n  \\hat V_t(z_t) &= \\min_{a_t \\in \\ALPHABET A} \\hat Q_t(z_t, a_t).\n\\end{align}\\] Then, we have the following: for any \\(h_t\\) and \\(a_t\\), \\[\\begin{equation} \\label{eq:history-info}\n  Q_t(h_t, a_t) = \\hat Q_t(φ_t(h_t), a_t)\n  \\quad\\text{and}\\quad\n  V_t(h_t) = \\hat V_t(φ_t(h_t)).\n\\end{equation}\\] Any strategy \\(\\hat π = (\\hat π_1, \\dots, \\hat π_T)\\), where \\(\\hat π_t \\colon\n\\ALPHABET Z \\to \\ALPHABET A\\), is optimal if and only if \\[\\begin{equation}\\label{eq:info-verification}\n    \\hat π_t(z_t) \\in \\arg\\min_{a_t \\in \\ALPHABET A} \\hat Q_t(z_t, a_t).\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs usual, we prove the result by backward induction. By construction, Eq. \\eqref{eq:history-info} is true at time \\(T+1\\). This forms the basis of induction. Now assume that \\eqref{eq:history-info} is true at time \\(t+1\\) and consider the system at time \\(t\\). Then, \\[\\begin{align*}\nQ_t(h_t, a_t) &= \\EXP[ c_t(S_t, A_t) + V_{t+1}(H_{t+1}) | H_t = h_t, A_t = a_t\n] \\\\\n&\\stackrel{(a)}= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}( φ_t(H_{t+1}) ) | H_t =\nh_t, A_t = a_t ]  \\\\\n&\\stackrel{(b)}= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}( φ_t(H_{t+1}) ) | Z_t =\nφ_t(h_t), A_t = a_t ]  \\\\\n&\\stackrel{(c)}= \\hat Q_t(φ_t(h_t), a_t),\n\\end{align*}\\] where \\((a)\\) follows from the induction hypothesis, \\((b)\\) follows from the properties (P1) and (P2) of the information state, and \\((c)\\) follows from the definition of \\(\\hat Q_t\\). This shows that the action value functions are equal. By minimizing over the actions, we get that the value functions are also equal.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pomdps/intro.html#belief-state-based-dynamic-program",
    "href": "pomdps/intro.html#belief-state-based-dynamic-program",
    "title": "21  Introduction",
    "section": "21.5 Belief state based dynamic program",
    "text": "21.5 Belief state based dynamic program\nAs shown in Example 21.2, the belief state \\(b_t\\) is an information state. Therefore, Theorem 21.3 implies that we can write a dynamic program based on \\(b_t\\). This is an important and commonly used formulation, so we study it separately and present some properties of the value functions. The belief state based dynamic program is given by: \\(V_{T+1}(b_{T+1}) = 0\\) and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\n  Q_t(b_t, a_t) =\n  \\EXP [ c_t(S_t, A_t) + V_{t+1}(B_{t+1}) \\mid B_t = b_t, A_t = a_t ].\n\\] and \\[ V_t(b_t) = \\min_{a_t \\in \\ALPHABET A} Q_t(b_t, a_t). \\]\nDefine \\[ \\PR(y_{t+1} | b_t, a_t) =\n   \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t) b_t(s_t).\n\\] Then, the belief update expression in Lemma 21.2 can be written as: \\[\n  b_{t+1}(s_{t+1}) =\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n  { \\PR(y_{t+1} | b_t, a_t) }.\n\\] For the ease of notation, we write this expression as \\(b_{t+1} = ψ(b_t,\ny_{t+1}, a_t)\\).\n\\[\\begin{align*}\n  Q_t(b_t, a_t) &= \\sum_{s_t \\in \\ALPHABET S} c_t(s_t, a_t) b_t(s_t) \\\\\n  & \\quad +  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  V_{t+1}( φ(b_t, y_{t+1}, a_t) ).\n\\end{align*}\\]\nA key property of the belief-state based value functions is the following.\n\nTheorem 21.4 The belief-state based value functions are piecewise linear and concave.\n\n\n\n\n\n\nAn illustration of a piecewise linear and concave function. Move the points around to see how the shape of the function changes.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs usual, we prove the result using backward induction. For any \\(a_T\\), \\[ Q_T(b_T, a_T) = \\sum_{s_T \\in \\ALPHABET S} c_T(s_T, a_T) b_T(s_T) \\] is linear in \\(b_T\\). Therefore, \\[ V_T(b_T) = \\min_{a_T \\in \\ALPHABET A} Q_T(b_T, a_T) \\] is the minimum of a finite number of linear functions. Hence \\(V_T(b_T)\\) is piecewise linear and concave.\nNow assume that \\(V_{t+1}(b_{t+1})\\) is piecewise linear and concave (PWLC). Any PWLC function can be represented as a minimum of a finite number of hyperplanes. Therefore, we can find a finite set of vectors \\(\\{ A_i\n\\}_{i \\in I}\\) indexed by finite set \\(I\\) such that \\[\n  V_{t+1}(b) = \\min_{i \\in I} \\langle A_i, b \\rangle.\n\\]\nWe need to show that \\(V_t(b_t)\\) is piecewise linear and concave (PWLC). We first show that \\(Q_t(b_t, a_t)\\) is PWLC. For any fixed \\(a_t\\), the first term \\(\\sum_{s_t} c_t(s_t, a_t) b_t(s_t)\\) is linear in \\(b_t\\). Now consider the second term: \\[\\begin{align*}\n  \\hskip 1em & \\hskip -1em\n  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  V_{t+1}( φ(b_t, y_{t+1}, a_t) ) \\\\\n  &=\n  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  \\min_{i \\in I}\n  \\left\\langle A_i,\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n  { \\PR(y_{t+1} | b_t, a_t) } \\right\\rangle \\\\\n  &=\n  \\sum_{y_{t+1} \\in \\ALPHABET Y}\n  \\min_{i \\in I}\n  \\Big\\langle A_i,\n   \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t)\n   \\Big\\rangle\n\\end{align*}\\] which is the sum of PWLC functions of \\(b_t\\) and therefore PWLC in \\(b_t\\).\nThus, \\(Q_t(b_t, a_t)\\) is PWLC. Hence, \\(V_t(b_t)\\) which is the pointwise minimum of PWLC functions is PWLC. Hence, the result holds due to principle of induction.\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nSince the value function is PWLC, we can identify a finite index set \\(I_t\\), and a set of vectors \\(\\{ A^i_t \\}_{i \\in I_t}\\) such that \\[\n    V_t(b) = \\min_{i \\in I_t} \\langle A^i_t, b \\rangle.\n\\] Smallwood and Sondik (1973) presented a “one-pass” algorithm to recursively compute \\(I_t\\) and \\(\\{ A^i_t \\}_{i \\in I_t}\\) which allows us to exactly compute the value function. Various efficient refinements of these algorithms have been presented in the literature, e.g., the linear-support algorithm (Cheng 1988), the witness algorithm (Cassandra et al. 1994), incremental pruning (Zhang and Liu 1996; Cassandra et al. 1997), duality based approach (Zhang 2009), and others. See https://pomdp.org/ for an accessible introduction to these algorithms.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pomdps/intro.html#exercises",
    "href": "pomdps/intro.html#exercises",
    "title": "21  Introduction",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 21.1 Consider an MDP where the state space \\(\\ALPHABET S\\) is a symmetric subset of integers of the form \\(\\{-L, -L + 1, \\dots, L -1 , L\\}\\) and the action space \\(\\ALPHABET A\\) is discrete. Suppose the transition matrix \\(P(a)\\) and the cost function \\(c_t(s,a)\\) satisfy properties (A1) and (A2) of Exercise 8.8. Show that \\(Z_t = |S_t|\\) is an information state.\n\n\nExercise 21.2 Consider a linear system with state \\(x_t \\in \\reals^n\\), observations \\(y_t \\in \\reals^p\\), and action \\(u_t \\in \\reals^m\\). Note that we will follow the standard notation of linear systems and denote the system variables by lower case letters \\((x,u)\\) rather than upper case letter \\((S,A)\\). The dynamics of the system are given by \\[\\begin{align*}\n  x_{t+1} &= A x_t + B u_t + w_t  \\\\\n  y_t &= C x_t + n_t\n\\end{align*}\\] where \\(A\\), \\(B\\), and \\(C\\) are matrices of appropriate dimensions. The per-step cost is given by \\[\n  c(x_t, u_t) = x_t^\\TRANS Q x_t + u_t^\\TRANS R u_t,\n\\] where \\(Q\\) is a positive semi-definite matrix and \\(R\\) is a positive definite matrix. We make the standard assumption that the primitive random variables \\(\\{s_1, w_1, \\dots, w_T, n_1, \\dots, n_T \\}\\) are independent.\nShow that if the primitive variables are Guassian, then the conditional estimate of the state \\[\n  \\hat x_t = \\EXP[ x_t | y_{1:t}, u_{1:t-1} ]\n\\] is an information state.\n\n\nExercise 21.3 Consider a machine which can be in one of \\(n\\) ordered state where the first state is the best and the last state is the worst. The production cost increases with the state of the machine. The state evolves in a Markovian manner. At each time, an agent has the option to either run the machine or stop and inspect it for a cost. After inspection, the agent may either repair the machine (at a cost that depends on the state) or replace it (at a fixed cost). The objective is to identify a maintenance policy to minimize the cost of production, inspection, repair, and replacement.\nLet \\(τ\\) denote the time of last inspection and \\(S_τ\\) denote the state of the machine after inspection, repair, or replacement. Show that \\((S_τ,\nt-τ)\\) is an information state.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pomdps/intro.html#notes",
    "href": "pomdps/intro.html#notes",
    "title": "21  Introduction",
    "section": "Notes",
    "text": "Notes\nThe discussion in this section is taken from Subramanian et al. (2022). Information state may be viewed as a generalization of the traditional notion of state Nerode (1958), which is defined as a statistic (i.e., a function of the observations) sufficient for input-output mapping. In contrast, we define an information state as a statistic sufficient for performance evaluation (and, therefore, for dynamic programming). Such a definition is hinted in Witsenhausen (1976). The notion of information state is also related to sufficient statistics for optimal control defined in Striebel (1965) for systems with state space models.\nAs far as we are aware, the informal definition of information state was first proposed by Kwakernaak (1965) for adaptive control systems. Formal definitions for linear control systems were given by Bohlin (1970) for discrete time systems and by Davis and Varaiya (1972) for continuous time systems. Kumar and Varaiya (1986) define an information state as a compression of past history which satisfies property (P2a) but do not formally show that such an information state always leads to a dynamic programming decomposition.\n\n\n\n\nBohlin, T. 1970. Information pattern for linear discrete-time models with stochastic coefficients. IEEE Transactions on Automatic Control (TAC) 15, 1, 104–106.\n\n\nCassandra, A., Littman, M.L., and Zhang, N.L. 1997. Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes. Proceedings of the thirteenth conference on uncertainty in artificial intelligence.\n\n\nCassandra, A.R., Kaelbling, L.P., and Littman, M.L. 1994. Acting optimally in partially observable stochastic domains. AAAI, 1023–1028.\n\n\nCheng, H.-T. 1988. Algorithms for partially observable markov decision processes. PhD thesis, University of British Columbia, Vancouver, BC.\n\n\nDavis, M.H.A. and Varaiya, P.P. 1972. Information states for linear stochastic systems. Journal of Mathematical Analysis and Applications 37, 2, 384–402.\n\n\nKumar, P.R. and Varaiya, P. 1986. Stochastic systems: Estimation identification and adaptive control. Prentice Hall.\n\n\nKwakernaak, H. 1965. Theory of self-adaptive control systems. In: Springer, 14–18.\n\n\nNerode, A. 1958. Linear automaton transformations. Proceedings of American Mathematical Society 9, 541–544.\n\n\nSmallwood, R.D. and Sondik, E.J. 1973. The optimal control of partially observable markov processes over a finite horizon. Operations Research 21, 5, 1071–1088. DOI: 10.1287/opre.21.5.1071.\n\n\nStriebel, C. 1965. Sufficient statistics in the optimal control of stochastic systems. Journal of Mathematical Analysis and Applications 12, 576–592.\n\n\nSubramanian, J., Sinha, A., Seraj, R., and Mahajan, A. 2022. Approximate information state for approximate planning and reinforcement learning in partially observed systems. Journal of Machine Learning Research 23, 12, 1–83. Available at: http://jmlr.org/papers/v23/20-1165.html.\n\n\nWitsenhausen, H.S. 1975. On policy independence of conditional expectation. Information and Control 28, 65–75.\n\n\nWitsenhausen, H.S. 1976. Some remarks on the concept of state. In: Y.C. Ho and S.K. Mitter, eds., Directions in large-scale systems. Plenum, 69–75.\n\n\nZhang, H. 2009. Partially observable Markov decision processes: A geometric technique and analysis. Operations Research.\n\n\nZhang, N. and Liu, W. 1996. Planning in stochastic domains: Problem characteristics and approximation. Hong Kong Univeristy of Science; Technology.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html",
    "href": "pomdps/sequential-hypothesis.html",
    "title": "22  Sequential hypothesis testing",
    "section": "",
    "text": "22.1 Dynamic programming decomposition\nWe use the belief-state as an information state to obtain a dynamic programming decomposition. Recall that the beief state is two-dimensional pdf where \\[ b_t(h) = \\PR(H = h | Y_{1:t}), \\quad h \\in \\{h_0, h_1\\}. \\]\nThe dynamic program for the above model is then given by \\[\n  V_T(b_T) = \\min\\{ \\EXP[ \\ell(h_0, H) | B_T = b_T],\n                    \\EXP[ \\ell(h_1, H) | B_T = b_T] \\}\n\\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\), \\[ V_t(b_t) = \\min \\{ \\EXP[ \\ell(h_0, H) | B_t = b_t],\n                      \\EXP[ \\ell(h_1, H) | B_t = b_t],\n                     c + \\EXP[V_{t+1}(ψ(b_t, Y_{t+1})) | B_t = b_t] \\},\n\\] where \\(ψ(b, y)\\) denotes the standard non-linear filtering update (there is no dependence on \\(a\\) here because there are no state dynamics in this model).\nWe introduce some notation to simplify the discussion. Define\nThen, the above DP can be written as \\[\n  V_T(b_T) = \\min\\{ L_0(b_T), L_1(b_T) \\}\n\\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\), \\[ V_t(b_t) = \\min \\{ L_0(b_t), L_1(b_t), W_t(b_t) \\}. \\]",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sequential hypothesis testing</span>"
    ]
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#dynamic-programming-decomposition",
    "href": "pomdps/sequential-hypothesis.html#dynamic-programming-decomposition",
    "title": "22  Sequential hypothesis testing",
    "section": "",
    "text": "Remarks\n\n\nWe are only conditioning on \\(Y_{1:t}\\) and not adding \\(A_{1:t-1}\\) in the conditioning. This is because we are taking the standard approach used in optimal stopping problems where we are only defining the state for case when the stopping decision hasn’t been taken so far and all previous actions are continue. Taking a continue action does not effect the observations. For this reason, we do not condition on \\(A_{1:t-1}\\).\nIt is possible to exploit the fact that \\(b_t = [p_t, 1 - p_t]^T\\) and write a simplified DP in terms of \\(p_t\\). In these notes, I don’t make this simplification so that we can see how these results will extend to the case of non-binary hypothesis.\n\n\n\n\n\n\n\\(L_i(b) = \\EXP[ \\ell(h_i, H) | B = b]\n= \\sum_{h \\in \\{h_0, h_1\\}} \\ell(h_i, h) b(h)\\).\n\\(W_t(b_t) = c + \\EXP[V_{t+1}(ψ(b_t, Y_{t+1})) | B_t = b_t]\\).",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sequential hypothesis testing</span>"
    ]
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#structure-of-the-optimal-policy",
    "href": "pomdps/sequential-hypothesis.html#structure-of-the-optimal-policy",
    "title": "22  Sequential hypothesis testing",
    "section": "22.2 Structure of the optimal policy",
    "text": "22.2 Structure of the optimal policy\nWe start by establishing simple properties of the different functions defined above.\n\nLemma 22.1 The above functions statisfy the following properties:\n\n\\(L_i(b)\\) is linear in \\(b\\).\n\\(V_t(b)\\) and \\(W_t(b)\\) is concave in \\(b\\).\n\\(V_t(b)\\) and \\(W_t(b)\\) are increasing in \\(t\\).\n\n\n\n\n\n\n\nAn illustration of the minimum of two straight lines and a concave function. Move the points around to see how the shape of the function changes.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe linearity of \\(L_i(b)\\) follows from definition. From the discussion on POMDPs, we know that \\(V_{t+1}(b)\\) is concave in \\(b\\) and so is \\(\\EXP[V_{t+1}(ψ(b, Y_{t+1})) | B_t = b]\\). Therefore \\(W_t(b)\\) is concave in \\(b\\).\nFinally, by construction, we have that \\(V_{T-1}(b) \\le V_T(b)\\). The monotonicity in time then follows from Q2 of Assignment 2. Sincen \\(V_t\\) is monotone in time, it implies that \\(W_t\\) is also monotone.\n\n\n\nNow define stopping sets \\(D_t(h) = \\{ b \\in Δ^2 : π_t(b) = h \\}\\) for \\(h \\in\n\\{h_0, h_1\\}\\). The key result is the following.\n\nTheorem 22.1 For all \\(t\\) and \\(h \\in \\{h_0, h_1\\}\\), the set \\(D_t(h)\\) is convex. Moreover, \\(D_t(h_i) \\subseteq D_{t+1}(h_i)\\).\n\n\n\n\n\n\nAn illustration of the stopping sets. Move the points around to see how the shape of the stopping set changes.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNote that we can write \\(D_t(h) = A_t(h) \\cap B_t(h)\\), where \\[ A_t(h_i) = \\{ b \\in Δ^2 : L_i(b)  \\le L_j(b) \\}\n   \\quad\\text{and}\\quad\n   B_t(h_i) = \\{ b \\in Δ^2 : L_i(b) \\le W_t(b)  \\}. \\]\n\\(A_t(h_i)\\) is a the set of \\(b\\) where one linear function of \\(b\\) is less than or equal to another linear function of \\(b\\). Therefore, \\(A_t(h_i)\\) is a convex set.\nSimilarly, \\(B_t(h_i)\\) is the set of \\(b\\) where a linear function of \\(b\\) is less than or equal to a concave function of \\(b\\). Therefore \\(B_t(h_i)\\) is also a convex set.\n\\(D_t(h_i)\\) is the intersection of two convex sets, and hence convex.\nThe monotonicty of \\(D_t(h_i)\\) in time follows from the monotonicity of \\(W_t\\) in time.\n\n\n\n\nTheorem 22.2 Suppose the stopping cost satisfy the following: \\[\\begin{equation} \\label{eq:cost-ass}\n\\ell(h_0, h_0) \\le c \\le \\ell(h_0, h_1)\n  \\quad\\text{and}\\quad\n  \\ell(h_1, h_1) \\le c \\le \\ell(h_1, h_0).\n\\end{equation}\\] Then, \\(e_i \\in D_t(h_i)\\), where \\(e_i\\) denotes the standard unit vector (i.e., \\(e_0 = [1, 0]^T\\) and \\(e_1 = [0, 1]^T\\)).\n\n\n\n\n\n\n\nRemark\n\n\n\nThe assumption on observation cost states that: (i) the cost of observation is greater than the cost incurred when the DM chooses the right hypothesis, and (ii) the cost of observation is less than the cost incurred when the DM chooses the wrong hypothesis. Both these assumptions are fairly natural.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNote that \\(L_i(e_0) = \\ell(h_i, h_0)\\) and \\(L_i(e_1) = \\ell(h_1, h_1)\\). Moreover, by construction, \\(W_t(b) \\ge c\\). Thus, under the above assumption on the cost, \\[ L_0(e_0) = \\ell(h_0, h_0) \\le c \\le W_t(e_0) \\] and \\[ L_0(e_0) = \\ell(h_0, h_0) \\le \\ell(h_1, h_0) = L_1(e_0). \\] Thus, \\(e_0 \\in D_t(h_0)\\).\nBy a symmetric argument, we can show that \\(e_1 \\in D_t(h_1)\\).\n\n\n\nTheorem 22.1 and Theorem 22.2 imply that the optimal stopping regions are convex and include the “corner points” of the simplex. Note that although we formulated the problem for binary hypothesis, all the steps of the proof hold in general as well.\n\n\n\n\n\nStopping regions for multiple hypothesis\n\n\nFor binary hypothesis, we can present a more concerete characterizatin of the optimal policy. Note that the two-dimensional simplex is equivalent to the interval \\([0,1]\\). In particular, any \\(b = Δ^2\\) is equal to \\([p, 1-p]\\), where \\(p \\in [0,1]\\). Now define:\n\n\\(\\displaystyle β_t = \\min\\left\\{ p \\in [0, 1] :\nπ_t\\left(\\begin{bmatrix} p \\\\ 1-p \\end{bmatrix}\\right) = h_0\n\\right\\}.\\)\n\\(\\displaystyle α_t = \\max\\left\\{ p \\in [0, 1] :\nπ_t\\left(\\begin{bmatrix} p \\\\ 1-p \\end{bmatrix}\\right) = h_1\n\\right\\}.\\)\n\nThen, by definition, the optimal policy has the following threshold property:\n\nProposition 22.1 Let \\(\\bar π_t(p) = π_t([p, 1-p]^T)\\). Then, under \\eqref{eq:cost-ass}, \\[ \\bar π_t(p) = \\begin{cases}\n   h_1, & \\text{if } p \\le α_t \\\\\n   \\mathsf{C}, & \\text{if } α_t &lt; p &lt; β_t \\\\\n   h_0, & \\text{if } β_t \\le p.\n  \\end{cases} \\]\nFurthermore, the decision thresholds are monotone in time. In particular, for all \\(t\\), \\[ α_t \\le α_{t+1} \\le β_{t+1} \\le β_t. \\]\n\nThe above property is simplies stated slighted in terms of the likelihood ratio. In particular, define \\(λ_t = b_t(0)/b_t(1) = p_t/(1 - p_t)\\). Then, we have the following:\n\nProposition 22.2 Let \\(\\hat π_t(λ) = π_t([λ/(1+λ), 1/(1+λ)]^T)\\). Then, under \\eqref{eq:cost-ass}, \\[ \\hat π_t(λ) = \\begin{cases}\n   h_1, & \\text{if } λ \\le α_t/(1 - α_t) \\\\\n   \\mathsf{C}, & \\text{if } α_t/(1 - α_t) &lt; λ &lt; β_t/(1 - β_t)_t \\\\\n   h_0, & \\text{if } β_t/(1 - β_t)_t \\le λ.\n  \\end{cases} \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(β, β \\in [0, 1]\\), \\[ α \\le β \\iff \\frac{α}{1-α} \\le \\frac{β}{1-β}.\\]\n\n\n\nThe result of Proposition 22.2 is called the sequential likelihood ratio test (SLRT) or sequential probability ratio test (SPRT) to contrast it with the standard :likelihood ratio test in hypotehsis testing.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sequential hypothesis testing</span>"
    ]
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#infinite-horizon-setup",
    "href": "pomdps/sequential-hypothesis.html#infinite-horizon-setup",
    "title": "22  Sequential hypothesis testing",
    "section": "22.3 Infinite horizon setup",
    "text": "22.3 Infinite horizon setup\nAssume that \\(T = ∞\\) so that the continuation alternative is always available. Then, we have the following.\n\nTheorem 22.3 Under \\eqref{eq:cost-ass}, an optimal decision rule always exists, is time-homogeneous, and is given by the solution of the following DP: \\[ V(b) = \\min\\{ L_0(b) , L_1(b) , W(b) \\} \\] where \\[ W(b) = c + \\int_{y} [ pf_0(y) + (1-p)f_1(y)] V(ψ(b,y)) dy. \\]\nTherefore, the optimal thresholds \\(a\\) and \\(b\\) are time-homogeneous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe result follows from standard results on non-negative dynamic programming. We did not cover non-negative DP. Essentially it determines conditions under which undiscounted infinite horizon problems have a solution when the per-step cost is non-negative.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sequential hypothesis testing</span>"
    ]
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#upper-bound-on-the-expected-number-of-measurements",
    "href": "pomdps/sequential-hypothesis.html#upper-bound-on-the-expected-number-of-measurements",
    "title": "22  Sequential hypothesis testing",
    "section": "22.4 Upper bound on the expected number of measurements",
    "text": "22.4 Upper bound on the expected number of measurements\nFor simplicity, we assume that \\(\\ell(h_0, h_0) = \\ell(h_1, h_1) = 0\\). For the infinite horizon model, we can get upper bound on the expected number of measurements that an optimal policy will take. Let \\(τ\\) denote the number of measurements taken under policy \\(π\\) and \\(A_τ\\) denote the terminal action after stopping. Then, the performance of policy \\(π\\) is given by \\[\n  J(π) = \\EXP[ c τ + \\ell(H, A_\\tau) \\mid \\Pi = b ].\n\\] Note that \\(\\ell(H, A_\\tau) \\ge 0\\). Therefore, the performance of the optimal policy is lower bounded by \\[\n  J^* \\ge c\\, \\EXP^{π^*}[  τ \\mid \\Pi = b] .\n\\] Now, consider a policy \\(\\tilde π\\) which does not consider continuation action and takes the best stopping decision. The performance of \\(\\tilde π\\) is given by \\[ J(\\tilde π) = \\min \\{ \\ell(h_1, h_0) b_1, \\ell(h_0, h_1) b_0 \\}. \\] Since \\(J(\\tilde π) \\ge J^*\\), we get \\[\n  \\EXP^{π^*}[ τ  \\mid \\Pi = b ] \\le \\frac 1c\n  \\min \\{ \\ell(h_1, h_0) b_1, \\ell(h_0, h_1) b_0 \\}. \\]",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sequential hypothesis testing</span>"
    ]
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#exercises",
    "href": "pomdps/sequential-hypothesis.html#exercises",
    "title": "22  Sequential hypothesis testing",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 22.1 Consider the following modification of the sequential hypothesis testing. As in the model discussed above, there are two hypothesis \\(h_0\\) and \\(h_1\\). The a priori probability that the hypothesis is \\(h_0\\) is \\(p\\).\nIn contrast to the model discussed above, there are \\(N\\) sensors. If the underlying hypothesis is \\(h_i\\) and sensor \\(m\\) is used at time \\(t\\), then the observation \\(Y_t\\) is distrubted according to pdf (or pmf) \\(f^m_i(y)\\). The cost of using sensor \\(m\\) is \\(c_m\\).\nWhenever the decision maker takes a measurement, he picks a sensor \\(m\\) uniformly at random from \\(\\{1, \\dots, N\\}\\) and observes \\(Y_t\\) according to the distribution \\(f^m_i(\\cdot)\\) and incurs a cost \\(c_m\\).\nThe system continues for a finite time \\(T\\). At each time \\(t &lt; T\\), the decision maker has three options: stop and declare \\(h_0\\), stop and declare \\(h_1\\), or continue to take another measurement. At time \\(T\\), the continue alternative is unavailable.\n\nFormulate the above problem as a POMDP. Identify an information state and write the dynamic programming decomposition for the problem.\nShow that the optimal control law has a threshold property, similar to the threshold propertly for the model described above.\n\n\n\nExercise 22.2 In this exercise, we will derive an approximate method to compute the performance of a given threshold based policy for infinite horizon sequential hypothesis testing problem. Let \\[ θ_i(π,p) = \\EXP^{π}[ τ | H = h_i] \\] denote the expected number of samples when using stopping rule \\(π\\) assuming that the true hypothesis is \\(h_i\\). Note that for any belief state based stopping rule, \\(θ_i\\) depends on the initial belief \\([p,\n1-p]\\). Furthermore, let \\[ ξ_i(h_k ;π, p) = \\PR^π(A_τ = h_k | H = h_i) \\] denote the probability that the stopping action is \\(h_k\\) when using stopping rule \\(π\\) assuming that the true hypothesis is \\(h_i\\).\n\nArgue that the performance of any policy \\(π\\) can be written as \\[\\begin{align*}\n  V_π(p) &= c [ p θ_0(π, p) + (1-p) θ_1(π,p) ] \\\\\n  & \\quad + p \\sum_{a \\in \\{h_0, h_1\\}} \\ell(a, h_0) ξ_0(a; π, p) \\\\\n  & \\quad + (1-p) \\sum_{a \\in \\{h_0, h_1\\}} \\ell(a, h_1) ξ_1(a; π, p).\n  \\end{align*}\\] Thus, approximately computing \\(θ_i\\) and \\(ξ_i\\) gives an approximate value of \\(V_π(p)\\).\nNow assume that the policy \\(π\\) is of a threshold form with thresholds \\(a\\) and \\(b\\). To avoid trivial cases, we assume that \\(p \\in (a,b)\\). The key idea to compute \\(θ_i\\) and \\(ξ_i\\) is that the evolution of \\(p_t =\n  \\PR(H = h_t | Y_{1:t})\\) is a Markov chain which starts at a state \\(p\n  \\in (a,b)\\) and stops the first time \\(p_t\\) goes below \\(a\\) or above \\(b\\).\n\n\n\nDiscretization of the state space\n\n\nSuppose we discretize the state space space \\([0, 1]\\) into \\(n+1\\) grid points \\(\\ALPHABET D_n = \\{0, \\frac1n, \\dots, 1\\}\\). Assume that \\(p\\), \\(a\\), and \\(b\\) lie on this discrete grid. Discreteize \\(p_t\\) to the closest grid point and let \\(P_i\\) denote the transition matrix of the discretized \\(p_t\\) when the true hypothesis is \\(h_i\\). Partition the \\(P_i\\) as \\[ \\left[\\begin{array}{c|c|c}\n     A_i & B_i & C_i \\\\\n     \\hline\n     D_i & E_i & F_i \\\\\n     \\hline\n     G_i & H_i & J_i\n    \\end{array}\\right] \\] where the lines correspond to the index for \\(a\\) and \\(b\\). The transition matrix of the absorbing Markov chain is given by \\[ \\hat P_i = \\left[\\begin{array}{c|c|c}\n       I & 0 & I \\\\\n       \\hline\n       D_i & E_i & F_i \\\\\n       \\hline\n       I & 0 & I\n      \\end{array}\\right] \\] Now suppose \\(j\\) is the index of \\(p\\) in \\(\\ALPHABET D_n\\). Using properties of absorbing Markov chains, show that\n\n\\(ξ_i(h_0; \\langle a, b \\rangle, p) \\approx\n    [ (I - E_i)^{-1} F_i \\mathbf{1} ]_j\\)\n\\(ξ_i(h_1; \\langle a, b \\rangle, p) \\approx\n    [ (I - E_i)^{-1} D_i \\mathbf{1} ]_j\\)\n\\(θ_i(\\langle a, b \\rangle, p) \\approx\n    [ (I - E_i)^{-1} \\mathbf{1} ]_j\\)",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sequential hypothesis testing</span>"
    ]
  },
  {
    "objectID": "pomdps/sequential-hypothesis.html#notes",
    "href": "pomdps/sequential-hypothesis.html#notes",
    "title": "22  Sequential hypothesis testing",
    "section": "Notes",
    "text": "Notes\nFor more details on sequential hypothesis testing, incuding an approximate method to determine the thresholds, see Wald (1945). The optimal of sequential likelihood ratio test was proved in Wald and Wolfowitz (1948). The model described above was first considered by Arrow et al. (1949). See DeGroot (1970).\nThe upper bound on expected number of measurements is adapted from an argument presented in Hay et al. (2012).\nExercise 22.1 is from Bai et al. (2015). Exercise 22.2 is from Woodall and Reynolds (1983).\n\n\n\n\n\nArrow, K.J., Blackwell, D., and Girshick, M.A. 1949. Bayes and minimax solutions of sequential decision problems. Econometrica 17, 3/4, 213. DOI: 10.2307/1905525.\n\n\nBai, C.-Z., Katewa, V., Gupta, V., and Huang, Y.-F. 2015. A stochastic sensor selection scheme for sequential hypothesis testing with multiple sensors. IEEE transactions on signal processing 63, 14, 3687–3699.\n\n\nDeGroot, M. 1970. Optimal statistical decisions. Wiley-Interscience, Hoboken, N.J.\n\n\nHay, N., Russell, S., Tolpin, D., and Shimony, S.E. 2012. Selecting computations: Theory and applications. UAI. Available at: http://www.auai.org/uai2012/papers/123.pdf.\n\n\nWald, A. 1945. Sequential tests of statistical hypotheses. The Annals of Mathematical Statistics 16, 2, 117–186. DOI: 10.1214/aoms/1177731118.\n\n\nWald, A. and Wolfowitz, J. 1948. Optimum character of the sequential probability ratio test. The Annals of Mathematical Statistics 19, 3, 326–339. DOI: 10.1214/aoms/1177730197.\n\n\nWoodall, W.H. and Reynolds, M.R. 1983. A discrete markov chain representation of the sequential probability ratio test. Communications in Statistics. Part C: Sequential Analysis 2, 1, 27–44. DOI: 10.1080/07474948308836025.",
    "crumbs": [
      "POMDPs",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sequential hypothesis testing</span>"
    ]
  },
  {
    "objectID": "approx-mdps/approx-DP.html",
    "href": "approx-mdps/approx-DP.html",
    "title": "23  Approximate dynamic programming",
    "section": "",
    "text": "23.1 Approximate value iteration\nIf we use a periodic policy with period \\(M\\), then the above bound can be improved by a factor of \\(1/(1-γ)\\).",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Approximate dynamic programming</span>"
    ]
  },
  {
    "objectID": "approx-mdps/approx-DP.html#approximate-value-iteration",
    "href": "approx-mdps/approx-DP.html#approximate-value-iteration",
    "title": "23  Approximate dynamic programming",
    "section": "",
    "text": "Theorem 23.1 Generate \\(\\{V_k\\}_{k \\ge 0}\\) and \\(\\{π_k\\}_{k \\ge 0}\\) such that \\[\\NORM{V_{k+1} - \\mathcal B V_k} \\le δ\n\\quad\\text{and}\\quad\n  \\NORM{\\mathcal B_{π_k} V_k - \\mathcal B V_k} \\le ε. \\] Then,\n\n\\(\\displaystyle \\lim_{k \\to ∞} \\NORM{V_k - V^*} \\le\n\\frac {δ}{(1-γ)}.\\)\n\\(\\displaystyle \\lim_{k \\to ∞} \\NORM{V_{π_k} - V^*} \\le\n\\frac {ε}{(1-γ)} + \\frac{2γδ}{(1-γ)^2}.\\)\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTo prove the first part, note that repeatedly combining the contraction property of the Bellman operator (Proposition 12.2) with the fact that \\(\\NORM{V_{k+1} - \\mathcal\nB V_k} \\le δ\\), we get that \\[\\begin{equation}\\label{eq:B1}\n  \\NORM{\\mathcal B^m V_{k+1} - \\mathcal B^{m+1} V_k} \\le γ^m δ.\n\\end{equation}\\]\nNow, from the triangle inequality, we have that \\[\\begin{align*}\n\\NORM{V_k - \\mathcal B^k V_0} &\\le\n  \\NORM{V_k - \\mathcal B V_{k-1}} + \\NORM{\\mathcal B V_{k-1} - \\mathcal B^2 V_{k-2}}\n  + \\cdots + \\NORM{B^{k-1} V_1 - \\mathcal B^k V_0} \\\\\n  &\\stackrel{(a)}\\le δ + γδ + \\dots + γ^{k-1}δ \\\\\n  &= \\left(\\frac{1 - γ^k}{1-γ}\\right) δ,\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:B1}. Taking the limit as \\(k \\to ∞\\) gives the first result.\nNow, to prove the second part, we again apply the triangle inequality \\[\\begin{align*}\n  \\NORM{\\mathcal B_{π_k} V^* - V^*} &\\le\n  \\NORM{\\mathcal B_{π_k} V^* - \\mathcal B_{π_k} V_k} +\n  \\NORM{\\mathcal B_{π_k} V_k - \\mathcal B V_k} +\n  \\NORM{\\mathcal B V_k - V^*} \\\\\n  &\\stackrel{(b)}\\le γ \\NORM{V^* - V_k} + ε + γ \\NORM{V_k - V^*} \\\\\n  &\\stackrel{(c)}\\le ε + \\frac{2γδ}{(1-γ)} =: m,\n\\end{align*}\\] where the first term in \\((b)\\) uses the contraction property, the second term uses the fact that \\(π_k\\) is an \\(ε\\)-optimal policy and the third term uses the fact that \\(V^*\\) is the fixed point of \\(\\mathcal B\\) and thus \\(V^* = \\mathcal B\nV^*\\) and then uses the contraction property. The inequality in \\((c)\\) use the result from the first part.\nNow, from the discounting property of the Bellman operator (Proposition 12.4), \\(\\NORM{\\mathcal B_{π_k} V^* - V^*}\n\\le m\\) implies \\[ \\NORM{V_{π_k} - V^*} \\le \\frac{m}{(1-γ)}\\] which proves the second part.",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Approximate dynamic programming</span>"
    ]
  },
  {
    "objectID": "approx-mdps/approx-DP.html#approximate-policy-iteration",
    "href": "approx-mdps/approx-DP.html#approximate-policy-iteration",
    "title": "23  Approximate dynamic programming",
    "section": "23.2 Approximate policy iteration",
    "text": "23.2 Approximate policy iteration\nBefore stating the approximate policy iteration algorithm, we state a preliminary result that serves as the main step in proving the error bounds for approximate policy iteration.\n\nProposition 23.1 Suppose \\(V\\), \\(π\\), and \\(h\\) satisfy \\[ \\NORM{V - V_π} \\le δ\n   \\quad\\text{and}\\quad\n   \\NORM{\\mathcal B_h V - \\mathcal B V} \\le ε.\n\\] Then, \\[ \\NORM{V_h - V^*} \\le γ \\NORM{V_π - V^*} + \\frac{ε + 2γδ}{(1-γ)}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the bounds on \\(V\\), \\(π\\), and \\(h\\) and the discounting property of the Bellman operator (Proposition 12.4), we have that \\[\\begin{equation}\\label{eq:P1}\n  \\mathcal B_h V_π \\le \\mathcal B_h V + γδ \\le \\mathcal B V + ε + γ δ.\n\\end{equation}\\]\nAgain, from the bounds on \\(V\\) and \\(π\\) and the discounting property of the Bellman operator, we have \\(\\mathcal B V \\le \\mathcal B V_π + γδ\\). Thus, \\[\\begin{equation}\\label{eq:P2}\n  \\mathcal B_h V_π \\le \\mathcal B V_π + ε + 2γδ\n\\end{equation}\\] For ease of notation, let \\(m := ε + 2γδ\\).\nMoreover, from the definition of the Bellman operator \\[ \\mathcal B V_π \\le \\mathcal B_π V_π = V_π.\\] Substituting the above in \\eqref{eq:P2}, we get that \\[ \\mathcal B_h V_π \\le V_π + m. \\] Therefore, by the discounting property of Bellman operator, we get \\[\\begin{equation}\\label{eq:P3}\n  V_h \\le V_π + \\frac{m}{(1-γ)}.\n\\end{equation}\\]\nUsing \\eqref{eq:P3} and the discounting property, we get that \\[V_h = \\mathcal B_h V_h = \\mathcal B_h V_π + \\big( \\mathcal B_h V_h -\n\\mathcal B_h V_π \\big) \\le \\mathcal B_h V_π + γ \\frac{m}{(1-γ)}. \\]\nSubtracting \\(V^*\\) from both sides we get \\[\\begin{align*}\nV_h - V^* &\\le \\mathcal B_h V_π - V^* + \\frac{mγ}{(1-γ)} \\\\\n&\\stackrel{(a)}\\le \\mathcal B V_π + m - V^* + \\frac{mγ}{(1-γ)} \\\\\n&\\stackrel{(b)}= \\mathcal B V_π - \\mathcal B V^* + \\frac{m}{(1-γ)} \\\\\n&\\stackrel{(c)}{\\le} γ \\NORM{V_π - V^*} + \\frac{m}{(1-γ)},\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:P1}, \\((b)\\) uses the fact that \\(V^*\\) is the fixed point of \\(\\mathcal B\\) and \\((c)\\) uses the contraction property. Substituting the value of \\(m\\) in the above equation gives the result.\n\n\n\n\nTheorem 23.2 Generate a sequence \\(\\{π_k\\}_{k \\ge 0}\\) and \\(\\{V_k\\}_{k \\ge 0}\\) such that \\[ \\NORM{V_k - V_{π_k}} \\le δ\n\\quad\\text{and}\\quad\n\\NORM{\\mathcal B_{π_k} V_k - \\mathcal B V_k} \\le ε.\\] Then, \\[ \\limsup_{k\\to ∞} \\NORM{V_{π_k} - V^*} \\le\n   \\frac{ε+2γδ}{(1-γ)^2}. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\n\nBoth approximate VI and approximate PI have similar error bounds (proportional to \\(1/(1-γ)^2\\).)\nWhen \\(ε = δ = 0\\), then Proposition 23.1 implies that \\(\\NORM{V_{π_{k+1}} - V^*} \\le γ \\NORM{V_{π_k} - V^*}\\). Thus, standard policy iteration has a geometeric rate of convergence (similar to value iteration), though in practice it converges much faster.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom Proposition 23.1 we have\n\\[ \\NORM{V_{π_{k+1}} - V^*} \\le γ \\NORM{V_{π_k} - V^*} + \\frac{ε + 2γδ}{(1-γ)}.\\]\nThe result follows from taking the limit \\(k \\to ∞\\).\n\n\n\n\nProposition 23.2 If the successive policies in approximate policy iteration converge (in general, it may not), i.e.  \\[ π_{k+1} = π_k = π,\n   \\quad \\text{for some $k$}. \\] Then, \\[ \\NORM{V_π - V^*} \\le \\frac{ε + 2γδ}{(1-γ)}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(V\\) be the approximate value function obtained at iteration \\(k\\), i.e., \\[\\NORM{V - V_π} \\le δ\n  \\quad\\text{and}\\quad\n  \\NORM{\\mathcal B_π V - \\mathcal V} \\le ε.\\]\nThen, from the triangle inequality, we have \\[\\begin{align*}\n  \\NORM{\\mathcal B V_π - V_π } &\\le\n  \\NORM{\\mathcal B V_π - \\mathcal B V} +\n  \\NORM{\\mathcal B V - \\mathcal B_π V} +\n  \\NORM{\\mathcal B_π V - \\mathcal B_π V_π} \\\\\n  &\\stackrel{(a)}\\le\n  γ\\NORM{V_π - V} + ε + γ \\NORM{V - V_π} \\\\\n  &\\stackrel{(b)}\\le\n  ε + 2γδ,\n\\end{align*}\\] where \\((a)\\) follows from the fact that \\(V_π = \\mathcal B_π V_π\\) and the contraction property and \\((b)\\) follows from the assumption on \\(V\\). Now, from the discounting property, we get the result.\n\n\n\n\nProposition 23.3 Suppose the successive value functions obtained by approximate policy iteration are “not too different”, i.e., \\[ \\NORM{V - V_π} \\le δ, \\quad\n   \\NORM{B_h V - \\mathcal B V} \\le ε,\n   \\quad\\text{and}\\quad\n   \\NORM{B_π V - \\mathcal B_h V} \\le ζ.\\] Then, \\[ \\NORM{V_π - V^*} \\le \\frac{ε + ζ + 2γδ}{(1-γ)}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe result follows by replacing \\(ε\\) in \\((a)\\) above by \\(ε+ζ\\).",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Approximate dynamic programming</span>"
    ]
  },
  {
    "objectID": "approx-mdps/approx-DP.html#notes",
    "href": "approx-mdps/approx-DP.html#notes",
    "title": "23  Approximate dynamic programming",
    "section": "Notes",
    "text": "Notes\nThe results presented in this section are taken from Bertsekas (2013).\n\n\n\n\nBertsekas, D.P. 2013. Abstract dynamic programming. Athena Scientific Belmont. Available at: https://web.mit.edu/dimitrib/www/abstractdp_MIT.html.",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Approximate dynamic programming</span>"
    ]
  },
  {
    "objectID": "approx-mdps/policy-loss.html",
    "href": "approx-mdps/policy-loss.html",
    "title": "24  Upper bounds on policy loss",
    "section": "",
    "text": "24.1 Approximate Bellman update\nThe definition of \\(π_{\\hat V}\\) assumes that we can perform a Bellman update exactly. Similar to the setup in approximate DP, suppose all we can guarantee is a policy \\(\\hat π\\) such that \\[\n\\| \\BELLMAN \\hat V - \\BELLMAN_{\\hat π} \\hat V\\| \\le ε\n\\] Then, we have the following.",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Upper bounds on policy loss</span>"
    ]
  },
  {
    "objectID": "approx-mdps/policy-loss.html#approximate-bellman-update",
    "href": "approx-mdps/policy-loss.html#approximate-bellman-update",
    "title": "24  Upper bounds on policy loss",
    "section": "",
    "text": "Theorem 24.2 \\[\n\\| V^* - V^{\\hat π}\\|_{∞}\n\\le\n\\frac{2 γ}{1-γ} \\| V^* - \\hat V\\|_{∞}\n+\n\\frac{ε}{1-γ}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom triangle inequality, we have \\[\\begin{align*}\n  \\| V^* - V^{\\hat π}\\|_{∞}\n  &\\le\n  \\| \\BELLMAN V^* - \\BELLMAN \\hat V \\|_{∞}\n  +\n  \\textcolor{red}{\\| \\BELLMAN \\hat V - \\BELLMAN_{\\hat π} \\hat V\\|_{∞}}\n  \\notag \\\\\n  & \\quad\n  +\n  \\| \\BELLMAN_{\\hat π} \\hat V - \\BELLMAN_{\\hat π} V^* \\|_{∞}\n  +\n  \\| \\BELLMAN_{\\hat π} V^* - \\BELLMAN_{\\hat π} V^{\\hat π} \\|_{∞}\n  \\\\\n  &\\le\n  γ \\| V^* - \\hat V \\|_{∞}\n  +\n  \\textcolor{red}{ε}\n  +\n  γ \\| \\hat V -  V^* \\|_{∞}\n  +\n  γ \\| V^* -  V^{\\hat π} \\|_{∞}\n\\end{align*}\\] The result follows from rearranging the terms.",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Upper bounds on policy loss</span>"
    ]
  },
  {
    "objectID": "approx-mdps/policy-loss.html#policy-loss-for-q-learning",
    "href": "approx-mdps/policy-loss.html#policy-loss-for-q-learning",
    "title": "24  Upper bounds on policy loss",
    "section": "24.2 Policy loss for \\(Q\\)-learning",
    "text": "24.2 Policy loss for \\(Q\\)-learning\nA related setting is what happens in \\(Q\\)-learning. Suppose \\(Q^*\\) is the optimal action-value function and we obtain an approximation \\(\\hat Q\\). Let \\(π_{\\hat Q}\\) be the greedy policy with respect to \\(\\hat Q\\), i.e., \\[\n  π_{\\hat Q}(s) \\in \\arg \\min_{a \\in \\ALPHABET A} \\hat Q(s,a).\n\\] Then, we have the following.\n\nTheorem 24.3 \\[\n\\| V^* - V^{π_{\\hat Q}}\\|_{∞}\n\\le\n\\frac{2}{1-γ} \\| Q - \\hat Q\\|_{∞}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor ease of notation, we use \\(\\hat π\\) to denote \\(π_{\\hat Q}\\). Let \\(α = \\| Q - \\hat Q\\|_{∞}\\). Now consider \\[\\begin{align}\nV^{\\hat π}(s) - V^*(s)\n&=\nQ^{\\hat π}(s, \\hat π(s)) - Q^*(s, π^*(s))\n\\notag \\\\\n&\\le\nQ^{\\hat π}(s, \\hat π(s)) - Q^*(s, \\hat π(s)) + 2 α\n\\label{eq:policy-loss-QL-step1}\n\\end{align}\\] where the last inequality uses the fact that \\[\nQ^*(s, \\hat π(s)) - α\n\\le\n\\hat Q(s, \\hat π(s)) \\le Q^*(s, π^*(s))\n\\le\nQ^*(s, π^*(s)) + α.\n\\] Now observe that \\[\nQ^{\\hat π}(s, \\hat π(s)) - Q^*(s, \\hat π(s))\n=\nγ \\sum_{s' \\in \\ALPHABET S}P(s'|s, \\hat π(s)) [ V^{\\hat π}(s') - V^*(s') ]\n\\le\nγ \\| V^{\\hat π} - V^* \\|_{∞}.\n\\] Substituting this in \\eqref{eq:policy-loss-QL-step1} and rearranging the terms gives us the result.",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Upper bounds on policy loss</span>"
    ]
  },
  {
    "objectID": "approx-mdps/policy-loss.html#notes",
    "href": "approx-mdps/policy-loss.html#notes",
    "title": "24  Upper bounds on policy loss",
    "section": "Notes",
    "text": "Notes\nThe material in this section is adapted from Singh and Yee (1994). The proof of Theorem 24.1 is from Tsitsiklis and Roy (1996).\n\n\n\n\nSingh, S.P. and Yee, R.C. 1994. An upper bound on the loss from approximate optimal-value functions. Machine Learning 16, 3, 227–233. DOI: 10.1007/bf00993308.\n\n\nTsitsiklis, J.N. and Roy, B. van. 1996. Feature-based methods for large scale dynamic programming. Machine Learning 22, 1-3, 59–94. DOI: 10.1007/bf00114724.",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Upper bounds on policy loss</span>"
    ]
  },
  {
    "objectID": "approx-mdps/model-approximation.html",
    "href": "approx-mdps/model-approximation.html",
    "title": "25  Model approximation",
    "section": "",
    "text": "25.1 Bounds for model approximation",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model approximation</span>"
    ]
  },
  {
    "objectID": "approx-mdps/model-approximation.html#bounds-for-model-approximation",
    "href": "approx-mdps/model-approximation.html#bounds-for-model-approximation",
    "title": "25  Model approximation",
    "section": "",
    "text": "25.1.1 Policy and value error bounds\nLet \\(\\BELLMAN^π\\) and \\(\\BELLMAN^*\\) denote the Bellman operator for policy \\(π\\) and the optimality Bellman operator for model \\(\\ALPHABET M\\). Let \\(\\hat\n{\\BELLMAN}^π\\) and \\(\\hat {\\BELLMAN}^*\\) denote the corresponding quantities for model \\(\\widehat {\\ALPHABET M}\\). Define the Bellman mismatch functionals \\(\\MISMATCH^π\\) and \\(\\MISMATCH^*\\) as follows: \\[\\begin{align*}\n  \\MISMATCH^π v &= \\| \\BELLMAN^π v - \\hat {\\BELLMAN}^π v \\|_∞,\n  \\\\\n  \\MISMATCH^* v &= \\| \\BELLMAN^* v - \\hat {\\BELLMAN}^* v \\|_∞ .\n\\end{align*}\\]\nAlso define the maximum Bellman mismatch as \\[\\begin{align*}\n  \\MISMATCH^{\\max} v &=\n  \\max_{(s,a) \\in \\ALPHABET S, A} \\biggl\\lvert\n    c(s,a) + γ \\sum_{s' \\in \\ALPHABET S} P(s'|s,a)v(s')  \\notag \\\\\n  & \\hskip 6em\n   -\\hat c(s,a) - γ \\sum_{s' \\in \\ALPHABET S} \\hat P(s'|s,a) v(s')\n   \\biggr\\rvert.\n\\end{align*}\\]\n\nLemma 25.1 The following inequalities hold:\n\n\\(\\sup_{π \\in Π} \\MISMATCH^π v = \\MISMATCH^{\\max} v\\)\n\\(\\MISMATCH^* v \\le \\MISMATCH^{\\max} v\\).\n\n\nThe Bellman mismatch functional can be used to bound the performance difference of a policy between the true and approximate models.\n\nProposition 25.1 (Policy error) For any (possibly randomized) policy \\(π\\), \\[\\begin{equation}\\label{eq:policy-error}\n   \\| V^{π} - \\hat V^{π} \\|_∞ \\le\n   \\frac{1}{1-γ} \\min\\{ \\MISMATCH^π V^{π}, \\MISMATCH^π \\hat V^{π} \\}.\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe bound the left hand side of \\eqref{eq:policy-error} in two ways. The first way is as follows: \\[\\begin{align}\n  \\| V^{π} - \\hat V^{π} \\|_∞\n  &=\n  \\| \\BELLMAN^π V^π - \\hat {\\ALPHABET  B}^π \\hat V^π \\|_∞\n  \\notag \\\\\n  &\\le\n  \\| \\BELLMAN^π V^π - \\hat {\\ALPHABET  B}^π V^π \\|_∞\n  +\n  \\| \\hat {\\BELLMAN}^π V^π - \\hat {\\ALPHABET  B}^π \\hat V^π \\|_∞\n  \\notag \\\\\n  &\\le\n  \\MISMATCH^π V^π + γ \\| V^π - \\hat V^π \\|_∞\n  \\label{eq:ineq-3}\n\\end{align}\\] where the first inequality follows from the triangle inequality, and the second inequality follows from the definition of the Bellman mismatch functional and the contraction property of Bellman operators. Rearranging terms in \\eqref{eq:ineq-3} gives us \\[\\begin{equation}\n\\| V^{π} - \\hat V^{π} \\|_∞ \\le \\frac{ \\MISMATCH^π V^{π}}{1 - γ}.\n\\label{eq:ineq-4}\\end{equation}\\] This gives the first bound.\nThe second bound is symmetric and obtained by interchanging the roles of \\(V^π\\) and \\(\\hat V^π\\). \\[\\begin{align}\n  \\| V^{π} - \\hat V^{π} \\|_∞\n  &=\n  \\| \\BELLMAN^π V^π - \\hat {\\ALPHABET  B}^π \\hat V^π \\|_∞\n  \\notag \\\\\n  &\\le\n  \\| \\BELLMAN^π V^π - \\ALPHABET  B^π \\hat V^π \\|_∞\n  +\n  \\| \\BELLMAN^π \\hat V^π - \\hat {\\ALPHABET  B}^π \\hat V^π \\|_∞\n  \\notag \\\\\n  &\\le\n  γ \\| V^π - \\hat V^π \\|_∞\n  +\n  \\MISMATCH^π \\hat V^π\n  \\label{eq:ineq-13}\n\\end{align}\\] Rearranging terms in \\eqref{eq:ineq-13} gives us \\[\\begin{equation}\n\\| V^{π} - \\hat V^{π} \\|_∞ \\le \\frac{ \\MISMATCH^π \\hat V^{π}}{1 - γ}.\n\\label{eq:ineq-14}\\end{equation}\\] This gives the second bound.\n\n\n\nSimilar to the above, we can also bound the difference between the optimal value function of the true and approximate models.\n\nProposition 25.2 (Value error) Let \\(V^*\\) and \\(\\hat V^*\\) denote the optimal value functions for \\(\\ALPHABET\nM\\) and \\(\\widehat {\\ALPHABET M}\\), respectively. Then, \\[\\begin{equation}\\label{eq:value-error}\n    \\| V^* - \\hat V^* \\|_∞ \\le\n    \\frac{1}{1-γ} \\min\\{ \\MISMATCH^* V^*, \\MISMATCH^* \\hat V^* \\}\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof argument is almost the same as the proof argument for Proposition 25.1. The first was is as follows: \\[\\begin{align}\n  \\| V^{*} - \\hat V^{*} \\|_∞\n  &=\n  \\| \\BELLMAN^* V^* - \\hat {\\ALPHABET  B}^* \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  \\| \\BELLMAN^* V^* - \\hat {\\ALPHABET  B}^* V^* \\|_∞\n  +\n  \\| \\hat {\\BELLMAN}^* V^* - \\hat {\\ALPHABET  B}^* \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  \\MISMATCH^* V^* + γ \\| V^* - \\hat V^* \\|_∞\n  \\label{eq:ineq-1}\n\\end{align}\\] where the first inequality follows from the triangle inequality, and the second inequality follows from the definition of the Bellman mismatch functional and the contraction property of Bellman operators. Rearranging terms in \\eqref{eq:ineq-1} gives us \\[\\begin{equation}\n\\| V^* - \\hat V^* \\|_∞ \\le \\frac{  \\MISMATCH^* V^*}{1 - γ}.\n\\label{eq:ineq-2}\\end{equation}\\] This gives the first bound.\nThe second bound is symmetric and obtained by interchanging the roles of \\(V^*\\) and \\(\\hat V^*\\). \\[\\begin{align}\n  \\| V^{*} - \\hat V^{*} \\|_∞\n  &=\n  \\| \\BELLMAN^* V^* - \\hat {\\ALPHABET  B}^* \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  \\| \\BELLMAN^* V^* - \\ALPHABET  B^* \\hat V^* \\|_∞\n  +\n  \\| \\BELLMAN^* \\hat V^* - \\hat {\\ALPHABET  B}^* \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  γ \\| V^* - \\hat V^* \\|_∞\n  +\n  \\MISMATCH^* \\hat V^*\n  \\label{eq:ineq-11}\n\\end{align}\\] Rearranging terms in \\eqref{eq:ineq-11} gives us \\[\\begin{equation}\n\\| V^{*} - \\hat V^{*} \\|_∞ \\le \\frac{ \\MISMATCH^* \\hat V^{*}}{1 - γ}.\n\\label{eq:ineq-12}\\end{equation}\\] This gives the second bound.\n\n\n\n\n\n25.1.2 Model approximation error\nTo bound the model error, we observe that from triangle inequality we have \\[\\begin{equation} \\label{eq:triangle-1}\n  \\| V^* - V^{\\hat π^*} \\|_∞ \\le\n  \\| V^* - \\hat V^{\\hat π^*} \\|_∞\n  +\n  \\| V^{\\hat π^*} - \\hat V^{\\hat π^*} \\|_∞.\n\\end{equation}\\]\nProposition 25.1 and Proposition 25.2 provide bounds of both of the terms of \\(\\eqref{eq:triangle-1}\\). Choosing appropriate values for both terms gives us the following\n\nTheorem 25.1 (Model approximation error) The policy \\(\\hat π^*\\) is an \\(α\\)-optimal policy of \\(\\ALPHABET M\\) where \\[\n    α := \\| V^* - V^{\\hat π^*} \\|_∞ \\le\n    \\frac{1}{1-γ} \\bigl[ \\MISMATCH^* \\hat V^* + \\MISMATCH^{\\hat\n    π^*} \\hat V^* \\bigr].\n\\] Moreover, since \\(\\MISMATCH^{\\max} \\hat V^*\\) is an upper bound for both \\(\\MISMATCH^{\\hat π^*} \\hat V^*\\) and \\(\\MISMATCH^*\n\\hat V^*\\), we have \\[\n    α \\le \\frac{2}{(1-γ)}  \\MISMATCH^{\\max}  \\hat V^*.\n\\]\n\nIn some applications, it is useful to have a bound on model approximation error that depends on \\(V^*\\) rather than \\(\\hat V^*\\). We provide such a bound below.\n\nTheorem 25.2 (Model approximation error) The policy \\(\\hat π^*\\) is an \\(α\\)-optimal policy of \\(\\ALPHABET M\\) where \\[\n    α := \\| V^* - V^{\\hat π^*} \\|_∞ \\le\n    \\frac{1}{1-γ} \\MISMATCH^{\\hat π^*} V^*\n    +\n    \\frac{(1+γ)}{(1-γ)^2} \\MISMATCH^* V^* .\n\\]\nMoreover, since \\(\\MISMATCH^{\\max} V^*\\) is an upper bound for both \\(\\MISMATCH^{\\hat π^*} V^*\\) and \\(\\MISMATCH^* V^*\\), we have \\[\n    α \\le \\frac{2}{(1-γ)^2}  \\MISMATCH^{\\max}  V^*.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe bound the first term of \\(\\eqref{eq:triangle-1}\\) by Proposition 25.2 But instead of bounding the second term of \\(\\eqref{eq:triangle-1}\\) by Proposition 25.1, we consider the following: \\[\\begin{align}\n  \\| V^{\\hat π^*} - \\hat V^{\\hat π^*} \\|_∞\n  &=\n  \\| V^{\\hat π^*} - \\hat V^{*} \\|_∞\n  = \\| \\BELLMAN^{\\hat π^*} V^{\\hat π^*} -\n       \\hat {\\BELLMAN}^{\\hat π^*} \\hat V^{*} \\|_∞\n  \\notag \\\\\n  &\\le \\| \\BELLMAN^{\\hat π^*} V^{\\hat π^*} -\n          \\BELLMAN^{\\hat π^*} V^{*} \\|_∞\n    +  \\| \\BELLMAN^{\\hat π^*} V^{*} -\n       \\hat {\\BELLMAN}^{\\hat π^*} V^{*} \\|_∞\n    +\n       \\| \\hat {\\BELLMAN}^{\\hat π^*} V^{*} -\n       \\hat {\\BELLMAN}^{\\hat π^*} \\hat V^{*} \\|_∞\n  \\notag \\\\\n  &\\le γ \\| V^* - V^{\\hat π^*} \\|_∞ + \\MISMATCH^{\\hat π^*} V^*\n  + γ \\| V^* - \\hat V^* \\|_∞\n  \\label{eq:ineq-21}.\n\\end{align}\\] where the first inequality follows from the triangle inequality and the second inequality follows from the definition of Bellman mismatch functional and contraction property of Bellman operator.\nSubstituting \\(\\eqref{eq:ineq-21}\\) in \\(\\eqref{eq:triangle-1}\\) and rearranging terms, we get \\[\\begin{align}\n  \\| V^* - V^{\\hat π^*} \\|_∞\n  &\\le\n  \\frac{1}{1-γ} \\MISMATCH^{\\hat π^*} V^*\n  +\n  \\frac{1+γ}{1-γ} \\| V^* - \\hat V^* \\|_∞\n  \\notag \\\\\n  &\\le\n  \\frac{1}{1-γ} \\MISMATCH^{\\hat π^*} V^*\n  +\n  \\frac{(1+γ)}{(1-γ)^2} \\MISMATCH^* V^* .\n\\end{align}\\] where the second inequality follows from Proposition 25.2.\n\n\n\n\n\n\n\n\n\nRemark:\n\n\n\nNote that the bound of Theorem 25.2 is tighter by a factor of \\(1/(1-γ)\\) but that bound is in terms of \\(\\hat V^*\\). In some settings, a bound in terms of \\(V^*\\) is more desirable. Using Theorem 25.1 in such settings leads to scaling by \\(1/(1-γ)\\).\n\n\n\n\n\n\n\n\nComparison with policy error bound\n\n\n\nIt is interesting to compare the bound of Theorem 25.2 with the policy loss error derived in Theorem 24.1. In particular, instead of using policy \\(\\hat π^*\\), suppose we use the policy \\(μ = \\GREEDY(\\hat V^*)\\), which is the greedy policy (in the true model) w.r.t. \\(\\hat V^*\\). From Theorem 24.1, we know that \\[\n  \\NORM{V^* - V^{μ}}_{∞} \\le \\frac{2 γ}{1 - γ} \\NORM{V^* - \\hat V^*}_{∞}\n  \\le \\frac{2 γ}{(1-γ)^2} \\MISMATCH^{\\max} \\hat V^*\n\\] where we have used Proposition 25.2 and Lemma 25.1 for the last inequality. In contast, the bound of Theorem 25.2 is tighter by a factor of \\(1/(1-γ)\\). In principle, this policy should be better than \\(\\hat π^*\\). The above comparison shows that it may be possible to tighten the policy error loss in Theorem 24.1.\nNote that when we use the upper bounds in terms of \\(\\MISMATCH^{\\max} V^*\\), then the two bounds match in \\(1/(1-γ)\\) factors. In particular, \\[\n  \\NORM{V^* - V^{μ}}_{∞} \\le \\frac{2 γ}{1 - γ} \\NORM{V^* - \\hat V^*}_{∞}\n  \\le \\frac{2 γ}{(1-γ)^2} \\MISMATCH^{\\max} V^*\n\\] which is slightly tighter than the bound in Theorem 25.2 on \\(\\NORM{V^* - V^{\\hat π^*}}_{∞}\\).",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model approximation</span>"
    ]
  },
  {
    "objectID": "approx-mdps/model-approximation.html#ipm-based-bounds-on-model-approximation-error",
    "href": "approx-mdps/model-approximation.html#ipm-based-bounds-on-model-approximation-error",
    "title": "25  Model approximation",
    "section": "25.2 IPM based bounds on model approximation error",
    "text": "25.2 IPM based bounds on model approximation error\nSometimes, it is easier to think in terms of explicit bounds between the models. For example, we may characterize the error between models \\(\\ALPHABET M\\) and \\(\\hat {\\ALPHABET M}\\) as the distance between their cost functions and transition dynamics. For formalize this notion, we need to specify a metric on probability spaces. It turns out that integral probability metrics (IPM) are ideally suited for this task.\nLet \\(\\def\\F{\\mathfrak{F}}\\F\\) be a convex and balanced set of functions from \\(\\ALPHABET S\\) to \\(\\reals\\). Then, the IPM distance (w.r.t. \\(\\F\\)) between two probability laws \\(μ_1\\) and \\(μ_2\\) is given by \\[\n  d_{\\F}(ν_1, ν_2) = \\sup_{f \\in \\F}\n  \\left| \\int f d μ_1 - \\int f d μ_2 \\right|.\n\\] For our discussion, we will assume that \\(\\F\\) is a maximal generator. See the notes on IPM for more details, in particular the notion of gauge or Minkowski functional \\(ρ_{\\F}\\) of an IPM and Proposition 35.4, which states that for any function \\(f\\), \\[\\begin{equation}\\label{eq:IPM-ineq}\n  \\left| \\int f d μ_1 - \\int f d μ_2 \\right|\n  \\le\n  ρ_{\\F}(f) d_{\\F}(μ_1, μ_2).\n\\end{equation}\\]\nNow, we define a notion of distance between models.\n\nDefinition 25.1 Given a function class \\(\\F\\), we say that a model \\(\\hat {\\ALPHABET M}\\) is an \\((ε,δ)\\)-approximation of model \\(\\ALPHABET M\\) if for all \\((s,a) \\in \\ALPHABET S × \\ALPHABET A\\), we have:\n\n\\(\\ABS{ c(s,a) - \\hat c(s,a) } \\le ε\\)\n\\(d_{\\F}( P(\\cdot \\mid s,a) , \\hat P(\\cdot \\mid s,a) ) \\le δ\\).\n\n\nNote that given any two models \\(\\ALPHABET M\\) and \\(\\hat {\\ALPHABET M}\\), we can always say that \\(\\hat {\\ALPHABET M}\\) is an \\((ε,δ)\\) approximation of \\(\\ALPHABET M\\) with \\[\nε = \\NORM{ c - \\hat c }_{∞}\n\\quad\\text{and}\\quad\nδ = \\sup_{(s,a) \\in \\ALPHABET S × \\ALPHABET A}\n    d_{\\F}( P(\\cdot \\mid s,a) , \\hat P(\\cdot \\mid s,a) ).\n\\]\nAn immediate implication of the above definition is the following.\n\nLemma 25.2 If \\(\\hat {\\ALPHABET M}\\) is an \\((ε,δ)\\) approximation of \\(\\ALPHABET M\\) with respect to \\(\\F\\), then for any \\(v \\colon \\ALPHABET S \\to \\reals\\) \\[\n  \\MISMATCH^{\\max} v \\le ε + γ δ \\, ρ_{\\F}(v).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the definition of maximum Bellman mismatch, we have \\[\\begin{align*}\n  \\MISMATCH^{\\max} v &=\n  \\max_{(s,a) \\in \\ALPHABET S, A} \\biggl\\lvert\n    c(s,a) + γ \\sum_{s' \\in \\ALPHABET S} P(s'|s,a)v(s')\n   -\\hat c(s,a) - γ \\sum_{s' \\in \\ALPHABET S} \\hat P(s'|s,a) v(s')\n   \\biggr\\rvert\n  \\\\\n  &\\stackrel{(a)}\\le\n  \\max_{(s,a) \\in \\ALPHABET S, A}\n  \\bigg\\{\n  \\bigl\\lvert c(s,a) - \\hat c(s,a) \\bigr\\rvert\n  +\n  γ \\biggl\\lvert\n  \\sum_{s' \\in \\ALPHABET S} P(s' | s,a) v(s')\n  -\n  \\sum_{s' \\in \\ALPHABET S} \\hat P(s' | s,a) v(s')\n  \\biggr|\n  \\biggr\\}\n  \\\\\n  &\\stackrel{(b)}\\le\n  \\max_{(s,a) \\in \\ALPHABET S, A} \\Bigl\\{\n  \\bigl\\lvert c(s,a) - \\hat c(s,a) \\bigr\\rvert\n  + γ ρ_{\\F}(v)\n  d_{\\F}(P(\\cdot | s,a), \\hat P(\\cdot | s,a) )\n  \\Bigr\\}\n  \\notag \\\\\n  &\\stackrel{(c)}\\le ε + γ δ ρ_{\\F}(v)\n\\end{align*}\\] where \\((a)\\) follows from triangle inequality, \\((b)\\) follows from \\(\\eqref{eq:IPM-ineq}\\) and \\((c)\\) follows from the definition of \\((ε,δ)\\).\n\n\n\nAn immediate consequence of Lemma 25.2 when combined with Theorem 25.2 and Theorem 25.1 is the following.\n\nTheorem 25.3 (Model approximation error) If model \\(\\hat {\\ALPHABET M}\\) is an \\((ε,δ)\\)-approximation of model \\(\\ALPHABET M\\), then the policy \\(\\hat π^*\\) is an \\(α\\)-optimal policy of \\(\\ALPHABET M\\) where \\[\n    α := \\| V^* - V^{\\hat π^*} \\|_∞ \\le\n    \\frac{2}{1-γ} \\bigl[ ε + γ δ ρ_{\\F}(\\hat V^*)\\bigr].\n\\] Another upper bound on \\(α\\) is \\[\n    α  \\le\n    \\frac{2}{(1-γ)^2} \\bigl[ ε + γ δ ρ_{\\F}(V^*)\\bigr].\n\\]\n\nNote that the above bounds require the knowledge of \\(\\hat V^*\\). For specific choices of IPM, it is possible to obtain bounds which do not require the knowledge of \\(\\hat V^*\\). We can get looser upper bounds which do not require explicit knowledge of \\(\\hat V^*\\).\n\nCorollary 25.1 (Instance independent model approximation error bounds)  \n\nIf model \\(\\hat {\\ALPHABET M}\\) is an \\((ε,δ)\\)-approximation of model \\(\\ALPHABET M\\) with respect to total variation, then the policy \\(\\hat π^*\\) is an \\(α\\)-optimal policy of \\(\\ALPHABET M\\) where \\[\n  α \\le \\frac{2 ε}{1-γ} + \\frac{ γ δ\\SPAN(\\hat r)}{(1-γ)^2},\n\\] or \\[\n  α \\le \\frac{2 ε}{(1-γ)^2} + \\frac{ γ δ\\SPAN(r)}{(1-γ)^3}.\n\\]\nIf the approximation is with respect the the Wasserstein distance, then we have the following:\n\nIf the approximate model \\(\\hat {\\ALPHABET M}\\) is \\((\\hat L_r, \\hat L_P)\\) Lipschitz (see Definition 19.1) with \\(γ \\hat L_P &lt; 1\\), then \\[\n  α \\le \\frac{2}{1-γ}\\biggl[\n  ε + \\frac{γ δ\\hat L_r}{1 - γ \\hat L_{P}} \\biggr]\n\\]\nIf the original model \\({\\ALPHABET M}\\) is \\((L_r, L_P)\\) Lipschitz with \\(γ L_P &lt; 1\\), then \\[\n  α \\le \\frac{2}{(1-γ)^2}\\biggl[\n  ε + \\frac{γ δ L_r}{1 - γ L_{P}} \\biggr]\n\\]\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nThe result follows from Theorem 25.3 the observation that \\(\\SPAN(\\hat V^*) \\le (1-γ)^{-1} \\SPAN(\\hat r)\\) and \\(\\SPAN(V^*) \\le (1-γ)^{-1} \\SPAN(r)\\).\nThe result follows from Theorem 25.3 and Theorem 19.1.",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model approximation</span>"
    ]
  },
  {
    "objectID": "approx-mdps/model-approximation.html#example-inventory",
    "href": "approx-mdps/model-approximation.html#example-inventory",
    "title": "25  Model approximation",
    "section": "25.3 Example: Inventory management with incorrect demand distribution",
    "text": "25.3 Example: Inventory management with incorrect demand distribution\nLet’s consider the inventory management problem. Suppose that the demand process has a PDF \\(f_W\\) but we choose a policy \\(\\hat π^*\\) according to an estimated demand PDF \\(f_{\\hat W}\\). What is the model approximation error if the policy \\(\\hat π^*\\) is used instead of \\(π^*\\)?\nTo upper bound the model approximation error, we use the instance independent bounds of Corollary 25.1 in terms of the Wasserstein distance. Note that since there is no error in modeling the per-step cost, \\(ε = 0\\). To bound \\(δ\\), note that pdf of the dynamics are given by: \\[\n  p(\\cdot | s,a) = f_W(\\cdot -s+a)\n\\] which is the same as the demand distribution shifted by \\((s-a)\\). Thus, \\[\n  \\ALPHABET K(p(\\cdot | s,a), \\hat p(\\cdot | s,a))\n  =\n  \\ALPHABET K(f_W(\\cdot - s + a), f_{\\hat W}(\\cdot - s + a))\n  =\n  \\ALPHABET K(f_W, f_{\\hat W})\n\\] where the last equality uses the fact that shifting two distributions does not change their Wasserstein distance1. Thus, \\[\n  δ = \\ALPHABET K(f_W, f_{\\hat W})\n\\] is the Wasserstein distance between the\n1 For our 1-dimension setting, this fact can be seen immediately from the formula of Wasserstein distance in terms of the CDF of random varaibles: \\[\n  \\ALPHABET K(X,Y) = \\int_{-∞}^∞ \\ABS{ F_X(t) - F_Y(t) } dt.\n\\] For a more general argument, see the next section.We know from Example 19.1 that the inventory management model is \\((p + \\max\\{c_h,cs\\}, 1)\\)-Lipschitz. Therefore, the approximate value function \\(\\hat V^*\\) is \\(\\hat L_V\\)-Lipschitz (see Theorem 19.1) with \\[\n  \\hat L_V \\le \\frac{p + \\max\\{c_h, c_s\\}}{1 - γ}.\n\\]\nTherefore, we get that the policy \\(\\hat π^*\\) is \\(α\\)-optimal, where \\[\n  α \\le \\frac{2 γ}{(1-γ)^2}(p + \\max\\{c_h, c_s\\}) \\ALPHABET K(f_W, f_{\\hat W}).\n\\]",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model approximation</span>"
    ]
  },
  {
    "objectID": "approx-mdps/model-approximation.html#example-certainty-equivalence",
    "href": "approx-mdps/model-approximation.html#example-certainty-equivalence",
    "title": "25  Model approximation",
    "section": "25.4 Example: Performance loss in using certainty equivalent control",
    "text": "25.4 Example: Performance loss in using certainty equivalent control\nCertainty equivalence refers to the following design methodology to determine a control policy for a stochastic control problem. Replace the random variables in the stochastic control problem by their (conditional) expectations, solve the resulting deterministic control problem to determine a feedback control policy, and use the resulting certainty equivalent control policy in the original stochastic system. See notes on linear quadratic regulation for an example where certainty equivalence is optimal.\nBut this is not the case in general. In this section, we use the results of Theorem 25.3 to characterize the performance loss when using certainty equivalence for dynamic models.\nConsider a system with state space \\(\\reals^n\\), action space \\(\\reals^m\\), and dynamics \\[\\begin{equation}\\label{eq:stochastic}\n    S_{t+1} = f(S_t, A_t) + N_t\n\\end{equation}\\] where \\(f\\) is a measurable function and \\(\\{N_t\\}_{t \\ge 1}\\) is a zero-mean i.i.d. noise sequence with control law \\(\\nu_N\\). The per-step cost is given by \\(c(S_t, A_t)\\).\nNow consider a deterministic model obtained by assuming that the noise sequence in \\(\\eqref{eq:stochastic}\\) takes its expected value, i.e., the dynamics are \\[\\begin{equation}\\label{eq:deterministic}\n    S_{t+1} = f(S_t, A_t).\n\\end{equation}\\] The per-step cost is the same as before.\nLet \\(\\ALPHABET M\\) denote the stochastic model and \\(\\hat {\\ALPHABET M}\\) denote the deterministic model. Then, the certainty equivalent design is to use the control policy \\(\\hat \\pi^*\\) in original stochastic model \\(\\ALPHABET M\\). We use the Wasserstein distance based bounds in Corollary 25.1 to bound \\(\\NORM{V^{\\hat \\pi^*} - V^*}_{∞}\\). We assume that there is some norm \\(\\| \\cdot \\|\\) on \\(\\reals^n\\) and the Wasserstein distance and Lipschitz constant are computed with respect to this norm.\nSince the costs are the same for both models, \\(ε = 0\\). We now characterize \\(\\delta\\). For ease of notation, given random variables \\(X\\) and \\(Y\\) with probability laws \\(\\nu_X\\) and \\(\\nu_Y\\), we will use \\(\\ALPHABET K(X,Y)\\) to denote \\(\\ALPHABET K(\\nu_X, \\nu_Y)\\). Recall that Wasserstein distance is defined as (Villani et al. 2008) \\[\\begin{equation}\\label{eq:Kantorovich}\n    \\ALPHABET K(\\nu_X, \\nu_Y) = \\inf_{ \\substack{ \\tilde X \\sim \\nu_X \\\\ \\tilde Y \\sim \\nu_Y} }\n    \\EXP[ \\| \\tilde X - \\tilde Y \\| ].\n\\end{equation}\\] Now, for a fixed \\((s,a)\\), define \\(X = f(s,a) + N\\), where \\(N \\sim \\nu_N\\), and \\(Y = f(s,a)\\). Then, the Wasserstein distance between \\(P(\\cdot | s,a)\\) and \\(\\hat P(\\cdot | s,a)\\) is equal to \\(\\ALPHABET K(X,Y)\\), which by \\(\\eqref{eq:Kantorovich}\\) equals \\(\\EXP[\\| N \\|]\\), which does not depend on \\((s,a)\\). Thus, \\[\n  δ = \\EXP[\\NORM{N}].\n\\]\nThus, by Corollary 25.1, we get \\[\\begin{equation}\\label{eq:CE-bound}\n    \\NORM{ V^{\\hat \\pi^*} - V^*}_{∞} \\le \\frac{2\\gamma}{1- \\gamma} \\EXP[ \\| N \\| ] L_{\\hat V^*}.\n\\end{equation}\\] This bound precisely quantifies the engineering intuition that certainty equivalent control laws are good when the noise is “small”. This bound may be viewed as a generalization of the bounds on certainty equivalence for stochastic optimization presented earlier.",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model approximation</span>"
    ]
  },
  {
    "objectID": "approx-mdps/model-approximation.html#notes",
    "href": "approx-mdps/model-approximation.html#notes",
    "title": "25  Model approximation",
    "section": "Notes",
    "text": "Notes\nThe material in this section is adapted from Bozkurt et al. (2023), where the results were presented for unbounded per-step cost. The IPM-based bounds of Theorem 25.3 are due to Müller (1997), but the proof is adapted from Bozkurt et al. (2023), where some generalizations of Theorem 25.3 are also presented. The total variation bound in Corollary 25.1 is due to Müller (1997). The Wasserstein distance based bound in Corollary 25.1 is due to Asadi et al. (2018).\nThe approximation bound for the inventory management example is from Müller (1997). The approximation bound for certainty equivalence is from Bozkurt et al. (2023).\n\n\n\n\nAsadi, K., Misra, D., and Littman, M. 2018. Lipschitz continuity in model-based reinforcement learning. Proceedings of the 35th international conference on machine learning, PMLR, 264–273. Available at: https://proceedings.mlr.press/v80/asadi18a.html.\n\n\nBozkurt, B., Mahajan, A., Nayyar, A., and Ouyang, Y. 2023. Weighted norm bounds in MDPs with unbounded per-step cost.\n\n\nMüller, A. 1997. Integral probability metrics and their generating classes of functions. Advances in Applied Probability 29, 2, 429–443. DOI: 10.2307/1428011.\n\n\nVillani, C. et al. 2008. Optimal transport: Old and new. Springer.",
    "crumbs": [
      "Approx DP",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model approximation</span>"
    ]
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-utility.html",
    "href": "risk-sensitive/risk-sensitive-utility.html",
    "title": "26  Risk Sensitive Utility",
    "section": "",
    "text": "26.1 A simple LQG example\nSuppose \\(x \\in \\reals\\) is the distance of an object from its desired position and the application of a control \\(u \\in \\reals\\) will bring it to \\(x - u\\). Suppose the cost of this maneuver is \\[\n  C =  \\tfrac{1}{2}[ R u^2 + S (x-u)^2] .\n\\]\nHere, the two terms represent the cost of control and the final displacement from the desired position. Elementary calculus shows that the optimal value of \\(u\\) and the minimum cost are \\[\n  u = \\frac{S x}{S + R },\n  \\qquad\n  V(x) = \\frac{1}{2} \\cdot \\frac{RS x^2}{S + R}.\n\\]\nNow suppose there is noise so that \\(x- u\\) is replaced by \\(x - u + w\\). We’ll assume that \\(w \\sim {\\cal N}(0, Σ)\\). The cost then becomes \\[\n  C =  R u^2 + S (x-u + w)^2 .\n\\]\nIn the risk neutral case, the optimal control is same as earlier and the minimum cost \\(V(x)\\) simply increases by \\(\\frac12 SΣ\\). This a special case of a general phenomenon known as certainty equivalence. See the notes of linear quadratic regulator for details.\nNow consider a risk-sensitive version of the problem, in which \\(u\\) is chosen to minimize \\[\n  C_θ =  \\frac{1}{θ} \\log \\EXP[ \\exp(θ C) ].\n\\]\nIn the risk-averse case (i.e., \\(θ &gt; 0\\)), minimizing \\(C_θ\\) is equivalent to minimizing \\[ \\begin{equation} \\label{eq:cost}\n\\EXP[ \\exp(θ C)] =\n\\int \\exp\\Bigl( \\frac{θ}{2} \\Bigl( Ru^2 + S(x-u+w)^2 - \\frac{w^2}{θΣ}\\Bigr)\\Bigr) dw.\n\\end{equation} \\] Let us write the right hand side as \\(\\int \\exp(\\frac{1}{2} θQ((x,u), w) dw\\). Note that \\[\n  \\frac{∂^2 Q((x,u), w)}{∂w^2} = S - \\frac{1}{θΣ}.\n\\] Therefore, \\(Q\\) is negative definite in \\(w\\) if \\(S - 1/θΣ &lt; 0\\), or equivalently (recall \\(θ &gt; 0\\)), \\[\\begin{equation} \\label{eq:critical}\n  θΣS - 1 &lt; 0\n  \\iff\n   0 &lt; θ &lt; \\frac{1}{SΣ}.\n\\end{equation} \\] For now, we assume that \\(θΣS &lt; 1\\) and we will return to what happens when \\(θΣS = 1\\) later.\nSince \\(Q\\) is negative definite in \\(w\\) (and \\(θ &gt; 0\\)), \\(-\\frac{1}{2}θQ((x,u),w))\\) is positive definite in \\(w\\). Therefore, by using Lemma 26.1 in the appendix, we know that \\[ \\begin{equation} \\label{eq:simplify}\n  \\int\\exp\\Bigl( \\frac{θ}{2} Q((x,u),w) \\Bigr) dw\n  = \\sqrt{\\frac{2π (1 - θΣS)}{Σ}}\n  \\exp\\Bigl( \\frac{θ}{2} \\max_{w}Q((x,u),w) \\Bigr).\n\\end{equation} \\] Now, the maximizing value of \\(w\\) is \\(-\\frac{θΣS}{1 - θΣS}(x-u)\\) and therefore we get \\[\n  \\max_{w} Q((x,u), w) = R u^2 + \\frac{S}{1-θΣS}(x-u)^2\n\\]\nSubstituting this base in \\eqref{eq:simplify} and then in \\eqref{eq:cost}, we get \\[\n  \\EXP[\\exp(θC)]\n  = \\sqrt{\\frac{2π (1 - θΣS)}{Σ}}\n  \\exp\\Bigl(\\frac{θ}{2}\\Bigl(R u^2 + \\frac{S}{1 -\n  θΣS}(x-u)^2\\Bigr).\n\\]\nNow, minimizing \\(\\EXP[\\exp(θC)]\\) is same as minimizing the term in coefficient of \\(θ/2\\) (recall \\(θ\\) is positive), which is minimized by \\[\n  u = \\frac{Sx}{S + R - θΣSR}.\n\\] The corresponding minimum value of effective cost is \\[\n  V_θ(x) =\n  \\frac{1}{2} \\cdot \\frac{RS x^2}{R + S - θΣSR}\n  + \\frac{1}{2θ} \\log\\frac{2π (1 - θΣS)}{Σ}.\n\\]\nNote that both the expression for control action and the value become infinity as \\(θ\\) increases through the critical value: \\[\n  θ_{\\text{crit}} =  \\frac{1}{Σ}\\left( \\frac{1}{S} + \\frac{1}{R} \\right)\n\\] First note that for \\(θ &lt; θ_{\\text{crit}}\\), the constraint \\eqref{eq:critical} is automatically satisfied. The value \\(θ = θ_{\\text{crit}}\\) marks a point at which the decision maker is so pessimistic that his apprehension of uncertainties completely overrides the assurances given by known statistical behavior. This is called neurotic breakdown. There is a corresponding optimistic extreme, euphoria, if the cost function contains quadratic reward terms.",
    "crumbs": [
      "Risk sensitive MDPs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Risk Sensitive Utility</span>"
    ]
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-utility.html#a-simple-lqg-example",
    "href": "risk-sensitive/risk-sensitive-utility.html#a-simple-lqg-example",
    "title": "26  Risk Sensitive Utility",
    "section": "",
    "text": "Remark\n\n\n\nWhittle calls the term \\(Q((x,u),w)\\) as the stress. Note that in the above calculations, we choose \\(u\\) to minimize the stress and choose \\(w\\) to maximize the stress. It is as though there is an another agent, the “phantom other”, who exerts the control \\(w\\) at the same time as the optimizer exerts the control \\(u\\). When \\(θ\\) is negative, then the phantom other is opposing the optimizer and trying to maximize the stress. (Note that the minimizing value of \\(w\\) is \\(-\\frac{θΣS}{1 - θΣS}(x-u)\\), which can also be written as \\(θΣRu\\)). So, what started out as a one-person control problem has turned into a two-person game.",
    "crumbs": [
      "Risk sensitive MDPs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Risk Sensitive Utility</span>"
    ]
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-utility.html#appendix",
    "href": "risk-sensitive/risk-sensitive-utility.html#appendix",
    "title": "26  Risk Sensitive Utility",
    "section": "Appendix",
    "text": "Appendix\n\nLemma 26.1 Suppose that \\(Q(z,w)\\) is a quadratic function of vectors \\(z\\) and \\(w\\), positive definite in \\(w\\).\nLet \\(Q_{ww} = ∂^2 Q(z,w)/∂w^2\\). Since \\(Q(z,w)\\) is a quadratic function, \\(Q_{ww}\\) does not depend on \\(z\\). Since \\(Q\\) is positive definite in \\(w\\), \\(Q_{ww} &gt; 0\\).\nSuppose \\(w \\in \\reals^r\\). Define \\(q = \\log[ (2π)^{r/2}\n\\det(Q_{ww})^{-1/2}]\\). Then, for a fixed value of \\(z\\) \\[\n  \\int \\exp\\bigl[ -Q(z,w)\\bigr] dw = \\exp\\bigl[ q - \\inf_{w \\in \\reals^r}\n  Q(z,w) \\bigr].\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThe point of the lemma is that, if one replaces an integration with respect to \\(w\\) by a minimization of \\(Q\\) with respect to \\(w\\), then the result is correct as far as terms dependent on the second argument \\(z\\) are concerned.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor the fixed value of \\(z\\), let \\(\\hat w\\) be the minimizing value of \\(Q(z,w)\\). Then, one can write\n\\[ Q(z,w) = Q(z, \\hat w) + \\tfrac 12 (w-\\hat w)^\\TRANS Q_{ww} (w - \\hat w). \\]\nThe result follows from substituting this in the left hand side of the expression in the Lemma and observing that (e.g., from the form of the density function of a multi-nominal Gaussian),\n\\[\\begin{equation}\n  \\int \\exp[ - \\tfrac 12 (w - \\hat w)^\\TRANS Q_{ww} (w - \\hat w) ] dw\n  = \\exp[-q].\n\\end{equation}\\]",
    "crumbs": [
      "Risk sensitive MDPs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Risk Sensitive Utility</span>"
    ]
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-utility.html#notes",
    "href": "risk-sensitive/risk-sensitive-utility.html#notes",
    "title": "26  Risk Sensitive Utility",
    "section": "Notes",
    "text": "Notes\nThe material in this section is taken from Whittle (2002).\n\n\n\n\nWhittle, P. 2002. Risk sensitivity, A strangely pervasive concept. Macroeconomic Dynamics 6, 1, 5–18. DOI: 10.1017/s1365100502027025.",
    "crumbs": [
      "Risk sensitive MDPs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Risk Sensitive Utility</span>"
    ]
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-mdps.html",
    "href": "risk-sensitive/risk-sensitive-mdps.html",
    "title": "27  Risk Sensitive MDPs",
    "section": "",
    "text": "27.1 Finite horizon setup\nConsider an MDP with state space \\(\\ALPHABET\nX\\), action space \\(\\ALPHABET U\\), per-step cost \\(c \\colon \\ALPHABET X ×\n\\ALPHABET U \\to \\reals\\), and controlled transition matrix \\(P\\). However, instead of the risk neutral optimization criteria that we consider previously, we consider a risk-sensitive objective. In particular, the performance of any (possibly non-Markovian) strategy \\(g =\n(g_1, \\dots, g_T)\\) is given by \\[\n  \\bar J_θ(g) = \\frac{1}{θ} \\log \\EXP\\Bigl[ \\exp\\Bigl(\n    θ \\sum_{t=1}^T c(X_t, U_t)\n  \\Bigr) \\Bigr].\n\\]\nRecall that this is the effective cost for an exponential disutility function. Note that \\(J_θ(g) = \\exp(θ \\bar J(g))\\) may be viewed as a multiplicative cost. Based on the argument for multiplicative cost, we can write the dynamic program for \\(J_θ(g)\\) as follows.\nThe dynamic program of \\eqref{eq:DP-1}–\\eqref{eq:DP-2} can be made to resemble the standard dynamic program by defining the disutility matrix \\[\n    D_{xy}(u) = \\exp(θ c(x,u)) P_{xy}(u).\n\\] Note that the elements of \\(D\\) are non-negative. The expression \\eqref{eq:DP-1} can then be written in “standard” form: \\[\n  Q_{t+1}(x) = \\sum_{y \\in \\ALPHABET X} D_{xy}(u) V_{t+1}(y).\n\\]",
    "crumbs": [
      "Risk sensitive MDPs",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Risk Sensitive MDPs</span>"
    ]
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-mdps.html#finite-horizon-setup",
    "href": "risk-sensitive/risk-sensitive-mdps.html#finite-horizon-setup",
    "title": "27  Risk Sensitive MDPs",
    "section": "",
    "text": "Dynamic program\n\n\n\nInitialize \\(V_{T+1}(x) = 1\\) and recursively compute\n\\[ \\begin{align}\n    Q_t(x,u) &= \\exp(θ c(x,u)) \\sum_{y \\in \\ALPHABET X} P_{xy}(u) V_{t+1}(y),\n    \\label{eq:DP-1}\\\\\n    V_t(x) &= \\min_{u \\in \\ALPHABET X} Q_t(x,u).\n    \\label{eq:DP-2}\n    \\end{align} \\]\nOr, equivalently, working with the effective cost value function:\n\\[ \\begin{align}\n    Q_t(x,u) &= \\exp(θ c(x,u)) \\sum_{y \\in \\ALPHABET X} P_{xy}(u)\n    \\exp(θ \\bar V_{t+1}(y)),\n    \\\\\n    \\bar V_t(x) &= \\frac{1}{θ} \\log \\bigl( \\min_{u \\in \\ALPHABET X} Q_t(x,u) \\bigr).\n    \\end{align} \\]",
    "crumbs": [
      "Risk sensitive MDPs",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Risk Sensitive MDPs</span>"
    ]
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-mdps.html#infinite-horizon-average-cost-setup",
    "href": "risk-sensitive/risk-sensitive-mdps.html#infinite-horizon-average-cost-setup",
    "title": "27  Risk Sensitive MDPs",
    "section": "27.2 Infinite horizon average cost setup",
    "text": "27.2 Infinite horizon average cost setup\nThe objective for infinite-horizon average cost setup is to minimize: \\[\n  J^* = \\inf_{g} \\limsup_{T \\to ∞}\n  \\frac{1}{θT} \\EXP \\Bigl[ \\exp\\Bigl(\n  θ \\sum_{t=1}^T c(X_t, U_t)\n  \\Bigr) \\Bigr].\n\\]\nWhen the matrix \\(P(u)\\) is irreducible and aperiodic for each \\(u\\), the dynamic program for average cost MDP can be written as follows:\n\nTheorem 27.1 Suppose there exist constant \\(J\\) and a bounded function \\(v \\colon \\ALPHABET\nX \\to \\reals\\) such that \\[ \\begin{equation} \\label{eq:avg}\n  \\exp(θ (J + v(x))) =\n  \\min_{u \\in \\ALPHABET U}\n   \\sum_{y \\in \\ALPHABET X} P_{xy}(u) \\exp( θ( c(x, u) + v(y)))\n\\end{equation} \\] Furthermore, let \\(g^* \\colon \\ALPHABET X \\to \\ALPHABET U\\) denote the policy that achieves the arg min in the right hand side. Then, \\(J\\) is the optimal performance and the time-homogeneous policy \\(g^*\\) achieves that performance.\n\nThe dynamic program of \\eqref{eq:avg} can be written in an alternative form using a Legendre-type transformation and the duality relationship between relative entropy function and the logarithmic moment generating function.\nLet \\(Δ(\\ALPHABET X)\\) denote the set of probability vectors on \\(\\ALPHABET X\\). Then, for any \\(ν \\in Δ(\\ALPHABET X)\\), the relative entropy \\(I(\\cdot \\| ν)\n\\colon Δ(\\ALPHABET X) \\to \\reals \\cup \\{+∞\\}\\) is by given by \\[\n  I(μ \\| ν) = \\begin{cases}\n    \\sum_{x \\in \\ALPHABET X} \\log(λ(x)) μ(x),\n    & \\text{if } μ \\ll ν, \\\\\n    +∞, & \\text{otherwise}.\n  \\end{cases} \\] where \\[ λ(x) = \\begin{cases}\n  \\frac{μ(x)}{ν(x)}, & \\text{if } ν(x) \\neq 0, \\\\\n  1, & \\text{otherwise}.\n\\end{cases}\\]\n\nLemma 27.1 Let \\(w \\colon \\ALPHABET X \\to \\reals\\) be a bounded function and \\(ν \\in\nΔ(\\ALPHABET X)\\). Then, \\[\n  \\log \\sum_{x \\in \\ALPHABET X} ν(x) \\exp( w(x)) =\n  \\sup_{μ \\in Δ(\\ALPHABET X)} \\Bigl\\{\n     \\sum_{x \\in \\ALPHABET X}  μ(x) w(x) -\n    I(μ \\| ν)\n  \\Bigr\\},\n\\] where the supremum is attained at the unique probability measure \\(μ^*\\) given by \\[\n  μ^*(x) = \\frac{e^{θv(x)}}{\\int e^{θv(x)}ν(x) dx} ν(x).\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nSuch a dual-representation is a fundamental property of all coherent risk measures and not just the entropic risk measure that we are working with here. See, for example, Föllmer and Schied (2010).\n\n\nUsing Lemma 27.1, the dynamic program of \\eqref{eq:avg} can be written as \\[ \\begin{equation}\n  J + v(x) = \\min_{u \\in \\ALPHABET U} \\sup_{μ \\in Δ(\\ALPHABET X)}\n  \\Bigl\\{\n    c(x,u) + \\sum_{y \\in \\ALPHABET X} μ(y) v(y) - \\frac{1}{θ}\n    I(μ \\| P(\\cdot | x, u) )\n  \\Bigr\\}.\n\\end{equation} \\] This equation corresponds to the Issacs equation associated with a stochastic dynamic game with average cost-per unit time criterion.",
    "crumbs": [
      "Risk sensitive MDPs",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Risk Sensitive MDPs</span>"
    ]
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-mdps.html#infinite-horizon-discounted-cost-setup",
    "href": "risk-sensitive/risk-sensitive-mdps.html#infinite-horizon-discounted-cost-setup",
    "title": "27  Risk Sensitive MDPs",
    "section": "27.3 Infinite horizon discounted cost setup",
    "text": "27.3 Infinite horizon discounted cost setup\nWhittle (2002) has a discussion on why discounted cost setup for risk-sensitive MDP is tricky and the solution depends on the interpretation of discounting.",
    "crumbs": [
      "Risk sensitive MDPs",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Risk Sensitive MDPs</span>"
    ]
  },
  {
    "objectID": "risk-sensitive/risk-sensitive-mdps.html#notes",
    "href": "risk-sensitive/risk-sensitive-mdps.html#notes",
    "title": "27  Risk Sensitive MDPs",
    "section": "Notes",
    "text": "Notes\nThe basic risk-sensitive MDP was first considered in Howard and Matheson (1972). See Howard and Matheson (1972) for a policy iteration algorithm. It is also mentioned that a version of this result is presented in Bellman’s book. See Hernandez-Hernández and Marcus (1996) and Hernández-Hernández (1999) for a detailed treatment of the average cost case.\n\n\n\n\n\nFöllmer, H. and Schied, A. 2010. Convex risk measures. In: Encyclopedia of quantitative finance. American Cancer Society. DOI: 10.1002/9780470061602.eqf15003.\n\n\nHernandez-Hernández, D. and Marcus, S.I. 1996. Risk sensitive control of markov processes in countable state space. Systems & Control Letters 29, 3, 147–155. DOI: 10.1016/s0167-6911(96)00051-5.\n\n\nHernández-Hernández, D. 1999. Existence of risk-sensitive optimal stationary policies for controlled markov processes. Applied Mathematics and Optimization 40, 3, 273–285. DOI: 10.1007/s002459900126.\n\n\nHoward, R.A. and Matheson, J.E. 1972. Risk-sensitive markov decision processes. Management Science 18, 7, 356–369. DOI: 10.1287/mnsc.18.7.356.\n\n\nWhittle, P. 2002. Risk sensitivity, A strangely pervasive concept. Macroeconomic Dynamics 6, 1, 5–18. DOI: 10.1017/s1365100502027025.",
    "crumbs": [
      "Risk sensitive MDPs",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Risk Sensitive MDPs</span>"
    ]
  },
  {
    "objectID": "linear-systems/lqr.html",
    "href": "linear-systems/lqr.html",
    "title": "28  Linear quadratic regulation",
    "section": "",
    "text": "28.1 Solution to linear quadratic regulation\nWe start from a simple observation.\nNote that with the above notation, the terms defined in Lemma 28.1 may be written as \\[ P_{+} = \\RICCATI P\n\\quad\\text{and}\\quad\nK = \\GAIN P. \\]\nNow, an immediate implication of Proposition 28.1 is the following:",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear quadratic regulation</span>"
    ]
  },
  {
    "objectID": "linear-systems/lqr.html#solution-to-linear-quadratic-regulation",
    "href": "linear-systems/lqr.html#solution-to-linear-quadratic-regulation",
    "title": "28  Linear quadratic regulation",
    "section": "",
    "text": "A useful property\n\n\n\nLet \\(x \\in \\reals^n\\) be a random variable with mean \\(μ\\) and covariance \\(Σ\\). Then, \\[\n\\EXP[ x^\\TRANS S x ] = μ^\\TRANS S μ + \\TR(S Σ)\n\\]\n\n\n\n\nLemma 28.1 (Completion of squares) Let \\(x \\in \\reals^n\\), \\(u \\in \\reals^m\\), and \\(w \\in \\reals^n\\) be random variables defined on a common probability space. Suppose \\(w\\) is zero mean with finite covariance and independent of \\((x,u)\\). Let \\(x_{+} = Ax + Bu + w\\), where \\(A\\) and \\(B\\) are matrices of appropriate dimensions. Then, given matrices \\(P\\), \\(Q\\), \\(S\\), and \\(R\\) of appropriate dimensions, \\[\n\\EXP\\left[\n\\MATRIX{x \\\\ u}^\\TRANS \\MATRIX{Q & S \\\\ S^\\TRANS & R } \\MATRIX{ x \\\\ u}\n+ x_{+} P x_{+} \\right]\n=\n\\EXP\\bigl[ x^\\TRANS P_{+} x  \n    +\n    (u + Kx)^\\TRANS Δ (u + Kx)\n    +\n    w^\\TRANS P w\n    \\bigr].\n\\] where\n\n\\(Δ = R + B^\\TRANS P B\\)\n\\(K = Δ^{-1}[ S^\\TRANS + B^\\TRANS P A ]\\)\n\\(P_{+} = Q + A^\\TRANS P A - K^\\TRANS Δ K\\)\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\(w\\) is zero mean and independent of \\((x,u)\\), we have \\[\n  \\EXP[ x_{+}^\\TRANS P x ]\n  =\n  \\EXP\\bigl[ (Ax + Bu)^\\TRANS P (Ax + Bu)\n      + w^\\TRANS P w \\bigr].\n\\]\nThe proof follows immediately by completing the square on the left hand side. In particular \\[\\begin{align*}\n& u^\\TRANS R u + 2 x^\\TRANS S u +  (Ax+Bu)^\\TRANS P (Ax + Bu) \\\\\n& \\quad = u^\\TRANS (R + B^\\TRANS P B) u + 2 u^\\TRANS( S^\\TRANS + B^\\TRANS P A) x\n  + x^\\TRANS A^\\TRANS P A x \\\\\n& \\quad = u^\\TRANS Δ u + 2 u^\\TRANS Δ K x + x^\\TRANS A^\\TRANS P A x \\\\\n& \\quad =\n  (u + K x)^\\TRANS Δ (u + Kx) - x^\\TRANS K^\\TRANS Δ K x\n  +  x^\\TRANS A^\\TRANS P A x \\\\\n\\end{align*}\\]\n\n\n\n\nDefinition 28.1 Given the system model \\((A,B)\\) and per-step cost \\((Q,S,R)\\), we define the Riccati operator \\(\\RICCATI \\colon \\mathbb{S}^{n × n}_{\\ge 0} \\to \\mathbb{S}^{n × n}_{\\ge 0}\\) as follows: \\[ \\RICCATI P = Q + A^\\TRANS P A\n- (S^\\TRANS + B^\\TRANS P A)^\\TRANS (R + B^\\TRANS P B)^{-1}\n  (S^\\TRANS + B^\\TRANS P A).\\] Moreover, define the Gain operator \\(\\GAIN \\colon \\mathbb{S}^{n × n}_{\\ge 0} \\to \\reals^{m × n}\\) as \\[ \\GAIN P = - (R + B^\\TRANS P B)^{-1}(S^\\TRANS + B^\\TRANS P A). \\]\n\n\n\n\n\n\n\n\nRiccati equations\n\n\n\nRiccati equations are named after Jacopo Riccati (1670–1754) who studied the differential equations of the form \\[\\dot x = a x^2 + b t + c t^2\\] and its variations. In continuous time, such equations arise in optimal control and filtering. The discrete-time version of these equations are also named after Riccati.\nI am calling the updates in Lemma 28.1 as Riccati operators because they are similar to Bellman operators considered earlier.\n\n\n\n\n\n\n\n\nAlternative forms of the Riccati operator\n\n\n\nFor the special case when \\(S = 0\\) (i.e., no cross terms in the cost), the Riccati operator is given by: \\[\n  \\RICCATI P = Q + A^\\TRANS P A -\n  A^\\TRANS P B (R + B^\\TRANS P B)^{-1} B^\\TRANS P A\n\\] The following are equivalent to the Riccati operator:\n\n\\(A^\\TRANS P(I + B R^{-1} B^\\TRANS P)^{-1}A + Q\\).\n\\(A^\\TRANS(P^{-1} + B R^{-1} B^\\TRANS)^{-1} A + Q\\).\n\nHere the first equality follows from the simplified Sherman-Morrison-Woodbudy formula for \\((I + (B R^{-1})(B^\\TRANS P))^{-1}\\) and the second follows simple algebra.\n\n\n\nProposition 28.1 Recursively define the matrices \\(\\{P_t\\}_{t \\ge 1}\\) in a backwards manners as follows: \\(P_T = Q_T\\) and then for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[\\begin{equation}\\label{eq:riccati}\n  P_t = \\RICCATI P_{t+1}.\n\\end{equation}\\] Moreover, define the gains \\(\\{K_t\\}_{t \\ge 1}\\) as: \\[\\begin{equation}\\label{eq:gain}\n  K_t = \\GAIN P_t.\n\\end{equation}\\]\nThen, for any control policy \\(g\\), the total cost \\(J(g)\\) given by \\(\\eqref{eq:cost}\\) may be written as \\[\\begin{equation}\n  J(g) = \\bar J(g) + \\tilde J\n\\end{equation}\\] where the controlled part of the cost is \\[ \\bar J(g) =\n  \\EXP\\biggl[ \\sum_{t=1}^{T-1}\n  (u_t + K_t x_t)^\\TRANS Δ_t (u_t + K_t x_t) \\biggr]\n\\] with \\(Δ_t = R + B^\\TRANS P_{t+1} B\\) and the control-free part of the cost is \\[\n  \\tilde J =\n  \\EXP\\biggl[ x_1^\\TRANS P_1 x_1 +\n  \\sum_{t=1}^{T-1} w_t^\\TRANS P_{t+1} w_t \\biggr]\n\\]\n\n\n\n\n\n\n\nDiscrete Riccati equation\n\n\n\nWe will use the notation: \\[\n  K_{1:T-1} = \\LQR_T(A,B,Q,R;Q_T)\n  \\quad\\hbox{or}\\quad\n  (K_{1:T-1}, P_{1:T}) = \\LQR_T(A,B,Q,R;Q_T)\n\\] to denote the LQR gains \\(K_{1:T-1}\\) and \\(P_{1:T}\\) computed via \\(\\eqref{eq:riccati}\\) and \\(\\eqref{eq:gain}\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe result follows by repeatedly applying Lemma 28.1 starting at time \\(t = T-1\\) and moving backwards.\n\n\n\n\n\nTheorem 28.1 The optimal control policy for the LQR problem \\(\\eqref{eq:cost}\\) is given by \\[\n    u_t = - K_t x_t\n  \\] where the feedback gains \\(\\{K_t\\}_{t \\ge 1}\\) are computed as described in Proposition 28.1. The performance of the optimal strategy is given by: \\[\n      J^* = \\tilde J = \\TR(X_1 P_1) + \\sum_{t=1}^{T-1} \\TR(W_t P_{t+1})\n  \\] where \\(X_1\\) is the covariance of the initial state and \\(\\{W_t\\}_{t \\ge 1}\\) is the covariance of the noise process.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFirst observe that \\(\\tilde J\\) does not depend on the control policy. So, minimizing \\(\\bar J(g)\\) is the same as minimizing \\(J(g)\\).\nNow recall that \\(\\{R_t\\}_{t \\ge 1}\\) are positive definite. Therefore, \\(Δ_t = [R_t + B^\\TRANS P B]\\) are positive definite. Hence, for any policy \\(g\\), \\(\\bar J(g) \\ge 0\\). The proposed policy achieves \\(\\bar J(g) = 0\\) and is, therefore, optimal.",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear quadratic regulation</span>"
    ]
  },
  {
    "objectID": "linear-systems/lqr.html#salient-features-of-the-result",
    "href": "linear-systems/lqr.html#salient-features-of-the-result",
    "title": "28  Linear quadratic regulation",
    "section": "28.2 Salient features of the result",
    "text": "28.2 Salient features of the result\n\nWe derived the results for time-invariant dynamics and cost, but the argument trivially generalizes to time-varying dynamics and cost as well.\nThe optimal gains \\(\\{K_t\\}_{t \\ge 1}\\) do not depend on the distribution of the noise \\(\\{W_t\\}_{t \\ge 1}\\). Thus, the noise in the dynamics does not change the closed loop control policy but changes the optimal cost by a term that depends on the noise covariance (but does not depend on the policy).\nA special case of the above observation is that the optimal control policy of the stochastic LQR problem is the same as the optimal control policy of the deterministic LQR problem (where the noise \\(w_t ≡ 0\\)). This result is sometimes called the certainty equivalence principle.\nSuppose the noise was not white but Gaussian and correlated over time (and still independent of the initial state \\(x_1\\)). Then, the optimal control action at time \\(t\\) will be the same as that of the deterministic system \\[\n  x_{τ + 1} = A x_{τ} + B u_{τ} + w_{τ|t},\n  \\quad τ \\ge t,\n\\] where \\(w_{τ|t}\\) is \\(\\EXP[ w_{\\tau} \\mid w_{1:t} ]\\). That is, at time \\(t\\), one replaces future stochastic noise \\(w_τ\\) (\\(τ \\ge t\\)) by an ‘equivalent’ deterministic noise \\(w_{τ|t}\\) and then applies the method of deterministic LQR to deduce the optimal feedback control in terms of the predicted noise. This is also a special instance of the general certainty equivalence principle, which also extends to the case when the state is not perfectly observed.\nThe assumption that \\(R\\) is positive definite is not necessary. In order to get a unique control law, it is sufficient that \\(R + B^\\TRANS P_{t} B\\) be positive definite for all \\(t\\), which is possible even when \\(R\\) is not positive definite.\nA particular instance where \\(R\\) is not positive definite is when \\(R = 0\\)! This is called minimum variance control. \n\nThe term minimum variance is used because the objective is equivalent to minimizing the variance of \\(y = C x\\), where \\(C\\) is such that \\(C^\\TRANS C = Q\\).",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear quadratic regulation</span>"
    ]
  },
  {
    "objectID": "linear-systems/lqr.html#lq-tracking-of-a-reference-trajectory",
    "href": "linear-systems/lqr.html#lq-tracking-of-a-reference-trajectory",
    "title": "28  Linear quadratic regulation",
    "section": "28.3 LQ tracking of a reference trajectory",
    "text": "28.3 LQ tracking of a reference trajectory\nNow we consider the tracking problem. We are given a pre-specified trajectory \\(\\{r_t\\}_{t=1}^T\\) and the objective is to minimize a per-step cost given by \\[ \\begin{align*}\n    c_t(x_t, u_t) &= (C x_t - r_t)^\\TRANS Q (x_t - r_t) + u_t^\\TRANS R u_t\n    \\\\\n    \\text{and}\\quad\n    c_T(x_T) &= (x_T - r_T)^\\TRANS Q_T (x_T - r_T).\n  \\end{align*} \\]\nThe simplest way to derive a solution to the tracking problem is to convert it to a regulation problem by using state augmentation. In particular, define \\(z_t = \\VEC(x_t, 1)\\). Then per-step cost is quadratic in \\(z_t\\): \\[\n  c(z_t, u_t) =\n  z^\\TRANS \\bar Q z + u^\\TRANS R u\n  ,\n  \\quad\\text{where }\n  \\bar Q =\n  \\MATRIX{C & -r_t}^\\TRANS\n  Q\n  \\MATRIX{I & -r_t}.\n\\] Moreover, \\(z_t\\) has linear dynamics of the form: \\[\n  z_{t+1} = \\bar A z_t + \\bar B u_t + \\bar B w_t\n  ,\n  \\quad\\hbox{where }\n  \\bar A = \\MATRIX{A & 0 \\\\ 0 & 1}\n  \\hbox{ and }\n  \\bar B = \\MATRIX{B \\\\ 0}.\n\\] Thus, we can compute the optimal gains using the solution of the regulation problem (with time-varying cost): \\[\n\\bar K_{1:T-1} = \\LQR_T(\\bar A, \\bar B, \\bar Q_{1:T}, R).\n\\] It can be shown (Lewis et al. 2012) that the optimal control can be written as \\[\n  u_t = \\bar K_t z_t = - K_t x_t + K ^∘_t v_{t+1}\n\\] where \\[\\begin{align*}\n  (K_{1:T-1}, P_{1:T}) &= \\LQR_T(A,B,C^\\TRANS Q C,R; C^\\TRANS Q_T C),\n  \\\\\n  K ^∘_t &= (R + B^\\TRANS P_{t+1} B)^{-1} B^\\TRANS,\n\\end{align*}\\] and the offset process \\(v_{1:T}\\) is computed backwards by solving \\[\n  v_t = (A - BK_t)^\\TRANS v_{t+1} + C^\\TRANS Q r_t,\n  \\quad\n  v_T = C^\\TRANS Q r_T.\n\\]",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear quadratic regulation</span>"
    ]
  },
  {
    "objectID": "linear-systems/lqr.html#an-example-second-order-integrator",
    "href": "linear-systems/lqr.html#an-example-second-order-integrator",
    "title": "28  Linear quadratic regulation",
    "section": "28.4 An example: second order integrator",
    "text": "28.4 An example: second order integrator\nAs an example, consider a discretized model of a second-order integrator, which models the dynamics of a point-mass in one-dimensional space under time-varying force. The continuous-time dynamics of a second-order integrator are given by \\[\n  \\dot x(t) = \\MATRIX{0 & 1 \\\\ 0 & 0} x(t) + \\MATRIX{0\\\\ \\frac 1m}u(t)\n\\] where \\(x(t) \\in \\reals^2\\) with \\(x_1(t)\\) indicating position and \\(x_2(t)\\) indicating velocity, \\(u(t) \\in \\reals\\) denotes force, and \\(m\\) is a parameter denoting mass. We discretize the dynamics using zero-order hold (Wittenmark et al. 2002) with a sampling time of \\(Δt\\). First observe that the matrix \\(A_c = \\left[ \\begin{smallmatrix}0 & 1 \\\\ 0 & 0\\end{smallmatrix}\\right]\\) is nilpotent because \\(A_c^2 = 0\\). Therefore, the matrix exponential simplifies to \\[\n  e^{A_ct} = I + A_c t =\n  \\MATRIX{1 & t \\\\ 0 & 1}.\n\\] Therefore, the discretized model is \\[\\begin{align*}\n  A &= e^{A_c Δt} = \\MATRIX{1 & Δt \\\\ 0 & 1},\n  &\n  B &= \\int_{0}^{Δt} e^{A_ct} B_c dt = \\MATRIX{\\tfrac12 Δt^2 \\\\ \\frac{Δt}{m}}\n\\end{align*}\\]\nWe further assume that there is a disturbance with variance \\(σ^2\\) at the actuation, so the discretized model is \\[\n  x_{t+1} = A x_t + B (u_t + w_t)\n\\] where \\(w_t \\sim {\\cal N}(0, σ^2)\\)1.\n1 This model may be viewed as a model of the form \\(\\eqref{eq:dynamics}\\) by considering the noise covariance in \\(\\eqref{eq:dynamics}\\) to be \\(σ^2 B B^\\TRANS\\).Suppose \\[\n  Q = Q_T = \\MATRIX{1 & 0 \\\\ 0 & 0}\n  \\quad\\text{and}\\quad\n  R = ρ\n\\]\n\nRegulation\nWe first solve the regulation problem. Assume \\(m = 1\\,\\text{kg}\\), \\(Δt = 0.1\\,\\text{s}\\) and noise covariance \\(σ^2 = 0.05\\). The output (i.e., position) and the input (i.e., force) for a \\(T = 10\\,\\text{s}\\) simulation of the system are shown in Figure 28.1. Note that, as expected, as the cost of applying control increases, less force is applied and it takes longer for the position to become close to zero.\n\n\n\n\n\n\nviewof R = Object.assign(Inputs.range([0.1, 2.5], {label: \"R\", step: 0.1, value: 0.1 }), {style: '--label-width:20px'})\n\n\n\n\n\n\nPositionPlot = Plot.plot({\n  grid: true,\n  y: { domain: [-0.25, 1] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(sims.filter(d =&gt; d.R == R), {x: \"time\", y: \"position\", curve:\"step-after\"}),\n  ]}\n)\nForcePlot = Plot.plot({\n  grid: true,\n  y: { domain: [-3, 0.5] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(sims.filter(d =&gt; d.R == R), {x: \"time\", y: \"force\", curve: \"step-before\"}),\n  ]}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Position over time\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Force over time\n\n\n\n\n\n\n\nFigure 28.1: Optimal regulation of second order integrator for different choices of control cost\n\n\n\n\n\nTracking\nWe now solve the tracking problem. Conisder the same parameters as before. We consider tracking a “square-wave” signal. The output (i.e., position) and the input (i.e., force) for a \\(T = 60\\,\\text{s}\\) simulation of the system are shown in Figure 28.2.\n\n\n\n\n\n\nviewof Ro = Object.assign(Inputs.range([0.1, 2.5], {label: \"R\", step: 0.1, value: 0.1 }), {style: '--label-width:20px'})\n\n\n\n\n\n\nPositionPloto = Plot.plot({\n  grid: true,\n  y: { domain: [-0.25, 1] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(tracking.filter(d =&gt; d.R == Ro), {x: \"time\", y: \"reference\", curve:\"step-after\", stroke: \"red\"}),\n    Plot.line(tracking.filter(d =&gt; d.R == Ro), {x: \"time\", y: \"position\", curve:\"step-after\"}),\n  ]}\n)\nForcePloto = Plot.plot({\n  grid: true,\n  y: { domain: [-3, 0.5] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(tracking.filter(d =&gt; d.R == R), {x: \"time\", y: \"force\", curve: \"step-before\"}),\n  ]}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Position over time (the red curve denotes the reference)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Force over time\n\n\n\n\n\n\n\nFigure 28.2: Optimal regulation of second order integrator for different choices of control cost",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear quadratic regulation</span>"
    ]
  },
  {
    "objectID": "linear-systems/lqr.html#exercises",
    "href": "linear-systems/lqr.html#exercises",
    "title": "28  Linear quadratic regulation",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 28.1 What is the optimal solution to the LQR problem when \\(Q = 0\\)?\n\n\nExercise 28.2 What is the optimal solution to the LQR problem when \\(Q \\succ 0\\) and \\(R = 0\\)?\n\n\nExercise 28.3 (Noise coupled subsystems) Consider a system with two subsystems: subsytem 1 with state \\(x^1_t \\in \\reals^{n^1}\\) and control \\(u^1_t \\in \\reals^{m^1}\\) and subsystem 2 with state \\(x^2_t \\in \\reals^{n^2}\\) and control \\(u^2_t \\in \\reals^{m^2}\\). The dynamics are coupled only though the noise, i.e., \\[\\begin{align*}\n  x^1_{t+1} &= A^{11} x^1_t + B^1 u^1_t + w^1_t \\\\\n  x^2_{t+1} &= A^{22} x^2_t + B^2 u^2_t + w^2_t\n\\end{align*}\\] where the noise process \\(\\{ (w^1_t, w^2_t)\\}_{t \\ge 1}\\) is correlated across subsystems but independent across time.\nLet per-step cost is decoupled across the subsystems and of the form: \\[\n  c(x_t, u_t) =\n  (x^1_t)^\\TRANS Q^{11} x^1_t + (u^1_t)^\\TRANS R^{11} u^1_t\n  +\n  (x^2_t)^\\TRANS Q^{22} x^2_t + (u^2_t)^\\TRANS R^{22} u^2_t.\n\\] The terminal cost has a similar structure. Show that the optimal control law is of the form: \\[\nu^1_t = - K^1_t x^1_t\n\\quad\\text{and}\\quad\nu^2_t = - K^2_t x^2_t\n\\] where the gains \\(K^1_t\\) and \\(K^2_t\\) are obtained by solving two separate Riccati equations.\nHint: There are two ways to solve this problem. An algebraic method using where one can argue that the Riccati gain \\(P\\) is diagonal and a simpler method that uses certainty equivalence.",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear quadratic regulation</span>"
    ]
  },
  {
    "objectID": "linear-systems/lqr.html#notes",
    "href": "linear-systems/lqr.html#notes",
    "title": "28  Linear quadratic regulation",
    "section": "Notes",
    "text": "Notes\nSee Athans (1971) for a general discussion of the philosophical approach of approximating general stochastic control problems as linear quadratic models. See Dorato and Levis (1971) for a general overview of discrete-time LQR including a summary of how such models might arise and different (dated) approaches to numerically solve the Riccati equation. LQR for continuous time systems was proposed by Kalman (1960) for deterministic systems and by Wonham (1968) for stochastic systems. The proof idea of completion of squares is due to Åström (1970) but we loosely follow the proof outlines adapted from Afshari and Mahajan (2023).\nExercise 28.3 is modified from the proof idea used in Arabneydi and Mahajan (2016) and Gao and Mahajan (2022).\nThe term certainty equivalence is due to Simon (1956), who was looking at a static problem; a similar result had earlier been shown by Theil (1954). A result which is essentially equivalent to the stochastic LQR problem is proved by Theil (1957).\n\n\n\n\nAfshari, M. and Mahajan, A. 2023. Decentralized linear quadratic systems with major and minor agents and non-gaussian noise. IEEE Transactions on Automatic Control 68, 8, 4666–4681. DOI: 10.1109/tac.2022.3210049.\n\n\nArabneydi, J. and Mahajan, A. 2016. Linear quadratic mean field teams: Optimal and approximately optimal decentralized solutions. Available at: https://arxiv.org/abs/1609.00056v2.\n\n\nÅström, K.J. 1970. Introduction to stochastic control theory. Dover.\n\n\nAthans, M. 1971. The role and use of the stochastic linear-quadratic-gaussian problem in control system design. IEEE Transactions on Automatic Control 16, 6, 529–552. DOI: 10.1109/tac.1971.1099818.\n\n\nDorato, P. and Levis, A. 1971. Optimal linear regulators: The discrete-time case. IEEE Transactions on Automatic Control 16, 6, 613–620. DOI: 10.1109/tac.1971.1099832.\n\n\nGao, S. and Mahajan, A. 2022. Optimal control of network-coupled subsystems: Spectral decomposition and low-dimensional solutions. IEEE Transactions on Control of Network Systems 9, 2, 657–669. DOI: 10.1109/tcns.2021.3124259.\n\n\nKalman, R.E. 1960. Contributions to the theory of optimal control. Boletin de la Sociedad Matematica Mexicana 5, 102–119.\n\n\nLewis, F.L., Vrabie, D., and Syrmos, V.L. 2012. Optimal control. John Wiley & Sons.\n\n\nSimon, H.A. 1956. Dynamic programming under uncertainty with a quadratic criterion function. Econometrica 24, 1, 74–81. DOI: 10.2307/1905261.\n\n\nTheil, H. 1954. Econometric models and welfare maximization. Wirtschaftliches Archiv 72, 60–83. DOI: 10.1007/978-94-011-2410-2_1.\n\n\nTheil, H. 1957. A note on certainty equivalence in dynamic planning. Econometrica, 346–349. DOI: 10.1007/978-94-011-2410-2_3.\n\n\nWittenmark, B., Åström, K.J., and Årzén, K.-E. 2002. Computer control: An overview. In: IFAC professional brief. IFAC. Available at: https://www.ifac-control.org/publications/list-of-professional-briefs/pb_wittenmark_etal_final.pdf.\n\n\nWonham, W.M. 1968. On a matrix riccati equation of stochastic control. SIAM Journal on Control 6, 4, 681–697. DOI: 10.1137/0306044.",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Linear quadratic regulation</span>"
    ]
  },
  {
    "objectID": "linear-systems/large-scale-systems.html",
    "href": "linear-systems/large-scale-systems.html",
    "title": "29  Large scale systems",
    "section": "",
    "text": "29.1 Mean-field control\nConsider a system consisting of \\(N\\) subsystems, indexed by the set \\(\\ALPHABET N \\coloneqq \\{1, \\dots, N\\}\\). Each subsystem \\(i \\in \\ALPHABET N\\) has a state \\(x^i_t \\in \\reals^{n}\\) and a control input \\(u^i_t \\in \\reals^m\\). The dynamics of each subsystem are given as \\[\\begin{equation}\\label{eq:dynamics}\n  x^i_{t+1} = A x^i_t + B u^i_t + D \\bar x_t + E \\bar u_t + w^i_t\n\\end{equation}\\] where \\[\\begin{equation}\n  \\bar x_t \\coloneqq \\frac 1N \\sum_{i \\in \\ALPHABET N} x^i_t\n  \\quad\\text{and}\\quad\n  \\bar u_t \\coloneqq \\frac 1N \\sum_{i \\in \\ALPHABET N} u^i_t\n\\end{equation}\\] are the emperical mean-field of the state and control, respectively and \\(A\\), \\(B\\), \\(D\\), \\(E\\) are matrices of appropriate dimensions. The noise processes \\(\\{w^i_t\\}_{t \\ge 1}\\), \\(i \\in \\ALPHABET N\\) are correlated across subsystem but are assumed to be independent across time.\nWe use \\(\\pmb x_t \\coloneqq (x^1_t, \\dots, x^N_t)\\) and \\(\\pmb u_t \\coloneqq(u^1_t, \\dots, u^N_t)\\) to denote the global state and control of the system. The system incurs a per-step cost given by \\[\\begin{equation}\\label{eq:mf-cost}\n  c(\\pmb x_t, \\pmb u_t) =\n  \\bar x_t^\\TRANS \\bar Q \\bar x_t + \\bar u_t^\\TRANS \\bar R \\bar u_t\n  +\n  \\frac 1N \\sum_{i \\in \\ALPHABET N}\n  \\bigl[\n  (x^i_t)^\\TRANS Q x^i_t + (u^i_t)^\\TRANS R u^i_t\n  \\bigr]\n\\end{equation}\\] and a terminal cost \\[\\begin{equation}\\label{eq:mf-cost-T}\n  c_T(\\pmb x_T) =\n  \\bar x_T^\\TRANS \\bar Q_T \\bar x_T\n  +\n  \\frac 1N \\sum_{i \\in \\ALPHABET N}\n  (x^i_T)^\\TRANS Q_T x^i_T\n\\end{equation}\\]\nWe are interested in identifying policies \\(g = (g_1, \\dots, g_{T-1})\\) where \\(\\pmb u_t = g_t(\\pmb x_t)\\) to minimize \\[\\begin{equation}\\label{eq:performance}\n  J(g) = \\EXP^g\\biggl[\\sum_{t=1}^{T-1} c(\\pmb x_t, \\pmb u_t) + c_T(\\pmb x_T)\n  \\biggr]\n\\end{equation}\\]",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Large scale systems</span>"
    ]
  },
  {
    "objectID": "linear-systems/large-scale-systems.html#mean-field-control",
    "href": "linear-systems/large-scale-systems.html#mean-field-control",
    "title": "29  Large scale systems",
    "section": "",
    "text": "Weakly coupled dynamics and cost\n\n\n\nNote that the subsystems are weakly coupled in the dynamics and cost. A naive solution using by solving a single Riccati equation has a complexity of \\(\\mathcal O(n^3 N^3)\\), which scales cubically in the number of agents.\n\n\n\n29.1.1 State space decomposition\nWe now present a decomposition method to simplify the above optimization problem. Note that \\(\\eqref{eq:dynamics}\\) implies that \\[\\begin{equation}\\label{eq:mf-dynamics}\n  \\bar x_t = (A + D) \\bar x_t + (B + E) \\bar u_t + \\bar w_t\n\\end{equation}\\] where \\(\\bar w_t = (\\sum_{i \\in \\ALPHABET N}w^i_t)/N\\). Define \\[\n  \\breve x^i_t = x^i_t - \\bar x_t\n  \\quad\\text{and}\\quad\n  \\breve u^i_t = u^i_t - \\bar u_t.\n\\] Then, subtracting \\(\\eqref{eq:mf-dynamics}\\) from \\(\\eqref{eq:dynamics}\\), we get \\[\\begin{equation}\\label{eq:breve-dynamics}\n  \\breve x^i_t = A \\breve x^i_t + B \\breve u^i_t + \\breve w^i_t\n\\end{equation}\\] where \\(\\breve w^i_t = w^i_t - \\bar w_t\\).\nWe can think of \\(\\bar x_t\\) as the “center of mass” of the system and \\(\\breve x^i_t\\) to be the relative coordinates of subsystem \\(i\\) wrt the center of mass. Building on this intuition, we make the following simple observation, which may be viewed as an analog of the :parallel axis theorem in physics.\n\nLemma 29.1 We have the following:\n\n\\(\\displaystyle \\frac 1N\\sum_{i \\in \\ALPHABET N} (x^i_t)^\\TRANS Q x^i_t\n=\n\\bar x_t^\\TRANS Q \\bar x_t\n+\n\\frac 1N \\sum_{i \\in \\ALPHABET N} (\\breve x^i_t)^\\TRANS Q \\breve x^i_t\\).\n\\(\\displaystyle \\frac 1N\\sum_{i \\in \\ALPHABET N} (u^i_t)^\\TRANS Q u^i_t\n=\n\\bar u_t^\\TRANS Q \\bar u_t\n+\n\\frac 1N \\sum_{i \\in \\ALPHABET N} (\\breve u^i_t)^\\TRANS Q \\breve u^i_t\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof follows from the observation that \\(\\sum_{i \\in \\ALPHABET N} \\breve x^i_t = 0\\) and elementary algebra.\n\n\n\nAn immediate implication of Lemma 29.1 is that the per-step cost can be decomposed as follows: \\[\\begin{equation}\n  c(\\pmb x_t, \\pmb u_t) = \\bar c(\\bar x_t, \\bar u_t) +\n  \\frac 1N \\sum_{i \\in \\ALPHABET N} \\breve c^i(\\breve x^i_t, \\breve u^i_t)\n\\end{equation}\\] where \\[\\begin{align*}\n  \\bar c(\\bar x_t, \\bar u_t) &=\n  \\bar x_t^\\TRANS (Q + \\bar Q) \\bar x_t + \\bar u_t^\\TRANS (R + \\bar R) \\bar u_t,\n  \\notag \\\\\n  \\breve c^i(\\breve x^i_t, \\breve u^i_t) &=\n   \\frac 1N \\sum_{i \\in \\ALPHABET N}\n   \\bigl[\n    (\\breve x^i_t)^\\TRANS Q \\breve x^i_t\n    +\n    (\\breve u^i_t)^\\TRANS Q \\breve u^i_t\n  \\bigr].\n\\end{align*}\\] A similar decomposition also holds for the terminal cost \\(c_t(\\pmb x_t, \\pmb u_t)\\).\nThus, the original system is equivalent to \\(N+1\\) coupled subsystems:\n\na mean-field subsystem with state \\(\\bar x_t\\), control input \\(\\bar u_t\\), and per-step cost \\(\\bar c(\\bar x_t, \\bar u_t)\\).\n\\(N\\) auxiliary subsytems, where subsystem \\(i \\in \\ALPHABET N\\) has state \\(\\breve x^i_t\\), control input \\(\\breve u^i_t\\), and per-step cost \\(\\breve c(\\breve x^i_t, \\breve u^i_t)\\).\n\nNote that the only coupling between the subsystems is through the noise. Therefore, by the argument presented in Exercise 28.3, the optimal control strategy is of the following form.\n\nProposition 29.1 The optimal control policy for the mean-field control system described above is given by \\[\n    u_t = - \\bar K_t \\bar x_t + \\breve K_t (x^i_t - \\bar x_t)\n  \\] where\n\n\\(\\bar K_{1:T-1} = \\LQR_T(A+D, B+E, Q + \\bar Q, R + \\bar R; Q_T + \\bar Q_T)\\)\n\\(\\breve K_{1:T-1} = \\LQR_T(A,B, Q, R; Q_T)\\).\n\n\n\n\n\n\n\n\nSignificance of the result\n\n\n\nThe above result is significant, both for synthesis and implementation.\nFor synthesis, rather than solving one Riccati equation with state dimension \\(nN\\), we need to solve two Riccati equations with dimension \\(n\\). Thus, the complexity of computing the optimal controller gains does not depend on the number \\(N\\) of subsystems.\nFor implementation, each subsystem does not need access to the global state \\(\\pmb x_t\\); instead it just needs access to the mean-field \\(\\bar x_t\\) in addition to its local state \\(x^i_t\\).",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Large scale systems</span>"
    ]
  },
  {
    "objectID": "linear-systems/large-scale-systems.html#network-coupled-subsystems",
    "href": "linear-systems/large-scale-systems.html#network-coupled-subsystems",
    "title": "29  Large scale systems",
    "section": "29.2 Network coupled subsystems",
    "text": "29.2 Network coupled subsystems",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Large scale systems</span>"
    ]
  },
  {
    "objectID": "linear-systems/large-scale-systems.html#notes",
    "href": "linear-systems/large-scale-systems.html#notes",
    "title": "29  Large scale systems",
    "section": "Notes",
    "text": "Notes\nThe results for mean-field control are adapted from Arabneydi and Mahajan (2016). The discussion above is restricted to the simplest setting of homogenous subsystems. Generalization to hetrogeneous subsystems and infinite horizon settings are also presented in Arabneydi and Mahajan (2016). A similar result for \\(N \\to \\infty\\) is also presented in Elliott et al. (2013).\nThe results for network coupled subsystems are adapted from Gao and Mahajan (2022).\n\n\n\n\nArabneydi, J. and Mahajan, A. 2016. Linear quadratic mean field teams: Optimal and approximately optimal decentralized solutions. Available at: https://arxiv.org/abs/1609.00056v2.\n\n\nElliott, R., Li, X., and Ni, Y.-H. 2013. Discrete time mean-field stochastic linear-quadratic optimal control problems. Automatica 49, 11, 3222–3233. DOI: 10.1016/j.automatica.2013.08.017.\n\n\nGao, S. and Mahajan, A. 2022. Optimal control of network-coupled subsystems: Spectral decomposition and low-dimensional solutions. IEEE Transactions on Control of Network Systems 9, 2, 657–669. DOI: 10.1109/tcns.2021.3124259.",
    "crumbs": [
      "Linear systems",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Large scale systems</span>"
    ]
  },
  {
    "objectID": "rl/stochastic-approximation.html",
    "href": "rl/stochastic-approximation.html",
    "title": "30  Stochastic approximation",
    "section": "",
    "text": "30.1 List of assumptions\nLet \\(\\mathcal F_t = σ(θ_{1:t}, ξ_{1:t})\\). We state the following set of assumptions but note that not every assumption is needed for every result.",
    "crumbs": [
      "RL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic approximation</span>"
    ]
  },
  {
    "objectID": "rl/stochastic-approximation.html#list-of-assumptions",
    "href": "rl/stochastic-approximation.html#list-of-assumptions",
    "title": "30  Stochastic approximation",
    "section": "",
    "text": "30.1.1 Assumptions on the function \\(f\\)\nF1. \\(θ^*\\) is a solution of the equation \\(f(θ) = 0\\).\nF1’. \\(θ^*\\) is the unique solution of the equation \\(f(θ) = 0\\).\nF2. The function \\(f\\) is globally Lipschitz-continuous with constant \\(L\\), i.e., for any \\(θ_1, θ_2 \\in \\reals^d\\), \\[\n  \\| f(θ_1) - f(θ_2) \\|_{2} \\le L \\| θ_1 - θ_2 \\|_2.\n\\] F2’. The function \\(f\\) is twice differentiable and is globally Lipschitz continuous with constant \\(L\\).\n\n\n\n\n\n\nImplication\n\n\n\nAssumption (F2) implies that for each \\(θ \\in \\reals^d\\), there is a unique function \\(s(\\cdot, θ)\\) that satisfies the ODE \\[\n  \\frac{ds(t,θ)}{dt} = f(s(t,θ)), \\quad\n  s(0,θ) = θ.\n\\]\n\n\nF3. The equilibrium \\(θ^*\\) of the ODE \\(\\dot θ = f(θ)\\) is globally asymptotically stable.\nF3’. The equilibrium \\(θ^*\\) of the ODE \\(\\dot θ = f(θ)\\) is globally exponentially stable. Thus, there exists constants \\(μ \\ge 1\\) and \\(γ &gt; 0\\) such that \\[\n\\| s(t,θ) - θ^*\\|_2 \\le μ\\|θ - θ^*\\|_2 \\exp(-γ t),\n\\quad\n\\forall t \\ge 0, \\forall θ \\in \\reals^d.\n\\]\nF4. There is a finite constant \\(K\\) such that \\[\n\\| \\nabla^2 f_i(θ) \\|_{S} \\cdot \\| θ - θ^*\\|_2 \\le K,\n\\quad\n\\forall i \\in \\{1, \\dots, d\\},\n\\forall θ \\in \\reals^d,\n\\] where \\(\\|\\cdot\\|_S\\) denotes the spectral norm of a matrix (i.e., the largest singular value).\n\n\n\n\n\n\nImplication\n\n\n\nAssumption (F4) implies that \\[\n\\left| \\frac{∂^2 f_i(θ)}{∂θ_j ∂θ_k}\\right| \\cdot \\| θ - θ^*\\|_2 \\le K,\n\\quad\n\\forall i.j,k, \\in \\{1,\\dots, d\\},\n\\forall θ \\in \\reals^d.\n\\]\n\n\n\n\n30.1.2 Conditions on the noise\nN1. \\(\\{ξ_t\\}_{t \\ge 0}\\) is a martingale difference sequence with respect to \\(\\{ \\mathcal F_t\\}_{t \\ge 1}\\), i.e., \\[ \\EXP[ ξ_{t+1} | \\mathcal F_t ] = 0, \\text{ a.s.}, \\quad \\forall t \\ge 1. \\]\nN2. The noise \\(\\{ξ_t\\}_{t \\ge 1}\\) satisfies \\[\n\\EXP[ \\| ξ_{t+1}^2 \\|_2^2 \\mid \\mathcal F_t ] \\le\nσ^2( 1 + \\| θ_t - θ^*\\|_{2}^2),\n\\quad \\text{a.s. } \\forall t \\ge 1\n\\] for some finite constant \\(σ^2\\).\n\n\n30.1.3 Conditions on the learning rate\nR1. \\(\\sum_{t \\ge 1} α_t^2 &lt; ∞\\).\nR2. \\(\\sum_{t \\ge 1} α_t = ∞\\).\nR3. There exists constants \\(\\underline α, \\bar α \\in (0,1)\\) such that \\(\\underline α \\le α_t \\le \\bar α\\) for all \\(t \\ge 1\\).",
    "crumbs": [
      "RL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic approximation</span>"
    ]
  },
  {
    "objectID": "rl/stochastic-approximation.html#gladyshevs-result",
    "href": "rl/stochastic-approximation.html#gladyshevs-result",
    "title": "30  Stochastic approximation",
    "section": "30.2 Gladyshev’s result",
    "text": "30.2 Gladyshev’s result\nThe following is a restatement of the result of Gladyshev (1965).\n\nTheorem 30.1 Suppose assumptions (F1’), (N1), and (N2) hold. In addition, the function \\(f(\\cdot)\\) is passive, i.e., for each \\(0 &lt; ε &lt; M &lt; ∞\\), \\[ \\sup_{ε &lt; \\| θ - θ^*\\|_2 &lt; M}\n  \\langle θ - θ^*, f(θ) \\rangle\n&lt; 0\\] and \\[\\|f(θ)\\|_2 \\le K \\|θ - θ^*\\|_2, \\quad K &lt; ∞.\\] Then,\n\nIf (R1) holds, then \\(\\{θ_t\\}\\) is bounded almost surely.\nIn addition, if (R2) holds, then \\(θ_t \\to θ^*\\) almost surely as \\(t \\to ∞\\).\n\n\n\n\n\n\n\n\nRemark\n\n\n\nThe second assumption \\(\\|f(θ)\\|_2 \\le K \\| θ - θ^*\\|_2\\) implies that \\(f(⋅)\\) is continuous as \\(θ^*\\), but it need not be continuous anywhere else.",
    "crumbs": [
      "RL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic approximation</span>"
    ]
  },
  {
    "objectID": "rl/stochastic-approximation.html#sec-borkar-meyn",
    "href": "rl/stochastic-approximation.html#sec-borkar-meyn",
    "title": "30  Stochastic approximation",
    "section": "30.3 Borkar-Meyn’s result",
    "text": "30.3 Borkar-Meyn’s result\nThe following is a restatement of the result of Borkar and Meyn (2000).\n\nTheorem 30.2 Suppose assumptions (F2), (N1), and (N2) hold. In addition:\n\nThere exists a limit function \\(f_{∞} \\colon \\reals^d \\to \\reals^d\\) such that \\[\n  \\lim_{r \\to ∞} \\frac{f(r θ)}{r} = f_{∞}(θ), \\quad\n  \\forall θ \\in \\reals^d.\n\\]\nOrigin is asymptotically stable equilibrium of the ODE \\[\n  \\dot θ(t) = f_{∞}(θ(t)).\n\\]\n\nThen,\n\nIf (R1) and (R2) hold, then \\(\\{θ_t\\}_{t \\ge 1}\\) is bounded almost surely.\nIn addition, if (F3) holds, then \\(θ_t \\to θ^*\\) almost surely as \\(t \\to ∞\\).\nIf (F3) and (R3) hold, then:\n\nthere exists a \\(α^* &gt; 0\\) and \\(C_1 &lt; ∞\\) such that if \\(\\bar α \\le α^*\\) then \\[\n   \\limsup_{n \\to ∞} \\EXP[ \\| θ_k\\|^2 ] \\le C_1.\n\\]\nif \\(\\bar α \\le α^*\\) then \\(θ_t \\to θ^*\\) in probability. In particular, for any \\(ε &gt; 0\\), there exists a \\(b_1 = b_1(ε) &lt; ∞\\) such that \\[\n   \\limsup_{n \\to ∞} \\PR( \\| θ_k - θ^* \\| \\ge ε ) \\le b_1 \\bar α.\n\\]\nIn addition, if (F3’) holds, then \\(θ_t \\to θ^*\\) in mean square. In particular, there exists a \\(b_2 &lt; ∞\\) such that for any initial condition \\(θ_0 \\in \\reals^d\\), \\[\n   \\limsup_{n \\to ∞} \\EXP[ \\| θ_k - θ^*\\|^2 ] \\le b_2 \\bar α.\n\\]\n\n\n\n\n\n\n\n\n\nRates of convergence\n\n\n\nBorkar and Meyn (2000) also provides rates of convergence of stochastic approximation under stronger assumptions.",
    "crumbs": [
      "RL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic approximation</span>"
    ]
  },
  {
    "objectID": "rl/stochastic-approximation.html#vidyasagars-result",
    "href": "rl/stochastic-approximation.html#vidyasagars-result",
    "title": "30  Stochastic approximation",
    "section": "30.4 Vidyasagar’s result",
    "text": "30.4 Vidyasagar’s result\nThe following is a restatement of the result of Vidyasagar (2023).\n\n\n\n\n\n\nFunction classes\n\n\n\nConsider a continuous function \\(f \\colon \\reals_{\\ge 0} \\to \\reals_{\\ge 0}\\).\n\nThe function \\(f\\) is said to belong to class \\(\\mathcal K\\) if \\(f(0) = 0\\) and \\(f(\\cdot)\\) is strictly increasing.\nThe function \\(f \\in \\ALPHABET K\\) is said to belong to class \\(\\ALPHABET K \\ALPHABET R\\) if, in addition, \\(f(r) \\to ∞\\) as \\(s \\to ∞\\).\nThe function \\(f\\) is said to belong to class \\(\\ALPHABET B\\) if \\(f(0) = 0\\) and, in addition for all \\(0 &lt; ε &lt; M &lt; ∞\\), we have \\[\n\\inf_{ε \\le r \\le M} f(r) &gt; 0.\n\\]\n\n\n\n\nExample 30.2 Observe that every function \\(f\\) of class \\(\\ALPHABET K\\) also belongs to class \\(\\ALPHABET B\\) but the converse is not true. For example, let \\[\n  f(r) = \\begin{cases}\n  r, & \\text{if } r \\in [0,1], \\\\\n  e^{-(r-1)}, & \\text{if } r &gt; 1.\n  \\end{cases}\n\\] Then, \\(f\\) belongs to class \\(\\ALPHABET B\\).\nHowever, since \\(f(r) \\to 0\\) as \\(r \\to ∞\\), \\(f\\) cannot be bounded below by any function of class \\(\\ALPHABET K\\).\n\n\nTheorem 30.3 Suppose assumptions (F1), (F2), (N1), and (N2) hold. In addition, suppose that there exists a twice differentiable Lyapunov function \\(V \\colon \\reals^d \\to \\reals_{\\ge 0}\\) that satisfies the following conditions:\n\nThere exist constants \\(a, b &gt; 0\\) such that \\[\\begin{equation}\\label{eq:vidyasagar-cond-1}\n  a \\| θ - θ^*\\|_2^2 \\le V(θ) \\le b \\| θ - θ^* \\|_2^2,\n  \\quad \\forall θ \\in \\reals^d.\n\\end{equation}\\]\nThere is a finite constant \\(M\\) such that \\[\\begin{equation}\\label{eq:vidyasagar-cond-2}\n\\| \\GRAD^2 V(θ) \\|_S \\le 2M,\n  \\quad \\forall θ \\in \\reals^d.\n\\end{equation}\\] Then,\n\n\nIf \\(\\dot V(θ) \\coloneqq \\langle \\GRAD V(θ), f(θ) \\rangle \\le 0\\) for all \\(θ \\in \\reals^d\\) and (R1) holds, then the iterates \\(\\{θ_t\\}_{t \\ge 1}\\) are bounded almost surely.\nIf, in addition, (R2) holds and there exists a function \\(\\phi \\in \\ALPHABET B\\) such that \\[\n\\dot V(θ) \\le - \\phi(\\| θ - θ^*\\|_2),\n\\quad \\forall θ \\in \\reals^d.\n\\] Then, \\(θ_t \\to θ^*\\) almost surely as \\(t \\to ∞\\).\n\n\n\n\n\n\n\n\nDiscussion of the conditions\n\n\n\nIt is worthwhile to compare the conditions of Theorem 30.2 and Theorem 30.3.\n\nIn Theorem 30.2, it is assumed that (F1’) holds while in Theorem 30.3, it is assumed that (F1) holds. That is, there is no assumption that \\(θ^*\\) is the unique solution of \\(f(θ) = 0\\).\nThe assumptions on \\(\\dot V\\) in part 1 of Theorem 30.3 imply only that \\(θ^*\\) is a locally stable equilibrim of the ODE \\eqref{eq:ODE}. This is in contrast to Theorem 30.2 imply that \\(θ^*\\) is globally asymptotically stable.\nThe assumptions in part 2 of Theorem 30.4 ensure that \\(θ^*\\) is globally asymptotically stable equilibrium of the ODE \\eqref{eq:ODE}. Therefore, assumption (F1’) is implicit in the second part of Theorem 30.3.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe first start by establishing a bound on \\(\\EXP[V(θ_{t+1}) \\mid \\ALPHABET  F_t]\\). To do so, observe that by Taylor series, we have \\[\n  V(θ + η) = V(θ) + \\langle \\GRAD V(θ), η \\rangle\n  + \\frac 12 \\langle η, \\GRAD^2 V(θ + λη)η \\rangle\n\\] for some \\(λ \\in [0,1]\\). Since \\(\\NORM{\\GRAD^2 V(θ+λη)}_S \\le 2M\\), it follows that \\[\n  V(θ + η) \\le V(θ) + \\langle \\GRAD V(θ), η \\rangle\n  + M \\NORM{η}_2^2.\n\\] Now apply the above bound with \\(θ = θ_t\\) and \\(η = θ_{t+1} - θ_t = α_t f(θ_t) + α_t ξ_{t+1}\\). This gives \\[\\begin{align*}\nV(θ_{t+1}) &\\le V(θ_t)\n+ α_t \\langle \\GRAD V(θ_t), f(θ_t) \\rangle\n+ α_t \\langle \\GRAD V(θ_t), ξ_{t+1} \\rangle\n  \\notag \\\\\n&\\quad  + α_t^2 M \\bigl[ \\NORM{f(θ_t)}_2^2 + \\NORM{ξ_{t+1}}_2^2 +\n2 \\langle f(θ_t), ξ_{t+1} \\rangle\n\\bigr]\n\\end{align*}\\] Recall that \\(\\langle V(θ), f(θ) \\rangle \\eqqcolon \\dot V(θ)\\). Now, we can bound \\(\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t]\\) using assumptions (N1) and (N2). \\[\\begin{equation*}\n\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t] \\le V(θ_t)\n+ α_t \\dot V(θ_t)\n+ α_t^2 M \\bigl[ \\NORM{f(θ_t)}_2^2 + σ^2(1 + \\NORM{θ_t - θ^*}_2^2)\n\\bigr]\n\\end{equation*}\\] Assumption (F1) and (F2) implies that \\[\n\\NORM{f(θ_t)}_2^2 = \\NORM{f(θ_t) - f(θ^*)}_2^2 \\le L^2 \\NORM{θ_t - θ^*}_2^2.\n\\] Substituting in the above bound, we get: \\[\\begin{equation}\\label{eq:vidyasagar-1-pf-step-1}\n\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t] \\le V(θ_t)\n+ α_t \\dot V(θ_t)\n+ α_t^2 M \\bigl[ σ^2 + (σ^2 + L^2)\\NORM{θ_t - θ^*}_2^2 \\bigr].\n\\end{equation}\\]\n\nProof of part 1.\nUnder the stated assumptions, we can simplify \\eqref{eq:vidyasagar-1-pf-step-1} \\[\\begin{align*}\n\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t]\n&\\stackrel{(a)}\\le\nV(θ_t)\n+ α_t^2 M \\bigl[ σ^2 + (σ^2 + L^2)\\NORM{θ_t - θ^*}_2^2 \\bigr]\n  \\notag \\\\\n&\\stackrel{(b)}\\le\n\\biggl[ 1 + \\frac{α_t^2 M}{a} (L^2 + σ^2) \\biggr] V(θ_t)\n+\n  α_t^2 M σ^2\n\\end{align*}\\] where \\((a)\\) follows from the assumption that \\(\\dot V(θ) &lt; 0\\) and \\((b)\\) follows from \\(V(θ) \\ge a \\NORM{θ - θ^*}_2^2\\).\nThus, \\(\\{V(θ_t)\\}_{t \\ge 1}\\) is an “almost” supermartingale. Apply Theorem 37.2 with \\(X_t = V(θ_t)\\), \\(β_t = α_t^2 M(L^2 + σ^2)/a\\), \\(Y_t = α_t^2 M σ^2\\), and \\(Z_t = 0\\). Then, from (R1) it follows that \\(\\lim_{t \\to ∞} V(θ_t)\\) exists almost surely and is finite. Condition \\eqref{eq:vidyasagar-cond-1} implies that \\(\\{θ_t\\}_{t \\ge 1}\\) is almost surely bounded.\n\n\nProof of part 2.\nUnder the stated assumptions, we can simplify \\eqref{eq:vidyasagar-1-pf-step-1} \\[\\begin{align*}\n\\EXP[V(θ_{t+1}) \\mid \\ALPHABET F_t]\n&\\stackrel{(c)}\\le\n\\biggl[ 1 + \\frac{α_t^2 M}{a} (L^2 + σ^2) \\biggr] V(θ_t)\n+\n  α_t^2 M σ^2\n- α_t \\phi(\\NORM{θ_t - θ^*}_2)\n\\end{align*}\\] where the first two terms are simplified in the same way as above and the last term corresponds to the upper bound on \\(\\dot V(θ_t) \\le -\\phi(\\NORM{θ_t - θ^*}_2^2)\\).\nWe can again apply Theorem 37.2 with \\(X_t = V(θ_t)\\), \\(β_t = α_t^2 M(L^2 + σ^2)/a\\), \\(Y_t = α_t^2 M σ^2\\), and \\(Z_t = α_t \\phi(\\NORM{θ_t - θ^*}_2)\\). Thus, we can conclude that there exists a random variable \\(ζ\\) such that \\(V(θ_t) \\to ζ\\) and \\[\\begin{equation}\\label{eq:vidyasagar-1-pf-step-2}\n\\sum_{t \\ge 1} α_t \\phi(\\NORM{θ_t - θ^*}_2^2) &lt; ∞,\n\\quad \\mathrm{a.s.}\n\\end{equation}\\]\nLet \\(Ω_1 \\subset Ω\\) denote the values of \\(ω\\) for which \\[\n  \\sup_{t \\ge 1} V(θ_t(ω)) &lt; ∞,\n  \\lim_{t \\to ∞} V(θ_t(ω)) = ζ(ω),\n  \\sum_{t \\ge 1} α_t \\phi(\\NORM{θ_t(ω) - θ^*}_2) &lt; ∞.\n\\] From Theorem 37.2, we know that \\(P(Ω_1) = 1\\). We will now show that \\(ζ(ω) = 0\\) for all \\(ω \\in Ω_1\\) by contradiction. Assume that for some \\(ω \\in Ω_1\\), we have \\(ζ(ω) = 2 ε &gt; 0\\). Choose a \\(T\\) such that \\(V(θ_t(ω)) \\ge ε\\) for all \\(t \\ge T\\). Define \\(V_M = \\sup_{t \\ge 1} V(θ_t(ω))\\). Then, we have that \\[\n  \\sqrt{\\frac{ε}{b}} \\le \\NORM{θ_t}_2 \\le \\sqrt{\\frac{V_M}{a}},\n  \\quad \\forall t \\ge T.\n\\]\nDefine \\(δ = \\inf_{\\sqrt{ε/b} \\le r \\le \\sqrt{V_M/a}} \\phi(r)\\) and observe that \\(δ &gt; 0\\) because \\(\\phi\\) belongs to class \\(B\\). Therefore, \\[\n\\sum_{t \\ge T} α_t \\phi(\\NORM{θ_t - θ^*}_2) \\ge\n\\sum_{t \\ge T} α_t δ = ∞,\n\\] due to (R2). But this contradicts \\eqref{eq:vidyasagar-1-pf-step-2}. Hence, there is no \\(ω \\in Ω_1\\) such that \\(ζ(ω) &gt; 0\\). Therefore, \\(ζ = 0\\) almost surely, i.e., \\(V(θ_t) \\to 0\\) almost surely. Finally, it follows from \\eqref{eq:vidyasagar-cond-1} that \\(θ_t \\to θ^*\\) almost surely as \\(t \\to ∞\\).\n\n\n\n\nTheorem 30.3 requires the existence of a suitable Lyapunov function that satisfies various conditions. Verifying whether or not such a function exists can be a bottleneck.\nIf can be shown (see Theorem 4 of Vidyasagar (2023)) that the conditions on \\(V\\) in Theorem 30.3 ensure that the equilibrium \\(θ^*\\) of the ODE \\eqref{eq:ODE} is globally asymptotically stable. By strengthening this assumption to global exponential stability of \\(θ^*\\) and adding a few other conditions, it is possible to establish a “converse” Lyapunov theorem that establishes the existence of such a \\(V\\). This is done below.\n\nTheorem 30.4 Suppose assumptions (F1’), (F2’), (F3) and (F4) hold. Then, there exists a twice differentiable function \\(V \\colon \\reals^d \\to \\reals_{\\ge 0}\\) such that \\(V\\) and its derivative \\(\\dot V \\colon \\reals^d \\to \\reals_{\\ge 0}\\) defined as \\(\\dot V(θ) \\coloneqq \\langle \\langle \\GRAD V(θ), f(θ) \\rangle\\) together satisfy the following conditions: there exist positive constants \\(a\\), \\(b\\), \\(c\\), and a finite constant \\(M\\) such that for all \\(θ \\in \\reals^d\\):\n\n\\(a\\NORM{θ - θ^*}_2^2 \\le V(θ) \\le b\\NORM{θ - θ^*}_2^2\\),\n\\(\\dot V(θ) \\le -c\\NORM{θ - θ^*}_2^2\\),\n\\(\\NORM{\\GRAD^2 V(θ)}_S \\le 2M\\).\n\n\nCombining Theorem 30.3 and Theorem 30.4, we get the following “self-contained” theorem:\n\nTheorem 30.5 Suppose assumptions (F1’), (F2’), (F3), and (F4) as well as assumptions (N1) and (N2) hold. Then,\n\nIf (R1) holds then \\(\\{θ_t\\}_{t \\ge 1}\\) is bounded almost surely.\nIf, in addition, (R2) holds then \\(\\{θ_t\\}_{t \\ge 1}\\) converges almost surely to \\(θ^*\\) as \\(t \\to ∞\\).",
    "crumbs": [
      "RL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic approximation</span>"
    ]
  },
  {
    "objectID": "rl/stochastic-approximation.html#notes",
    "href": "rl/stochastic-approximation.html#notes",
    "title": "30  Stochastic approximation",
    "section": "Notes",
    "text": "Notes\nThe stochastic approximation algorithm was introduced by Robbins and Monro (1951). See Lai (2003) for a historical overview. The classical references on this material is Borkar (2008), Chen and Guo (1991), Kushner and Yin (1997).\nExample 30.1 is borrowed from Borkar (2008), who points out that it was proposed by Arthur (1994) to model the phenomenon of decreasing returns in economics.\nThe material in this section is adapted from Vidyasagar (2023).\n\n\n\n\nArthur, W.B. 1994. Increasing returns and path dependence in the economy. University of Michigan Press. DOI: 10.3998/mpub.10029.\n\n\nBorkar, V.S. 2008. Stochastic approximation. Hindustan Book Agency. DOI: 10.1007/978-93-86279-38-5.\n\n\nBorkar, V.S. and Meyn, S.P. 2000. The o.d.e. Method for convergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization 38, 2, 447–469. DOI: 10.1137/s0363012997331639.\n\n\nChen, H.-F. and Guo, L. 1991. Identification and stochastic adaptive control. Birkhäuser Boston. DOI: 10.1007/978-1-4612-0429-9.\n\n\nGladyshev, E.G. 1965. On stochastic approximation. Theory of Probability and Its Applications 10, 2, 275–278. DOI: 10.1137/1110031.\n\n\nKushner, H.J. and Yin, G.G. 1997. Stochastic approximation algorithms and applications. Springer New York. DOI: 10.1007/978-1-4899-2696-8.\n\n\nLai, T.L. 2003. Stochastic approximation: Invited paper. The Annals of Statistics 31, 2. DOI: 10.1214/aos/1051027873.\n\n\nRobbins, H. and Monro, S. 1951. A stochastic approximation method. The Annals of Mathematical Statistics 22, 3, 400–407. DOI: 10.1214/aoms/1177729586.\n\n\nVidyasagar, M. 2023. Convergence of stochastic approximation via martingale and converse Lyapunov methods. Mathematics of Control, Signals, and Systems 35, 2, 351–374. DOI: 10.1007/s00498-023-00342-9.",
    "crumbs": [
      "RL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic approximation</span>"
    ]
  },
  {
    "objectID": "dec-pomdps/designers-approach.html",
    "href": "dec-pomdps/designers-approach.html",
    "title": "31  Designer’s Approach",
    "section": "",
    "text": "31.1 The designer’s approach\nThe standard “dynamic-progarmming viewpoint” is to evaluate the value function (or the cost-to-go function) from the point of view of the agent at each time. Instead, consider the problem from the point of view of the system designer before the system starts operating. The system designer knows the system model and the statistics of the primitive random variables but does not know the observation of the agent. The designer is concerned with the optimal decision rule for the agent before the system starts operating.\nThe designer may view the system as a stochastic input-output system: the stochastic inputs are the primitive random variables, the control inputs are the decision rules, and the output is the instantenous cost. The input-output relationship can be described consistently by \\((S_t, Y_t)\\), which represents the state of the environment and the state of the agent.1 However, this state cannot be used for optimization because it is not observed by the system designer. So, the optimization problem at the system designer is conceptually equivalent to a POMDP.\nHence the designer can obtain a sequential decomposition by forming a belief on the state (sufficient for input-output mapping) of the system, based on all the history of observations and actions available to it (i.e., all the past decision rules, since the designer doesn’t observe anything else). This belief can be described by \\[\n   b_t = \\PR(S_t, Y_t \\mid π_{1:t-1}),\n\\] which is the “conditional probabilty law” of the “state” conditioned on all the past observations and “control actions” of the designer. Technically, \\(b_t\\) is not a conditional probability law, rather it is unconditional probability law; but this fact is a techinicality which does not affect the solution methodology.\nThe belief state sastisfies the usual properties of an information state.\nThe decision rule \\(ψ_t \\colon b_t \\mapsto π_t\\) followed by the designer is called a meta-policy (to distinguish it from the policy of the agent). The optimal meta-policy \\(ψ = (ψ_1, \\dots, ψ_T)\\) can be obtained by solving the following dynamic program.",
    "crumbs": [
      "Dec-POMDPs",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Designer's Approach</span>"
    ]
  },
  {
    "objectID": "dec-pomdps/designers-approach.html#the-designers-approach",
    "href": "dec-pomdps/designers-approach.html#the-designers-approach",
    "title": "31  Designer’s Approach",
    "section": "",
    "text": "1 In this particular example, we could have worked with just \\(S_t\\) as the state sufficient for input-output mapping, but we carry the “state of the agent” so that the same approach continues to work for more general models.\n\n\nLemma 31.1 The belief state satisfies the following properties.\n\nSufficient for predicting itself. There exists a linear transformation \\(H_t(π_t)\\) such that \\[\n   b_{t+1} = H_t(π_t) b_t.\n\\]\nSufficient for performance evaluation. There exists functions \\(\\bar c_t \\colon [ \\ALPHABET S \\to \\ALPHABET A] \\to \\reals\\) such that the expected per-step cost can be expressed as \\[\n\\EXP[ c_t(S_t, A_t) \\mid π_{1:t} ] =\n\\bar c_t(b_t, π_t).\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the “belief” is an unconditional probability, it evolves similar to the probability distribution of a Markov chain. In particular, :\\[\\begin{align*}\n  b_{t+1}(s_{t+1}, y_{t+1}) &= \\PR(s_{t+1}, y_{t+1} \\mid π_{1:t}) \\\\\n  &= \\PR(y_{t+1} \\mid s_{t+1})\n  \\sum_{\\substack{s_t \\in \\ALPHABET S \\\\ y_t \\in \\ALPHABET Y}}\n  \\PR(y_t \\mid s_t) \\PR(s_{t+1} \\mid S_t = s_t, A_t = π_t(y_t))\n  \\PR(s_t \\mid π_{1:t-1})\n  \\\\\n  &\\eqqcolon \\bigl[ H_t(π_t) b_t \\bigr](s_{t+1}, y_{t+1}).\n\\end{align*}\\] This proves the first part of the result. Note that the transformation \\(H_t(π_t)\\) is linear in the sense that for any belief \\(b_t\\) given as a linear combination of two beliefs \\(b^{(1)}_t\\) and \\(b^{(2)}_t\\), i.e., \\[\n  b_t = λ b^{(1)}_t + (1 - λ)b^{(2)}_t\n\\] we have \\[\n  H_t(π_t) b_t = λ H_t(π_t) b^{(1)}_t + (1-λ) H_t(π_t) b^{(2)}_t.\n\\]\nFor the second part, note that \\[\\begin{align*}\n\\EXP[ c_t(S_t, A_t) \\mid π_{1:t} ]\n&=\n\\sum_{s_t \\in \\ALPHABET S} c_t(s_t, π_t(s_t))\n\\PR(s_t \\mid π_{1:t-1}) \\\\\n&\\eqqcolon \\bar c_t(b_t, π_t).\n\\end{align*}\\] This proves the second part of the result.\n\n\n\n\n\n\n\n\n\n\nDynamic program\n\n\n\nInitialize \\(V_{T+1}(b) ≡ 0\\). Then, for all \\(t \\in \\{T, \\dots, 1\\}\\), recursively define: \\[\\begin{align*}\n  Q_t(b,π) &= \\bar c_t(b, π) + V_{t+1}( H_t(π) b ) \\\\\n  V_t(b)   &= \\min_{π \\in Π} Q_t(b,π)\n\\end{align*}\\]\nLet \\(ψ_t(b)\\) denote the arg min of \\(Q_t(b,π)\\). Then, the meta-strategy \\(ψ = (ψ_1, \\dots, ψ_T)\\) is optimal.\n\n\n\n\n\n\n\n\nSome properties of the solution\n\n\n\n\nThe set \\(Π\\) above may be restricted to all deterministic maps from \\(\\ALPHABET S\\) to \\(\\ALPHABET A\\) without any loss of optimality.\nAs for POMDPs, we can show that the value function \\(V_t\\) is piecewise linear and convex.\n\n\n\n\n\n\n\n\n\nHow to determine the policy?\n\n\n\nThe dynamic program above determines a meta-policy \\(ψ\\). The corresponding policy \\(π = (π_1, \\dots, π_T)\\) may be determined as follows.\n\nStart with the initial belief \\(b_1\\) (which is the same as the initial distribution of the state \\(S_1\\)). Choose \\(π_1 = ψ_1(b_1)\\).\nLet \\(b_2 = H_1(π_1)b_1\\). Choose \\(π_2 = ψ_2(b_2)\\).\nLet \\(b_3 = H_2(π_2)b_2\\). Choose \\(π_3 = ψ_3(b_3)\\).\n…\n\n\n\n\n\n\n\n\n\nOptimal policies for infinite horizon are not time-homogeneous\n\n\n\nAlthough we presented the result for the finite-horizon setting, the argument naturally extends to the infinite horizon setting as well. In particular, consider the infinite horizon discounted cost setting. Then, the optimal meta-policy is given by the unique bounded fixed point of the following equation: \\[\n    V(b) = \\min_{π \\in Π} \\bigl\\{ \\bar c(b, π) + γ V(H(π)b) \\bigr\\},\n    \\quad \\forall b \\in Δ(\\ALPHABET S × \\ALPHABET Y).\n   \\] This implies that the meta-policy \\(ψ\\) is time-homogeneous but the policy \\(π = (π_1, π_2, \\dots)\\) (chosen according to the procedure described above) is not time-homogeneous.",
    "crumbs": [
      "Dec-POMDPs",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Designer's Approach</span>"
    ]
  },
  {
    "objectID": "dec-pomdps/designers-approach.html#the-general-idea",
    "href": "dec-pomdps/designers-approach.html#the-general-idea",
    "title": "31  Designer’s Approach",
    "section": "31.2 The general idea",
    "text": "31.2 The general idea\nNow we generalize the discussion above to a general multi-agent control problem. Suppose that the system has a state \\(S_t \\in \\ALPHABET S\\). There are \\(N\\) controllers, indexed by the set \\(\\ALPHABET N \\coloneqq \\{1, \\dots, N\\}\\). Let \\(Y^n_t \\in \\ALPHABET Y\\) and \\(A^n_t \\in \\ALPHABET A^n\\) denote the observation and action, respectively, of agent \\(n \\in \\ALPHABET N\\) at time \\(t\\). We assume that the observations are given by \\[\n  Y^n_t = \\ell^n_t(S_t, W^n_t), \\quad n \\in \\ALPHABET N\n\\] and the state evolves as \\[\n  S_{t+1} = f^0_t(S_t, A_t, W^0_t)\n\\] where \\(A_t\\) denotes the vector \\((A^1_t, \\dots, A^N_t)\\) of all actions. We assume that the noise processes \\(\\{W^0_t\\}_{t \\ge 1}\\), \\(\\{W^n_t\\}_{t \\ge 1}\\), \\(n \\in \\ALPHABET N\\), are independent across time and also mutually independent.\nWe assume that all agents use finite state controllers. In particular, agent \\(n \\in \\ALPHABET N\\) uses a local state \\(Z^n_t \\in \\ALPHABET Z^n\\). At time \\(t\\), agent \\(n\\) first updates its state using a state-update function \\(f^n_t\\), as follows: \\[\n  Z^n_t = f^n_t(Z^n_{t-1}, Y^n_t)\n\\] and then uses the updated state to choose an action as follows: \\[\n  A^n_t = π^n_t(Z^n_t).\n\\] We call \\(Z^n_t\\) as agent state.\nA commonly used example of agent state is \\(Z^n_t = (Y^n_{t-d+1}, Y^n_{t-d+2}, \\dots, Y^n_t)\\) where the agent uses a sliding window of the last \\(d\\) observations as a local state. The example presented in the previous section is a special case of a sliding window controller with window size \\(d=1\\).\nThus, we can view the problem from the time of a view of a system designer. The state sufficient for input-output mapping is \\((S_t, Z^1_t, \\dots, Z^n_t)\\); the control input is \\((f^1_t, \\dots, f^n_t, π^1_t, \\dots, π^n_t)\\) and the output is the instanteous cost. We separate the belief state before taking actions and the belief state before updating the local memory: \\[\\begin{align*}\n  b_t &= \\PR(S_t, Z^1_t, \\dots, Z^N_t \\mid f^{1:N}_{1:t}, π^{1:N}_{1:t-1}), \\\\\n  \\bar b_t &= \\PR(S_{t+1}, Z^1_t, \\dots, Z^N_t \\mid f^{1:N}_{1:t}, π^{1:N}_{1:t}).\n\\end{align*}\\]\nAs for Lemma 31.1, we can show the following:\n\nLemma 31.2 The belief state satisfies the following properties.\n\nSufficient for predicting itself. There exists a linear transformations \\(H_t( π^{1:N}_{t})\\) and \\(\\bar H_t(f^{1:N}_t)\\) such that \\[\\begin{align*}\n   \\bar b_t &= H_t(π^{1:N}_{t}) b_t, \\\\\n   b_{t+1} &= \\bar H_t(f^{1:N}_{t}) \\bar b_t.\n\\end{align*}\\]\nSufficient for performance evaluation. There exists functions \\(\\bar c_t \\colon [ \\ALPHABET S \\to \\ALPHABET A] \\to \\reals\\) such that the expected per-step cost can be expressed as \\[\n\\EXP[ c_t(S_t, A_t) \\mid f^{1:N}_{1:t}, π^{1:N}_{1:t} ] =\n\\bar c_t(b_t, π^{1:N}_t).\n\\]\n\n\nTherefore, we can obtain the following dynamic programming decomposition.\n\n\n\n\n\n\nDynamic program\n\n\n\nInitialize \\(V_{T+1}(b) ≡ 0\\). Then, for all \\(t \\in \\{T, \\dots, 1\\}\\), recursively define: \\[\\begin{align*}\n  Q_t(b, π^{1:N}) &= \\bar c_t(b, f^{1:N}, π^{1:N}) + \\bar V_{t}( H_t(π^{1:N}) b ) \\\\\n  \\bar Q_t(\\bar b, f^{1:N}) &= V_{t+1}(\\bar H_t(f^{1:N}) \\bar b), \\\\\n\\end{align*}\\] and \\[\\begin{align*}\n  V_t(b)   &= \\min_{π^{1:N}} Q_t(b, π^{1:N}), \\\\\n  \\bar V_t(\\bar b)   &= \\min_{f^{1:N}} \\bar Q_t(\\bar b, f^{1:N}).\n\\end{align*}\\]\nLet \\(ψ_t(b)\\) and \\(\\bar ψ_t(\\bar b)\\) denote the arg min of \\(Q_t(b, π^{1:N})\\) and \\(\\bar Q_t(\\bar b, f^{1:N})\\). Then, the meta-strategy \\((ψ_1,\\bar ψ_1, ψ_2, \\bar ψ_2, \\dots, ψ_T)\\) is optimal.\n\n\n\n\n\n\n\n\nSimplifying the dynamic program\n\n\n\nInstead of optimizing the dynamic program over all policies \\(f^{1:N}\\) and \\(π^{1:N}\\) at once, we can optimize them one by one. In particular, define an intermediate beliefs \\[\\begin{align*}\nb^1_t &= \\PR(S_t, Z^1_t, \\dots Z^N_t, A^1_t \\mid f^{1:N}_{1:t}, π^{1}_{1:t}, π^{2:N}_{1:t-1}), \\\\\nb^2_t &= \\PR(S_t, Z^1_t, \\dots Z^N_t, A^{1:2}_t \\mid f^{1:N}_{1:t}, π^{1:2}_{1:t}, π^{3:N}_{1:t-1}), \\\\\n\\cdots &= \\cdots \\\\\nb^N_t &= \\PR(S_t, Z^1_t, \\dots Z^N_t, A^{1:N}_t \\mid f^{1:N}_{1:t}, π^{1:N}_{1:t}),\n\\end{align*}\\] and \\[\\begin{align*}\n\\bar b^1_t &= \\PR(S_{t+1}, Z^1_{t+1}, Z^{2:N}_t \\mid f^{1}_{1:t+1}, f^{2:N}_{1:t}, π^{1:N}_{1:t}), \\\\\n\\bar b^2_t &= \\PR(S_{t+1}, Z^{1:2}_{t+1}, Z^{3:N}_t \\mid f^{1:2}_{1:t+1}, f^{3:N}_{1:t}, π^{1:N}_{1:t}), \\\\\n\\cdots &= \\cdots \\\\\n\\bar b^N_t &= \\PR(S_{t+1}, Z^{1:N}_{t+1} \\mid f^{1:N}_{1:t+1} π^{1:N}_{1:t}).\n\\end{align*}\\]\nThen, we can show that these beliefs update as: \\[\n  b_t\n  \\xrightarrow{H^1_t(π^1_t)}\n  b^1_t\n  \\xrightarrow{H^2_t(π^2_t)}\n  \\cdots\n  \\xrightarrow{H^N_t(π^N_t)}\n  b^N_t\n  \\rightarrow\n  \\bar b_t\n  \\xrightarrow{\\bar H^1_t(f^1_{t+1})}\n  \\bar b^1_t\n  \\xrightarrow{\\bar H^2_t(f^2_{t+1})}\n  \\cdots\n  \\bar b^N_t = b_{t+1}.\n\\] We can then decompose the DP over time in a similar manner and just optimize over only one of \\(π^n_t\\) or \\(f^n_t\\) at each step.",
    "crumbs": [
      "Dec-POMDPs",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Designer's Approach</span>"
    ]
  },
  {
    "objectID": "dec-pomdps/designers-approach.html#notes",
    "href": "dec-pomdps/designers-approach.html#notes",
    "title": "31  Designer’s Approach",
    "section": "Notes",
    "text": "Notes\nThe presentation here is borrowed from Mahajan (2008). The general idea first appeared in Witsenhausen (1973), where it was called the standard form. The model in Witsenhausen (1973) was fairly general. It was specialized to POMDPs with finite memory in Sandell (1974). Witsenhausen’s standard form was rediscovered in Dibangoye et al. (2016), where it was called occupation MDP.\n\n\n\n\nDibangoye, J.S., Amato, C., Buffet, O., and Charpillet, F. 2016. Optimally solving dec-POMDPs as continuous-state MDPs. Journal of Artificial Intelligence Research 55, 443–497. DOI: 10.1613/jair.4623.\n\n\nMahajan, A. 2008. Sequential decomposition of sequential dynamic teams: Applications to real-time communication and networked control systems. PhD thesis, University of Michigan, Ann Arbor, MI.\n\n\nSandell, J., Nils R. 1974. Control of finite-state, finite-memory stochastic systems. PhD thesis, Massachussets Institute of Technology, Cambridge, MA.\n\n\nWitsenhausen, H.S. 1973. A standard form for sequential stochastic control. Mathematical Systems Theory 7, 1, 5–11. DOI: 10.1007/bf01824800.",
    "crumbs": [
      "Dec-POMDPs",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Designer's Approach</span>"
    ]
  },
  {
    "objectID": "probability/convergence.html",
    "href": "probability/convergence.html",
    "title": "32  Convergence of random variables",
    "section": "",
    "text": "Convergence of expected values\n\n\n\nSuppose \\(X_n \\to X\\) almost surely. Then, each of the following is a sufficient condition for \\(\\EXP[X_n] \\to \\EXP[X]\\):\n\nMonotone Convergence Theorem. \\(0 \\le X_1 \\le X_2 \\cdots\\).\nBounded Convergence Theorem. there exists a constant \\(b\\) such that \\(|X_n| \\le b\\) for all \\(n\\).\nDominated Convergence Theorem. there exists a random variable \\(Y\\) such that \\(|X_n| \\le Y\\) almost surely for all \\(n\\) and \\(\\EXP[Y] &lt; ∞\\).\n\\(\\{X_n\\}_{n \\ge 1}\\) is uniformly integrable.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "probability/sub-gaussian.html",
    "href": "probability/sub-gaussian.html",
    "title": "33  Sub-Gaussian random variables",
    "section": "",
    "text": "33.1 Prelim: Concentration inequality of sum of Gaussian random variables\nLet \\(\\phi(\\cdot)\\) denote the density of \\(\\mathcal{N}(0,1)\\) Gaussian random variable: \\[ \\phi(x) = \\frac{1}{\\sqrt{2π}} \\exp\\biggl( - \\frac{x^2}{2} \\biggr). \\]\nNote that if \\(X \\sim \\mathcal{N}(μ,σ^2)\\), then the density of \\(X\\) is \\[\n\\frac{1}{σ}\\phi\\biggl( \\frac{x-μ}{σ} \\biggr)\n= \\frac{1}{\\sqrt{2π}\\,σ} \\exp\\biggl( - \\frac{(x-μ)^2}{2 σ^2} \\biggr). \\]\nThe tails of Gaussian random variables decay fast which can be quantified using the following inequality.\nThe fact that a Gaussian random variable has tails that decay to zero exponentially fast can be be seen in the moment generating function: \\[\n  M(s) = \\EXP[ \\exp(sX) ] = \\exp\\bigl( sμ + \\tfrac12 s^2 σ^2\\bigr).\n\\]\nA useful application of Mills inequality is the following concentration inequality.\nAnother useful result is the following:\nSee these notes for a lower bound with the same rate!",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Sub-Gaussian random variables</span>"
    ]
  },
  {
    "objectID": "probability/sub-gaussian.html#prelim-concentration-inequality-of-sum-of-gaussian-random-variables",
    "href": "probability/sub-gaussian.html#prelim-concentration-inequality-of-sum-of-gaussian-random-variables",
    "title": "33  Sub-Gaussian random variables",
    "section": "",
    "text": "Proposition 33.1 (Mills inequality) If \\(X \\sim \\mathcal{N}(0, 1)\\), then for any \\(t &gt; 0\\), \\[ \\PR( |X| &gt; t ) \\le \\frac{2\\phi(t)}{t}  \\]\nMore generally, if \\(X \\sim \\mathcal{N}(0, σ^2)\\), then for any \\(t &gt; 0\\), \\[ \\PR( |X| &gt; t ) \\le 2\\frac{σ}{t} \\phi\\biggl(\\frac{t}{σ}\\biggr) =\n\\sqrt{\\frac{2}{π} } \\frac{σ}{t}\n  \\exp\\biggl( - \\frac{t^2}{2σ^2} \\biggr). \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nIn the communication theory literature, this bound is sometimes known as the bound on the erfc or \\(Q\\) function.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe’ll first prove the result for unit variance random variable. Note that \\(X\\) is symmetric around origin. Therefore, \\[ \\PR(|X| &gt; t) = 2\\PR(X &gt; t). \\]\nNow, by using an idea similar to the proof of Markov’s inequality, we have \\[\\begin{align*}\nt \\cdot \\PR( |X| &gt; t) &= t \\int_{t}^∞ \\phi(x) dx  \\\\\n& \\le \\int_{t}^∞ x \\phi(x) dx \\\\\n& = \\int_{t}^∞ \\frac{1}{\\sqrt{2π}} x \\exp\\biggl( - \\frac{x^2}{2} \\biggr) dx \\\\\n&= \\frac{1}{\\sqrt{2π}} \\int_{t}^∞ - \\frac{∂}{∂x} \\exp\\biggl( -\\frac{x^2}{2}\n\\biggr) dx \\\\\n& = \\frac{1}{\\sqrt{2π}} \\exp\\biggl( - \\frac{t^2}{2} \\biggr)\n\\end{align*}\\]\nThe proof for the general case follows by observing that \\[\n\\PR(|X| &gt; t) = \\PR\\biggl( \\biggl| \\frac{X}{σ} \\biggr| &gt; \\frac{t}{σ} \\biggr)\n\\] where \\(X/σ \\sim \\mathcal{N}(0,1)\\).\n\n\n\n\n\n\nProposition 33.2 (Concentration inequality.) Let \\(X_i \\sim \\mathcal{N}(0, σ^2)\\) (not necessarily independent). Then, for any \\(t &gt; 0\\), \\[\n  \\PR\\Bigl( \\max_{1 \\le i \\le n} |X_i| &gt; t\\Bigr) \\le\n  2n \\frac{σ}{t} \\phi\\biggl( \\frac{t}{σ} \\biggr).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\nThis follows immediately from Mills inequality and the union bound.\n\n\n\n\nProposition 33.3 (Max of Gaussian random variables.) Let \\(X_i \\sim \\mathcal{N}(0,σ^2)\\) (not necessarily independent). Then, \\[\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} X_i \\Bigr] \\le σ \\sqrt{2 \\log n}\n\\] and \\[\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} |X_i| \\Bigr] \\le σ \\sqrt{2 \\log 2n}.\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the first inequality. The second follows by considering \\(2n\\) random variables \\(X_1, \\dots, X_n\\), \\(-X_1, \\dots, -X_n\\).\nFor any \\(s &gt; 0\\), \\[\\begin{align*}\n\\EXP\\Bigl[ \\max_{1 \\le i \\le n} X_i \\Bigr] &=\n\\frac{1}{s}\n\\EXP\\Bigl[ \\log \\Bigl( \\exp\\Bigl( s \\max_{1 \\le i \\le n} X_i \\Bigr) \\Bigr) \\Bigr]\n\\\\\n&\\stackrel{(a)}\\le\n\\frac{1}{s}\n\\log \\Bigl( \\EXP\\Bigl[ \\exp\\Bigl( s \\max_{1 \\le i \\le n} X_i \\Bigr) \\Bigr] \\Bigr)\n\\\\\n&\\stackrel{(b)}=\n\\frac{1}{s}\n\\log \\Bigl( \\EXP\\Bigl[ \\max_{1 \\le i \\le n} \\exp( s X_i ) \\Bigr] \\Bigr)\n\\\\\n&\\stackrel{(c)}\\le\n\\frac{1}{s}\n\\log \\Bigl(\\sum_{i=1}^n \\EXP\\bigl[ \\exp( s X_i ) \\bigr] \\Bigr)\n\\\\\n&\\stackrel{(d)}=\n\\log \\Bigl( \\sum_{i=1}^n\\exp\\Bigl( \\frac{s^2 σ^2}{2} \\Bigr) \\Bigr)\n\\\\\n&= \\frac{\\log n}{s} + \\frac{s^2 σ^2}{2}\n\\end{align*}\\] where \\((a)\\) follows from Jensen’s inequality, \\((b)\\) follows from monotonicity of \\(\\exp(\\cdot)\\), \\((c)\\) follows from definition of max, \\((d)\\) follows from the definition of moment generating function of Gaussian random variables. We get the result by setting \\(s = \\sqrt{2 \\log n}/σ\\) (which minimizes the upper bound).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nWe have stated and proved these inequalities for real-valued random variables. But a version of them continue to hold for vector valued Gaussian variables as well. For a complete treatment, see Picard (2007).",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Sub-Gaussian random variables</span>"
    ]
  },
  {
    "objectID": "probability/sub-gaussian.html#sub-gaussian-random-variables",
    "href": "probability/sub-gaussian.html#sub-gaussian-random-variables",
    "title": "33  Sub-Gaussian random variables",
    "section": "33.2 Sub-Gaussian random variables",
    "text": "33.2 Sub-Gaussian random variables\nIt turns out that the concentration inequalities of the form above continue to hold for more general distributions than the Gaussian. In particular, consider the bound on the max of Gaussian random variables that we established above. The only step which depends on the assumption that the random variables \\(X_i\\) were Gaussian in step \\((d)\\). Thus, as long as \\(\\EXP[ \\exp(s X_i) ] \\le\n\\exp(\\tfrac12 s^2 σ^2)\\), the result will continue to hold! This motivates the definition of sub-Gaussian random variables.\n\nDefinition 33.1 (Sub-Gaussian random variable) A random variable \\(X \\in \\reals\\) is said to be sub-Gaussian with variance proxy \\(σ^2\\) if \\(\\EXP[X] = 0\\) and its moment generating function satisfies \\[ \\EXP[ \\exp(sX) ] \\le \\exp( \\tfrac12 s^2 σ^2),\n\\quad \\forall s \\in \\reals. \\]\n\nThe reason the parameter \\(σ^2\\) is called a variance proxy is because by a straight forward application of Taylor series expansion and comparing coefficients, it can be shown that \\(\\text{var}(X) \\le σ^2\\). See Rivasplata (2012) for a proof.\nThis definition can be generalized to random vectors and matrices. A random vector \\(X \\in \\reals^d\\) is said the be sub-Gaussian with variance proxy \\(σ^2\\) if \\(\\EXP[X] = 0\\) and for any unit vector \\(u \\in \\reals^d\\), \\(u^\\TRANS X\\) is sub-Gaussian with variance proxy \\(σ^2\\).\nSimilarly, a random matrix \\(X \\in \\reals^{d_1 × d_2}\\) is said to be sub-Gaussian with variance proxy \\(σ^2\\) if \\(\\EXP[X] = 0\\) and for any unit vectors \\(u \\in \\reals^{d_1}\\) and \\(v \\in \\reals^{d_2}\\), \\(u^\\TRANS X v\\) is sub-Gaussian with variance proxy \\(σ^2\\).\nWe will use the phrase “\\(σ\\)-sub-Gaussian” as a short form of “sub-Gaussian with variance proxy \\(σ^2\\)”. One typically writes \\(X \\sim \\text{subG}(σ^2)\\) to denote a random variable with sub-Gaussian distribution with variance proxy \\(σ^2\\). (Strictly speaking, this notation is a bit ambiguous since \\(\\text{subG}(σ^2)\\) is a class of distributions rather than a single distribution.)",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Sub-Gaussian random variables</span>"
    ]
  },
  {
    "objectID": "probability/sub-gaussian.html#examples-of-sub-gaussian-distributions",
    "href": "probability/sub-gaussian.html#examples-of-sub-gaussian-distributions",
    "title": "33  Sub-Gaussian random variables",
    "section": "33.3 Examples of sub-Gaussian distributions",
    "text": "33.3 Examples of sub-Gaussian distributions\n\nIf \\(X\\) be a Rademacher random variable, i.e., \\(X\\) takes the values \\(\\pm 1\\) with probability \\(1/2\\). Then, \\[ \\EXP[ \\exp(sX) ] = \\frac12 e^{-s} + \\frac12 e^s = \\cosh s \\le\n\\exp(\\tfrac12 s^2), \\] so \\(X\\) is\nIf \\(X\\) is uniformly distributed over \\([-a, a]\\). Then, for any \\(s \\neq 0\\), \\[ \\EXP[ \\exp(s X) ] = \\frac{1}{2as}[ e^{as} - e^{-as} ]\n   = \\sum_{n=0}^∞ \\frac{(as)^{2n}}{(2n+1)!}. \\] Using the inequality \\((2n+1)! \\ge n!2^n\\), we get that \\(X\\) is \\(a\\)-sub-Gaussian.\nIt can be shown that (see Rivasplata (2012) ) if \\(X\\) is a random variable with \\(\\EXP[X] = 0\\) and \\(|X| &lt; 1\\) a.s., then \\[ \\EXP[ \\exp(sX) ] \\le \\cosh s, \\quad \\forall s \\in \\reals. \\] Therefore, \\(X\\) is 1-sub-Gaussian.\nAn immediate corollary of the previous example is that if \\(X\\) is a random variable with \\(\\EXP[X] = 0\\) and \\(|X| \\le b\\) a.s., then \\(X\\) is \\(b\\)-sub-Gaussian.\nBy a similar arguement, we can show that if \\(X\\) is a zero mean random variable supported on some interval \\([a,b]\\), then \\(X\\) is \\((b-a)/2\\) sub-Gaussian.\nIf \\(X\\) is \\(σ^2\\) sub-Gaussian, then for any \\(α \\in \\reals\\), \\(α X\\) is \\(|α|σ\\)-sub-Gaussian.\nIf \\(X_1\\) and \\(X_2\\) are \\(σ_1\\) and \\(σ_2\\)-sub-Gaussian, then \\(X_1 + X_2\\) is \\(\\sqrt{σ_1^2 + σ_2^2}\\)-sub-Gaussian.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Sub-Gaussian random variables</span>"
    ]
  },
  {
    "objectID": "probability/sub-gaussian.html#characterization-of-sub-gaussian-random-variables",
    "href": "probability/sub-gaussian.html#characterization-of-sub-gaussian-random-variables",
    "title": "33  Sub-Gaussian random variables",
    "section": "33.4 Characterization of sub-Gaussian random variables",
    "text": "33.4 Characterization of sub-Gaussian random variables\nSub-Gaussian random variables satisfy a concentration result similar to Mills inequality.\n\nLemma 33.1 Let \\(X \\in \\reals\\) be \\(σ\\)-sub-Gaussian. Then, for any \\(t &gt; 0\\), \\[\\begin{equation}\\label{eq:sG-tail-bounds}\n  \\PR(X &gt; t) \\le \\exp\\biggl( - \\frac{t^2}{2σ^2} \\biggr)\n  \\quad\\text{and}\\quad\n  \\PR(X &lt; t) \\le \\exp\\biggl( - \\frac{t^2}{2σ^2} \\biggr)\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis follows from Chernoff’s bound and the definition of sub-Gaussianity. In particular, for any \\(s &gt; 0\\) \\[\n\\PR(X &gt; t) = \\PR(\\exp(sX) &gt; \\exp(st)) \\le \\frac{ \\EXP[\\exp(sX) ]} { \\exp(st) }\n\\le \\exp\\biggl( \\frac{s^2 σ^2}{2} - st \\biggr).\n\\] Now, to find the tightest possible bound, we minimize the above bound with respect to \\(s\\), which is attained at \\(s = t/σ^2\\). Substituting this in the above bound, we get the first inequality. The second inequality follows from a similar argument.\n\n\n\nRecall that the moments of \\(Z \\sim \\mathcal{N}(0,σ^2)\\) are given by \\[\n  \\EXP[ |Z|^k ] = \\frac{1}{\\sqrt{π}} (2σ^2)^{k/2} Γ\\biggl(\\frac{k+1}{2}\\biggr),\n\\] where \\(Γ(\\cdot)\\) denotes the Gamma function. The next result shows that the tail bounds \\eqref{eq:sG-tail-bounds} are sufficient to show that the absolute moments of \\(X \\sim \\text{subG}(σ^2)\\) can be bounded by those of \\(Z \\sim \\mathcal{N}(0,σ^2)\\) up to multiplicative constants.\n\nLemma 33.2 Let \\(X\\) be a random variable such that \\[ \\PR( |X| &gt; t) \\le 2 \\exp\\biggl(- \\frac{t^2}{2σ^2} \\biggr),\\] then for any positive integer \\(k \\ge 1\\), \\[ \\EXP[ |X|^k ] \\le (2σ^2)^{k/2} k Γ(k/2). \\]\n\nNote that for the special case of \\(k=1\\), the above bound implies \\(\\EXP[ |X| ]\n\\le σ \\sqrt{2π}\\) and for \\(k=2\\), \\(\\EXP[|X|^2] \\le 4σ^2\\).\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis is a simple application of the tail bound. \\[\\begin{align*}\n\\EXP[ |X|^k ] &= \\int_{0}^∞ \\PR( |X|^k &gt; t ) dt \\\\\n&= \\int_{0}^∞ \\PR( |X| &gt; t^{1/k}) dt \\\\\n&\\le 2 \\int_{0}^∞ \\exp\\biggl( - \\frac{t^{2/k}}{2σ^2} \\biggr) dt \\\\\n&= (2σ^2)^{k/2} k \\int_{0}^∞ e^{-u} u^{k/2 - 1} du,\n\\qquad u = \\frac{t^{2/k}}{2σ^2} \\\\\n&= (2σ^2)^{k/2}k Γ(k/2).\n\\end{align*}\\]\nThe result for \\(k=1\\) follows from \\(Γ(1/2) = \\sqrt{π/2}\\).\n\n\n\nUsing moments, we can bound the moment generating function in terms of the tail bounds.\n\nLemma 33.3 Let \\(X\\) be a random variable such that \\[ \\PR( |X| &gt; t) \\le 2 \\exp\\biggl(- \\frac{t^2}{2σ^2} \\biggr)\\] then, \\[\\EXP[ \\exp(sX) ] \\le \\exp(4 s^2 σ^2). \\]\n\nFor this reason, sometimes it is stated that \\(X \\sim \\text{subG}(s^2)\\) when it satisfies the tail bound \\eqref{eq:sG-tail-bounds}.\nThe proof follows from the following Taylor series bound on the exponential function. \\[\n\\exp(sX) \\le 1 + \\sum_{k=2}^∞ \\frac{s |X|^k}{k!}\n\\] and apply the result of Lemma 33.2. See Rigollet (2015) for details.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Sub-Gaussian random variables</span>"
    ]
  },
  {
    "objectID": "probability/sub-gaussian.html#properties-of-sub-gaussian-random-vectors",
    "href": "probability/sub-gaussian.html#properties-of-sub-gaussian-random-vectors",
    "title": "33  Sub-Gaussian random variables",
    "section": "33.5 Properties of sub-Gaussian random vectors",
    "text": "33.5 Properties of sub-Gaussian random vectors\n\nTheorem 33.1 Let \\(X = (X_1, \\dots, X_n)\\) be a vector of independent \\(σ\\)-sub-Gaussian random variables. Then, the random vector \\(X\\) is \\(σ\\)-sub-Gaussian.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any unit vector \\(u \\in \\reals^n\\), and any \\(s \\in \\reals\\) \\[\\begin{align*}\n\\EXP[ \\exp( s u^\\TRANS X) ] &= \\prod_{i=1}^n \\EXP[ \\exp(s u_i X_i) ] \\\\\n&\\le \\prod_{i=1}^n \\exp\\bigl( \\tfrac{1}{2} s^2 u_i^2 σ^2 \\bigr) \\\\\n&= \\exp\\bigl( \\tfrac{1}{2} s^2 \\| u \\|^2 σ^2 \\bigr) \\\\\n&= \\exp\\bigl( \\tfrac{1}{2} s^2 σ^2 \\bigr).\n\\end{align*}\\]",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Sub-Gaussian random variables</span>"
    ]
  },
  {
    "objectID": "probability/sub-gaussian.html#concentration-inequalities",
    "href": "probability/sub-gaussian.html#concentration-inequalities",
    "title": "33  Sub-Gaussian random variables",
    "section": "33.6 Concentration inequalities",
    "text": "33.6 Concentration inequalities\nRecall that if \\(X_1\\) and \\(X_2\\) and \\(σ_1\\) and \\(σ_2\\)-sub-Gaussian, then \\(X_1 +\nX_2\\) is sub-Gaussian with variance proxy \\(σ_1^2 + σ_2^2\\). An immediate implication of this property is the following:\n\nProposition 33.4 (Hoeffding inequality) Suppose that variables \\(X_i\\), \\(i \\in \\{1,\\dots,n\\}\\), are independent and \\(X_i\\) has mean \\(μ_i\\) and \\(σ_i\\)-sub-Gaussian. Then, for all \\(t &gt; 0\\), we have\n\\[ \\PR\\biggl( \\sum_{i=1}^n( X_i - μ_i) \\ge t \\biggr)\n   \\le \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n σ_i^2 } \\biggr).\n\\]\n\nThe Hoeffding inequality is often stated for the special case of bounded random variables. In particular, if \\(X_i \\in [a,b]\\), then we know that \\(X_i\\) is sub-Gaussian with parameter \\(σ = (b-a)/2\\), so we obtain the bound \\[ \\PR\\biggl( \\sum_{i=1}^n( X_i - μ_i) \\ge t \\biggr)\n   \\le \\exp\\biggl( - \\frac{2t^2}{\\sum_{i=1}^n n(b-a)^2 } \\biggr).\n\\]\nThe Hoeffding inequality can be generalized to Martingales. Recall that a sequence \\(\\{ (D_i, \\mathcal F_i)\\}_{i \\ge 1}\\) is called a martingale difference sequence is for all \\(i \\ge 1\\), \\(D_i\\) is \\(\\mathcal{F}_i\\) measurable, \\[ \\EXP[ |D_i| ] &lt; ∞ \\quad\\text{and}\\quad\n   \\EXP[ D_{i+1} \\mid \\mathcal{F}_i ] = 0. \\]\n\nProposition 33.5 (Asuma-Hoeffding Inequality.) Let \\(\\{ (D_i, \\mathcal{F}_i)\\}_{i \\ge 1}\\) be a martingale difference sequence and suppose that \\(|D_i| \\le b_i\\) almost surely for all \\(i \\ge 1\\). Then for all \\(t \\ge 0\\) \\[ \\PR\\biggl( \\biggl| \\sum_{i=1}^n D_i \\biggr| \\ge t \\biggr)\n   \\le 2 \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n b_i^2 } \\biggr).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\(|D_i| \\le b_i\\), \\(D_i\\) is \\(b_i\\)-subGaussian. Using the smoothing property of conditional expectation, we have \\[\\begin{align}\n\\EXP\\biggl[ \\exp\\biggl( s \\biggl( \\sum_{i=1}^n D_i \\biggr) \\biggr) \\biggl]\n&=\n\\EXP\\biggl[ \\exp\\biggl( s \\biggl( \\sum_{i=1}^{n-1} D_i \\biggr) \\biggr) \\biggl]\n\\,\n\\EXP\\bigl[ \\exp\\bigl( s D_n \\bigr) \\bigm| \\mathcal{F}_{n-1} \\bigl]\n\\notag \\\\\n&\\le\n\\EXP\\biggl[ \\exp\\biggl( s \\biggl( \\sum_{i=1}^{n-1} D_i \\biggr) \\biggr) \\biggl]\n\\, \\exp\\bigl( \\tfrac12 s^2 b_n^2 \\bigr),\n\\end{align}\\] where the inequality followed from \\(D_n\\) being \\(b_n\\)-subGaussian. Iterating backwards this way, we get \\[ \\PR\\biggl(  \\sum_{i=1}^n D_i  \\ge t \\biggr)\n   \\le  \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n b_i^2 } \\biggr).\n\\] By a symmetric argument, we can show that \\[ \\PR\\biggl(  \\sum_{i=1}^n D_i  \\le -t \\biggr)\n   \\le  \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n b_i^2 } \\biggr).\n\\] Conbining these two, we get the stated result.\n\n\n\nNote that we can easily generalize the above inequality to the case when \\(D_k\n\\in [a_i, b_i]\\) because in that case \\(D_k\\) will be \\((b_i - a_i)/2\\) sub-Gaussian.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Sub-Gaussian random variables</span>"
    ]
  },
  {
    "objectID": "probability/sub-gaussian.html#maximal-inequalities",
    "href": "probability/sub-gaussian.html#maximal-inequalities",
    "title": "33  Sub-Gaussian random variables",
    "section": "33.7 Maximal inequalities",
    "text": "33.7 Maximal inequalities\nAs we explained in the motivation for the definition of sub-Gaussian random variables, the definition implies that sub-Gaussian random variables will satisfy the concentration and maximal inequalities for Gaussian random variables. In particular, we have the following general result.\n\nTheorem 33.2 Let \\(X_i \\in \\reals\\) be \\(σ\\)-sub-Gaussian random variables (not necessarily independent). Then, \\[\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} X_i \\Bigr] \\le σ \\sqrt{2 \\log n}\n\\quad\\text{and}\\quad\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} |X_i| \\Bigr] \\le σ \\sqrt{2 \\log 2n}.\n\\] Moreover, for any \\(t &gt; 0\\), \\[\n  \\PR\\Bigl( \\max_{1 \\le i \\le n} X_i &gt; t\\Bigr) \\le\n  n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr)\n\\quad\\text{and}\\quad\n  \\PR\\Bigl( \\max_{1 \\le i \\le n} |X_i| &gt; t\\Bigr) \\le\n  2n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr).\n\\]\n\nThe proof is exactly the same as the Gaussian case!\nNow we state two generalizations without proof. See Rigollet (2015) for proof.\n\nMaximum over a convex polytope\n\nTheorem 33.3 Let \\(\\mathsf{P}\\) be a polytope with \\(n\\) vertices \\(v^{(1)}, \\dots, v^{(n)} \\in\n\\reals^d\\) and let \\(X \\in \\reals^d\\) be a random variable such that \\([\nv^{(i)} ]^\\TRANS X\\), \\(i \\in \\{1, \\dots, n\\}\\) are \\(σ\\)-sub-Gaussian random variables. Then, \\[\n  \\EXP\\Bigl[ \\max_{θ \\in \\mathsf{P}} θ^\\TRANS X \\Bigr] \\le σ \\sqrt{2 \\log n}\n\\quad\\text{and}\\quad\n  \\EXP\\Bigl[ \\max_{θ \\in \\mathsf{P}} | θ^\\TRANS X | \\Bigr] \\le σ \\sqrt{2 \\log 2n}.\n\\] Moreover, for any \\(t &gt; 0\\), \\[\n  \\PR\\Bigl(  \\max_{θ \\in \\mathsf{P}} θ^\\TRANS X &gt; t\\Bigr) \\le\n  n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr)\n\\quad\\text{and}\\quad\n  \\PR\\Bigl(  \\max_{θ \\in \\mathsf{P}} |θ^\\TRANS X| &gt; t\\Bigr) \\le\n  2n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr).\n\\]\n\n\n\nMaximum over the \\(\\ell_2\\) ball\n\nTheorem 33.4 Let \\(X \\in \\reals^d\\) be a \\(σ\\)-sub-Gaussian random variable. Then, \\[ \\EXP[ \\max_{ \\| θ \\| \\le 1 } θ^\\TRANS X ] =\n   \\EXP[ \\max_{ \\| θ \\| \\le 1 } | θ^\\TRANS X | ] \\le 4σ \\sqrt{d}.\n\\] Moreover, for any \\(t &gt; 0\\) \\[ \\PR( \\max_{ \\| θ \\| \\le 1 } θ^\\TRANS X &gt; t) =\n   \\PR( \\max_{ \\| θ \\| \\le 1 } | θ^\\TRANS X | &gt; t ) \\le\n  6^d \\exp\\biggl(- \\frac{t^2}{8σ^2} \\biggr).\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nFor any \\(δ &gt; 0\\), take \\(t = σ\\sqrt{8d \\log 6} + 2σ\\sqrt{2 \\log(1/δ)}\\), we obtain that with probability less than \\(1-δ\\), it holds that \\[\n  \\max_{\\|θ\\| \\le 1} θ^\\TRANS X\n  =\n  \\max_{\\|θ\\| \\le 1} | θ^\\TRANS X |\n  \\le 4σ\\sqrt{d} + 2σ \\sqrt{2\\log(1/δ)}.\n\\]",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Sub-Gaussian random variables</span>"
    ]
  },
  {
    "objectID": "probability/sub-gaussian.html#lipschitz-functions-of-gaussian-variables.",
    "href": "probability/sub-gaussian.html#lipschitz-functions-of-gaussian-variables.",
    "title": "33  Sub-Gaussian random variables",
    "section": "33.8 Lipschitz functions of Gaussian variables.",
    "text": "33.8 Lipschitz functions of Gaussian variables.\nRecall that a function \\(f \\colon \\reals^d \\to \\reals\\) is \\(L\\)-Lipschitz with respect to the Eucledian norm if \\[\n  | f(x) - f(y) | \\le L \\| x - y \\|_2,\n  \\quad \\forall x, y \\in \\reals^d.\n\\]\nThe following results shows that any Lipschitz function of a Gaussian random variable is \\(L\\)-sub-Gaussian.\n\nTheorem 33.5 Let \\(X = (X_1, \\dots, X_n)\\) be a vector of i.i.d. standard Gaussian random variables and let \\(f \\colon \\reals^n \\to \\reals\\) be \\(L\\)-Lipschitz with respect to the Euclidean norm. Then, the variable \\(f(X) - \\EXP[ f(X) ]\\) is \\(L\\)-sub-Gaussian and therefore \\[\n  \\PR\\bigl[ \\bigl| f(X) - \\EXP[f(X)] \\bigr| \\ge t \\bigr]\n  \\le 2 \\exp\\biggl(- \\frac{t^2}{2L^2} \\biggr).\n\\]\n\nThis result is remarkable because it guarantees that any \\(L\\)-Lipschitz function of a standard Gaussian random vector, irrespective of the dimension, exhibits concetration like a scalar Gaussian variable with variance \\(L^2\\).\nFor a proof, see Chapter 2 of Wainwright (2019).\n\n\n\n\nPicard, J. 2007. Concentration inequalities and model selection. Springer Berlin Heidelberg. DOI: 10.1007/978-3-540-48503-2.\n\n\nRigollet, P. 2015. High-dimensional statistics. Available at: https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/.\n\n\nRivasplata, O. 2012. Subgaussian random variables: An expository note. Available at: http://stat.cmu.edu/~arinaldo/36788/subgaussians.pdf.\n\n\nWainwright, M.J. 2019. High-dimensional statistics. Cambridge University Press. DOI: 10.1017/9781108627771.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Sub-Gaussian random variables</span>"
    ]
  },
  {
    "objectID": "probability/change-of-measure.html",
    "href": "probability/change-of-measure.html",
    "title": "34  Change of Measure",
    "section": "",
    "text": "34.1 Change of measure of a single random variable.\nGiven two measures \\(\\mu\\) and \\(\\nu\\) on a measurable space \\((\\Omega, \\mathcal F)\\), we say that the measure \\(\\mu\\) is absolutely continuous with respect to \\(\\nu\\) (denoted by \\(\\mu \\ll \\nu\\)) if for any \\(A \\in \\mathcal F\\), \\[\n  \\nu(A) = 0 \\implies \\mu(A) = 0.\n\\]",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change of Measure</span>"
    ]
  },
  {
    "objectID": "probability/change-of-measure.html#change-of-measure-of-a-single-random-variable.",
    "href": "probability/change-of-measure.html#change-of-measure-of-a-single-random-variable.",
    "title": "34  Change of Measure",
    "section": "",
    "text": "Theorem 34.1 Let \\((\\Omega, \\mathcal F, P)\\) be a probability space and \\(\\Lambda\\) be an almost surely non-negative random variable such that \\(\\EXP[\\Lambda] = 1\\). For any \\(A \\in \\mathcal F\\), define \\[ P^\\dagger(A) = \\int_A \\Lambda(\\omega) dP(\\omega). \\] Then,\n\n\\(P^\\dagger\\) is a probability measure.\nFor any random variable \\(X\\), \\[ \\EXP^\\dagger[X] = \\EXP[ \\Lambda X]. \\]\nIf \\(\\Lambda\\) is almost surely positive, then \\[ \\EXP[X] = \\EXP^\\dagger \\left[ \\frac{X}{\\Lambda} \\right]. \\]\n\n\n\n\n\n\n\n\nProof\n\n\n\nBy definition. \\(P^\\dagger(\\emptyset) = 0\\) and \\(P^\\dagger(\\Omega) = \\EXP[ \\Lambda] =\n1\\). Since \\(\\Lambda\\) is almost surely non-negative, \\(P^\\dagger(A) \\ge 0\\). Hence, \\(P^\\dagger\\) is a probability measure.\nThe second and the third part follow from observing that \\[ dP^\\dagger(\\omega) = \\Lambda(\\omega) dP(\\omega). \\]\n\n\n\n\nTheorem 34.2 (Radon-Nikodym) Given two probability measures \\(P\\) and \\(P^\\dagger\\) on a measurable space, if \\(P^\\dagger\\) is absolutely continuous with respect to \\(P\\), then there exists an almost surely positive random variable \\(\\Lambda\\) such that \\(\\EXP[\\Lambda] = 1\\) and for any \\(A \\in \\mathcal F\\), \\[\n  P^\\dagger(A) = \\int_A \\Lambda(\\omega) dP(\\omega).\n\\] Such a \\(\\Lambda\\) is called the Radon-Nikodym derivative of \\(P^\\dagger\\) with respect to \\(P\\), and is written as \\[\n  \\Lambda = \\frac{ dP^\\dagger } {dP}.\n\\]\n\n\nRemark\n\n\nThe Radon-Nikodym theorem provides the reverse property of Theorem 34.1. Given two measures \\(μ \\ll ν\\), \\[\n  \\int_{A} f dν = \\int_A f \\frac{dν}{dμ} dμ.\n\\] Thus, in Theorem 34.1, we are constructing a new probaility measure \\(P^\\dagger\\) such that \\(dP^\\dagger/dP = Λ\\).\nThe Radon-Nikodym Theorem is typically stated for \\(σ\\)-finite measures. The above statement is a specialization of Radon-Nikodym Theorem to probability measures.\nIn statistical signal processing literature, the Radon-Nikodym derivative is sometimes known as the likelihood ratio. In the reinforcement learning literature, it is called importance sampling.\nThe density of a random variable is the Radon-Nikodym derivative with respect to the Lebesgue measure.\nThe Radon-Nikodym derivative satisfies the product rule. If \\(μ \\ll ν \\ll\nλ\\), then \\[\n  \\frac {dμ}{dλ} = \\frac {dμ}{dν} \\frac {dν}{dλ},\n  \\quad λ~\\text{a.s.}.\n\\]\nThe Kullback-Leibler divergence between two probability measures \\(P\\) and \\(Q\\) defined on \\((\\Omega, \\mathcal F)\\) may be written as \\[\n  D_{\\text{KL}}( P \\| Q) = \\int_\\Omega \\log \\left ( \\frac {dP}{dQ} \\right)\n  dP.\n\\]",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change of Measure</span>"
    ]
  },
  {
    "objectID": "probability/change-of-measure.html#conditional-expectation-under-change-of-measure",
    "href": "probability/change-of-measure.html#conditional-expectation-under-change-of-measure",
    "title": "34  Change of Measure",
    "section": "34.2 Conditional expectation under change of measure",
    "text": "34.2 Conditional expectation under change of measure\n\nTheorem 34.3 Consider two probability measures \\(P\\) and \\(P^\\dagger\\) on \\((Ω, \\mathcal F)\\) such that \\(P^\\dagger \\ll P\\). Let \\(Λ\\) denote the Radon-Nikodym derivative of \\(P^\\dagger\\) with respect to \\(P\\) and \\(\\mathcal G\\) be any sub sigma-field of \\(\\mathcal F\\). Then, for any random variable \\(X\\) \\[\n  \\EXP^\\dagger[ X | \\mathcal G ] =\n  \\dfrac{ \\EXP[ Λ X | \\mathcal G ] } { \\EXP [ Λ | \\mathcal G ] },\n  \\quad P^\\dagger~\\text{a.s.}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(G \\in \\mathcal G\\). Then:\n\\[\\begin{align*}\n  \\int_G \\EXP[ Λ X | \\mathcal G] dP\n  &\\stackrel{(a)}= \\int_G Λ X dP \\\\\n  &\\stackrel{(b)}= \\int_G X dP^\\dagger \\\\\n  &\\stackrel{(c)}= \\int_G \\EXP^\\dagger[ X | \\mathcal G] dP^\\dagger \\\\\n  &\\stackrel{(d)}= \\int_G \\EXP^\\dagger[ X | \\mathcal G] Λ dP \\\\\n  &\\stackrel{(e)}= \\int_G \\EXP[ \\EXP^\\dagger[ X | \\mathcal G]  Λ | \\mathcal G] dP \\\\\n  &\\stackrel{(f)}= \\int_G \\EXP^\\dagger[ X | \\mathcal G]  \\EXP[ Λ | \\mathcal G] dP \\\\\n\\end{align*}\\] where (a), (c), and (e) follow from the definition of conditional expectation, (b) and (d) follow from change of measures, and (f) follows because \\(\\EXP^\\dagger[\nX | \\mathcal G]\\) is \\(\\mathcal G\\)-measurable. Thus,\n\\[ \\EXP[ Λ X | \\mathcal G ] = \\EXP^\\dagger[ X | \\mathcal G ] \\EXP[ Λ | \\mathcal G]. \\]",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change of Measure</span>"
    ]
  },
  {
    "objectID": "probability/change-of-measure.html#change-of-measure-for-a-process",
    "href": "probability/change-of-measure.html#change-of-measure-for-a-process",
    "title": "34  Change of Measure",
    "section": "34.3 Change of measure for a process",
    "text": "34.3 Change of measure for a process\nConsider a probability space \\((Ω, \\mathcal F)\\) and let \\(P\\) and \\(P^\\dagger\\) be two probability measures on \\((Ω, \\mathcal F)\\) such that \\(P^\\dagger \\ll P\\). Let \\(Λ\\) denote the Radon-Nikodym derivative of \\(P^\\dagger\\) with respect to \\(P\\).\nLet \\(\\{\\mathcal F_t\\}_{t \\ge 0}\\) be a filtration on \\((Ω, \\mathcal F)\\). Then, we can define the Radon-Nikodym derivative process \\[\n  Λ_t = \\EXP[ Λ | \\mathcal F_t ].\n\\]\n\nTheorem 34.4  \n\nThe Radon-Nikodym derivative process \\(\\{Λ_t\\}_{t \\ge 0}\\) is a martingale with respect to \\(\\{\\mathcal F_t\\}_{t \\ge 0}\\), i.e., for any \\(s \\le t\\), \\[ \\EXP[ Λ_t | \\mathcal F_s ] = Λ_s. \\]\nLet \\(X_t\\) be an \\(\\mathcal F_t\\) measurable random variable. Then \\[ \\EXP^\\dagger[X_t] = \\EXP[Λ X_t ] = \\EXP[ Λ_t X_t ]. \\]\nThus, \\(Λ_t\\) may be viewed as \\(\\dfrac {dP^\\dagger}{dP} \\Bigg|_{\\mathcal\nF_t}\\).\nLet \\(X_t\\) be an \\(\\mathcal F_t\\) measurable random varaible. Then for any \\(s &lt; t\\), \\[ \\EXP^\\dagger[X_t | \\mathcal F_s ] =\n   \\dfrac{1}{Λ_s} \\EXP[ Λ_t X_t | \\mathcal F_s ] .\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe fact Radon-Nikodym derivate process is a martingale immediately follows from the towering property of conidtional expectation:\n\\[\n\\EXP[ Λ_t | \\mathcal F_s ] = \\EXP[ \\EXP[ Λ | \\mathcal F_t ] | \\mathcal F_s ]\n= \\EXP[ Λ | \\mathcal F_s ] = Λ_s.\n\\]\nBy definition of Radon-Nikodym derivative, \\(\\EXP^\\dagger[X_t] = \\EXP[Λ X_t]\\). Now, by the towering property of conditional expectation, we have \\[\n  \\EXP[Λ X_t ] = \\EXP[ \\EXP[ Λ X_t | \\mathcal F_t ] ]\n  = \\EXP[ X_t \\EXP[ Λ | \\mathcal F_t ] ] = \\EXP [Λ_t X_t].\n\\] This proves the second part.\nTo prove the third part, Theorem 34.3 implies that\n\\[\\begin{equation}\n\\EXP^\\dagger[ X_t | \\mathcal F_s ] =\n   \\frac{ \\EXP[ Λ X_t | \\mathcal F_s ]} { \\EXP[ Λ | \\mathcal F_s ] } =\n   \\frac{ \\EXP[ Λ X_t | \\mathcal F_s ]} { Λ_s }.\n   \\label{eq:step-1}\n\\end{equation}\\]\nNow, consider the numerator:\n\\[\n\\EXP[ Λ X_t | F_s ] = \\EXP[ \\EXP [ Λ X_t | \\mathcal F_t ] | \\mathcal F_s ]\n= \\EXP [ X_t \\EXP[ Λ | \\mathcal F_t ] ] = \\EXP [ X_t Λ_t ] .\n\\] Substituting this in \\eqref{eq:step-1} completes the proof of the third part.\n\n\n\nAn immediate implication of Theorem 34.4 is the following.\n\nCorollary 34.1 A process \\(\\{X_t\\}_{t \\ge 0}\\) is a \\(P^\\dagger\\)-martingale with respect to \\(\\{\\mathcal F_t\\}_{t \\ge 0}\\) if and only if the process \\(\\{ Λ_t X_t \\}_{t\n\\ge 0}\\) is a \\(P\\)-martingale.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change of Measure</span>"
    ]
  },
  {
    "objectID": "probability/IPM.html",
    "href": "probability/IPM.html",
    "title": "35  Integral Probablity Metrics",
    "section": "",
    "text": "35.1 Definition of Integral Probability Metrics",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Integral Probablity Metrics</span>"
    ]
  },
  {
    "objectID": "probability/IPM.html#definition-of-integral-probability-metrics",
    "href": "probability/IPM.html#definition-of-integral-probability-metrics",
    "title": "35  Integral Probablity Metrics",
    "section": "",
    "text": "Notation\n\nLet \\((\\ALPHABET X,d)\\) be a complete separable metric space, i.e., a Polish space.\nLet \\(w \\colon \\ALPHABET X \\to [1,∞)\\) be a measurable function and define the weighted norm of any function \\(f \\colon \\ALPHABET X \\to \\reals\\) as \\[ \\NORM{f}_w = \\sup_{x \\in \\ALPHABET X} \\frac{f(x)}{w(x)}. \\]\nLet \\(\\ALPHABET M(\\ALPHABET X)\\) denote the set of all measurable real-valued functions on \\(\\ALPHABET X\\) and let \\(\\ALPHABET M_w(\\ALPHABET X)\\) denote the subset with bounded \\(w\\)-norm, i.e., \\[ \\ALPHABET M_w(\\ALPHABET X) =\n\\{ f \\in \\ALPHABET M(\\ALPHABET X) : \\NORM{f}_w &lt; ∞ \\} \\]\nLet \\(\\mathfrak X\\) denote the Borel \\(σ\\)-algebra of \\(\\ALPHABET X\\).\nLet \\(\\ALPHABET P(\\ALPHABET X)\\) denote the set of all probaility measures on \\((\\ALPHABET X, \\mathfrak X)\\) and \\(\\ALPHABET P_w(\\ALPHABET X) = \\{ μ \\in \\ALPHABET P(\\ALPHABET X) : \\int w dμ &lt; ∞ \\}\\).\n\n\\(\\def\\F{\\mathfrak F}\\) Integral probability metrics (IPMs) are pseudo-metrics on \\(\\ALPHABET P_w(\\ALPHABET X)\\) defined as follows.\n\nDefinition 35.1 The integral probability metric \\(d_{\\mathfrak F}\\) is a pseudo-metric on \\(\\ALPHABET P_w(\\ALPHABET X)\\) generated by the function class \\(\\F \\subset \\ALPHABET M_w(\\ALPHABET X)\\) is defined as \\[\n    d_{\\F}(μ_1, μ_2) =\n    \\sup_{f \\in \\F}\n    \\biggl|\n    \\int f d μ_1 - \\int f d μ_2\n    \\biggr|,\n    \\quad\n    \\forall μ_1, μ_2 \\in \\ALPHABET P_w(\\ALPHABET X).\n  \\] The function \\(f\\) is called the witness function and the function class \\(\\F\\) is called the generator.\n\nAn IPM is a pseudo-metric, i.e., it satisfies the following properties: for all \\(μ_1, μ_2, μ_3 \\in \\ALPHABET P_w(\\ALPHABET X)\\)\n\n\\(d_{\\F}(μ_1, μ_1) = 0\\)\nSymmetry. \\(d_{\\F}(μ_1, μ_2) = d_{\\F}(μ_2, μ_1)\\)\nTriangle inequality. \\(d_{\\F}(μ_1, μ_3) \\le d_{\\F}(μ_1, μ_2) + d_{\\F}(μ_2, μ_3)\\).\n\nIPM is a metric when \\(d(μ_1, μ_2) = 0 \\implies μ_1 = μ_2\\).\n\n\nExamples\n\nThe Kolmogorov-Smirnov metric. The Kolmogorov-Smirnov metric on \\(\\reals\\) is defined as \\[\n   \\text{KS}(μ, ν) = \\sup_{t \\in \\reals} \\ABS{ M(t) - N(t) }\n\\] where \\(M\\) and \\(N\\) are CDFs corresponding to \\(μ\\) and \\(ν\\), respectively. It has the following properties:\n\n\\(\\text{KS}(μ,ν) \\in [0, 1]\\)\n\\(\\text{KS}(μ,ν) = 0\\) iff \\(μ = ν\\)\n\\(\\text{KS}(μ, ν) = 1\\) iff the two distributions have disjoint support, i.e., there exists a \\(s \\in \\reals\\) such that \\(μ( (-∞, s] ) = 1\\) and \\(ν((-∞, s]) = 0\\)\n\\(\\text{KS}\\) is a metric\n\nThe Kolmogorov-Smirnov metric is an IPM with \\(\\F = \\{ \\IND_{[t, ∞)} : t \\in \\reals \\}\\).\nTotal variation metric Let \\((\\ALPHABET S, \\mathfrak S)\\) be a probability space. The total variation metric between two probability measures \\(μ\\) and \\(ν\\) on \\((\\ALPHABET S, \\mathfrak S)\\) is defined as \\[\n\\text{TV}(μ,ν) = \\sup_{B \\in \\mathfrak S} \\ABS{ μ(B) - ν(B) }.\n\\] It has the following properties:\n\n\\(\\text{TV}(μ,ν) \\in [0, 1]\\)\n\\(\\text{TV}(μ,ν) = 0\\) iff \\(μ = ν\\)\n\\(\\text{TV}(μ, ν) = 1\\) iff there exists an event \\(B \\in \\mathfrak S\\) such that \\(μ(B) = 1\\) and \\(μ(B) = 0\\).\n\\(\\text{TV}\\) is a metric\n\nWhen \\(\\ALPHABET S = \\reals^k\\) the measures \\(μ\\) and \\(ν\\) have densities \\(f\\) and \\(g\\) with respect to a measure \\(λ\\), then\n\n\\(\\text{TV}(μ,ν) = \\tfrac 12 \\int \\ABS{ f(x) - g(x) } λ(dx)\\).\n\\(\\text{TV}(μ,ν) = 1 - \\int \\min\\{ f(x), g(x) \\} λ(dx)\\).\n\\(\\text{TV}(μ,ν) = μ(B) - ν(B)\\), where \\(B = \\{ x : f(x) \\ge g(x) \\}\\).\n\nOne may view the total variation metric as the total variation norm of the signed measure \\(μ - ν\\). An implication of the first property is that total variation can be expressed as an IPM with the generator \\[\n   \\F = \\{ f : \\NORM{f}_{∞} \\le 1 \\}.\n\\]\nKantorovich-Rubinstein metric or Wasserstein distance.\nMaximum-mean discrepency.\nWeighted total variation.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Integral Probablity Metrics</span>"
    ]
  },
  {
    "objectID": "probability/IPM.html#maximal-generators",
    "href": "probability/IPM.html#maximal-generators",
    "title": "35  Integral Probablity Metrics",
    "section": "35.2 Maximal generators",
    "text": "35.2 Maximal generators\nSo far, we haven’t imposed any restriction on the generator \\(\\F\\). We now present a canonical representation of a generator.\n\nDefinition 35.2 For a function class \\(\\F \\in \\ALPHABET M_w(\\ALPHABET X)\\), define the maximal generator \\(\\ALPHABET G_{\\F}\\) as \\[\n    \\ALPHABET G_{\\F} =\n    \\biggl\\{\n      f \\in \\ALPHABET M_w(\\ALPHABET X) :\n      \\biggl| \\int f d μ_1 - \\int f d μ_2 \\biggr| \\le d_{\\F}(μ_1, μ_2), \\space\n      \\text{for all } μ_1, μ_2 \\in \\ALPHABET P_w(\\ALPHABET X)\n      \\biggr\\}.\n  \\]\n\nMaximal generators have the following properties.\n\nProposition 35.1 Let \\(\\F \\subset \\ALPHABET M_w(\\ALPHABET X)\\) be an arbitary generator. Then:\n\n\\(\\ALPHABET G_{\\F}\\) contains the convex hull of \\(\\F\\).\nIf \\(f \\in \\ALPHABET G_{\\F}\\), then \\(α f + β \\in \\ALPHABET G_{\\F}\\) for all \\(|α| \\le 1\\) and \\(β \\in \\reals\\).\nIf the sequence \\(\\{f_n\\}_{n \\ge 1} \\in \\ALPHABET G_{\\F}\\) converges uniformly to \\(f\\), then \\(f \\in \\ALPHABET G_{\\F}\\).\n\n\nMaximal generators also allow us to compare IPMs with different generators.\n\n\n\n\n\n\nBalanced set\n\n\n\nA set \\(\\ALPHABET S\\) is said to be balanced if for all scalars \\(a\\) satisfying \\(|a| \\le 1\\), we have \\(a \\ALPHABET S \\subset \\ALPHABET S\\).\n\n\n\nProposition 35.2 Let \\(\\F \\subset \\mathfrak D \\subset \\ALPHABET M_w(\\ALPHABET X)\\) and \\(μ_1, μ_2 \\in \\ALPHABET P_w(\\ALPHABET X)\\). Then\n\n\\(d_{\\F}(μ_1, μ_2) \\le d_{\\mathfrak D}(μ_1, μ_2)\\).\n\\(\\ALPHABET G_{\\F} \\subset \\ALPHABET G_{\\mathfrak D}\\).\nIf \\(\\mathfrak D \\subset \\ALPHABET G_{\\F}\\) then \\(d_{\\F}\\) and \\(d_{\\mathfrak D}\\) are identical.\nIf \\(\\F \\subset \\mathfrak D \\subset \\ALPHABET G_{\\F}\\) and \\(\\mathfrak D\\) is convex and balanced, contains the constant functions, and is closed with respect to pointwise convergence, then \\(\\mathfrak D = \\ALPHABET G_{\\F}\\).\n\n\n\nExamples\n\nKolmogorov-Smirnov metric. The maximal generator of Kolmogorov-Smirnov metric is \\[\n     \\ALPHABET G_{\\F} = \\{ f : V(f) \\le 1 \\}\n\\] where \\(V(f)\\) denotes the variation of function \\(f\\), which is defined as \\[\n     V(f) = \\sup_{P} \\sum_{k=1}^{n} \\ABS{ f(x_k) - f(x_{k-1}) }.\n\\] where the suprimum is over all partitions \\(P = \\{x_0, \\dots, x_n\\}\\) of \\(\\reals\\).\nTotal-variation metric. The maximal generator of total-variation metric is \\[\n   \\ALPHABET G_{\\F} = \\{ f : \\tfrac 12 \\SPAN(f) \\le 1 \\}\n\\]\nKantorovich-Rubinstein metric or Wasserstein distance.\nMaximum-mean discrepency.\nWeighted total variation.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Integral Probablity Metrics</span>"
    ]
  },
  {
    "objectID": "probability/IPM.html#Minkowski",
    "href": "probability/IPM.html#Minkowski",
    "title": "35  Integral Probablity Metrics",
    "section": "35.3 Minkowski functional or gauge of an IPM",
    "text": "35.3 Minkowski functional or gauge of an IPM\nWe will assume that the generator \\(\\F\\) is the maximal generator. In particular, this means that \\(0 \\in \\F\\) is an interior point. Define \\((0,∞)\\F \\coloneqq \\cup_{0 &lt; r &lt; ∞} r\\F\\), which is a subset of \\(\\ALPHABET M_w(\\ALPHABET X)\\).\nFor any function \\(f \\in \\ALPHABET M_w(\\ALPHABET X)\\) define the Minkowski functional (or gauge) as follows: \\[\n  ρ_{\\F}(f) = \\inf \\big\\{ ρ &gt; 0 : f/ρ \\in \\F \\bigr\\}.\n\\] where the infimum of an empty set is defined as infinity. Therefore, \\(ρ_{\\F}(f) &lt; ∞\\) if \\(f \\in (0,∞)\\F\\); otherwise \\(ρ_{\\F}(f) = ∞\\).\nIn all the above examples, the Minkowski functional is a semi-norm, which is a general property.\n\nProposition 35.3 The Minkowski functional is a semi-norm, i.e., it satisfies the following properties:\n\nNon-negativity. \\(ρ(f) \\ge 0\\).\nAbsolute homogeneity \\(ρ_{\\F}(a f) = |a| ρ_{\\F}(f)\\) for all scalars \\(a\\).\nSubadditivity. \\(ρ_{\\F}(f_1 + f_2) \\le ρ_{\\F}(f_1) + ρ_{\\F}(f_2)\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNon-negativity holds by definition. Absolute homogeneity holds because \\(\\F\\) is balanced, in particular \\(\\F = -\\F\\). Note that absolute homogeneity implies \\(ρ_{\\F}(0) = 0\\).\nTo see why subadditivity holds, let \\(ρ_1 = ρ_{\\F}(f_1)\\) and \\(ρ_2 = ρ_{\\F}(f_2)\\). Then, since \\(\\F\\) is convex, we have for any \\(λ \\in [0,1]\\), \\[\n  λ \\frac{f_1}{ρ_1} + (1-λ) \\frac{f_2}{ρ_2} \\in \\F.\n\\] Take \\(λ = ρ_1/(ρ_1 + ρ_2)\\), which implies \\((f_1 + f_2)/(ρ_1 + ρ_2) \\in \\F\\). Thus, \\(ρ_{\\F}(f_1 + f_2) \\le ρ_1 + ρ_2\\).\n\n\n\n\nProposition 35.4 For any \\(f \\in (0,∞)\\F\\) such that \\(ρ_{\\F}(f) \\neq 0\\) and any \\(μ_1, μ_2 \\in \\ALPHABET P_w(\\ALPHABET X)\\), we have \\[\n  \\left|\n  \\int f d μ_1 - \\int f d μ_2\n  \\right|\n  \\le\n  ρ_{\\F}(f) d_{\\F}(μ_1, μ_2).\n  \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy the assumptions imposed on \\(f\\), we know that \\(ρ_{\\F}(ρ) \\in (0,∞)\\). Define \\(f' = f/ρ_{\\F}(f) \\in \\F\\). Then, \\[\n\\left| \\int f d μ_1 - \\int f d μ_2 \\right|\n=\nρ_{\\F}(f)\n\\left| \\int f' d μ_1 - \\int f' d μ_2 \\right|\n\\le\nρ_{\\F}(f) d_{\\F}(μ_1, μ_2)\n\\] where the last inequality uses the fact that \\(f' \\in \\F\\).\n\n\n\n\n\n\n\n\n\nWhy care about maximal generators\n\n\n\nProposition 35.2 implies that the maximal generator of \\(\\F\\) is the smallest superset of \\(\\F\\) that is convex and balanced, contains the constant functions, and is closed with respect to pointwise convergence. The IPM distance wrt a generator and its maximal generator are the same but the Minkowski functionals are not! In particular if \\(\\F \\subset \\mathfrak D\\), then \\(ρ_{\\F}(f) \\ge ρ_{\\mathfrak D}(f)\\). So, working with the maximal generators gives us the tightest version of the bound in Proposition 35.4.\nFor example, we know that \\(\\{ f : \\NORM{f}_{∞} \\le 1 \\}\\) is a generator for total-variation distance. Using this, the result of Proposition 35.4 implies that \\[\\begin{equation}\\label{eq:IPM-TV-1}\n\\left| \\int f d μ_1 - \\int f d μ_2 \\right|\n\\le\n\\NORM{f}_{∞} \\text{TV}(μ_1, μ_2).\n\\end{equation}\\] Note that the left hand side is invariant to addition by a constant, but the right hand side is not. Therefore, this bound cannot be tight.\nNow consider the maximal generator for total-variation distance is \\(\\{ f : \\tfrac 12 \\SPAN{f} \\le 1 \\}\\). Using this, the result of Proposition 35.4 implies that \\[\\begin{equation}\\label{eq:IPM-TV-2}\n\\left| \\int f d μ_1 - \\int f d μ_2 \\right|\n\\le\n\\tfrac 12 \\SPAN(f)\\text{TV}(μ_1, μ_2).\n\\end{equation}\\] In this case, the right hand side is invariant to additional by a constant. Moreover, \\(\\NORM{f}_{∞} \\le \\tfrac 12 \\SPAN(f)\\) (see Exercise 35.1). Therefore, the bound of \\(\\eqref{eq:IPM-TV-2}\\) is tighter than that of \\(\\eqref{eq:IPM-TV-1}\\).\n\n\n\nExamples\n\nKolmogorov-Smirnov metric. The Minkowski functional of a function \\(f\\) for Kolmogorov-Smirnov metric is its total variation \\(V(f)\\). Thus, for any function with bounded total variation: \\[\n   \\biggl| \\int f d μ - \\int f dν \\biggr|\n   \\le \\text{KS}(μ,v) V(f).\n\\]\nTotal variation metric. The Minkowski functional of a function \\(f\\) for total variation metric is \\(\\tfrac \\SPAN(f)\\). Thus, for any function with bounded span: \\[\n   \\biggl| \\int f d μ - \\int f dν \\biggr|\n   \\le \\text{TV}(μ,v) \\tfrac 12 \\SPAN(f).\n\\]\nKantorovich-Rubinstein metric or Wasserstein distance.\nMaximum-mean discrepency.\nWeighted total variation.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Integral Probablity Metrics</span>"
    ]
  },
  {
    "objectID": "probability/IPM.html#properties-of-ipms",
    "href": "probability/IPM.html#properties-of-ipms",
    "title": "35  Integral Probablity Metrics",
    "section": "35.4 Properties of IPMs",
    "text": "35.4 Properties of IPMs\n\nDefinition 35.3 Let \\((\\ALPHABET S, d)\\) be a metric vector space, \\(w \\colon \\ALPHABET S \\to [1, ∞)\\) a weight function, and \\(d_{\\F}\\) any semi-metric on \\(\\ALPHABET P_w\\). Then, \\(d_{\\F}\\) is said to have\n\nProperty (R) if \\(d_{\\F}(δ_a, δ_b) = d(a,b)\\).\nProperty (M) if \\(d_{\\F}(aX, aY) = a d_{\\F}(X,Y)\\).\nProperty (C) if \\(d_{\\F}(P_1 * Q, P_2 * Q) \\le d_{\\F}(P_1, P_2)\\) for all probability measures \\(Q\\).\n\n\nThe next result is from Müller (1997), Theorem~4.7.\n\nSuppose \\(d_{\\F}\\) is an IPM with a maximal generator \\(\\ALPHABET G_{\\F}\\). Then\n\nProperty (R) holds if and only if \\[\n\\sup_{f \\in \\ALPHABET F}\n\\frac{ \\ABS{ f(x) - f(y) } }{ d(x,y) }\n= 1\n, \\quad\n\\text{for all } x,y \\in \\ALPHABET S, x \\neq y.\n  \\]\nProperty (C) holds if any only if \\(\\ALPHABET G_{\\F}\\) is invariant under translations.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Integral Probablity Metrics</span>"
    ]
  },
  {
    "objectID": "probability/IPM.html#exercises",
    "href": "probability/IPM.html#exercises",
    "title": "35  Integral Probablity Metrics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 35.1 Show that for any function \\(f\\): \\[\n\\tfrac 12 \\SPAN(f) \\le \\NORM{f}_{∞}.\n\\]\n\nHint: Suppose \\(f\\) is such that \\(\\max(f) = - \\min(f)\\), i.e., the function is centered at \\(0\\). Then, the inequality holds with equality. What can you say when the function is not centered at \\(0\\)?",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Integral Probablity Metrics</span>"
    ]
  },
  {
    "objectID": "probability/IPM.html#notes",
    "href": "probability/IPM.html#notes",
    "title": "35  Integral Probablity Metrics",
    "section": "Notes",
    "text": "Notes\nIPMs were first defined by Zolotarev (1984), where they were called probability metrics with \\(ζ\\)-structure. They are studied in detail in Rachev (1991). The term IPM was coined by Müller (1997). The discussion here is mostly borrowed from Müller (1997).\n\n\n\n\nMüller, A. 1997. Integral probability metrics and their generating classes of functions. Advances in Applied Probability 29, 2, 429–443. DOI: 10.2307/1428011.\n\n\nRachev, S.T. 1991. Probability metrics and the stability of stochastic models. Wiley, New York.\n\n\nZolotarev, V.M. 1984. Probability metrics. Theory of Probability & Its Applications 28, 2, 278–302. DOI: 10.1137/1128025.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Integral Probablity Metrics</span>"
    ]
  },
  {
    "objectID": "probability/markov-chains.html",
    "href": "probability/markov-chains.html",
    "title": "36  Markov chains",
    "section": "",
    "text": "36.1 Classification of states\nThe states of a time-homogeneous Markov chain can be classified as follows.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "probability/markov-chains.html#classification-of-states",
    "href": "probability/markov-chains.html#classification-of-states",
    "title": "36  Markov chains",
    "section": "",
    "text": "We say that a state \\(j\\) is accessible from \\(i\\) (abbreviated as \\(i \\rightsquigarrow j\\)) if there is exists an \\(m \\in \\integers_{\\ge 0}\\) (which may depend on \\(i\\) and \\(j\\)) such that \\([P^m]_{ij} &gt; 0\\). The fact that \\([P^m]_{ij} &gt; 0\\) implies that there exists an ordered sequence of states \\((i_0, \\dots, i_m)\\) such that \\(i_0 = i\\) and \\(i_m = j\\) such that \\(P_{i_k i_{k+1}} &gt; 0\\); thus, there is a path of positive probability from state \\(i\\) to state \\(j\\).\nAccessibility is an transitive relationship, i.e., if \\(i \\rightsquigarrow j\\) and \\(j \\rightsquigarrow k\\) implies that \\(i \\rightsquigarrow k\\).\nTwo distinct states \\(i\\) and \\(j\\) are said to communicate (abbreviated to \\(i \\leftrightsquigarrow j\\)) if \\(i\\) is accessible from \\(j\\) (i.e., \\(j \\rightsquigarrow i\\)) and \\(j\\) is accessible from \\(i\\) (\\(i \\rightsquigarrow j\\)). Alternatively, we say that \\(i\\) and \\(j\\) communicate if there exist \\(m, m' \\in \\integers_{\\ge 0}\\) such that \\([P^{m}]_{ij} &gt; 0\\) and \\([P^{m'}]_{ji} &gt; 0\\).\nCommunication is an equivalence relationship, i.e., it is reflexive (\\(i \\leftrightsquigarrow i\\)), symmetric (\\(i \\leftrightsquigarrow j\\) if and only if \\(j \\leftrightsquigarrow i\\)), and transitive (\\(i \\leftrightsquigarrow j\\) and \\(j \\leftrightsquigarrow k\\) implies \\(i \\leftrightsquigarrow k\\)).\nThe states in a finite-state Markov chain can be partitioned into two sets: recurrent states and transient states. A state is recurrent if it is accessible from all states that is accessible from it (i.e., \\(i\\) is recurrent if \\(i \\rightsquigarrow j\\) implies that \\(j \\rightsquigarrow i\\)). States that are not recurrent are transient.\nIt can be shown that a state \\(i\\) is recurrent if and only if \\[\\sum_{t=1}^{\\infty} [ P^t ]_{ii} = \\infty.\\]\nStates \\(i\\) and \\(j\\) are said to belong to the same communicating class if \\(i\\) and \\(j\\) communicate. Communicating classes form a partition the state space. Within a communicating class, all states are of the same type, i.e., either all states are recurrent (in which case the class is called a recurrent class) or all states are transient (in which case the class is called a transient class).\nA Markov chain with a single communicating class (thus, all states communicate with each other and are, therefore, recurrent) is called irreducible.\nThe period of a state \\(i\\), denoted by \\(d(i)\\), is defined as \\[d(i) = \\gcd\\{ t \\in \\integers_{\\ge 1} : [P^t]_{ii} &gt; 0 \\}.\\] If the period is \\(1\\), the state is aperiodic, and if the period is \\(2\\) or more, the state is periodic. It can be shown that all states in the same class have the same period.\nA Markov chain is aperiodic, if all states are aperiodic. A simple sufficient (but not necessary) condition for an irreducible Markov chain to be aperiodic is that there exists a state \\(i\\) such that \\(P_{ii} &gt; 0\\). In general, for a finite and aperiodic Markov chain, there exists a positive integer \\(T\\) such that \\[_{ii} &gt; 0,\n        \\quad \\forall t \\ge T, i \\in \\ALPHABET S.\\]",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "probability/markov-chains.html#limit-behavior-of-markov-chains",
    "href": "probability/markov-chains.html#limit-behavior-of-markov-chains",
    "title": "36  Markov chains",
    "section": "36.2 Limit behavior of Markov chains",
    "text": "36.2 Limit behavior of Markov chains\nWe now state some special distributions for a time-homogeneous Markov chain.\n\nA PMF \\(\\xi\\) on \\(\\ALPHABET S\\) is called a stationary distribution if \\(\\xi = \\xi P\\). Thus, if a (time-homogeneous) Markov chain starts in a stationary distribution, it stays in a stationary distribution.\nA finite irreducible Markov chain has a unique stationary distribution. Moreover, when the Markov chain is also aperiodic, the stationary distribution is given by \\(\\pi_j = 1/m_j\\), where \\(m_j\\) is the expected return time to state \\(j\\).\nA PMF \\(\\xi\\) ion \\(\\ALPHABET S\\) is called a limiting distribution if \\[\\lim_{t \\to \\infty} [ P^t]_{ij} = \\xi_j,\n        \\quad \\forall i,j \\in \\ALPHABET S.\\]\nA finite irreducible Markov chain has a limiting distribution if and only if it is aperiodic. Therefore, for an aperiodic Markov chain, the limiting distribution is the same as the stationary distribution.\n\nThe limiting behavior of irreducible Markov chains can be characterized even when they are periodic.\n\nTheorem 36.1 Let \\(P\\) be irreducile. Then, there is an integer \\(d \\ge 1\\) (called the period of \\(P\\)) and a partition \\[\\ALPHABET S = \\ALPHABET C_0 \\cup \\ALPHABET C_1 \\cup \\cdots \\cup \\ALPHABET C_{d-1}\\] such that for every \\(r \\in \\{0, \\dots, d-1\\}\\),\n\nFor \\(i \\in C_r\\), \\([P^m]_{ij} &gt; 0\\) only if \\(j \\in C_{(r + m) \\bmod d}\\).\nFor sufficiently large \\(k\\), \\([P^{kd}]_{ij} &gt; 0\\) for all \\(i,j \\in C_r\\).\n\nFurthermore, suppose the initial distribution \\(\\xi_0\\) is such that \\(\\xi_0(\\ALPHABET C_0) = 1\\) (i.e., the Markov chain starts in cell \\(\\ALPHABET C_0\\)). Then, for any \\(r \\in \\{0,\\dots, d-1\\}\\) and \\(j \\in \\ALPHABET C_r\\), we have \\[\\label{eq:limit-of-P}\n        \\lim_{k \\to \\infty} [P^{(kd + r)}]_{ij} = \\frac{d}{m_j}.\\]\n\nThe following result describes the sample path behavior of the Markov chain.\n\nTheorem 36.2 (Strong law of large numbers for Markov chains) Suppose \\(P\\) is an irreducible Markov chain that starts in state \\(i \\in \\ALPHABET S\\). Then, \\[\\label{eq:SLLN}\n        \\lim_{T \\to \\infty} \\frac 1T \\sum_{t=0}^{T-1} \\IND \\{ S_t = j \\} = \\frac 1{m_j}.\\] Therefore, for any function \\(h \\colon \\ALPHABET S \\to \\reals\\), \\[\\label{eq:ergodic}\n        \\lim_{T \\to \\infty} \\frac 1T \\sum_{t=0}^{T-1} h(S_t) = \\sum_{j \\in \\ALPHABET S} \\frac {h(j)}{m_j}.\\]",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "probability/markov-chains.html#exercises",
    "href": "probability/markov-chains.html#exercises",
    "title": "36  Markov chains",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 36.1 Let \\(\\{X_t\\}_{t \\ge 1}\\) be a Markov chain and \\(\\ALPHABET A\\) and \\(\\ALPHABET B\\) be subsets of the state space.\n\nIs it true that \\(\\PR(X_2 \\in \\ALPHABET B \\mid X_1 = x_1, X_0 \\in \\ALPHABET A) = \\PR(X_2 \\in \\ALPHABET B \\mid X_1 = x_1)\\)?\nIs it true that \\(\\PR(X_2 \\in \\ALPHABET B \\mid X_1 \\in \\ALPHABET A, X_0 = x_0) = \\PR(X_2 \\in \\ALPHABET B \\mid X_1 \\in \\ALPHABET A)\\)?\n\nIn each case, either prove the result or provide a counterexample.\n\n\nExercise 36.2 Consider a Markov chain where the state space \\(\\ALPHABET X\\) is a symmetric subset of integers of the form \\(\\{-L, -L + 1, \\dots, L-1, L\\}\\) Let \\(\\ALPHABET X_{\\ge 0}\\) denote the set \\(\\{0, \\dots, L\\}\\).\nWhat are the conditions on the tranisition matrix \\(P\\) such that the absolute values of the state \\(Z_t = |X_t|\\), \\(Z_t \\in \\ALPHABET X_{\\ge 0}\\), to be a Markov chain?\nRemark: See Exercise 8.8 for a generalization of the idea to MDPs.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "probability/markov-chains.html#notes",
    "href": "probability/markov-chains.html#notes",
    "title": "36  Markov chains",
    "section": "Notes",
    "text": "Notes\nTheorem 36.1 is from (Norris 1998, Theorems 1.8.5 and 1.8.5). Theorem 36.2 goes under different names and I found it hard to find easily accessible references. One such reference is (Durrett 2019, Theorem 5.6.1)\n\n\n\n\nDurrett, R. 2019. Probability: Theory and examples. Cambridge University Press. DOI: 10.1017/9781108591034.\n\n\nNorris, J.R. 1998. Markov chains. Cambridge university press.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html",
    "href": "probability/martingales.html",
    "title": "37  Martingales",
    "section": "",
    "text": "37.1 Examples and Immediate Properties",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html#examples-and-immediate-properties",
    "href": "probability/martingales.html#examples-and-immediate-properties",
    "title": "37  Martingales",
    "section": "",
    "text": "Martingales generalize the theory of sums of independent random variables. Let \\(ξ_1, ξ_2, \\dots\\) be indepdent, integrable random variables. Define \\(X_0 = 0\\) and \\(X_t = ξ_1 + \\dots + ξ_t\\). If \\(\\EXP[X_t] = 0\\) for \\(t \\ge 1\\), then the sequence \\(\\{X_t\\}_{t \\ge 1}\\) is a martingale with respect to the natural filtration: \\[\n\\EXP[X_{t+1} \\mid X_{1:t}] = \\EXP[X_t + ξ_{t+1} \\mid X_{1:t}] = X_t.\n\\]\nSimilarly, if \\(ξ_t\\) has positive mean, \\(\\{X_t\\}_{t \\ge 1}\\) is a submartingale, and if \\(ξ_t\\) has negative mean, \\(\\{X_t\\}_{t \\ge 1}\\) is a supermartingale.\nLet \\(\\{X_t\\}_{t \\ge 1}\\) be martingale and let \\(f\\) be a convex function for which each \\(f(X_t)\\) is integrable. Then, a direct application of Jensen’s inequality implies that \\(\\{f(X_t)\\}_{t \\ge 1}\\) is a submartingale.\nA consequence of the previous result is the following. Let \\(\\{X_t\\}_{t \\ge 1}\\) be a martingale and \\(p \\ge 1\\) be such that \\(|X_t|^p\\) is integrable for all \\(t\\). Then \\(\\{|X_t|^p\\}_{t \\ge 1}\\) is a submartingale.\nLet \\(\\{X_t\\}_{t \\ge 1}\\) be submartingale and let \\(f\\) be a convex and increasing function for which each \\(f(X_t)\\) is integrable. Then, \\(\\{f(X_t)\\}_{t \\ge 1}\\) is a submartingale because \\[\n   \\EXP[ f(X_{t+1}) \\mid \\mathcal F_t]\n   \\ge\n   f( \\EXP[ X_{t+1} \\mid \\mathcal F_t ])\n   \\ge\n   f(X_t)\n\\] where the first inequality follows from Jensen’s inequality and the second from the fact that \\(\\{X_t\\}_{t \\ge 1}\\) is a sub-martingale and \\(f\\) is increasing.\nA consequence of the previous result is the following. Let \\(\\{X_t\\}_{t \\ge 1}\\) be a submartingale. Then, for any constant \\(a\\), \\(\\{(X_t - a)^+\\}_{t \\ge 1}\\) is also a submartingale.\nIf \\(\\{X_t\\}_{t \\ge 1}\\) and \\(\\{Y_t\\}_{t \\ge 1}\\) are martingales defined on the same family of \\(σ\\)-algebras, then \\(\\{a X_t + b Y_t\\}_{t \\ge 1}\\) is a martingale.\nThe previous result holds for supermartingales provided \\(a\\) and \\(b\\) are positive.\nIf \\(\\{X_t\\}_{t \\ge 1}\\) and \\(\\{Y_t\\}_{t \\ge 1}\\) are supermartingales defined on the same family of \\(σ\\)-algebras, then \\(\\{X_t \\wedge Y_t\\}_{t \\ge 1}\\) is a supermartingale.\nSuppose \\(X\\) is an integrable random variable and \\(\\{\\mathcal F_t\\}_{t \\ge 1}\\) is a filtration. Define \\(X_t \\coloneqq \\EXP[X \\mid \\mathcal F_t]\\). Then \\(\\{X_t\\}_{t \\ge 1}\\) is a martingale with respect to the filtration (follows from the smoothing property of conditional expectations).\nLikelihood ratios are martingales. Suppose \\(X_1\\), \\(X_2\\), are independent random variables with density \\(f\\). Imagine we are considering the alternative hypothesis that these random variables are independent with a different probability density \\(g\\) (but they are really distributed according to the density \\(f\\)). We assume that \\(f\\) and \\(g\\) have the same support. Define the likelihood ratio \\[\n  Λ_t = \\frac{g(X_1)}{f(X_1)} \\dots \\frac{g(X_t)}{f(X_t)}.\n   \\] Then, since \\(Λ_{t+1} = Λ_t g(X_{t+1})/f(X_{t+1})\\), we have \\[\\begin{align*}\n   \\EXP[Λ_{t+1} \\mid X_{1:t}]\n   &= Λ_t \\EXP\\biggl[ \\frac{g(X_{t+1})}{f(X_{t+1})} \\biggm| X_{1:t} \\biggr] \\\\\n   &= Λ_t \\EXP\\biggl[ \\frac{g(X_{t+1})}{f(X_{t+1})} \\biggr] \\\\\n   &= Λ_t \\int\\biggl( \\frac{g(x)}{f(x)} \\biggr) f(x)dx \\\\\n   &= Λ_t \\int g(x)dx \\\\\n   &= Λ_t.\n  \\end{align*}\\] Hence, \\(\\{Λ_t\\}_{t \\ge 1}\\) is a martingale.\nA martingale \\(\\{X_t\\}_{t \\ge 1}\\) may be written as a sum of increments: \\(X_t = X_0 + ξ_1 + \\dots + ξ_t\\). The increments \\(\\{ξ_t\\}_{t \\ge 1}\\) are called martingale differences. Each \\(ξ_t\\) is integrable and \\(\\EXP[ξ_t \\mid \\mathcal F_{t-1}] = 0\\) for all \\(t\\).",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html#doobs-decomposition",
    "href": "probability/martingales.html#doobs-decomposition",
    "title": "37  Martingales",
    "section": "37.2 Doob’s decomposition",
    "text": "37.2 Doob’s decomposition\n\nTheorem 37.1 (Doob’s decomposition) Every sequence \\(\\{X_t\\}_{t \\ge 1}\\) of integrable random variables adapted to a filtration \\(\\{\\mathcal F_t\\}_{t \\ge 1}\\) can be decomposed into a sum of a martingale and an integrable predictable process, i.e., \\[\n    X_t = M_t + A_t\n  \\] where \\(\\{M_t\\}_{t \\ge 1}\\) is a martingale and \\(\\{A_t\\}_{t \\ge 1}\\) is a predictable process\n\n\nExistence. We can identify the Doob’s decomposition by defining \\[ M_t = X_0 + \\sum_{s=1}^{t}\\bigl( X_s - \\EXP[ X_s \\mid \\mathcal F_{s-1} \\bigr],\\] and \\[A_t = \\sum_{s=1}^t \\bigl( \\EXP[ X_s \\mid \\mathcal F_{s-1} ] - X_{s-1} \\bigr).\\]\nUniqueness. The decomposition is almost surely unique. If \\(X_t = M'_t + A'_t\\) is another decomposition, then \\(Y_t = M_t - M'_t\\) being the difference of martingales is a martingale. But \\(M_t - M'_t = A_t - A'_t\\), which is a predictable process; combining this with the martingale property of \\(M_t - M'_t\\) implies that \\[\nA_t - A'_t = \\EXP[ A_t - A'_t \\mid \\mathcal F_t] =\n\\EXP[ M_t - M'_t \\mid \\mathcal F_t ] = M_{t-1} - M'_{t-1} = A_{t-1} - A'_{t-1}.\n\\] Since \\(A_0 = A'_0 = 0\\), we get that \\(A_t = A'_t\\) for all \\(t \\ge 1\\).\nA stochastic process \\(\\{X_t\\}_{t \\ge 1}\\) is a submartingale if and only if its Doob’s decomposition gives a predictable process that is almost surely increasing.\nA stochastic process \\(\\{X_t\\}_{t \\ge 1}\\) is a supermartingale if and only if its Doob’s decomposition gives a predictable process that is almost surely decreasing",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html#martingale-inequalities",
    "href": "probability/martingales.html#martingale-inequalities",
    "title": "37  Martingales",
    "section": "37.3 Martingale Inequalities",
    "text": "37.3 Martingale Inequalities\n\nBurkholder’s inequality\nLet \\(\\{X_t\\}_{t \\ge 1}\\) be a martingale and let \\(\\xi_1 = X_1\\) and \\(\\xi_t = X_t - X_{t-1}\\) be the corresponding martingale difference sequence. The quadratic variation process is defined as \\[\n  Q_t \\coloneqq \\sum_{s = 1}^t \\NORM{ξ_s}^2.\n\\]\n\nProposition 37.1 (Burkholder’s inequality) For any \\(p &gt; 1\\) be such that \\(\\EXP[|X_t|^p] &lt; ∞\\) for all \\(t\\), there exists positive constants \\(c_p\\) and \\(C_p\\) (depending on \\(p\\) alone) such that \\[\n  c_p \\EXP\\bigl[ Q_t^{p/2} \\bigr]\n  \\le\n  \\EXP\\Bigl[ \\sup_{1 \\le s \\le t} \\NORM{X_s}^p \\Bigr]\n  \\le\n  C_p \\EXP\\bigl[ Q_t^{p/2} \\bigr]\n\\]\n\nThe inequality is due to Burkholder (1966). See notes of David Pollard for an accessible proof based on martingale ideas and Bogdan and Więcek (2022) for an elementary proof using Bergman divergence.\n\n\nLenglart inequality\n\nProposition 37.2 (Lenglart inequality) Given a filtration \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\), let \\(\\{X_t\\}_{t \\ge 1}\\) be a non-negative adapted process and \\(\\{Y_t\\}_{t \\ge 1}\\) be non-negative non-decreasing predictable process such that for any bounded stopping time \\(τ\\), we have \\(\\EXP[X_{τ} \\mid \\ALPHABET F_0] \\le \\EXP[ Y_{τ} \\mid \\ALPHABET F_0]\\). Then:\n\nFor all \\(p \\in (0,1)\\), \\[\n\\EXP\\biggl[ \\Bigl( \\sup_{t \\ge 1} X_t \\Bigr)^p \\Bigm| \\ALPHABET F_0 \\biggr]\n\\le\nC_p\n\\EXP\\biggl[ \\Bigl( \\sup_{t \\ge 1} Y_t \\Bigr)^p \\Bigm| \\ALPHABET F_0 \\biggr],\n\\] where \\(C_p = p^{-p}/(1-p)\\) and the constant \\(C_p\\) is sharp.\nIn addition, if \\(\\{X_t\\}_{t \\ge 1}\\) is non-decreasing, then we have \\[\n\\EXP\\biggl[ \\Bigl( \\sup_{t \\ge 1} X_t \\Bigr)^p \\Bigm| \\ALPHABET F_0 \\biggr]\n\\le\np^{-p}\n\\EXP\\biggl[ \\Bigl( \\sup_{t \\ge 1} Y_t \\Bigr)^p \\Bigm| \\ALPHABET F_0 \\biggr],\n\\] and the constant \\(p^{-p}\\) is sharp.\nMoreover, for any \\(c,d &gt; 0\\), we have \\[\n\\PR\\Bigl( \\sup_{t \\ge 1} X_t &gt; c \\Bigm| \\ALPHABET F_0 \\Bigr)\n\\le\n\\frac 1c\n\\EXP\\Bigl[ \\sup_{t \\ge 1} Y_t \\wedge d \\Bigm| \\ALPHABET F_0 \\Bigr]\n+\n\\PR\\Bigl( \\sup_{t \\ge 1} Y_t &gt; d \\Bigm| \\ALPHABET F_0 \\Bigr)\n\\]\n\n\nThe result is due to Lenglart (1977). See Geiss and Scheutzow (2021) for the sharpness of the bounds.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html#convergence-of-nonnegative-supermartingales",
    "href": "probability/martingales.html#convergence-of-nonnegative-supermartingales",
    "title": "37  Martingales",
    "section": "37.4 Convergence of nonnegative supermartingales",
    "text": "37.4 Convergence of nonnegative supermartingales\nWe state certain properties of nonnegative supermartingales without proof. This material is taken from Neveu (1975).\n\nMaximal inequality for nonnegative supermartingales. For every nonnegative supermartingale \\(\\{X_t\\}_{t \\ge 1}\\), the random variable \\(\\sup_{t \\ge 1} X_t\\) is a.s. finite on the set \\(\\{X_0 &lt; ∞\\}\\) and, more precisely, satisfies the following inequality: \\[\n  \\PR\\Bigl( \\sup_{t \\ge 1} X_t \\ge a \\Bigm| \\mathcal F_t \\Bigr)\n  \\le\n  \\max\\biggl( \\frac{X_0}{a}, 1 \\biggr)\n\\]\nDoob’s inequalities. Let \\(\\{X_t\\}_{t \\ge 1}\\) be a generalized martingale or a nonnegative submartingale. Then for all \\(p \\in (1,∞)\\), and all \\(t\\), we have: \\[\n\\EXP[\\ABS{X_t}^p] \\le\n\\EXP\\Bigl[ \\sup_{1 \\le s \\le t} \\ABS{X_s}^p \\Bigr]\n\\le\n\\biggl(\\frac{p}{p-1}\\biggr)^p\n\\EXP[\\ABS{X_t}^p].\n  \\]\nSwitching principle for supermartingales. Given two nonnegative supermartingales \\(\\{X^{(i)}_t\\}_{t \\ge 1}\\), \\(i \\in \\{1, 2\\}\\), and a stopping time \\(τ\\) such that \\(X^{(1)}_{τ} \\ge X^{(2)}_{τ}\\) on \\(\\{τ &lt; ∞\\}\\), the formula \\[\nX_t(ω) = \\begin{cases}\nX^{(1)}_t(ω), & \\text{if $t &lt; τ(ω)$} \\\\\nX^{(2)}_t(ω), & \\text{if $t \\ge τ(ω)$}\n\\end{cases}\n\\] defines a new nonnegative supermartingale.\nSupermartingale convergence theorem. Every nonnegative supermartingale \\(\\{X_t\\}_{t \\ge 1}\\) converges almost surely. Furthermore, the limit \\(X_{∞} = \\lim_{t \\to ∞} X_t\\) satisfies the inequality \\[\n  \\EXP[ X_{∞} \\mid \\mathcal F_t] \\le X_t.\n\\]\nA nonnegative supermartingale converges in \\(\\mathcal L^1\\) to its limit \\(X_{∞}\\) if and only if \\(\\EXP[X_t] \\to \\EXP[X_{∞}]\\).\nFor a nonnegative supermartingale \\(\\{X_t\\}_{t \\ge 1}\\) and a stopping time \\(τ\\), the sequence \\(\\{X_{τ \\wedge t}\\}_{t \\ge 1}\\) “stopped at the stopping time \\(τ\\)” is also a non-negative supermartingale.\nStopping theorem Let \\(\\{X_t\\}_{t \\ge 1}\\) be a nonnegative supermartingale. Then, for any bounded stopping times \\(τ_1\\) and \\(τ_2\\), we have \\[\nX_{τ_1} \\ge \\EXP[ X_{τ_2} \\mid \\mathcal F_{τ_1}],\n\\quad \\text{ on } \\{τ_1 \\le τ_2 \\}.\n\\]\n\n\n\n\n\n\n\nInterpretation of supermartingales\n\n\n\n\n\nThe following interpretation is taken from Dellacherie and Meyer (1982).\nSuppose the random variable \\(X_t\\) represents a gambler’s fortune at time \\(t\\). Then his successive gains are presented by the random variable \\(ξ_t = X_t - X_{t-1}\\). The gambler may be in an arbitrarily complicated casino, where he may choose between all sorts of games, move from one table to another, bet on other player’s fortunes, etc., but it is understood that his decisions are unprophetic, i.e., they can only be taken as functions for the past and not as functions of the future, with the convention that the present, i.e., the game which is in the process of being played, forms part of the future.\nThe supermartingale inequality \\(\\EXP[ξ_t \\mid \\ALPHABET F_t] \\le 0\\) means that, whatever decisions are taken by the gambler just before the \\(t\\)-th game, the average profits from that will be negative. In other words, the game favors the casino—which is what happens in reality! (The martingale equality corresponds to the case of an equitable casino)\nNow imagine that the gambler, fearing that he is under the influence of an evil star, confides his fortune to a “luckier” (but still unprophetic) friend and goes out for fresh air and returns at random times \\(τ_1 &lt;  τ_2 &lt;  \\dots\\). The stopping theorem says that what he observes at these random instances is also a game favorable to the casino (or merely equitable in case of martingales). In other words, things are no better.\nThe restriction on the length of the stopping time has the following meaning: suppose the gambler tells his friend to “call me at the first moment \\(τ_1\\) when my gain \\(X_{τ_1} - X_0\\) is positive. Then call me again at time \\(τ_2\\) when my gain \\(X_{τ_2} - X_{τ_1}\\) is positive. And so on” At such moments, mean gain is also positive and hence stopping theorem seems to be contradicted; however, the stopping theorem affirms that the stopping times \\(τ_1\\), \\(τ_2\\), etc., are not finite.\n\n\n\nThe converges result continues to hold for “almost” supermartingales.\n\nTheorem 37.2 (Almost supermartingale convergence theorem) Suppose \\(\\{X_t\\}_{t \\ge 1}\\), \\(\\{β_t\\}_{t \\ge 1}\\), \\(\\{Y_t\\}_{t \\ge 1}\\), \\(\\{Z_t\\}_{t \\ge 1}\\) are nonnegative stochastic processes adapted to some filtration \\(\\{\\mathcal F_t\\}_{t \\ge 1}\\) that satisfy \\[\\begin{equation}\\label{eq:almost-supermartingale}\n  \\EXP[X_{t+1} \\mid \\mathcal F_t ] \\le\n  (1 + β_t) X_t + Y_t - Z_t,\n  \\quad t \\ge 1\n\\end{equation}\\] Define the set \\(Ω_0\\) by \\[\n  Ω_0 = \\biggl\\{ ω : \\sum_{t \\ge 1} β_t(ω) &lt; ∞ \\biggr\\}\n  \\cap\n  \\biggl\\{ ω : \\sum_{t \\ge 1} Y_t(ω) &lt; ∞ \\biggr\\}.\n\\] Then, for all \\(ω \\in Ω_0\\), we have that\n\n\\(\\lim_{t \\to ∞} X_t(ω)\\) exists and is finite\n\\(\\sum_{t \\ge 1} Z_t(ω) &lt; ∞\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe key idea is that we can fiddle with \\eqref{eq:almost-supermartingale} to make it a supermartingale. Let \\(b_t = 1/\\prod_{\\tau = 1}^{t-1} ( 1 + β_{τ} )\\). Note that \\(\\{b_t\\}_{t \\ge 1}\\) is \\(\\{\\mathcal F_t\\}_{t \\ge 1}\\) predictable. Define the nonnegative processes \\(\\{X'_t\\}_{t \\ge 1}\\) \\(\\{Y'_t\\}_{t \\ge 1}\\) and \\(\\{Z'_t\\}_{t \\ge 1}\\), where \\[\nX'_t = b_t X_t,\n\\quad\nY'_t = b_{t+1} Y_t,\n\\quad\nZ'_t = b_{t+1} Z_t.\n\\] \\[\\begin{align}\n    \\EXP[X'_{t+1} \\mid \\mathcal F_t] &=\n    b_{t+1} \\EXP[ X_{t+1} \\mid \\mathcal F_t] \\notag \\\\\n    &\\le b_{t+1}(1 + β_t) X_t + b_{t+1} Y_t  - b_{t+1} Z_t \\notag \\\\\n    &= X'_t + Y'_t - Z'_t.\n    \\label{eq:almost-supermartingale-st-1}\n\\end{align}\\]\nDefine \\[\\begin{equation}\\label{eq:almost-supermartingale-st-2}\nW_t = X'_t - \\sum_{s = 1}^{t-1} (Y'_s - Z'_s) .\n\\end{equation}\\] From \\eqref{eq:almost-supermartingale-st-1} we have \\[\n\\EXP[W_{t+1} \\mid \\mathcal F_t] =\n\\EXP\\biggl[ X'_{t+1} - \\sum_{s=1}^t(Y'_s - Z'_s) \\biggm| \\mathcal F_t \\biggr]\n\\le X_t - \\sum_{s=1}^{t-1} (Y'_s - Z'_s)\n= W_t.\n\\] Therefore, \\(\\{W_t\\}_{t \\ge 1}\\) is a supermartingale, but we cannot immediately apply the supermartingale convergence theorem because we don’t know that \\(W_t\\) is bounded from below.\nFor an arbitrary \\(a &gt; 0\\), define the stopping time \\(τ_a = \\inf \\bigl\\{ t : \\sum_{s=1}^{t} Y'_s &gt; a \\bigr\\}\\). Now define the stopped sequence \\[W^{(a)}_t := W_{τ_a \\wedge t} =\n\\begin{cases}\n  W_t, & \\hbox{if } t \\le τ_a \\\\\n  W_{τ_a}, & \\hbox{if } t &gt; τ_a\n\\end{cases},\n\\quad t \\ge 1.\\] Note that the stopped sequence satisfies: \\[\n\\EXP[W^{(a)}_{t+1} \\mid \\ALPHABET F_t] \\stackrel{(a)}=\n\\begin{cases}\n  \\EXP[W_{t+1} \\mid \\ALPHABET F_t] , & \\hbox{if } t+1 \\le τ_a \\\\\n  W_{τ_a}, & \\hbox{if } t + 1 &gt; τ_a\n\\end{cases}\n\\stackrel{(b)}\\le\n\\begin{cases}\n  W_t, & \\hbox{if } t &lt; τ_a \\\\\n  W_{τ_a}, & \\hbox{if } t \\ge τ_a\n\\end{cases}\n\\stackrel{(c)}= W^{(a)}_t,\n\\] where \\((a)\\) uses the definition of \\(W^{(a)}_t\\) and the fact that \\(\\EXP[W_{τ_a} \\mid \\ALPHABET F_t] = W_{τ_a}\\) since \\(τ_a \\le t\\); \\((b)\\) uses the fact that \\(\\{W_t\\}_{t \\ge 1}\\) is a supermartingale; and \\((c)\\) uses the fact that the two terms coincide when \\(t = τ_a\\).\nThus, the stopped sequence \\(\\{W^{(a)}_{t}\\}_{t \\ge 1}\\) is a supermartingale. Moreover, it is bounded from below since \\[\nW^{(a)}_t = W_{t \\wedge τ} \\ge - \\sum_{s = 1}^{τ \\wedge (t-1)} Y_s\n\\ge -a,\\] So, we can think of \\(W^{(a)}_{t} + a\\) as a nonnegative supermartingale. Therefore, by the supermartingale convergence theorem, for any given \\(a\\), \\(\\lim_{t \\to ∞} W^{(a)}_{t}\\) exists and is finite a.s.\nNow define \\(Ω^{(a)}\\) to be the set \\(\\{ ω : \\sum_{t \\ge 1} Y_t(ω) &lt; a \\}\\). We know that \\(Y'_t = b_{t+1} Y_t &lt; Y_t\\). So, on the set \\(Ω^{(a)}\\), \\(\\sum_{t \\ge 1} Y'_t(ω) &lt; a\\). Hence, for all \\(ω \\in Ω^{(a)}\\), we have \\(τ_{a}(ω) = ∞\\). Therefore, \\(W_t = W^{(a)}_t\\) and, by the previous argument, \\(\\{W_t\\}_{t \\ge 1}\\) converges to a finite limit.\nBy countable additivity, we get that \\(W_t\\) converges to a finite limit for all \\(a \\in \\integers\\).\nSide remark: It took me a while to understand why this is the case, so I am including a complete formal argument.\n\nFix an \\(a\\). We know that on \\(Ω^{(a)}\\), \\(W_t\\) has a finite limit. Denote that limit by \\(W_{∞}\\).\nFor any \\(ε &gt; 0\\), define the event \\(\\ALPHABET E = \\cap_{s = 1}^∞ \\cup_{t = s}^∞ |W_t(ω) - W_{∞}(ω)| &gt; ε\\).\nThe fact that \\(W_t\\) converges almost surely to \\(W_{∞}\\) on \\(Ω^{(a)}\\) means that \\(\\PR(Ω^{(a)} \\cap \\ALPHABET E) = 0\\).\nWe now restrict attention to integer values of \\(a\\). Since \\(Ω^{(a)} \\cap \\ALPHABET E\\) is an increasing sequence, we have \\[\n\\PR(Ω_0 \\cap \\ALPHABET E)\n=\n\\PR(\\lim_{a \\to ∞} Ω^{(a)} \\cap \\ALPHABET E)\n=\n\\lim_{a \\to ∞} \\PR(Ω^{(a)} \\cap \\ALPHABET E)\n=\n0.\n\\] Hence, \\(W_t\\) converges to \\(W_{∞}\\) almost surely on \\(Ω_0\\).\n\n\nNow \\eqref{eq:almost-supermartingale-st-2} implies that \\[\n  \\lim_{t \\to ∞} \\biggl \\{ X'_t - \\sum_{s=1}^{t-1} (Y'_s - Z'_s) \\biggr\\} &lt; ∞.\n\\] Hence, on \\(Ω_0\\), \\(\\sum_{s=1}^∞ Z'_s &lt; ∞\\) and \\(\\lim_{t \\to ∞} X'_t\\) exists and is finite.\nFrom Exercise 37.1, it follows that for \\(ω \\in Ω_0\\), \\(b_t(ω)\\) has a limit. \\[\n    X_t = X'_t/b'_t\n\\] has a limit. Moreover, the inequality, \\[\n  Z_t \\le Z'_t \\prod_{s \\ge 1} (1 + β_s)\n\\] implies that \\(\\sum_{t \\ge 1} Z_t &lt; ∞\\) for \\(ω \\in Ω_0\\).\n\n\n\nThe following is a determinisitic version of the above result, taken from Bertsekas and Tsitsiklis (2000).\n\nProposition 37.3 Let \\(\\{X_t\\}_{t \\ge 1}\\), \\(\\{Y_t\\}_{t \\ge 1}\\), and \\(\\{Z_t\\}_{t \\ge 1}\\) be sequences such that \\(\\{Z_t\\}_{t \\ge 1}\\) is nonnegative and the sequences satisfy: \\[\n  X_{t+1} \\le X_t + Y_t - Z_t\n\\] and \\(\\sum_{t \\ge 1} Y_t &lt; ∞\\). Then, either \\(X_t \\to -∞\\) or else \\(X_t\\) converges to a finite value and \\(\\sum_{t \\ge 1} Z_t &lt; ∞\\).",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html#convergence-of-submartingales",
    "href": "probability/martingales.html#convergence-of-submartingales",
    "title": "37  Martingales",
    "section": "37.5 Convergence of submartingales",
    "text": "37.5 Convergence of submartingales\n\nTheorem 37.3 (Krickeberg decomposition) Let \\(\\{S_t\\}_{t \\ge 1}\\) be a submartingale for which \\(\\sup_t \\EXP[S_t^{+}] &lt; ∞\\). Then, there exists a positive martingale \\(\\{M_t\\}_{t \\ge 1}\\) and a positive supermartingale \\(\\{X_t\\}_{t \\ge 1}\\) such that \\(S_t = M_t - X_t\\) almost sure for each \\(t\\).\n\n\n\n\n\n\n\nRemark\n\n\n\nThe finiteness of \\(\\sup_{t} \\EXP[ S_{t}^{+}]\\) is equivalent to the finiteness of \\(\\sup_{t} \\EXP[|S_t|]\\) because \\(|S_t| = 2S_t{+} - (S_t^{+} - S_t^{-})\\) and by the submartingale property, \\(\\EXP[S_t^{+} - S_t^{-}] = \\EXP[S_t]\\) increases with \\(t\\).\n\n\n\nCorollary 37.1 A submartingale with \\(\\sup_{t} \\EXP[S_t^{+}] &lt; ∞\\) converges almost surely to an integrable limit.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html#square-integrable-martingales",
    "href": "probability/martingales.html#square-integrable-martingales",
    "title": "37  Martingales",
    "section": "37.6 Square integrable martingales",
    "text": "37.6 Square integrable martingales\n\nIf \\(\\{X_t\\}_{t \\ge 1}\\) is an integrable sequence adapted to \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\). Define the compensator \\(\\{ \\langle X_t \\rangle\\}_{t \\ge 1}\\) as: \\(\\langle X_1 \\rangle = 0\\) and for \\(t &gt; 1\\): \\[\n  \\langle X_t \\rangle - \\langle X_{t-1} \\rangle\n  =\n\\EXP[ X_t \\mid \\ALPHABET F_{t-1} ] - X_{t-1}.\n\\] Note that the compensator is predictable and \\(X_t - \\langle X_t \\rangle\\) is a martingale.\nNow consder a square integrable martingle \\(\\{X_t\\}_{t \\ge 1}\\). The compensator is given by: \\[\n\\langle X_t^2 \\rangle - \\langle X_{t-1}^2 \\rangle\n=\n\\EXP[ X_t^2 - X_{t-1}^2 \\mid \\ALPHABET F_{t-1} ]\n=\n\\EXP[ (X_t - X_{t-1})^2 \\mid \\ALPHABET F_{t-1} ].\n  \\] Thus, the compensator \\(\\langle X_t^2 \\rangle\\) is increasing.\nConvergence of square integrable martinales. Let \\(\\langle X_{∞} \\rangle = \\lim_{t \\to ∞} \\langle X_t \\rangle\\). If \\(\\EXP[ \\langle X_{∞} \\rangle ] &lt; ∞\\), then \\(\\{X_t\\}_{t \\ge 1}\\) converges almost surely in the mean-square sense to a random variable \\(X_{∞}\\).\nLet \\(\\{X_t\\}_{t \\ge 1}\\) be a square integrable martingale adapted to \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\). Let \\(τ\\) be a stopping time such that \\(\\EXP[ \\langle X_{τ} \\rangle ] &lt; ∞\\). Then,\n\n\\(\\EXP[X_{τ} \\mid \\ALPHABET F_1] = X_1\\).\n\\(\\EXP[X_{τ}^2 \\mid \\ALPHABET F_1 ] = \\EXP[ \\langle M_{τ} \\rangle \\mid \\ALPHABET F_1]\\).",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html#strong-law-of-large-number-for-martingale-difference-sequence",
    "href": "probability/martingales.html#strong-law-of-large-number-for-martingale-difference-sequence",
    "title": "37  Martingales",
    "section": "37.7 Strong law of large number for martingale difference sequence",
    "text": "37.7 Strong law of large number for martingale difference sequence\nThe following is a generalization of the strong law of large numbers for martingale differences. See Stout (1974) (Theorem 3.3.1).\n\nTheorem 37.4 Suppose \\(\\{M_t\\}_{t \\ge 1}\\) is a martingale difference sequence with respect to the filtration \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\). Let \\(\\{a_t\\}_{t \\ge 1}\\) be positive sequence that is \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\) predictable and \\(\\lim_{t \\to ∞} a_t = ∞\\).\nIf for some \\(p \\in (0,2]\\), we have \\[\n  \\sum_{t=1}^∞ \\EXP \\biggl[ \\frac{|M_t|^p}{a_t^p} \\biggm|\n  \\ALPHABET F_{t-1} \\biggr] &lt; ∞,\n\\] then \\[\n  \\sum_{t=1}^∞ \\frac{M_t}{a_t} = 0, \\quad a.s.\n\\]",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html#exercises",
    "href": "probability/martingales.html#exercises",
    "title": "37  Martingales",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 37.1 The purpose of this exercise is to establish the following: For a sequence of nonnegative real numbers \\(\\{a_t\\}_{t \\ge 1}\\) \\[\\sum_{t \\ge 1} a_t &lt; ∞  \\iff\n\\prod_{t \\ge 1} (1 + a_t) \\text{ converges to a finite non-zero limit}.\n\\]\n\nShow that \\(\\sum_{t \\ge 1}a_t \\le \\prod_{t \\ge 1}(1 + a_t)\\).\nHint: Expand the right hand side.\nShow that \\(\\prod_{t \\ge 1}(1 + a_t) \\le \\exp\\Bigl(\\sum_{t \\ge 1} a_t\\Bigr)\\).\nHint: Use Taylor series expansion of \\(e^x\\).\n\n\n\nExercise 37.2 (Kronecker’s Lemma) This result is useful intermediate step for showing convergence of various iterative algorithms.\nSuppose \\(\\{a_t\\}_{t \\ge 1}\\) is a strictly positive increasing real sequence which diverges to \\(∞\\) and \\(\\{x_t\\}_{t \\ge 1}\\) is a real sequence such that the series \\(\\sum_{t=1}^∞ x_t/a_t\\) converges. Then, \\[\n  \\lim_{T \\to ∞} \\frac 1{a_T} \\sum_{t=1}^T x_t = 0.\n\\]\n\n\nExercise 37.3 Consider a sequence \\(\\{α_t\\}_{t \\ge 1}\\) adapted to \\(\\{\\mathcal F_t\\}\\) such that \\[\n  \\sum_{t=1}^{∞} α_t = \\infty\n  \\quad\\text{and}\\quad\n  \\sum_{t=1}^{∞} α_t^2 &lt; ∞.\n  \\] Now suppose \\(\\{X_t\\}_{t \\ge 1}\\) is an adapted sequence of square integrable random variables such that \\[\n    \\EXP[ X_{t+1} | \\mathcal F_t ] = 0\n    \\quad\\text{and}\\quad\n    \\EXP[ \\| X_{t+1}\\|_2^2 \\mid \\mathcal F_t ] \\le B\n  \\] where \\(B\\) is a deterministic constant. Then \\[\n    \\sum_{t=1}^T α_t X_{t+1}\n    \\quad\\text{and}\\quad\n    \\sum_{t=1}^T α_t^2 \\| X_t\\|_2^2,\n    \\quad\n    T = 1,2, \\dots\n  \\] converge to finite limits almost surely.\n\n\nExercise 37.4 (Generalization of Theorem 37.2) Suppose \\(\\{X_t\\}_{t \\ge 1}\\), \\(\\{a_t\\}_{t \\ge 1}\\), \\(\\{Y_t\\}_{t \\ge 1}\\) be nonnegative processes which are adapted to some filtration \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\) such that \\[\n  \\EXP[X_{t+1} \\mid \\ALPHABET F_t] \\le a_t X_t + Y_t.\n\\] Define the set \\(Ω_0\\) by \\[\n  Ω_0 = \\biggl\\{ ω : \\prod_{t \\ge 1} a_t(\\omega) &lt; ∞  \\biggr\\}\n  \\cap\n  \\biggl\\{ ω : \\sum_{t \\ge 1} Y_t(ω) &lt; ∞ \\biggr\\}\n\\] Then, show that for all \\(ω \\in Ω_0\\), we have that \\(\\lim_{t \\to ∞} X_t(ω)\\) exists and is finite.\n\n\nExercise 37.5 (Vector generalization of Theorem 37.2) In all the discussion of almost supermartingale results so far, we have assumed that the processes were scalar valued. Now consider the case where \\(\\{X_t\\}_{t \\ge 1}\\) and \\(\\{Y_t\\}_{t \\ge 1}\\) are processes in \\(\\reals_{\\ge 0}^n\\) and \\(\\{A_t\\}_{t \\ge 1}\\) is a process in \\(\\reals_{\\ge 0}\\), all adapted to a filteration \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\) such that \\[\n  \\EXP[X_{t+1} \\mid \\ALPHABET F_t] \\le A_t X_t + Y_t.\n\\]\nAssume that for all \\(t\\), the matrix \\(A_t\\) is of the form \\(P_t D_t\\), where \\(P_t\\) is a permutation matrix and \\(D_t\\) is a diagonal matrix with all positive elements.1\n1 Such matrices are called :positive monomial matrices and have the property that they are invertible and all entries of the inverse are non-negative.Define the set \\(Ω_0\\) by \\[\n  Ω_0 = \\biggl\\{ ω : \\prod_{t \\ge 1} A_t(\\omega) &lt; ∞ \\biggr\\}\n  \\cap\n  \\biggl\\{ ω : \\sum_{t \\ge 1} Y_t(ω) &lt; ∞ \\biggr\\}\n\\] Then, show that for all \\(ω \\in Ω_0\\), we have that \\(\\lim_{t \\to ∞} X_t(ω)\\) exists and is finite.\n\n\nExercise 37.6 (Delayed almost supermartingales) Suppose \\(\\{X_t\\}_{t \\ge 1}\\), \\(\\{β_t\\}_{t \\ge 1}\\), and \\(\\{Y_t\\}_{t \\ge 1}\\) be nonnegative processes which are adapted to some filteration \\(\\{\\ALPHABET F_t\\}_{t \\ge 1}\\) such that \\[\n  \\EXP[ X_{t+1} \\mid \\ALPHABET F_t] \\le (1 + β_t) X_{t-d} + Y_t,\n\\] where \\(d \\in \\integers_{\\ge 0}\\) is a constant. Define the set \\(Ω_0\\) by \\[\n  Ω_0 = \\biggl\\{ ω : \\sum_{t \\ge 1} β_t(\\omega) &lt; ∞ \\biggr\\}\n  \\cap\n  \\biggl\\{ ω : \\sum_{t \\ge 1} Y_t(ω) &lt; ∞ \\biggr\\}\n\\] Then, show that for all \\(ω \\in Ω_0\\), we have that \\(\\lim_{t \\to ∞} X_t(ω)\\) exists and is finite.\nHint: Write the dynamics equation in vector form and use Exercise 37.5\n\n\nExercise 37.7 Let \\(\\{X_t\\}_{t \\ge 1}\\) be square integrable martingale with \\(\\EXP[ X_T^2 ] \\le T\\) for all \\(T\\). Prove that \\(X_T/T \\to 0\\), a.s.\n[This may be viewed as a generalization of a SLLN for i.i.d. sequences.]\nHint: Use Abel’s lemma to show \\(\\sum_{t=1}^T \\EXP[ M_t^2 ]/t^2 &lt; ∞\\) and then use Theorem 37.4.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/martingales.html#notes",
    "href": "probability/martingales.html#notes",
    "title": "37  Martingales",
    "section": "Notes",
    "text": "Notes\nThe material in the introduction is taken from Pollard (2002), Chang (2007), and Dellacherie and Meyer (1982). The latter is an excellent reference for this material.\nTheorem 37.2 and its proof is adapted from Robbins and Siegmund (1971) and this post on math stackexchange. A version of Theorem 37.2 with \\(Z_t = 0\\) is stated as Exercise II-4 of Neveu (1975).\nExercise 37.3 is from Bertsekas and Tsitsiklis (2000). Exercise 37.5 is from Mahajan et al. (2024), which provides other vector generalizations of Theorem 37.2 as well.\n\n\n\n\nBertsekas, D.P. and Tsitsiklis, J.N. 2000. Gradient convergence in gradient methods with errors. SIAM Journal on Optimization 10, 3, 627–642. DOI: 10.1137/s1052623497331063.\n\n\nBogdan, K. and Więcek, M. 2022. Burkholder inequality by bregman divergence.\n\n\nBurkholder, D.L. 1966. Martingale transforms. The Annals of Mathematical Statistics 37, 6, 1494–1504. DOI: 10.1214/aoms/1177699141.\n\n\nChang, J.T. 2007. Stochastic processes. Available at: http://www.stat.yale.edu/~pollard/Courses/251.spring2013/Handouts/Chang-notes.pdf.\n\n\nDellacherie, C. and Meyer, P.-A. 1982. Probabilities and potential B: Theory of martingales. North-Holland Mathematical Studies.\n\n\nGeiss, S. and Scheutzow, M. 2021. Sharpness of Lenglart’s domination inequality and a sharp monotone version. Electronic Communications in Probability 26, none, 1–8. DOI: 10.1214/21-ECP413.\n\n\nLenglart, É. 1977. Relation de domination entre deux processus. Annales de l’institut henri poincaré. Section b. Calcul des probabilités et statistiques, 171–179.\n\n\nMahajan, A., Niculescu, S.-I., and Vidyasagar, M. 2024. A vector almost-sure supermartingale theorem and its applications. In: Submitted to IEEE conference on decision and control. IEEE.\n\n\nNeveu, J. 1975. Discrete parameter martingales. North Holland.\n\n\nPollard, D. 2002. A user’s guide to measure theoretic probability. Cambridge University Press.\n\n\nRobbins, H. and Siegmund, D. 1971. A convergence theorem for non-negative almost supermartingales and some applications. In: Optimizing methods in statistics. Elsevier, 233–257. DOI: 10.1016/b978-0-12-604550-5.50015-8.\n\n\nStout, W.F. 1974. Almost sure convergence. Academic Press.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Martingales</span>"
    ]
  },
  {
    "objectID": "probability/stochastic-stability.html",
    "href": "probability/stochastic-stability.html",
    "title": "38  Stochastic stability",
    "section": "",
    "text": "38.1 Different notions of stability\nFirst we recall some terminology related to almost sure convergence.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Stochastic stability</span>"
    ]
  },
  {
    "objectID": "probability/stochastic-stability.html#different-notions-of-stability",
    "href": "probability/stochastic-stability.html#different-notions-of-stability",
    "title": "38  Stochastic stability",
    "section": "",
    "text": "A random sequence \\(\\{X_t\\}_{t \\ge 1}\\) in a sample space \\(Ω\\) converges to a random variable \\(X\\) almost surely if \\[\n\\PR\\Bigl(ω \\in Ω : \\lim_{t \\to ∞} \\| X_t(ω) - X(ω) \\| = 0 \\Bigr) = 1.\n\\]\nThe convergence is said to be exponentially fast with rate no slower than \\(γ^{-1}\\) for some \\(γ &gt; 1\\) (not dependent on \\(ω\\)) if \\(γ^t \\| X_t - X \\|\\) converges almost surely to some \\(Δ \\ge 0\\).\nGiven a set \\(\\mathcal D \\in \\reals^n\\), a random sequence \\(\\{X_t\\}_{t \\ge 1}\\) is said to converge to \\(\\mathcal D\\) almost surely if \\[\n\\PR\\Bigl(ω \\in Ω : \\lim_{t \\to ∞} \\mathrm{dist}(X_t(ω), \\mathcal D) = 0 \\Bigr) = 1,\n\\] where \\(\\mathrm{dist}(x,\\mathcal D) = \\inf_{\\tilde x \\in \\mathcal D} \\| x - \\tilde x\\|\\).\n\n\nDefinition 38.1 The origin of \\eqref{eq:stability-dynamics} is said to be:\n\nStable in probability if \\(\\lim_{x_0 \\to 0} \\PR(\\sup_{t \\ge 1} \\| X_t \\| &gt; ε ) = 0\\) for any \\(ε &gt; 0\\).\nAsymptotically stable in prbability if it is stable in probability and moreover \\(\\lim_{x_0 \\to 0} \\PR(\\lim_{t \\to ∞} \\|X_t\\| = 0) = 1\\).\nExponentially stable in prbability if for some \\(γ &gt; 1\\) (not dependent on \\(ω\\)), \\(\\lim_{x_0 \\to 0} \\PR(\\lim_{t \\to ∞} \\|γ^t X_t\\| = 0) = 1\\).",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Stochastic stability</span>"
    ]
  },
  {
    "objectID": "probability/stochastic-stability.html#sufficient-conditions-for-stochastic-stability",
    "href": "probability/stochastic-stability.html#sufficient-conditions-for-stochastic-stability",
    "title": "38  Stochastic stability",
    "section": "38.2 Sufficient conditions for stochastic stability",
    "text": "38.2 Sufficient conditions for stochastic stability\n\nDefinition 38.2 Given a set \\(\\mathcal Q \\in \\reals^n\\) containing the origin, the origin of \\eqref{eq:stability-dynamics} is said to be:\n\nlocally a.s. asymptotically stable in \\(\\mathcal Q\\) if starting from \\(x_0 \\in \\mathcal Q\\) all the sample paths \\(X_t\\) stay in \\(\\mathcal Q\\) for all \\(t \\ge 1\\) and converge to origin almost surely.\nlocally a.s. exponentially stable in \\(\\mathcal Q\\) if it is locally a.s. asymptotically stable and the convergence is exponentially fast.\n\nIf the above properties hold for \\(\\mathcal Q = \\reals^n\\), the system is said to be globally a.s. asymptotically (or exponentially) stable.\n\n\n\n\n\n\n\nFunction class \\(\\mathcal K\\)\n\n\n\nA continuous function \\(h \\colon [0, a) \\to [0, ∞)\\) is said to belong to class \\(\\mathcal K\\) if it is strictly increasing and \\(h(0) = 0\\).\n\n\n\nTheorem 38.1 For the stochastic discrete-time system \\eqref{eq:stability-dynamics}, let \\(\\{X_t\\}_{t \\ge 1}\\) be Markov.\nLet \\(V \\colon \\reals^n \\to \\reals\\) be a continuous positive definite and radially unbounded function. Define the level set \\(\\mathcal Q_{λ} \\coloneqq \\{ x : 0 \\le V(x) &lt; λ \\}\\) for some \\(λ &gt; 0\\).\nLet \\(φ \\colon \\reals^n \\to \\reals\\) be a continious function that satisfies \\(φ(x) \\ge 0\\) for all \\(x \\in \\mathcal Q_{λ}\\).\nSuppose the following property holds: for all \\(x \\in \\reals^n\\), \\[\n  \\EXP[ V(X_{t+1}) \\mid X_t = x ] - V(x) \\le -φ(x), \\quad \\forall t \\ge 1.\n\\] Then:\n\nFor any initial condition \\(x_0 \\in \\mathcal Q_{λ}\\), \\(\\{X_t\\}_{t \\ge 1}\\) converges to \\(\\mathcal D_1 \\coloneqq \\{x \\in \\mathcal Q_{λ} : φ(x) = 0 \\}\\) with probability at least \\(1 - V(x_0)/λ\\).\nif moreover \\(φ(x)\\) is positive definite on \\(\\mathcal Q_{λ}\\) and there exist two calss \\(\\mathcal K\\) functions \\(h_1\\) and \\(h_2\\) such that \\(h_1(\\|x\\|) \\le V(x) \\le h_2(\\|x\\|)\\), then \\(x = 0\\) is asymptotically stable in probability.\n\n\nUnder slightly stronger conditions, it is also possible to characterize the rate of convergence.\n\nTheorem 38.2 For the stochastic discrete-time system \\eqref{eq:stability-dynamics}, let \\(\\{X_t\\}_{t \\ge 1}\\) be Markov.\nLet \\(V \\colon \\reals^n \\to \\reals\\) be a continous nonnegative function.\nSuppose the following condition holds: there exists an \\(α \\in (0,1)\\) such that \\[\n  \\EXP[ V(X_{t+1}) \\mid X_t = x ] - V(x) \\le -α V(x), \\quad \\forall t \\ge 1.\n\\] Then:\n\nFor any initial state \\(x_0\\), \\(V(X_t)\\) almost surely converges to \\(0\\) exponentially fast with a rate no slower than \\(1-α\\).\nIf moreover \\(V\\) satisfies \\(c_1 \\|x\\|^p \\le V(x) \\le c_2 \\|x\\|^p\\) for some \\(c_1, c_2, p &gt; 0\\), then \\(x = 0\\) is globally a.s. exponentially stable.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Stochastic stability</span>"
    ]
  },
  {
    "objectID": "probability/stochastic-stability.html#weaker-sufficient-conditions-for-stochastic-stability",
    "href": "probability/stochastic-stability.html#weaker-sufficient-conditions-for-stochastic-stability",
    "title": "38  Stochastic stability",
    "section": "38.3 Weaker sufficient conditions for stochastic stability",
    "text": "38.3 Weaker sufficient conditions for stochastic stability\n\nTheorem 38.3 For the stochastic discrete-time system \\eqref{eq:stability-dynamics}, let \\(V \\colon \\reals^n \\to \\reals\\) be a continuous nonnegative and radially unbounded function. Define the set \\(Q_{λ} \\coloneqq \\{x : V(x) &lt; λ \\}\\) for some \\(λ &gt; 0\\).\nSuppose the following conditions hold:\n\nFor any \\(t\\) such that \\(X_t \\in \\mathcal Q_{λ}\\), we have \\[ \\EXP[ V(X_{t+1}) \\mid \\mathcal F_t ] - V(X_t) \\le 0. \\]\nThere is an integer \\(T \\ge 1\\) (not depent on \\(ω\\)) and a continous function \\(φ \\colon \\reals^n \\to \\reals\\) that satisfies \\(φ(x) \\ge 0\\) for all \\(x \\in \\mathcal Q_{λ}\\) such that for any \\(t\\), \\[ \\EXP[ V(X_{t+\\color{red}{T}}) \\mid \\mathcal F_t ] - V(X_t) \\le -φ(X_t) \\]\n\nThen, the implications of Theorem 38.1 hold.\n\nThe sufficient conditions for exponential stability can be weakened in a similar manner.\n\nTheorem 38.4 Suppose assumptions a) and b) of Theorem 38.3 are satisfied with the inequality of b) strengthened to \\[ \\EXP[ V(X_{t+\\color{red}{T}}) \\mid \\mathcal F_t ] - V(X_t) \\le -αV(X_t) \\] for some \\(α \\in (0,1)\\).\nThen, the implications of Theorem 38.2 hold.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Stochastic stability</span>"
    ]
  },
  {
    "objectID": "probability/stochastic-stability.html#notes",
    "href": "probability/stochastic-stability.html#notes",
    "title": "38  Stochastic stability",
    "section": "Notes",
    "text": "Notes\nThe material in this section is adapted from Qin et al. (2020).\n\n\n\n\nQin, Y., Cao, M., and Anderson, B.D.O. 2020. Lyapunov criterion for stochastic systems and its applications in distributed computation. IEEE Transactions on Automatic Control 65, 2, 546–560. DOI: 10.1109/tac.2019.2910948.",
    "crumbs": [
      "Probability Appendix",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Stochastic stability</span>"
    ]
  },
  {
    "objectID": "linear-algebra/matrix-relationships.html",
    "href": "linear-algebra/matrix-relationships.html",
    "title": "39  Some useful matrix relationships",
    "section": "",
    "text": "39.1 Matrix identities",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Some useful matrix relationships</span>"
    ]
  },
  {
    "objectID": "linear-algebra/matrix-relationships.html#matrix-identities",
    "href": "linear-algebra/matrix-relationships.html#matrix-identities",
    "title": "39  Some useful matrix relationships",
    "section": "",
    "text": "If \\((I + U)\\) is invertible, then \\[ U(I + U)^{-1} = I - (I + U)^{-1}. \\] This can be verified my multiplying both sides with \\((I+U)\\).\nSimplified Sherman-Morrison-Woodbudy formula: If \\((I + UV)\\) or, equivalently, \\((I + VU)\\) is invertible, then \\[ (I + UV)^{-1} = I - U(I + VU)^{-1}V. \\] This can be verified by multiplying both sides with \\((I + UV)\\). This relationship can also be written as: \\[ (I + UV)^{-1} = I - UV^{1/2}(I + V^{1/2}UV^{1/2})^{-1}V^{1/2}. \\]\nA slight generalization of the above is: \\[ (I + U T^{-1} V)^{-1} = I - U(T + VU)^{-1}V. \\]\nIf \\((I + UV)\\) or, equivalently, \\((I + VU)\\) is invertible, then \\[ V(I + UV)^{-1} = (I + VU)^{-1}V. \\] This can be verified by left multiplying by \\((I + VU)\\) and right multiplying by \\((I + UV)\\).",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Some useful matrix relationships</span>"
    ]
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html",
    "href": "linear-algebra/positive-definite-matrix.html",
    "title": "40  Positive definite matrices",
    "section": "",
    "text": "40.1 Definite and basic properties",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Positive definite matrices</span>"
    ]
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#definite-and-basic-properties",
    "href": "linear-algebra/positive-definite-matrix.html#definite-and-basic-properties",
    "title": "40  Positive definite matrices",
    "section": "",
    "text": "Definition 40.1 A \\(n \\times n\\) symmetric matrix \\(M\\) is called\n\npositive definite (written as \\(M \\succ 0\\)) if for all \\(x \\in\n\\reals^n\\), \\(x \\neq 0\\), we have \\[x^\\TRANS M x &gt; 0.\\]\npositive semi definite (written as \\(M \\succeq 0\\)) if for all \\(x \\in\n\\reals^n\\), \\(x \\neq 0\\), we have \\[x^\\TRANS M x \\ge 0.\\]\n\n\n\nExamples\n\n\\(\\MATRIX{ 3 & 0 \\\\ 0 & 2 } \\succ 0\\) because \\(\\MATRIX{ x_1 & x_2 } \\MATRIX{ 3 & 0 \\\\ 0 & 2 } \\MATRIX{ x_1 \\\\ x_2 }\n= 3 x_1^2 + 2 x_2^2 &gt; 0.\\)\n\\(\\MATRIX{ 0 & 0 \\\\ 0 & 2 } \\succeq 0\\) because \\(\\MATRIX{ x_1 & x_2 } \\MATRIX{ 0 & 0 \\\\ 0 & 2 } \\MATRIX{ x_1 \\\\ x_2 }\n=  2 x_2^2 \\ge 0\\).",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Positive definite matrices</span>"
    ]
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#remarks-on-positive-definite-matrices",
    "href": "linear-algebra/positive-definite-matrix.html#remarks-on-positive-definite-matrices",
    "title": "40  Positive definite matrices",
    "section": "40.2 Remarks on positive definite matrices",
    "text": "40.2 Remarks on positive definite matrices\n\nBy making particular choices of \\(x\\) in the definition of positive definite matrix, we have that for a positive definite matrix \\(M\\),\n\n\\(M_{ii} &gt; 0\\) for all \\(i\\)\n\\(M_{ij} &lt; \\sqrt{M_{ii} M_{jj}}\\) for all \\(i \\neq j\\).\n\nHowever, satisfying these inequalities is not sufficient for positive definiteness.\nA symmetric matrix is positive definite (respt. postive semi-definite) if and only if all of its eigenvalues are positive (respt. non-negative).\nTherefore, a sufficient condition for a symmetric matrix to be positive definite is that all diagonal elements are positive and the matrix is diagonally dominant, i.e., \\(M_{ii} &gt; \\sum_{j \\neq i} | M_{ij}|\\) for all \\(i\\).\nIf \\(M\\) is symmetric positive definite, then so is \\(M^{-1}\\).\nIf \\(M\\) is symmetric positive definite, then \\(M\\) has a unique symmetric positive definite square root \\(R\\) (i.e., \\(RR = M\\)).\nIf \\(M\\) is symmetric positive definite, then \\(M\\) has a unique Cholesky factorization \\(M = T^\\TRANS T\\), where \\(T\\) is upper triangular with positive diagonal elements.\nThe set of positive semi-definite matrices forms a convex cone.\nPositive definiteness introduces a partial order on the convex cone of positive semi-definite matrices. In particular, we say that for two positive semi-definite matrices \\(M\\) and \\(N\\) of the same dimension, \\(M\n\\succeq N\\) if \\(M - N\\) is positive semi-definite. For this reason, often \\(M\n\\succ 0\\) and \\(M \\succeq 0\\) is used a short-hand to denote that \\(M\\) is positive definite and positive semi-definite.\nLet \\(M\\) is a symmetric square matrix. Let \\[ λ_1(M) \\ge λ_2(M) \\ge \\dots \\ge λ_n(M) \\] denote the ordered (real) eigenvalues of \\(M\\). Then \\[ λ_1(M)I \\succeq M \\succeq λ_n(M)I. \\]\nIf \\(M \\succeq N\\), then \\[ λ_k(M) \\ge λ_k(N), \\quad k \\in \\{1, \\dots, n\\}. \\]\nIf \\(M \\succeq N \\succ 0\\), then \\[ N^{-1} \\succeq M^{-1} \\succ 0. \\]\nIf \\(M \\succeq N\\) are \\(n × n\\) matrices and \\(T\\) is a \\(m × n\\) matrix, then \\[ T^\\TRANS M T \\succeq T^\\TRANS N T. \\]\nIf \\(M, N\\) are \\(n×\\) positive semi-definite matrices, then \\[ \\sum_{i=1}^k λ_i(M) λ_{n-i+1}(N) \\le\n  \\sum_{i=1}^k λ_i(MN) \\le\n  \\sum_{i=1}^k λ_i(M)λ_i(N),\n  \\quad k \\in \\{1, \\dots, n\\}.\n   \\] Note that this property does not require \\(M - N\\) to be positive or negative semi-definite.\nIf \\(M \\succ 0\\) and \\(T\\) are square matrices of the same size, then \\[ TMT + M^{-1} \\succeq 2T. \\]",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Positive definite matrices</span>"
    ]
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#a-useful-relationship.",
    "href": "linear-algebra/positive-definite-matrix.html#a-useful-relationship.",
    "title": "40  Positive definite matrices",
    "section": "40.3 A useful relationship.",
    "text": "40.3 A useful relationship.\nSymmetric block matrices of the form\n\\[ C = \\MATRIX{ A & X \\\\ X^\\TRANS & B } \\]\noften appear in applications. If \\(A\\) is non-singular, we can write\n\\[\n\\MATRIX{A & X \\\\ X^\\TRANS & B } =\n\\MATRIX{I & 0 \\\\ X^\\TRANS A^{-1} & I}\n\\MATRIX{A & 0 \\\\ 0 & B - X^\\TRANS A^{-1} X }\n\\MATRIX{I & A^{-1} X \\\\ 0 & I }\n\\] which shows that \\(C\\) is congruent to a block diagonal matrix, which is positive definite when its diagonal blocks are postive definite. Therefore, \\(C\\) is positive definite if and only if both \\(A\\) and \\(B - X^\\TRANS A^{-1} X\\) are positive definite. The matrix \\(B = X^\\TRANS A^{-1} X\\) is called the Shur complement of \\(A\\) in \\(C\\).",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Positive definite matrices</span>"
    ]
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#determinant-bounds",
    "href": "linear-algebra/positive-definite-matrix.html#determinant-bounds",
    "title": "40  Positive definite matrices",
    "section": "40.4 Determinant bounds",
    "text": "40.4 Determinant bounds\n\nProposition 40.1 (Fischer’s inequality) Suppose \\(A\\) and \\(C\\) are positive semidefinite matrix and \\[ M = \\MATRIX{A & B \\\\ B^\\TRANS & C}. \\] Then \\[ \\det(M) \\le \\det(A) \\det(C). \\]\n\nRecursive application of Fischer’s inequality gives the Hadamard’s inequality for a symmetric positive definite matrix: \\[ \\det(A) \\le A_{11} A_{22} \\cdots A_{nn}, \\] with equality if and only if \\(A\\) is diagonal.\n\nProposition 40.2 If \\(M \\succ N \\succ 0\\) are \\(n × n\\) matrices and \\(T\\) is a \\(m × n\\) matrix, then \\[ \\sup_{ T \\neq 0} \\frac{ \\| T^\\TRANS M T \\| }{ \\| T^\\TRANS N T \\|}\n   \\le \\frac{ \\det(M) }{ \\det(N) }, \\] where for any matrix \\(M\\), \\[\n  \\| M \\| = \\sup_{x \\neq 0} \\frac{ \\| M x \\|_2 }{ \\|x\\|_2 }\n\\] is the \\(2\\)-norm of the matrix.\n\nProposition 40.2 is taken from (Abbasi-Yadkori2011?).",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Positive definite matrices</span>"
    ]
  },
  {
    "objectID": "linear-algebra/positive-definite-matrix.html#references",
    "href": "linear-algebra/positive-definite-matrix.html#references",
    "title": "40  Positive definite matrices",
    "section": "References",
    "text": "References\nThe properties of positive definite matrices are stated in any book on the theory of matrices. See for example Marshall et al. (2011).\nHistorically, a matrix used as a test matrix for testing positive definiteness was the Wilson matrix \\[ W = \\MATRIX{5 & 7 & 6 & 5 \\\\ 7 & 10 & 8 & 7 \\\\ 6 & 8 & 10 & 9 \\\\ 5 & 7 & 9\n& 10}. \\] For a nice overview of Wilson matrix, see this blog post.\n\n\n\n\n\nMarshall, A.W., Olkin, I., and Arnold, B.C. 2011. Inequalities: Theory of majorization and its applications. Springer New York. DOI: 10.1007/978-0-387-68276-1.",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Positive definite matrices</span>"
    ]
  },
  {
    "objectID": "linear-algebra/infinite-product-of-matrices.html",
    "href": "linear-algebra/infinite-product-of-matrices.html",
    "title": "41  Infinite product of matrices",
    "section": "",
    "text": "Exercises",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Infinite product of matrices</span>"
    ]
  },
  {
    "objectID": "linear-algebra/infinite-product-of-matrices.html#exercises",
    "href": "linear-algebra/infinite-product-of-matrices.html#exercises",
    "title": "41  Infinite product of matrices",
    "section": "",
    "text": "Exercise 41.1 Suppose \\(A_t = \\MATRIX{0 & 1 + \\beta_t \\\\ 1 & \\gamma_t}\\), where \\(\\beta_t, \\gamma_t \\in [0,1]\\) Show that \\(\\prod_{t \\ge 1} A_t\\) converges if \\[\n  \\sum_{t \\ge 1} (\\beta_t + \\gamma_t) &lt; \\infty.\n\\]\nHint: Write \\(A_t = P + B_t\\), where \\(P = \\MATRIX{0 & 1 \\\\ 1 & 0}\\) is a permultation matrix. So \\(P^2 = I\\). Break the product \\(\\prod_{t \\ge 1} A_t\\) as \\[\n  \\cdots (A_4 A_3) (A_2 A_1)\n\\] and write \\(A_{2t}A_{2t-1}\\) in the form \\(I + C_t\\) and use property (C1).",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Infinite product of matrices</span>"
    ]
  },
  {
    "objectID": "linear-algebra/infinite-product-of-matrices.html#notes",
    "href": "linear-algebra/infinite-product-of-matrices.html#notes",
    "title": "41  Infinite product of matrices",
    "section": "Notes",
    "text": "Notes\nCondition (C1) is a standard result and stated as Theorem 1 in Trench (1999). Condition (C2) is Theorem 5 of Trench (1999). Condition (C3) is Theorem 2.1 combined with the remark on page 15 of Artzrouni (1986).\nExercise 41.1 is from Mahajan et al. (2024).\n\n\n\n\nArtzrouni, M. 1986. On the convergence of infinite products of matrices. Linear Algebra and its Applications 74, 11–21. DOI: 10.1016/0024-3795(86)90112-6.\n\n\nMahajan, A., Niculescu, S.-I., and Vidyasagar, M. 2024. A vector almost-sure supermartingale theorem and its applications. In: Submitted to IEEE conference on decision and control. IEEE.\n\n\nTrench, W.F. 1999. Invertibly convergent infinite products of matrices. Journal of Computational and Applied Mathematics 101, 1–2, 255–263. DOI: 10.1016/s0377-0427(98)00191-5.",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Infinite product of matrices</span>"
    ]
  },
  {
    "objectID": "linear-algebra/svd.html",
    "href": "linear-algebra/svd.html",
    "title": "42  Singular value decomposition",
    "section": "",
    "text": "42.1 Singular values\nLet \\(A\\) be a \\(n × d\\) matrix. Then, the matrix \\(A^\\TRANS A\\) is a symmetric \\(d × d\\) matrix, so its eigenvalues are real. Moreover, \\(A^\\TRANS A\\) is positive semi-definite, so the eigen values are non-negative. Let \\(\\{ λ_1, \\dots, λ_d \\}\\) denote the eigenvalues of \\(A^\\TRANS A\\), with repetitions. Order then so that \\(λ_1 \\ge λ_2 \\ge \\dots \\ge λ_d \\ge 0\\). Let \\(σ_i = \\sqrt{λ_i}\\), so that \\(σ_1 \\ge σ_2 \\ge \\dots σ_d \\ge 0\\). These numbers are called the singular values of \\(A\\).",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Singular value decomposition</span>"
    ]
  },
  {
    "objectID": "linear-algebra/svd.html#singular-values",
    "href": "linear-algebra/svd.html#singular-values",
    "title": "42  Singular value decomposition",
    "section": "",
    "text": "Properties of singular values\n\nThe number of non-zero singular values of \\(A\\) equals to the rank of \\(A\\). In particular, if \\(A\\) is \\(n × d\\) where \\(n &lt; d\\), then \\(A\\) has at most \\(n\\) nonzero singular values.\nIt can be shown that\n\\[ σ_1 = \\max_{\\|x\\| = 1}  \\| A x \\| . \\]\nLet \\(v_1\\) denote the arg-max of the above optimization. \\(v_1\\) is called the first singular vector of \\(A\\). Then,\n\\[ σ_2 = \\max_{ x \\perp v_1, \\|x \\| = 1}  \\| A x\\|. \\]\nLet \\(v_2\\) denote the arg-max of the above optimization. \\(v_2\\) is called the second singular vector of \\(A\\), and so on.\nLet \\(A\\) be a \\(n × d\\) matrix and \\(v_1, \\dots, v_r\\) be the singular vectors, where \\(r = \\text{rank}(A)\\). Then for any \\(k \\in \\{1, \\dots, r\\}\\), let \\(V_k\\) be the subspace spanned by \\(\\{v_1, \\dots, v_k\\}\\). Then, \\(V_k\\) is the best \\(k\\)-dimensional subspace for \\(A\\).\nFor any matrix \\(A\\), \\[ \\sum_{i =1}^r σ_i^2(A) = \\| A \\|_{F}^2\n   := \\sum_{j,k} a_{jk}^2. \\]\nAny vector \\(v\\) can be written as a linear combination of \\(v_1, \\dots, v_r\\) and a vector perpendicular to \\(V_r\\) (defined above). Now, \\(Av\\) can be written as the same linear combination of \\(Av_1, Av_2, \\dots, Av_r\\). So, \\(Av_1, \\dots, Av_r\\) form a fundamental set of vectors associated with \\(A\\). We normalize them to length one by \\[ u_i = \\frac{1}{σ_i(A)} A v_i. \\] The vectors \\(u_1, \\dots, u_r\\) are called the left singular vectors of \\(A\\). The \\(v_i\\) are called the right singular vectors.\nBoth the left and the right singular vectors are orthogonal.\n\n\n\n\n\n\n\nSingular value decomposition\n\n\n\nFor any matrix \\(A\\), \\[ A = \\sum_{i=1}^r σ_i u_i v_i^\\TRANS \\] where \\(u_i\\) and \\(v_i\\) are the left and right singular vectors, and \\(σ_i\\) are the singular values.\nEquivalently, in matrix notation: \\[ A = U D V^\\TRANS \\] where the columns of \\(U\\) and \\(V\\) consist of the left and right singular vectors, respectively, and \\(D\\) is a diagonal matrix whose diagonal entries are the singular values of \\(A\\).\n\n\nIf \\(A\\) is a positive definite square matrix, then the SVD and the eigen-decomposition coincide.",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Singular value decomposition</span>"
    ]
  },
  {
    "objectID": "linear-algebra/svd.html#best-rank-k-approximations",
    "href": "linear-algebra/svd.html#best-rank-k-approximations",
    "title": "42  Singular value decomposition",
    "section": "42.2 Best rank-\\(k\\) approximations",
    "text": "42.2 Best rank-\\(k\\) approximations\nThere are two important matrix norms, the Frobenius norm which is defined as \\[\n  \\| A \\|_{F} = \\sqrt{ \\sum_{i,j} a_{ij}^2 }\n\\] and the induced norm which is defined as \\[\n  \\| A \\|_2 = \\max_{\\|x \\| = 1} \\| A x \\|.\n\\]\nNote that the Frobenius norm is equal to the square root of the sum of squares of the singular values and the \\(2\\)-norm is the largest singular value.\nLet \\(A\\) be an \\(n × d\\) matrix and think of \\(A\\) as the \\(n\\) points in \\(d\\)-dimensional space. The Frobenius norm of \\(A\\) is the square root of the sum of squared distance of the points to the origin. The induced norm is the square root of the sum of squared distances to the origin along the direction that maximizes this quantity.\n\nProposition 42.1 (Best rank-\\(k\\) approximation) Let \\[ A = \\sum_{i = 1}^r σ_i u_i v_i^\\TRANS \\] be the SVD of \\(A\\). For \\(k \\in \\{1, \\dots, r\\}\\), let \\[ A_k = \\sum_{i=1}^k σ_i u_i v_i^\\TRANS \\] be the sum truncated after \\(k\\) terms.\nThen, \\(A_k\\) is the best rank \\(k\\) approximation to \\(A\\), when the error is measured in either the induced norm or the Frobenius norm.\n\n\n\n\n\n\n\nProof outline\n\n\n\n\n\nThis result is established by showing the following properties:\n\nThe rows of \\(A_k\\) are the projections of the rows of \\(A\\) onto the subspace \\(V_k\\) spanned by the first \\(k\\) right singular vectors of \\(A\\).\nFor any matrix \\(B\\) of rank at most \\(k\\) \\[ \\| A - A_k \\|_{F} \\le \\|A - B \\|_{F}. \\]\n\\(\\| A - A_k\\|_2^2 = σ_{k+1}^2.\\)\nFor any matrix \\(B\\) of rank at most \\(k\\) \\[ \\| A - A_k \\|_{2} \\le \\|A - B \\|_{2}. \\]",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Singular value decomposition</span>"
    ]
  },
  {
    "objectID": "linear-algebra/svd.html#notes",
    "href": "linear-algebra/svd.html#notes",
    "title": "42  Singular value decomposition",
    "section": "Notes",
    "text": "Notes\nThe chapter on SVD in Hopcroft and Kannan (2012) contains a nice intuitive explanation of SVD.\n\n\n\n\nHopcroft, J. and Kannan, R. 2012. Computer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf.",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Singular value decomposition</span>"
    ]
  },
  {
    "objectID": "linear-algebra/rkhs.html",
    "href": "linear-algebra/rkhs.html",
    "title": "43  Reproducing Kernel Hilbert Space",
    "section": "",
    "text": "43.1 Review of Linear Operators\nAs an example, suppose \\(\\mathcal F\\) is an inner product space. For a \\(g \\in\n\\mathcal F\\), the operator \\(A_g \\colon \\mathcal F \\to \\reals\\) defined by \\(A_g(f) = \\langle f, g \\rangle\\) is a linear operator. Such scalar valued operators are called functionals on \\(\\mathcal F\\).\nLinear operators satisfy the following property.",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Reproducing Kernel Hilbert Space</span>"
    ]
  },
  {
    "objectID": "linear-algebra/rkhs.html#review-of-linear-operators",
    "href": "linear-algebra/rkhs.html#review-of-linear-operators",
    "title": "43  Reproducing Kernel Hilbert Space",
    "section": "",
    "text": "Linear Operator\n\n\n\nLet \\(\\mathcal F\\) and \\(\\mathcal G\\) be normed vector spaces over \\(\\reals\\). A function \\(A \\colon \\mathcal F \\to \\mathcal G\\) is called a linear operator if it satisfies the following properties:\n\nHonogeneity: For any \\(α \\in \\reals\\) and \\(f \\in \\mathcal F\\), \\(A(αf) =\nα (Af)\\).\nAdditivity: For any \\(f,g \\in \\mathcal F\\), \\(A(f + g) = Af + Ag\\).\n\nThe operator norm of a linear operator is defined as \\[ \\NORM{A} = \\sup_{f \\in \\mathcal F} \\frac{ \\NORM{A f}_{\\mathcal G}}\n{\\NORM{f}}_{\\mathcal F}. \\]\nIf \\(\\NORM{A} &lt; ∞\\), then the operator is said to be a bounded operator.\n\n\n\n\n\nTheorem 43.1 If \\(A \\colon \\mathcal F \\to \\mathcal G\\) is a linear operator, then the following three conditions are equivalent:\n\n\\(A\\) is a bounded operator.\n\\(A\\) is continuous on \\(\\mathcal F\\).\n\\(A\\) is continious at one point of \\(\\mathcal F\\).",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Reproducing Kernel Hilbert Space</span>"
    ]
  },
  {
    "objectID": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "href": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "title": "43  Reproducing Kernel Hilbert Space",
    "section": "43.2 Dual of a linear operator",
    "text": "43.2 Dual of a linear operator\nThere are two notions of dual of a linear operator: algebraic dual and topological dual. If \\(\\mathcal F\\) is a normed space, then the space of all linear functionals \\(A \\colon \\mathcal F \\to \\reals\\) is the algebraic dual space of \\(\\mathcal F\\); the space of all continuous linear functions \\(A \\colon\n\\mathcal F \\to \\reals\\) is the topological dual space of \\(\\mathcal F\\).\nIn finite-dimensional space, the two notions of dual spaces coincide (every linear operator on a normed, finite dimensional space is bounded). But this is not the case for infinite dimensional spaces.\n\nTheorem 43.2 (Riesz representation) In a Hilbert space \\(\\mathcal F\\), all continuous linear functionals are of the form \\(\\langle\\cdot, g\\rangle\\), for some \\(g\n\\in \\mathcal F\\).\n\nTwo Hilbert spaces \\(\\mathcal F\\) and \\(\\mathcal G\\) are said to be isometrically isomorphic if there is a linear bijective map \\(U \\colon\n\\mathcal F \\to \\mathcal G\\) which preserves the inner product, i.e., \\(\\langle\nf_1, f_2 \\rangle_{\\mathcal F} = \\langle U f_1, U f_2 \\rangle_{\\mathcal G}\\).\nNote that Riesz representation theorem gives a natural isometric isomorphism \\(\\psi \\colon g \\mapsto \\langle \\cdot, g \\rangle_{\\mathcal F}\\) between \\(\\mathcal F\\) and its topological dual \\(\\mathcal F'\\), whereby \\(\\NORM{ψ(g)}_{\\mathcal F'} = \\NORM{g}_{\\mathcal F}\\).",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Reproducing Kernel Hilbert Space</span>"
    ]
  },
  {
    "objectID": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "href": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "title": "43  Reproducing Kernel Hilbert Space",
    "section": "43.3 Reproducing kernel Hilbert space",
    "text": "43.3 Reproducing kernel Hilbert space\nLet \\(\\mathcal H\\) be a Hilbert space of functions mapping from some non-empty set \\(\\ALPHABET X\\) to \\(\\reals\\). Note that for every \\(x \\in \\ALPHABET X\\), there is a very special functional on \\(\\mathcal H\\): the one that assigns to each \\(f\n\\in \\mathcal H\\), its value at \\(x\\). This is called the evaluation functional and denoted by \\(δ_x\\). In particular, \\(δ_x \\colon \\mathcal H \\to\n\\reals\\), where \\(δ_x \\colon f \\mapsto f(x)\\).\n\n\n\n\n\n\nReproducing kernel Hilbert space (RKHS)\n\n\n\nA Hilbert space \\(\\mathcal H\\) of functions \\(f \\colon \\ALPHABET X \\to \\reals\\) defined on a non-empty set \\(\\ALPHABET X\\) is said to be a RKHS if \\(δ_x\\) is continuous for all \\(x \\in \\ALPHABET X\\).\n\n\nIn view of Theorem 43.1, an equivalent definition is that a Hilbert space \\(\\mathcal H\\) is RKHS if the evaluation functionals \\(δ_x\\) are bounded, i.e., for every \\(x \\in \\ALPHABET X\\), there exists a \\(M_x\\) such that \\[ | δ_x | = | f(x) | \\le M_x \\| f \\|_{\\mathcal H}, \\quad \\forall f \\in \\mathcal H\\]\nAn immediate implication of the above property is that two functions which agree in RKHS norm agree at every point: \\[ | f(x) - g(x) | = | δ_x(f - g) | \\le M_x \\| f - g \\|_{\\mathcal H},\n   \\quad \\forall f,g \\in \\mathcal H. \\]\nFor example, the \\(L_2\\) space of square integrable functions i.e., \\(\\int_{\\reals^n} f(x)^2 dx &lt; ∞\\) with inner product \\(\\int_{\\reals^n} f(x)\ng(x)dx\\) is a Hilbert space, but not an RKHS because the delta function, which has the reproducing property \\[ f(x) = \\int_{\\reals^n} δ(x - y) f(y) dy \\] is not bounded.\nRKHS are particularly well behaved. In particular, if we have a sequence of functions \\(\\{f_n\\}_{n \\ge 1}\\) which converges to a limit \\(f\\) in the Hilbert-space norm, i.e., \\(\\lim_{n \\to ∞} \\NORM{f_n - f}_{\\mathcal H} = 0\\), then they also converge pointwise, i.e., \\(\\lim_{n \\to ∞} f_n(x) = f(x)\\) for all \\(x \\in \\ALPHABET X\\).",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Reproducing Kernel Hilbert Space</span>"
    ]
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-rhks",
    "href": "linear-algebra/rkhs.html#properties-of-rhks",
    "title": "43  Reproducing Kernel Hilbert Space",
    "section": "43.4 Properties of RHKS",
    "text": "43.4 Properties of RHKS\nRKHS has many useful properties:\n\nFor any RKHS, there exists a unique kernel \\(k \\colon \\ALPHABET X ×\n\\ALPHABET X \\to \\reals\\) such that\n\nfor any \\(x \\in \\ALPHABET X\\), \\(k(\\cdot, x) \\in \\mathcal H\\),\nfor any \\(x \\in \\ALPHABET X\\) and \\(f \\in \\mathcal H\\), \\(\\langle f, k(\\cdot, x) \\rangle = f(x)\\) (the reproducing property).\n\nIn particular, for any \\(x,y \\in \\ALPHABET X\\), \\[ k(x,y) = \\langle k(\\cdot, x), k(\\cdot, y) \\rangle. \\] Thus, the kernel is a symmetric function.\nThe kernel is positive definite, i.e., for any \\(n \\ge 1\\), for all \\((a_1,\n\\dots, a_n) \\in \\reals^n\\) and \\((x_1, \\dots, x_n) \\in \\ALPHABET X^n\\), \\[ \\sum_{i=1}^n \\sum_{j=1}^n a_i a_i h(x_i, x_j) \\ge 0 \\]\nA conseuqence of positive definiteness is that \\[| k(x, y)|^2 \\le k(x, x) k(y, y). \\]\n(Moore-Aronszajn Theorem) For every positive definite kernel \\(K\\) on \\(\\ALPHABET X × \\ALPHABET X\\), there is a unique RKHS on \\(\\ALPHABET X\\) with \\(K\\) as its reproducing kernel.",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Reproducing Kernel Hilbert Space</span>"
    ]
  },
  {
    "objectID": "linear-algebra/rkhs.html#examples-of-kernels",
    "href": "linear-algebra/rkhs.html#examples-of-kernels",
    "title": "43  Reproducing Kernel Hilbert Space",
    "section": "43.5 Examples of kernels",
    "text": "43.5 Examples of kernels\nSome common examples of symmetric positive definite kernels for \\(\\ALPHABET\nX = \\reals^n\\) are as follows:\n\nLinear kernel \\[ k(x,y) = \\langle x, y \\rangle\\]\nGaussian kernel \\[ k(x,y) = \\exp\\biggl( - \\frac{\\| x - y \\|^2}{σ^2} \\biggr),\n   \\quad σ &gt; 0. \\]\nPolynomail kernel \\[ k(x,y) = \\bigl( 1 + \\langle x, y \\rangle \\bigr)^d,\n   \\quad d \\in \\integers_{&gt; 0}. \\]",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Reproducing Kernel Hilbert Space</span>"
    ]
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-kernels",
    "href": "linear-algebra/rkhs.html#properties-of-kernels",
    "title": "43  Reproducing Kernel Hilbert Space",
    "section": "43.6 Properties of kernels",
    "text": "43.6 Properties of kernels\n\nSuppose \\(φ \\colon \\ALPHABET X \\to \\reals^n\\) is a feature map, then \\[ k(x,y) := \\langle φ(x), φ(y) \\rangle \\] is a kernel.\nNote that there are no conditions on \\(\\ALPHABET X\\) (e.g., \\(\\ALPHABET X\\) doesn’t need to be an inner product space).\nIf \\(k\\) is a kernel on \\(\\ALPHABET X\\), then for any \\(α &gt; 0\\), \\(αk\\) is also a kernel.\nIf \\(k_1\\) and \\(k_2\\) are kernels on \\(\\ALPHABET X\\), then \\(k_1 + k_2\\) is also a kernel.\nIf \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be arbitrary sets and \\(A \\colon\n\\ALPHABET X \\to \\ALPHABET Y\\) is a map. Let \\(k\\) be a kernel on \\(\\ALPHABET\nY\\). Then, \\(k(A(x_1), A(x_2))\\) is a kernel on \\(\\ALPHABET X\\).\nIf \\(k_1 \\colon \\ALPHABET X_1 × \\ALPHABET X_1 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_1\\) and \\(k_2 \\colon \\ALPHABET X_2 × \\ALPHABET X_2 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_2\\), then \\[ k( (x_1, x_2), (y_1, y_2) ) = k_1(x_1, y_1) k_2(x_2, y_2) \\] is a kernel on \\(\\ALPHABET X_1 × \\ALPHABET X_2\\).\n(Mercer-Hilber-Schmit theorems) If \\(k\\) is positive definite kernel (that is continous with finite trace), then there exists an infinite sequence of eiegenfunctions \\(\\{ φ_i \\colon \\ALPHABET X \\to \\reals\n\\}_{i \\ge 1}\\) and real eigenvalues \\(\\{λ_i\\}_{i \\ge 1}\\) such that we can write \\(k\\) as: \\[ k(x,y) = \\sum_{i=1}^∞ λ_i φ_i(x) φ_i(y). \\] This is analogous to the expression of a matrix in terms of its eigenvector and eigenvalues, except in this case we have functions and an infinity of them.\nUsing this property, we can define the inner product of RKHS in a simpler form. First, for any \\(f \\in \\mathcal H\\), define \\[ f_i = \\langle f, φ_i \\rangle.\\] Then, for any \\(f, g \\in \\mathcal H\\), \\[ \\langle f, g \\rangle = \\sum_{i=1}^∞ \\frac{ f_i g_i } { λ_i }. \\]",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Reproducing Kernel Hilbert Space</span>"
    ]
  },
  {
    "objectID": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "href": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "title": "43  Reproducing Kernel Hilbert Space",
    "section": "43.7 Kernel ridge regression",
    "text": "43.7 Kernel ridge regression\nGiven labelled data \\(\\{ (x_i, y_i) \\}_{i=1}^n\\), and a feature map \\(φ \\colon\n\\ALPHABET X \\to \\ALPHABET Z\\), define the RKHS \\(\\ALPHABET H\\) of functions from \\(\\ALPHABET Z \\to \\reals\\) with the kernel \\(k(x,y) = \\langle φ(x), φ(y)\n\\rangle_{\\mathcal H}\\). Now, consider the problem of minimizing\n\\[f^* = \\arg \\min_{f \\in \\ALPHABET H}\n\\biggl(\n  \\sum_{i=1}^n \\bigl( y_i - \\langle f, φ(x_i) \\rangle_{\\mathcal{H}} \\bigr)^2 +\n  λ \\NORM{f}^2_{\\mathcal H}\n\\bigr).\\]\n\nTheorem 43.3 (The representer theoreom (simple version)) Given a loss function \\(\\ell \\colon \\ALPHABET Z^n \\to \\reals\\) and a penalty function \\(Ω \\colon \\reals \\to \\reals\\), there is as a solution of \\[ f^* = \\arg \\min_{f \\in \\mathcal H} \\ell(f(x_1), \\dots, f(x_n))\n        + Ω(\\NORM{f}^2_{\\mathcal H}). \\] that takes the the form \\[ f^* = \\sum_{i=1}^n α_i k(\\cdot, x_i).\\]\nIf \\(Ω\\) is strictly increasing, all solutions have this form.\n\nUsing the representer theorem, we know that the solution is of the form \\[ f = \\sum_{i=1}^n α_i φ(x_i). \\] Then, \\[\n\\sum_{i=1}^n \\bigl( y_i - \\langle f, φ_i(x_i) \\rangle_{\\mathcal H} \\bigr)^2\n  + λ \\NORM{f}_{\\mathcal H}^2\n= \\NORM{ y - K α}^2 + λ α^\\TRANS K α. \\]\nDifferentiating wrt \\(α\\) and setting this to zero, we get \\[\n  α^* = (K + λI_n)^{-1} y.\n\\]",
    "crumbs": [
      "Linear Algebra Appendix",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Reproducing Kernel Hilbert Space</span>"
    ]
  },
  {
    "objectID": "convexity/convexity.html",
    "href": "convexity/convexity.html",
    "title": "44  Convex sets and convex functions",
    "section": "",
    "text": "44.1 Convexity\nA function is said to be concave when \\(-f\\) is convex.",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Convex sets and convex functions</span>"
    ]
  },
  {
    "objectID": "convexity/convexity.html#convexity",
    "href": "convexity/convexity.html#convexity",
    "title": "44  Convex sets and convex functions",
    "section": "",
    "text": "Definition 44.1 (Convex sets and convex functions) A subset \\(C\\) of \\(\\reals^n\\) is convex if for every \\(x_0, x_1 \\in C\\), the line segment \\([x_0, x_1] \\in C\\), i.e., \\[ (1-λ) x_0 + λ x_1 \\in C \\quad \\text{for all } λ \\in (0,1). \\]\nA function \\(f\\) on a convex set \\(C\\) is convex relative to \\(C\\) if for every \\(x_0, x_1 \\in C\\), one has \\[\nf( (1-λ)x_0 + λ x_1 ) \\le (1-λ) f(x_0) + λ f(x_1)\n\\quad \\text{for all } λ \\in (0,1).\n\\]\n\\(f\\) is strictly convex if this inequality is strict for points \\(x_0 \\neq x_1\\) with \\(f(x_0)\\) and \\(f(x_1)\\) finite.",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Convex sets and convex functions</span>"
    ]
  },
  {
    "objectID": "convexity/convexity.html#convex-combinations",
    "href": "convexity/convexity.html#convex-combinations",
    "title": "44  Convex sets and convex functions",
    "section": "44.2 Convex combinations",
    "text": "44.2 Convex combinations\nA convex combination of elements \\(x_0, x_1, \\dots, x_p \\in \\reals^n\\) is a linear combination \\(\\sum_{i=0}^p λ_i x_i\\) where the coefficients \\(λ_i\\) are non-negative and add to one. In case of two elements, convex combinations can be equivalently expressed as \\((1-λ)x_0 + λx_1\\) with \\(λ \\in [0,1]\\), which we have already seen. The next result shows that the definition of convexity in terms of two elements given earlier generalizes to multiple elements.\n\nProposition 44.1 A set \\(C \\subset \\reals^n\\) is convex if and only if \\(C\\) contains all convex combinations of its elements.\nA function \\(f\\) is convex relative to a convex set \\(C\\) if and only if for every choice of points \\(x_0, \\dots, x_p \\in C\\), we have: \\[\\begin{equation}\\label{eq:Jensen-inequality}\nf\\biggl( \\sum_{i=0}^p λ_i x_i \\biggr)\n\\le\n\\sum_{i=0}^p λ_i f(x_i)\n\\quad \\text{when }\nλ_i \\ge 0,\n\\sum_{i=0}^p λ_i = 1.\n\\end{equation}\\]\n\nInequality \\eqref{eq:Jensen-inequality} is also call Jensen’s inequality.\n\n\n\n\n\n\nExtended real line\n\n\n\n\n\nIn many applications, it is convenient to consider function \\(f\\) that are allowed to be extended-real-valued, i.e., take values in \\(\\bar \\reals = [-∞, ∞]\\) instead of just \\(\\reals = (-∞, ∞)\\). The extended real line has all the properties of a compact interval: every subset \\(A \\subset \\bar \\reals\\) has a supremum and an infimum, either of which could be infinite.\nWhen extending arithmetic operations to the extended real line, most rules extend in a natural manner but we have to be careful with two operations: \\(0 ⋅ ∞\\) and \\(∞ - ∞\\). We will define \\[\n0 ⋅ ∞ = 0 = 0 ⋅ (-∞).\n\\]\nFor convexity, one follows the inf-addition convention: \\[\n∞ + (-∞) = (-∞) + ∞ = ∞.\n\\]\nExtended arithematic then obeys the associative, commutative, and distributive laws of ordinary arithematic with one crucial exception: \\[\n  λ ⋅ (∞ - ∞) \\neq (λ ⋅ ∞ - λ ⋅ ∞)\n  \\quad \\text{when } λ &lt; 0.\n\\]\n\n\n\nThe definition of convex functions can be generalized to function defined over the extended real line as long as we invoke the inf-addition convention.\nAny convex function \\(f\\) on a convex set \\(C \\in \\reals^n\\) can be identified with a convex function on all of \\(\\reals^n\\) by defining \\(f(x) = ∞\\) for all \\(x \\not\\in C\\). Such functions are called proper convex functions (in contrast to improper convex functions which take the value \\(f(x) = -∞\\) for all \\(x \\in \\text{int}(\\text{dom} f)\\).",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Convex sets and convex functions</span>"
    ]
  },
  {
    "objectID": "convexity/convexity.html#properties-of-convex-function",
    "href": "convexity/convexity.html#properties-of-convex-function",
    "title": "44  Convex sets and convex functions",
    "section": "44.3 Properties of convex function",
    "text": "44.3 Properties of convex function\n\nProposition 44.2 Let \\(I\\) be an index set. Then:\n\nIntersection of convex functions is convex:  \\(\\bigcap_{i \\in I} C_i\\) is convex if each set \\(C_i\\) is convex.\nSupremum of convex functions is convex:  \\(\\sup_{i \\in I} f_i\\) is convex if each function \\(f_i\\) is convex.\nSupremum of finite number of strictly convex functions is strictly convex:  \\(\\sup_{i \\in I} f_i\\) is strictly convex if each function \\(f_i\\) is strictly convex and \\(I\\) is finite.\nPointwise supremum of a collection of convex functions is convex:  \\(f\\) is convex if \\(f(x) = \\lim\\sup_{i \\in I} f_i(x)\\) for all \\(x\\) and each \\(f_i\\) is convex.\n\n\nThe next result presents equivalent characterizations of convexity.\n\nProposition 44.3 For a differentiable function \\(f\\) on an open convex set \\(O \\subset \\reals^n\\), each of the following is both necessary and sufficient for \\(f\\) to be convex on \\(O\\):\n\n\\(\\langle x_1 - x_0, \\GRAD f(x_1) - \\GRAD f(x_0) \\rangle \\ge 0\\) for all \\(x_0, x_1 \\in O\\).\n\\(f(y) \\ge f(x) + \\langle \\GRAD f(x), y - x \\rangle\\) for all \\(x, y \\in O\\).\n\\(\\GRAD^2 f(x)\\) is pointwise-semidefinite for all \\(x \\in O\\) (\\(f\\) twice differentiable).",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Convex sets and convex functions</span>"
    ]
  },
  {
    "objectID": "convexity/convexity.html#notes",
    "href": "convexity/convexity.html#notes",
    "title": "44  Convex sets and convex functions",
    "section": "Notes",
    "text": "Notes\nThe material of these notes is adapted from Rockafellar and Wets (2009).\n\n\n\n\nRockafellar, R.T. and Wets, R.J.-B. 2009. Variational analysis. Springer Science & Business Media.",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Convex sets and convex functions</span>"
    ]
  },
  {
    "objectID": "convexity/duality.html",
    "href": "convexity/duality.html",
    "title": "45  Duality",
    "section": "",
    "text": "45.1 Basic Intuition\nWe start by stating a basic fact\nSmax = 2\nn = 100\nnp = 3\n\nf = function(s) { return s*s + 1 }\n// Legendre transform of f\ng = function(s) { return s*s/4 - 1 }\n\npoints = {\n\n  var points = new Array()\n  var idx = 0\n  for( var i = -n ; i &lt;= n; i++) {\n    var s = Smax*i/n\n    points[idx++] = { point: s, value: f(s) }\n  }\n  return points\n  }\n\nlegendre = {\n  var points = new Array()\n  var idx = 0\n\n  for( var i = -np; i &lt;= np; i++) {\n    var p = Smax*i/np\n    var gp = g(p)\n    points[idx++] = { x: -Smax, y: -Smax*p - gp, index: i }\n    points[idx++] = { x:  Smax, y:  Smax*p - gp, index: i }\n  }\n\n  return points\n}\nAn illustration of this fact is shown in Figure 45.1.\nThe Lengendre-Fenchel transform is a compact way of representing this basic fact. To fix ideas, consider a twice differentiable and strictly convex function \\(f \\colon \\reals \\to \\reals\\). Arbitrarily fix a point \\(x^∘\\) and consider the tangent \\(τ(x)\\) to the plot of \\(f(⋅)\\) at \\(x^∘\\). This tangent has a slope \\(p = f'(x^∘)\\). Let \\(g\\) denote the negative \\(y\\)-intercept of the tangent (using the negative \\(y\\)-intercept is just a matter of convention). Thus, the equation for the tangent is: \\[\n  τ(x) = p x - g, \\quad \\forall x \\in \\reals\n\\] at at the point \\(x\\), the tangent \\(τ\\) intercepts the function \\(f\\); thus, \\(τ(x^∘) = f(x^∘)\\), or equivalently: \\[\n  f(x^∘) = p x^∘ - g\n  \\quad\\hbox{or, equivalently}\\quad\n  g = p x^∘ - f(x^∘)\n\\]\nNote that the \\(g\\) above depends on \\(x^∘\\). Since \\(f\\) is strictly convex, \\(f'\\) is strictly increasing. Thus, there is a one-to-one relationship between the point \\(x^∘\\) and its slope \\(p = f'(x^∘)\\). The Lengendre-Fenchel transform is a method to define the intercept \\(g\\) as a function of \\(p\\). In particular, it is defined as \\[\n  g(p) = \\sup_{x \\in \\reals} \\{ px - f(x) \\}.\n\\]\nTo understand this definition, suppose \\(x^∘\\) achieves the supremum in the above equation. Then, (i) \\(f(x^∘) = p x^∘ - g(p)\\); that is the line \\(px - g(p)\\) touches the function \\(f\\) at \\(x^∘\\); and (ii) at all other points \\(x \\neq x^∘\\), \\(g(p) &gt; p x - f(x)\\) or \\(f(x) &gt; px - g(p)\\); that is the line \\(px - g(p)\\) lies below the function \\(f\\). Hence, \\(px - g(p)\\) is a :supporting line of \\(f\\) (and since \\(f\\) is differentiable, equal to its tangent).\nAn implication of this definition is that for differentiable and convex \\(f\\), the Legendre transform \\(g\\) of \\(f\\) solves \\[\n  g(p) = p x^∘ - f(x^∘)\n\\] where \\(x^∘\\) solves \\(p = f'(x^∘)\\), which is called the duality condition.\nAn interesting example of Fenchel-Young inequality is the following. Consider a real-valued, strictly increasing, continuous function \\(h\\) on \\(\\reals\\) which satisfies \\(h(0) = 0\\), \\(h(x) \\to -∞\\) as \\(x \\to -∞\\), and \\(h(x) \\to ∞\\) as \\(x \\to ∞\\). Since \\(h\\) is continuous and strictly increasing, it has an inverse. Define \\[\\begin{equation}\\label{eq:legendre-example}\n  f(x) = \\int_{0}^x h(s) ds\n  \\quad\\hbox{and}\\quad\n  g(y) = \\int_{0}^y h^{-1}(t) dt.\n\\end{equation}\\]\nexampleFunction = function(s) {\n    if (s &lt;= 1) {\n        return (s - s**2/2)\n    } else\n    {\n        return 1/2 + (s - 1)**2/2\n    }\n  }\n\nexamplePoints = {\n  var points = new Array()\n  var idx = 0\n\n  const n = 500;\n  const Smax = 2; \n\n  for(var i = 0; i &lt;= n; i++) {\n      var s = Smax*i/n\n      points[idx++] = { x: s, y: exampleFunction(s) } \n  }\n  return points\n}\nThe graph of \\(t = h(s)\\) is shown in Figure 45.2, where the shaded portions represent \\(f(x)\\) and \\(g(y)\\). From the plots, we can infer Fenchel-Young’s inequality: \\[\n  xy \\le f(x) + g(y)\n\\] with equality if and only if \\(y = h(x) = \\dot f(x)\\). This immediately implies that \\[\n  g(y) = \\sup_{x \\in \\reals} \\bigl\\{ xy - f(x) \\bigr\\}\n\\] and \\[\n  f(x) = \\sup_{y \\in \\reals} \\bigl\\{ xy - g(y) \\bigr\\}.\n\\]",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Duality</span>"
    ]
  },
  {
    "objectID": "convexity/duality.html#basic-intuition",
    "href": "convexity/duality.html#basic-intuition",
    "title": "45  Duality",
    "section": "",
    "text": "Characterization of convex functions\n\n\n\nAny convex function can be represented as a pointwise supremum of convex functions. More formally, for any convex \\(f \\colon \\reals^n \\to \\reals\\), there exists a countable index set \\(I\\) and a family of \\(\\{a_i \\in \\reals^n\\}_{i \\in I}\\) and \\(\\{b_i \\in \\reals\\}_{i \\in I}\\) such that \\[\n  f(x) = \\sup_{i \\in I} \\{ a_i^\\TRANS x + b_i \\}\n\\]\n\n\n\n\n\naveragePlot = Plot.plot({\n  grid: true,\n  y: { domain: [0, 5] },\n\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(legendre, {x:\"x\", y:\"y\", z:\"index\", stroke:\"gray\"} ),\n\n    Plot.line(points, {x:\"point\", y:\"value\",\n                      stroke:\"red\", strokeWidth: 4}),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 45.1: Convex function as a supremum of affine functions\n\n\n\n\n\n\n\n\n\n\nWhy countable index set?\n\n\n\n\n\nFrom the figure you can convince yourself of the result for an uncountable index set \\(\\bar I\\). Now consider the set \\(\\bar S = \\{a_i, b_i\\}_{i \\in \\bar I} \\subset \\reals^{n+1}\\) (for uncountable \\(\\bar I\\)). Since \\(\\reals^{n+1}\\) is separable, let \\(S = \\{a_i, b_i\\}_{i \\in I}\\) be a countable dense subset of \\(\\bar S\\) (where \\(I\\) is countable). Since the supremum over any set is the same as the supremum of its dense subset, we can replace \\(\\bar I\\) by \\(I\\).\n\n\n\n\n\n\n\n\nExample 45.1 Let \\(f(x) = x^2\\). For a fixed \\(p\\), the duality condition is \\(p = 2x^∘\\) or \\(x^∘ = p/2\\). Therefore, \\[\n  g(p) = p x^∘ - f(x^∘) = \\frac{p^2}{2} - \\frac{p^2}{4} = \\frac{p^2}{4}.\n\\]\n\n\nExample 45.2 Let \\(f(x) = (x^α)/α\\), where \\(α &gt; 1\\). For a fixed \\(p\\), the duality condition is \\(p = (x^∘)^{α-1}\\) or \\(x^∘ = p^{1/(p-1)}\\). Therefore, \\[\n  g(p) = p x^∘ - f(x^∘) = p^{α/(α-1)} - \\frac{1}{α} p^{α/(α-1)}\n  = \\frac{α-1}{α} p^{α/(α-1)}.\n\\] We can compactly write the above expression is \\(g(p) = p^β/β\\), where \\(1/β = 1 - 1/α\\).\n\n\nExample 45.3 Let \\(f(x) = x \\log x + (1-x) \\log (1-x)\\), where the domain is \\([0,1]\\). This is the negative binary entropy. For a fixed \\(p\\), the duality condition is \\[\np = f'(x^∘) = \\log(x^∘) + 1 - \\log(1-x^∘) - 1\n% = \\log(x^∘) - \\log(1-x^∘)\n= \\log \\frac{x^∘}{1-x^∘}\n\\] Thus, \\(x^∘ = e^p/(1 + e^p)\\). Therefore, \\[\n  g(p) = p x^∘ - f(x^∘) = \\log(1 + e^p),\n\\] where the last step follows after some algebra.\n\n\n\n\n\n\n\nFenchel-Young inequality\n\n\n\nLet \\(g\\) be the Legendre-Fenchel transform of \\(f\\). Then, by definition (changing the variable names for convenience), we have \\[\\begin{equation}\\tag{Fenchel's inequality}\nxy \\le f(x) + g(y).\n\\end{equation}\\] For the special case of Example 45.2, we have \\[\\begin{equation}\n\\tag{Young's inequality}\nxy \\le \\frac{x^p}{p} + \\frac{y^q}{q}\n\\end{equation}\\] where \\(p, q &gt; 1\\) are such that \\(\\frac 1p + \\frac 1q = 1\\).\n\n\n\n\nPlot.plot({\n  grid: true,\n  x : { label: \"s\"}, \n  y : { label: \"t\"}, \n\n  marks: [\n    // Fill\n    Plot.areaY(examplePoints.filter(pt =&gt; pt.x &lt;= 1), {x:\"x\", y:\"y\",\n               fill:\"lightblue\", fillOpacity: 0.5}),\n\n    Plot.areaX(examplePoints.filter(pt =&gt; pt.y &lt;= 0.7), {x:\"x\", y:\"y\",\n               fill:\"pink\", fillOpacity: 0.5}),\n\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n\n    Plot.line([ {x:1, y:0}, {x:1, y: exampleFunction(1)} ], {x:\"x\", y:\"y\", stroke: \"gray\"} ),\n\n    Plot.line([ {x:1, y: exampleFunction(1)}, {x:1, y: 0.7} ], {x:\"x\", y:\"y\", strokeDasharray: \"4\", stroke: \"gray\"} ),\n\n    Plot.line([ {x:0, y:0.7}, {x:1.62, y:0.7} ], {x:\"x\", y:\"y\", stroke: \"gray\"} ),\n\n    // Line\n    Plot.line(examplePoints, {x:\"x\", y:\"y\",\n              stroke:\"red\", strokeWidth: 4}),\n    \n  ]\n})\nPlot.plot({\n  grid: true,\n  x : { label: \"s\"}, \n  y : { label: \"t\"}, \n\n  marks: [\n    // Fill\n    Plot.areaY(examplePoints.filter(pt =&gt; pt.x &lt;= 1.4), {x:\"x\", y:\"y\",\n               fill:\"lightblue\", fillOpacity: 0.5}),\n\n    Plot.areaX(examplePoints.filter(pt =&gt; pt.y &lt;= 0.4), {x:\"x\", y:\"y\",\n               fill:\"pink\", fillOpacity: 0.5}),\n\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n\n    Plot.line([ {x:1.4, y:0}, {x:1.4, y: exampleFunction(1.4)} ], {x:\"x\", y:\"y\", stroke: \"gray\"} ),\n\n    Plot.line([ {x:0.55, y: 0.4}, {x:1.4, y: 0.4} ], {x:\"x\", y:\"y\", strokeDasharray: \"4\", stroke: \"gray\"} ),\n\n    Plot.line([ {x:0, y:0.4}, {x:0.55, y:0.4} ], {x:\"x\", y:\"y\", stroke: \"gray\"} ),\n\n    // Line\n    Plot.line(examplePoints, {x:\"x\", y:\"y\",\n              stroke:\"red\", strokeWidth: 4}),\n    \n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(x &lt; y\\), with \\(x = 1\\) and \\(y = 0.7\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(x &gt; y\\), with \\(x = 1.4\\) and \\(y = 0.4\\)\n\n\n\n\n\n\n\nFigure 45.2: The functions \\(f(x)\\) and \\(g(y)\\) defined in \\eqref{eq:legendre-example}. The red curve shows the graph of \\(t = h(s)\\). The shaded blue region shows the value of \\(f(x)\\) for a particular value of \\(x\\). The red curve shown the value of \\(g(y)\\) for a particular value of \\(y\\).",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Duality</span>"
    ]
  },
  {
    "objectID": "convexity/duality.html#general-definition",
    "href": "convexity/duality.html#general-definition",
    "title": "45  Duality",
    "section": "45.2 General definition",
    "text": "45.2 General definition\n\nDefinition 45.1 For any function \\(f \\colon \\reals^n \\to \\bar {\\reals}\\), the function \\(f^* \\colon \\reals^n \\to \\bar \\reals\\) defined by \\[\\begin{equation}\\label{eq:conjugate}\n  f^*(p) \\coloneqq \\sup_{x \\in \\reals^n} \\bigl\\{\n  \\langle p, x \\rangle - f(x) \\bigr\\}\n\\end{equation}\\] is conjugate to \\(f\\), while the function \\(f^{**} = (f^*)^*\\) defined by \\[\\begin{equation}\\label{eq:biconjugate}\n  f^{**}(x) \\coloneqq \\sup_{p \\in \\reals^n} \\bigl\\{\n  \\langle p, x \\rangle - f^*(p) \\bigr\\}\n\\end{equation}\\] is biconjugate to \\(f\\). The mapping \\(f \\mapsto f^*\\) is called Legendre-Fenchel transform.\n\nThe significance of the conjugate can be understood in terms of epigraph relationships. Note that \\eqref{eq:conjugate} implies that \\[\n  (p,β) \\in \\text{epi } f^*\n  \\iff\n  β \\ge \\langle p, x \\rangle - α\n  \\text{ for all } (x,α) \\in \\text{epi } f.\n\\] Let \\(\\ell_{p,β}(x) \\coloneqq \\langle p, x \\rangle - β\\), then we can write the above relationship as \\[\n  (p,β) \\in \\text{epi } f^*\n  \\iff\n  \\ell_{p,β} \\le f,\n\\] that is, \\(f^*\\) describes a family of affine functions majorized by \\(f\\). Similarly, \\[\n  β \\ge f^*(p)\n  \\iff\n  β \\ge \\ell_{x,α}(p)\n  \\text{ for all } (x,α) \\in \\text{epi } f.\n\\]\n\nTheorem 45.1 For any function \\(f \\colon \\reals^n \\to \\bar \\reals\\) with \\(\\text{con } f\\) proper, both \\(f^*\\) and \\(f^{**}\\) are proper, lsc, convex and \\[\n  f^{**} = \\text{cl con } f.\n\\] Thus, \\(f^{**} \\le f\\) and when \\(f\\) is itself proper, lsc, and convex, one has \\(f^{**} = f\\).",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Duality</span>"
    ]
  },
  {
    "objectID": "convexity/duality.html#properties-of-legendre-fenchel-transform",
    "href": "convexity/duality.html#properties-of-legendre-fenchel-transform",
    "title": "45  Duality",
    "section": "45.3 Properties of Legendre-Fenchel transform",
    "text": "45.3 Properties of Legendre-Fenchel transform\nThus, Legendre-Fenchel transform sets up a one-to-one correspondence in the class of proper, lsc, and convex functions: if \\(f\\) is conjugate to \\(g\\), then \\(g\\) is a conjugate to \\(f\\): \\[\nf \\xleftrightarrow{\\hskip0.5em*\\hskip0.5em} g\n\\text { when }\n\\begin{cases}\ng(p) = \\sup_{x \\in \\reals^n} \\bigl\\{ \\langle p, x \\rangle - f(x) \\bigr\\} \\\\\nf(x) = \\sup_{x \\in \\reals^n} \\bigl\\{ \\langle p, x \\rangle - g(p) \\bigr\\}\n\\end{cases}\n\\]\nGiven \\(f \\xleftrightarrow{\\hskip0.5em*\\hskip0.5em} g\\), we have the following properties:\n\nScaling properties. For any \\(λ &gt; 0\\), \\[\\begin{align*}\nλ f(x)\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\nλg(λ^{-1} p),\n\\\\\nf(λx)\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\ng(λ^{-1} p),\n\\end{align*}\\]\nTranslation properties. \\[\\begin{align*}\nf(x) - \\langle a, x \\rangle\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\ng(p + a) ,\n\\\\\nf(x + b)\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\ng(p) - \\langle p, b \\rangle,\n\\\\\nf(x) + c\n&\\xleftrightarrow{\\hskip0.5em*\\hskip0.5em}\ng(p) - c,\n\\\\\n\\end{align*}\\]\n\n\nProposition 45.1 Let \\(f\\) be a finite, coercive, twice differentiable, and strongly convex function, then the conjugate \\(g = f^*\\) is also finite, coercive, twice differentiable, and strongly convex. Moreover,\n\nthe gradient mapping \\(\\GRAD f\\) is one-to-one from \\(\\reals^n\\) to \\(\\reals^n\\), and its inverse is \\(\\GRAD g\\); one has \\[\\begin{align*}\ng(p) &= \\bigl\\langle (\\GRAD f)^{-1}(p), p \\bigr\\rangle\n- f\\bigl((\\GRAD f)^{-1}(p)\\bigr), \\\\\nf(x) &= \\bigl\\langle (\\GRAD g)^{-1}(x), x \\bigr\\rangle\n- g\\bigl((\\GRAD g)^{-1}(x)\\bigr),\n\\end{align*}\\]\nThe matrices \\(\\GRAD^2 f(x)\\) and \\(\\GRAD^2 g(p)\\) are inverse to one another when \\(p = \\GRAD f(x)\\) or, equivalently, \\(x = \\GRAD g(p)\\).\n\n\nStrongly convex functions on a simplex have the following properties.\n\nProposition 45.2 Let \\(Δ\\) be the simplex in \\(\\reals^n\\) and \\(f \\colon Δ \\to \\reals\\) be twice differentiable and strongly convex. Let \\(g \\colon \\reals^n \\to \\reals\\) be the Legendre-Fenchel conjugate of \\(f\\). Then,\n\nUnique maximizing argument: \\(\\GRAD g\\) is Lipschitz and satisfies \\[\\GRAD g(p) = \\arg\\max_{x \\in Δ}\\bigl\\{ \\langle x, p \\rangle - f(x) \\bigr\\}.\\]\nBoundedness: If there are constants \\(L\\) and \\(U\\) such that for all \\(x \\in Δ\\), we have \\(L \\le f(x) \\le U\\), then \\[\n   \\| p \\|_{∞} - U \\le g(p) \\le \\| p \\|_{∞} - L.\n\\]\nDistributivity: For any \\(c \\in \\reals\\), \\[\n   g(p + c \\ONES) = g(p) + c.\n\\]\nMonotonicity: If \\(p_1 \\le p_2\\), then \\(g(p_1) \\le g(p_2)\\).",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Duality</span>"
    ]
  },
  {
    "objectID": "convexity/duality.html#notes",
    "href": "convexity/duality.html#notes",
    "title": "45  Duality",
    "section": "Notes",
    "text": "Notes\nThe material on the intuition behind Legendre-Fenchel transform is adapted from Kennerly (2011). The example for Young-Fenchel inequality and Figure 45.2 is taken from Ellis (1985). The material on general definition and properties is adapted from Rockafellar and Wets (2009). Proposition 45.1 is stated as Example 11.9 in Rockafellar and Wets (2009). Proposition 45.2 is from Geist et al. (2019).\n\n\n\n\nEllis, R.S. 1985. Entropy, large deviations, and statistical mechanics. Springer New York. DOI: 10.1007/978-1-4613-8533-2.\n\n\nGeist, M., Scherrer, B., and Pietquin, O. 2019. A theory of regularized Markov decision processes. Proceedings of the 36th international conference on machine learning, PMLR, 2160–2169. Available at: https://proceedings.mlr.press/v97/geist19a.html.\n\n\nKennerly, S. 2011. A graphical derivation of the legendre transform. Available at: http://einstein.drexel.edu/~skennerly/maths/Legendre.pdf.\n\n\nRockafellar, R.T. and Wets, R.J.-B. 2009. Variational analysis. Springer Science & Business Media.",
    "crumbs": [
      "Convexity Appendix",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Duality</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Afshari, M. and Mahajan, A. 2023.\nDecentralized linear quadratic systems with major and minor agents and\nnon-gaussian noise. IEEE Transactions on Automatic\nControl 68, 8, 4666–4681. DOI: 10.1109/tac.2022.3210049.\n\n\nAltman, Eitan. 1999. Constrained\nmarkov decision processes. CRC Press. Available at: http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf.\n\n\nArabneydi, J. and Mahajan, A. 2016.\nLinear quadratic mean field teams: Optimal and approximately optimal\ndecentralized solutions. Available at: https://arxiv.org/abs/1609.00056v2.\n\n\nArrow, K.J., Blackwell, D., and Girshick,\nM.A. 1949. Bayes and minimax solutions of sequential decision\nproblems. Econometrica 17, 3/4, 213. DOI: 10.2307/1905525.\n\n\nArrow, K.J., Harris, T., and Marschak, J.\n1952. Optimal inventory policy. Econometrica 20, 1,\n250–272. DOI: 10.2307/1907830.\n\n\nArthur, W.B. 1994. Increasing returns\nand path dependence in the economy. University of Michigan Press.\nDOI: 10.3998/mpub.10029.\n\n\nArtzrouni, M. 1986. On the convergence of\ninfinite products of matrices. Linear Algebra and its\nApplications 74, 11–21. DOI: 10.1016/0024-3795(86)90112-6.\n\n\nAsadi, K., Misra, D., and Littman, M.\n2018. Lipschitz continuity in model-based reinforcement\nlearning. Proceedings of the 35th international conference on\nmachine learning, PMLR, 264–273. Available at: https://proceedings.mlr.press/v80/asadi18a.html.\n\n\nÅström, K.J. 1970. Introduction to\nstochastic control theory. Dover.\n\n\nAthans, M. 1971. The role and use of the\nstochastic linear-quadratic-gaussian problem in control system design.\nIEEE Transactions on Automatic Control\n16, 6, 529–552. DOI: 10.1109/tac.1971.1099818.\n\n\nBai, C.-Z., Katewa, V., Gupta, V., and Huang,\nY.-F. 2015. A stochastic sensor selection scheme for sequential\nhypothesis testing with multiple sensors. IEEE transactions on\nsignal processing 63, 14, 3687–3699.\n\n\nBaras, J.S., Dorsey, A.J., and Makowski,\nA.M. 1984. Two competing queues with linear costs: The μc-rule is\noften optimal. Advances in Applied Probability 16, 1,\n8–8. DOI: 10.1017/s000186780002187x.\n\n\nBellman, R., Glicksberg, I., and Gross,\nO. 1955. On the optimal inventory equation. Management\nScience 2, 1, 83–104. DOI: 10.1287/mnsc.2.1.83.\n\n\nBerry, R.A. 2000. Power and delay\ntrade-offs in fading channels. PhD thesis, Massachusetts Institute of\nTechnology. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay\ntradeoffs in fading channels—small-delay asymptotics. IEEE\nTransactions on Information Theory 59, 6, 3939–3952. DOI:\n10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002.\nCommunication over fading channels with delay constraints.\nIEEE Transactions on Information Theory\n48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M.\n2012. Energy-efficient scheduling under delay constraints for wireless\nnetworks. Synthesis Lectures on Communication Networks\n5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nBertsekas, D.P. 2011. Dynamic\nprogramming and optimal control. Athena Scientific. Available at:\nhttp://www.athenasc.com/dpbook.html.\n\n\nBertsekas, D.P. 2013. Abstract\ndynamic programming. Athena Scientific Belmont. Available at: https://web.mit.edu/dimitrib/www/abstractdp_MIT.html.\n\n\nBertsekas, D.P. and Tsitsiklis, J.N.\n2000. Gradient convergence in gradient methods with errors.\nSIAM Journal on Optimization 10, 3,\n627–642. DOI: 10.1137/s1052623497331063.\n\n\nBitar, E., Poolla, K., Khargonekar, P.,\nRajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind.\nHawaii international conference on system sciences, IEEE,\n1931–1937.\n\n\nBlackwell, D. 1964. Memoryless strategies\nin finite-stage dynamic programming. The Annals of Mathematical\nStatistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nBlackwell, D. 1965. Discounted dynamic\nprogramming. The Annals of Mathematical Statistics 36,\n1, 226–235. DOI: 10.1214/aoms/1177700285.\n\n\nBlackwell, D. 1970. On stationary\npolicies. Journal of the Royal Statistical Society. Series A\n(General) 133, 1, 33. DOI: 10.2307/2343810.\n\n\nBogdan, K. and Więcek, M. 2022.\nBurkholder inequality by bregman divergence.\n\n\nBohlin, T. 1970. Information pattern for\nlinear discrete-time models with stochastic coefficients. IEEE\nTransactions on Automatic Control (TAC) 15, 1, 104–106.\n\n\nBorkar, V.S. 2008. Stochastic\napproximation. Hindustan Book Agency. DOI: 10.1007/978-93-86279-38-5.\n\n\nBorkar, V.S. and Meyn, S.P. 2000. The\no.d.e. Method for convergence of stochastic approximation and\nreinforcement learning. SIAM Journal on Control and\nOptimization 38, 2, 447–469. DOI: 10.1137/s0363012997331639.\n\n\nBozkurt, B., Mahajan, A., Nayyar, A., and\nOuyang, Y. 2023. Weighted norm bounds in MDPs with unbounded\nper-step cost.\n\n\nBurkholder, D.L. 1966. Martingale\ntransforms. The Annals of Mathematical Statistics 37,\n6, 1494–1504. DOI: 10.1214/aoms/1177699141.\n\n\nBuyukkoc, C., Varaiya, P., and Walrand,\nJ. 1985. The cμ rule revisited. Advances in Applied\nProbability 17, 1, 237–238. DOI: 10.2307/1427064.\n\n\nCassandra, A., Littman, M.L., and Zhang,\nN.L. 1997. Incremental pruning: A simple, fast, exact method for\npartially observable Markov decision processes.\nProceedings of the thirteenth conference on uncertainty\nin artificial intelligence.\n\n\nCassandra, A.R., Kaelbling, L.P., and Littman,\nM.L. 1994. Acting optimally in partially observable stochastic\ndomains. AAAI, 1023–1028.\n\n\nChakravorty, J. and Mahajan, A. 2018.\nSufficient conditions for the value function and optimal strategy to be\neven and quasi-convex. IEEE Transactions on Automatic Control\n63, 11, 3858–3864. DOI: 10.1109/TAC.2018.2800796.\n\n\nChang, J.T. 2007. Stochastic processes.\nAvailable at: http://www.stat.yale.edu/~pollard/Courses/251.spring2013/Handouts/Chang-notes.pdf.\n\n\nChen, H.-F. and Guo, L. 1991.\nIdentification and stochastic adaptive control. Birkhäuser\nBoston. DOI: 10.1007/978-1-4612-0429-9.\n\n\nChen, X. 2017. L#-convexity and\nits applications in operations. Frontiers of Engineering\nManagement 4, 3, 283. DOI: 10.15302/j-fem-2017057.\n\n\nCheng, H.-T. 1988. Algorithms for\npartially observable markov decision processes. PhD thesis, University\nof British Columbia, Vancouver, BC.\n\n\nDaley, D.J. 1968. Stochastically monotone\nmarkov chains. Zeitschrift für\nWahrscheinlichkeitstheorie und verwandte Gebiete 10, 4,\n305–317. DOI: 10.1007/BF00531852.\n\n\nDavis, M.H.A. 1979. Martingale methods in\nstochastic control. In: Stochastic control theory and stochastic\ndifferential systems. Springer-Verlag, 85–117. DOI: 10.1007/bfb0009377.\n\n\nDavis, M.H.A. and Varaiya, P.P. 1972.\nInformation states for linear stochastic systems. Journal of\nMathematical Analysis and Applications 37, 2, 384–402.\n\n\nDeGroot, M. 1970. Optimal statistical\ndecisions. Wiley-Interscience, Hoboken, N.J.\n\n\nDellacherie, C. and Meyer, P.-A. 1982.\nProbabilities and potential B: Theory of\nmartingales. North-Holland Mathematical Studies.\n\n\nDevlin, S. 2014. Potential based reward\nshaping tutorial. Available at: http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf.\n\n\nDevlin, S. and Kudenko, D. 2012. Dynamic\npotential-based reward shaping. Proceedings of the 11th\ninternational conference on autonomous agents and multiagent\nsystems, International Foundation for Autonomous Agents; Multiagent\nSystems, 433–440.\n\n\nDibangoye, J.S., Amato, C., Buffet, O., and\nCharpillet, F. 2016. Optimally solving dec-POMDPs as\ncontinuous-state MDPs. Journal of Artificial Intelligence\nResearch 55, 443–497. DOI: 10.1613/jair.4623.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A.\n2016. On monotonicity of the optimal transmission policy in cross-layer\nadaptive m -QAM modulation.\nIEEE Transactions on Communications 64, 9, 3771–3785.\nDOI: 10.1109/TCOMM.2016.2590427.\n\n\nDorato, P. and Levis, A. 1971. Optimal\nlinear regulators: The discrete-time case. IEEE\nTransactions on Automatic Control 16, 6, 613–620. DOI: 10.1109/tac.1971.1099832.\n\n\nDubins, L.E. and Savage, L.J. 2014.\nHow to gamble if you must: Inequalities for stochastic\nprocesses. Dover Publications.\n\n\nDurrett, R. 2019. Probability: Theory\nand examples. Cambridge University Press. DOI: 10.1017/9781108591034.\n\n\nEdgeworth, F.Y. 1888. The mathematical\ntheory of banking. Journal of the Royal Statistical Society\n51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nElliott, R., Li, X., and Ni, Y.-H. 2013.\nDiscrete time mean-field stochastic linear-quadratic optimal control\nproblems. Automatica 49, 11, 3222–3233. DOI: 10.1016/j.automatica.2013.08.017.\n\n\nEllis, R.S. 1985. Entropy, large\ndeviations, and statistical mechanics. Springer New York. DOI: 10.1007/978-1-4613-8533-2.\n\n\nFeinberg, E.A. 2016. Optimality\nconditions for inventory control. In: Optimization challenges in\ncomplex, networked and risky systems. INFORMS, 14–45. DOI: 10.1287/educ.2016.0145.\n\n\nFeinberg, E.A. and He, G. 2020.\nComplexity bounds for approximately solving discounted MDPs\nby value iterations. Operations Research Letters. DOI: 10.1016/j.orl.2020.07.001.\n\n\nFerguson, T.S. 1989. Who solved the\nsecretary problem? Statistical science, 282–289.\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004.\nOptimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nFöllmer, H. and Schied, A. 2010. Convex\nrisk measures. In: Encyclopedia of quantitative finance.\nAmerican Cancer Society. DOI: 10.1002/9780470061602.eqf15003.\n\n\nFu, F. and Schaar, M. van der. 2012.\nStructure-aware stochastic control for transmission scheduling. IEEE\nTransactions on Vehicular Technology 61, 9, 3931–3945.\nDOI: 10.1109/tvt.2012.2213850.\n\n\nFu, M.C. 2018. Monte carlo tree search: A\ntutorial. 2018 winter simulation conference (WSC), IEEE. DOI:\n10.1109/wsc.2018.8632344.\n\n\nGao, S. and Mahajan, A. 2022. Optimal\ncontrol of network-coupled subsystems: Spectral decomposition and\nlow-dimensional solutions. IEEE Transactions on Control\nof Network Systems 9, 2, 657–669. DOI: 10.1109/tcns.2021.3124259.\n\n\nGeiss, S. and Scheutzow, M. 2021.\nSharpness of Lenglart’s domination\ninequality and a sharp monotone version. Electronic Communications\nin Probability 26, none, 1–8. DOI: 10.1214/21-ECP413.\n\n\nGeist, M., Scherrer, B., and Pietquin, O.\n2019. A theory of regularized Markov decision processes.\nProceedings of the 36th international conference on machine\nlearning, PMLR, 2160–2169. Available at: https://proceedings.mlr.press/v97/geist19a.html.\n\n\nGladyshev, E.G. 1965. On stochastic\napproximation. Theory of Probability and Its Applications\n10, 2, 275–278. DOI: 10.1137/1110031.\n\n\nGrzes, M. and Kudenko, D. 2009.\nTheoretical and empirical analysis of reward shaping in reinforcement\nlearning. International conference on machine learning and\napplications, 337–344. DOI: 10.1109/ICMLA.2009.33.\n\n\nHardy, G.H., Littlewood, J.E., and Pólya,\nG. 1952. Inequalities. Cambridge University Press.\n\n\nHarris, F.W. 1913. How many parts to make\nat once. The magazine of management 10, 2, 135–152.\nDOI: 10.1287/opre.38.6.947.\n\n\nHay, N., Russell, S., Tolpin, D., and Shimony,\nS.E. 2012. Selecting computations: Theory and applications.\nUAI. Available at: http://www.auai.org/uai2012/papers/123.pdf.\n\n\nHernandez-Hernández, D. and Marcus, S.I.\n1996. Risk sensitive control of markov processes in countable state\nspace. Systems & Control Letters 29,\n3, 147–155. DOI: 10.1016/s0167-6911(96)00051-5.\n\n\nHernández-Hernández, D. 1999. Existence\nof risk-sensitive optimal stationary policies for controlled markov\nprocesses. Applied Mathematics and Optimization 40, 3,\n273–285. DOI: 10.1007/s002459900126.\n\n\nHernández-Lerma, O. and Lasserre, J.B.\n1996. Discrete-time markov control processes. Springer New\nYork. DOI: 10.1007/978-1-4612-0729-0.\n\n\nHernández-Lerma, O. and Lasserre, J.B.\n1999. Further topics on discrete-time markov control processes.\nSpringer New York. DOI: 10.1007/978-1-4612-0561-6.\n\n\nHinderer, K. 2005. Lipschitz continuity\nof value functions in Markovian decision processes.\nMathematical Methods of Operations Research 62, 1,\n3–22. DOI: 10.1007/s00186-005-0438-1.\n\n\nHopcroft, J. and Kannan, R. 2012.\nComputer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf.\n\n\nHoward, R.A. 1960. Dynamic\nprogramming and markov processes. The M.I.T. Press.\n\n\nHoward, R.A. and Matheson, J.E. 1972.\nRisk-sensitive markov decision processes. Management Science\n18, 7, 356–369. DOI: 10.1287/mnsc.18.7.356.\n\n\nJenner, E., Hoof, H. van, and Gleave, A.\n2022. Calculus on MDPs: Potential shaping as a gradient. Available at:\nhttps://arxiv.org/abs/2208.09570v1.\n\n\nKalman, R.E. 1960. Contributions to the\ntheory of optimal control. Boletin de la Sociedad Matematica\nMexicana 5, 102–119.\n\n\nKaratzas, I. and Sudderth, W.D. 2010. Two\ncharacterizations of optimality in dynamic programming. Applied\nMathematics and Optimization 61, 3, 421–434. DOI: 10.1007/s00245-009-9093-x.\n\n\nKeilson, J. and Kester, A. 1977. Monotone\nmatrices and monotone markov processes. Stochastic Processes and\ntheir Applications 5, 3, 231–241.\n\n\nKelly, J.L., Jr. 1956. A new\ninterpretation of information rate. Bell System Technical\nJournal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nKennerly, S. 2011. A graphical derivation\nof the legendre transform. Available at: http://einstein.drexel.edu/~skennerly/maths/Legendre.pdf.\n\n\nKoole, G. 2006. Monotonicity in markov\nreward and decision chains: Theory and applications. Foundations and\nTrends in Stochastic Systems 1, 1, 1–76. DOI:\n10.1561/0900000002.\n\n\nKuhn, H.W. 1950. Extensive games.\nProceedings of the National Academy of Sciences 36,\n10, 570–576. DOI: 10.1073/pnas.36.10.570.\n\n\nKuhn, H.W. 1953. Extensive games and the\nproblem of information. In: H.W. Kuhn and A.W. Tucker, eds.,\nContributions to the theory of games. Princeton University\nPress, 193–216.\n\n\nKumar, P.R. and Varaiya, P. 1986.\nStochastic systems: Estimation identification and adaptive\ncontrol. Prentice Hall.\n\n\nKunnumkal, S. and Topaloglu, H. 2008.\nExploiting the structural properties of the underlying markov decision\nproblem in the q-learning algorithm. INFORMS Journal on\nComputing 20, 2, 288–301. DOI: 10.1287/ijoc.1070.0240.\n\n\nKushner, H.J. and Yin, G.G. 1997.\nStochastic approximation algorithms and applications. Springer\nNew York. DOI: 10.1007/978-1-4899-2696-8.\n\n\nKwakernaak, H. 1965. Theory of\nself-adaptive control systems. In: Springer, 14–18.\n\n\nLai, T.L. 2003. Stochastic approximation:\nInvited paper. The Annals of Statistics 31, 2. DOI: 10.1214/aos/1051027873.\n\n\nLenglart, É. 1977. Relation de domination\nentre deux processus. Annales de l’institut henri\npoincaré. Section b. Calcul des probabilités\net statistiques, 171–179.\n\n\nLevy, H. 1992. Stochastic dominance and\nexpected utility: Survey and analysis. Management Science\n38, 4, 555–593. DOI: 10.1287/mnsc.38.4.555.\n\n\nLevy, H. 2015. Stochastic dominance:\nInvestment decision making under uncertainty. Springer. DOI: 10.1007/978-3-319-21708-6.\n\n\nLewis, F.L., Vrabie, D., and Syrmos, V.L.\n2012. Optimal control. John Wiley & Sons.\n\n\nMahajan, A. 2008. Sequential\ndecomposition of sequential dynamic teams: Applications to real-time\ncommunication and networked control systems. PhD thesis, University of\nMichigan, Ann Arbor, MI.\n\n\nMahajan, A., Niculescu, S.-I., and Vidyasagar,\nM. 2024. A vector almost-sure supermartingale theorem and its\napplications. In: Submitted to IEEE conference on decision and\ncontrol. IEEE.\n\n\nMarshall, A.W., Olkin, I., and Arnold,\nB.C. 2011. Inequalities: Theory of majorization and its\napplications. Springer New York. DOI: 10.1007/978-0-387-68276-1.\n\n\nMorse, P. and Kimball, G. 1951.\nMethods of operations research. Technology Press of MIT.\n\n\nMüller, A. 1997a. Integral probability\nmetrics and their generating classes of functions. Advances in\nApplied Probability 29, 2, 429–443. DOI: 10.2307/1428011.\n\n\nMüller, A. 1997b. How does the value\nfunction of a markov decision process depend on the transition\nprobabilities? Mathematics of Operations Research 22,\n4, 872–885. DOI: 10.1287/moor.22.4.872.\n\n\nMurota, K. 1998. Discrete convex\nanalysis. Mathematical Programming 83, 1–3, 313–371.\nDOI: 10.1007/bf02680565.\n\n\nNain, P., Tsoucas, P., and Walrand, J.\n1989. Interchange arguments in stochastic scheduling. Journal of\nApplied Probability 26, 4, 815–826. DOI: 10.2307/3214386.\n\n\nNerode, A. 1958. Linear automaton\ntransformations. Proceedings of American Mathematical\nSociety 9, 541–544.\n\n\nNeveu, J. 1975. Discrete parameter\nmartingales. North Holland.\n\n\nNg, A.Y., Harada, D., and Russell, S.\n1999. Policy invariance under reward transformations: Theory and\napplication to reward shaping. ICML, 278–287. Available at: http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf.\n\n\nNorris, J.R. 1998. Markov\nchains. Cambridge university press.\n\n\nOh, S. and Özer, Ö. 2016. Characterizing\nthe structure of optimal stopping policies. Production and\nOperations Management 25, 11, 1820–1838. DOI: 10.1111/poms.12579.\n\n\nPicard, J. 2007. Concentration\ninequalities and model selection. Springer Berlin Heidelberg. DOI:\n10.1007/978-3-540-48503-2.\n\n\nPiunovskiy, A.B. 2011. Examples in\nmarkov decision processes. Imperial College Proess. DOI: 10.1142/p809.\n\n\nPollard, D. 2002. A user’s guide to\nmeasure theoretic probability. Cambridge University Press.\n\n\nPomatto, L., Strack, P., and Tamuz, O.\n2020. Stochastic dominance under independent noise. Journal of\nPolitical Economy 128, 5, 1877–1900. DOI: 10.1086/705555.\n\n\nPorteus, E.L. 1975. Bounds and\ntransformations for discounted finite markov decision chains.\nOperations Research 23, 4, 761–784. DOI: 10.1287/opre.23.4.761.\n\n\nPorteus, E.L. 2008. Building intuition:\nInsights from basic operations management models and principles. In: D.\nChhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nPuterman, M.L. 2014. Markov decision\nprocesses: Discrete stochastic dynamic programming. John Wiley\n& Sons. DOI: 10.1002/9780470316887.\n\n\nQin, Y., Cao, M., and Anderson, B.D.O.\n2020. Lyapunov criterion for stochastic systems and its applications in\ndistributed computation. IEEE Transactions on Automatic\nControl 65, 2, 546–560. DOI: 10.1109/tac.2019.2910948.\n\n\nRachelson, E. and Lagoudakis, M.G. 2010.\nOn the locality of action domination in sequential decision making.\nProceedings of 11th international symposium on artificial\nintelligence and mathematics. Available at: https://oatao.univ-toulouse.fr/17977/.\n\n\nRachev, S.T. 1991. Probability\nmetrics and the stability of stochastic models. Wiley, New York.\n\n\nRigollet, P. 2015. High-dimensional\nstatistics. Available at: https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/.\n\n\nRiis, J.O. 1965. Discounted\nMarkov programming in a periodic process. Operations\nResearch 13, 6, 920–929. DOI: 10.1287/opre.13.6.920.\n\n\nRivasplata, O. 2012. Subgaussian random\nvariables: An expository note. Available at: http://stat.cmu.edu/~arinaldo/36788/subgaussians.pdf.\n\n\nRobbins, H. and Monro, S. 1951. A\nstochastic approximation method. The Annals of Mathematical\nStatistics 22, 3, 400–407. DOI: 10.1214/aoms/1177729586.\n\n\nRobbins, H. and Siegmund, D. 1971. A\nconvergence theorem for non-negative almost supermartingales and some\napplications. In: Optimizing methods in statistics. Elsevier,\n233–257. DOI: 10.1016/b978-0-12-604550-5.50015-8.\n\n\nRockafellar, R.T. and Wets, R.J.-B. 2009.\nVariational analysis. Springer Science & Business Media.\n\n\nRoss, S.M. 1974. Dynamic programming and\ngambling models. Advances in Applied Probability 6, 3,\n593–606. DOI: 10.2307/1426236.\n\n\nRoy, A., Borkar, V., Karandikar, A., and\nChaporkar, P. 2022. Online reinforcement learning of optimal\nthreshold policies for Markov decision processes. IEEE\nTransactions on Automatic Control 67, 7, 3722–3729. DOI:\n10.1109/tac.2021.3108121.\n\n\nSandell, J., Nils R. 1974. Control of\nfinite-state, finite-memory stochastic systems. PhD thesis,\nMassachussets Institute of Technology, Cambridge, MA.\n\n\nSayedana, B. and Mahajan, A. 2020.\nCounterexamples on the monotonicity of delay optimal strategies for\nenergy harvesting transmitters. IEEE Wireless\nCommunications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E.\n2020. Cross-layer communication over fading channels with adaptive\ndecision feedback. International symposium on modeling and\noptimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nScherrer, B. 2016. On periodic markov\ndecision processes. Available at: https://ewrl.files.wordpress.com/2016/12/scherrer.pdf.\n\n\nSerfozo, R.F. 1976. Monotone optimal\npolicies for markov decision processes. In: Mathematical programming\nstudies. Springer Berlin Heidelberg, 202–215. DOI: 10.1007/bfb0120752.\n\n\nShwartz, A. 2001. Death and discounting.\nIEEE Transactions on Automatic Control\n46, 4, 644–647. DOI: 10.1109/9.917668.\n\n\nSimon, H.A. 1956. Dynamic programming\nunder uncertainty with a quadratic criterion function.\nEconometrica 24, 1, 74–81. DOI: 10.2307/1905261.\n\n\nSingh, S.P. and Yee, R.C. 1994. An upper\nbound on the loss from approximate optimal-value functions. Machine\nLearning 16, 3, 227–233. DOI: 10.1007/bf00993308.\n\n\nSkinner, B.F. 1938. Behavior of\norganisms. Appleton-Century.\n\n\nSmallwood, R.D. and Sondik, E.J. 1973.\nThe optimal control of partially observable markov processes over a\nfinite horizon. Operations Research 21, 5, 1071–1088.\nDOI: 10.1287/opre.21.5.1071.\n\n\nSmith, J.E. and McCardle, K.F. 2002.\nStructural properties of stochastic dynamic programs. Operations\nResearch 50, 5, 796–809. DOI: 10.1287/opre.50.5.796.365.\n\n\nStout, W.F. 1974. Almost sure\nconvergence. Academic Press.\n\n\nStriebel, C. 1965. Sufficient statistics\nin the optimal control of stochastic systems. Journal of\nMathematical Analysis and Applications 12, 576–592.\n\n\nStrusevich, V.A. and Rustogi, K. 2016.\nPairwise interchange argument and priority rules. In: Scheduling\nwith time-changing effects and rate-modifying activities. Springer\nInternational Publishing, 19–36. DOI: 10.1007/978-3-319-39574-6_2.\n\n\nSubramanian, J., Sinha, A., Seraj, R., and\nMahajan, A. 2022. Approximate information state for approximate\nplanning and reinforcement learning in partially observed systems.\nJournal of Machine Learning Research 23, 12, 1–83.\nAvailable at: http://jmlr.org/papers/v23/20-1165.html.\n\n\nTaylor, H.M. 1967. Evaluating a call\noption and optimal timing strategy in the stock market. Management\nScience 14, 1, 111–120. Available at: http://www.jstor.org/stable/2628546.\n\n\nTheil, H. 1954. Econometric models and\nwelfare maximization. Wirtschaftliches Archiv 72,\n60–83. DOI: 10.1007/978-94-011-2410-2_1.\n\n\nTheil, H. 1957. A note on certainty\nequivalence in dynamic planning. Econometrica, 346–349. DOI: 10.1007/978-94-011-2410-2_3.\n\n\nTopkis, D.M. 1998. Supermodularity\nand complementarity. Princeton University Press.\n\n\nTrench, W.F. 1999. Invertibly convergent\ninfinite products of matrices. Journal of Computational and Applied\nMathematics 101, 1–2, 255–263. DOI: 10.1016/s0377-0427(98)00191-5.\n\n\nTsitsiklis, J.N. 1984. Periodic review\ninventory systems with continuous demand and discrete order sizes.\nManagement Science 30, 10, 1250–1254. DOI: 10.1287/mnsc.30.10.1250.\n\n\nTsitsiklis, J.N. and Roy, B. van. 1996.\nFeature-based methods for large scale dynamic programming. Machine\nLearning 22, 1-3, 59–94. DOI: 10.1007/bf00114724.\n\n\nUrgaonkar, R., Wang, S., He, T., Zafer, M.,\nChan, K., and Leung, K.K. 2015. Dynamic service migration and\nworkload scheduling in edge-clouds. Performance Evaluation\n91, 205–228. DOI: 10.1016/j.peva.2015.06.013.\n\n\nVeinott, A.F. 1965. The optimal inventory\npolicy for batch ordering. Operations Research 13, 3,\n424–432. DOI: 10.1287/opre.13.3.424.\n\n\nVidyasagar, M. 2023. Convergence of\nstochastic approximation via martingale and converse\nLyapunov methods. Mathematics of Control, Signals, and\nSystems 35, 2, 351–374. DOI: 10.1007/s00498-023-00342-9.\n\n\nVillani, C. et al. 2008. Optimal\ntransport: Old and new. Springer.\n\n\nWainwright, M.J. 2019.\nHigh-dimensional statistics. Cambridge University Press. DOI:\n10.1017/9781108627771.\n\n\nWald, A. 1945. Sequential tests of\nstatistical hypotheses. The Annals of Mathematical Statistics\n16, 2, 117–186. DOI: 10.1214/aoms/1177731118.\n\n\nWald, A. and Wolfowitz, J. 1948. Optimum\ncharacter of the sequential probability ratio test. The Annals of\nMathematical Statistics 19, 3, 326–339. DOI: 10.1214/aoms/1177730197.\n\n\nWalrand, J. 1988. An introduction to\nqueueing networks. Prentice Hall.\n\n\nWang, S., Urgaonkar, R., Zafer, M., He, T.,\nChan, K., and Leung, K.K. 2019. Dynamic service migration in\nmobile edge computing based on Markov decision process.\nIEEE/ACM Transactions on Networking\n27, 3, 1272–1288. DOI: 10.1109/tnet.2019.2916577.\n\n\nWhitin, S. 1953. The theory of\ninventory management. Princeton University Press.\n\n\nWhittle, P. 1982. Optimization over\ntime: Dynamic programming and stochastic control. Vol. 1 and 2.\nWiley.\n\n\nWhittle, P. 1996. Optimal control:\nBasics and beyond. Wiley.\n\n\nWhittle, P. 2002. Risk sensitivity,\nA strangely pervasive concept. Macroeconomic\nDynamics 6, 1, 5–18. DOI: 10.1017/s1365100502027025.\n\n\nWhittle, P. and Komarova, N. 1988. Policy\nimprovement and the newton-raphson algorithm. Probability in the\nEngineering and Informational Sciences 2, 2, 249–255. DOI:\n10.1017/s0269964800000760.\n\n\nWiewiora, E. 2003. Potential-based\nshaping and q-value initialization are equivalent. Journal of\nArtificial Intelligence Research 19, 1, 205–208.\n\n\nWitsenhausen, H.S. 1969. Inequalities for\nthe performance of suboptimal uncertain systems. Automatica\n5, 4, 507–512. DOI: 10.1016/0005-1098(69)90112-5.\n\n\nWitsenhausen, H.S. 1970. On performance\nbounds for uncertain systems. SIAM Journal on Control\n8, 1, 55–89. DOI: 10.1137/0308004.\n\n\nWitsenhausen, H.S. 1973. A standard form\nfor sequential stochastic control. Mathematical Systems Theory\n7, 1, 5–11. DOI: 10.1007/bf01824800.\n\n\nWitsenhausen, H.S. 1975. On policy\nindependence of conditional expectation. Information and\nControl 28, 65–75.\n\n\nWitsenhausen, H.S. 1976. Some remarks on\nthe concept of state. In: Y.C. Ho and S.K. Mitter, eds., Directions\nin large-scale systems. Plenum, 69–75.\n\n\nWitsenhausen, H.S. 1979. On the structure\nof real-time source coders. Bell System Technical Journal\n58, 6, 1437–1451.\n\n\nWittenmark, B., Åström, K.J., and Årzén,\nK.-E. 2002. Computer control: An overview. In: IFAC\nprofessional brief. IFAC. Available at: https://www.ifac-control.org/publications/list-of-professional-briefs/pb_wittenmark_etal_final.pdf.\n\n\nWonham, W.M. 1968. On a matrix riccati\nequation of stochastic control. SIAM Journal on Control\n6, 4, 681–697. DOI: 10.1137/0306044.\n\n\nWoodall, W.H. and Reynolds, M.R. 1983. A\ndiscrete markov chain representation of the sequential probability ratio\ntest. Communications in Statistics. Part C: Sequential Analysis\n2, 1, 27–44. DOI: 10.1080/07474948308836025.\n\n\nYeh, E.M. 2012. Fundamental performance\nlimits in cross-layer wireless optimization: Throughput, delay, and\nenergy. Foundations and Trends in Communications and Information\nTheory 9, 1, 1–112. DOI: 10.1561/0100000014.\n\n\nZhang, H. 2009. Partially observable\nMarkov decision processes: A geometric technique and\nanalysis. Operations Research.\n\n\nZhang, N. and Liu, W. 1996. Planning\nin stochastic domains: Problem characteristics and approximation.\nHong Kong Univeristy of Science; Technology.\n\n\nZolotarev, V.M. 1984. Probability\nmetrics. Theory of Probability & Its Applications\n28, 2, 278–302. DOI: 10.1137/1128025.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "assignments/01.html",
    "href": "assignments/01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Exercise 1.1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 1.2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 2.3 from the notes on the newsvendor problem. Provide an analytic solution to the problem, similar to the derivation of the analytic solution for the case of continuous demand and actions in the notes.",
    "crumbs": [
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/02.html",
    "href": "assignments/02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Inventory Management. Consider a variation of the inventory management problem considered in class. Suppose that the inventory \\(S_t\\) and the actions \\(A_t\\) takes values in the set \\(\\mathbb S = \\{0, 1, \\dots,\nL \\}\\), and the dynamics are given by \\[\n   S_{t+1} = \\bigl[ S_t + A_t - W_t \\bigr]_0^L,\n\\] where \\([ s ]_{0}^L\\) is a function which clips the values between \\(0\\) and \\(L\\), i.e., \\[\n   [ s ]_0^L =\n   \\begin{cases}\n   0, & \\text{if $s &lt; 0$} \\\\\n   s, & \\text{if $0 \\le s \\le L$} \\\\\n   L, & \\text{if $s &gt; L$}.\n   \\end{cases}\n\\] The demand \\(W_t\\) takes values in the set \\(\\ALPHABET W = \\{0, 1, 2\\}\\) with probability mass function \\(P_W = [ 0.1, 0.7, 0.2 ]\\).\nThe per-step cost is given by \\[\n   c_t(S_t, A_t, W_t) = (S_t + A_t - W_t)^2.\n\\] Note that in this case, the inventoy cannot be negative but the per-step cost does take shortage into account.\nWrite the dynamic programming equation and numerically solve it for a horizon \\(T = 10\\) and inventory level \\(L = 5\\). Plot the value function and the optimal policy for \\(t=1\\) and \\(t=2\\).\n\n\nExercise 5.1 from notes on Finite horizon MDPs.\nExercise 5.2 from notes on Finite horizon MDPs.",
    "crumbs": [
      "Assignments",
      "Assignment 2"
    ]
  }
]