---
title: "Theory: Reward Shaping"
weight: 31
categories:
  - MDP
tags:
  - reward shaping
---

What are the conditions under which two MDPs which have the same dynamics but
different cost functions have the same optimal policy? This is an important
question in reinforcement learning (where one often _shapes_ the reward
function to speed up learning) and inverse reinforcement learning (where one
learns the reward function from the behavior of an expert). The following
result provides a complete answer to this question. These results are
typically established for inifinte horizon models. However, in my opinion, it is conceptually simpler to start with the finite horizon model. 

Let $M^1$ and $M^2$ denote two MDPs on the same state space $\ALPHABET S$ and
action space $\ALPHABET A$. Both MDPs have the same dynamics $P = (P_1, \dots,
P_T)$, but different cost functions $c^1
= (c^1_1, \dots, c^1_T)$ and $c^2 = (c^2_1, \dots, c^2_T)$. We assume that for
$t \in \{1, \dots, T-1\}$, the per-step cost is a function of the current
state, current action, and next state (see [cost depending on next
state](../mdp-functional#cost-depends-on-next-state))[^1] and for $t = T$, the
per-step cost function is just a function of the current state. 

[^1]: We choose the cost to depend on the next state only for convenience of
analysis. The result of @thm:reward-shaping can be established for models where
the cost depends only on the per-step cost by replacing property 2 in
@thm:reward-shaping by
$$ c^2_t(s,a) = c^1_t(s,a) + \sum_{s' \in \ALPHABET S} P_t(s'|s,a) Φ_{t+1}(s') -
Φ_t(s).$$

::: highlight :::
Theorem #thm:reward-shaping

:   Suppose the cost functions in MDPs $M^1$ and $M^2$ are related as follows:

    1. For $t = T$,
    $$ c^2_T(s) = c^1_T(s) - Φ_T(s).  $$

    2. For $t \in \{1, \dots, T-1\}$, 
    $$ c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) +  Φ_{t+1}(s_{+}) - Φ_t(s). $$

    Then, for any policy $π$,
    $$\begin{equation}\label{eq:result}
        Q^{\pi,2}_t(s,a) = Q^{\pi,1}_t(s,a) - Φ_t(s)
        \quad\text{and}\quad
        V^{\pi,2}_t(s) = V^{\pi,1}_t(s) - Φ_t(s).
    \end{equation}$$

:::

Remark #rem:sign

:   The sign of the potential function is irrelevant. So, we could also
    have written
       $$ c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) +  Φ_t(s) - Φ_{t+1}(s_{+}) $$
    and argued that 
    $$  V^{\pi,2}_t(s) = V^{\pi,1}_t(s) + Φ_t(s).$$

<details>
<summary>
#### Proof {-}
</summary>
<div>
We prove the result by backward induction. First note that
$$
  V^{\pi,2}_T(s) = c^2_T(s) = c^1_T(s) - Φ_T(s) = V^{\pi,1}_T(s) - Φ_T(s).
$$
This forms the basis of induction. Now suppose that \\eqref{eq:result} holds for
time $t+1$. Now consider
$$\begin{align*}
Q^{\pi,2}_t(s,a) &= \EXP[ c^2_t(s,a,S_{t+1}) + V^{\pi,2}_{t+1}(S_{t+1}) \mid S_t
= s, a ]
\\
&\stackrel{(a)}= \EXP[ c^1_t(s,a,S_{t+1}) - Φ_t(s) + Φ_{t+1}(S_{t+1}) \\
&\qquad + V^{\pi,1}_{t+1}(S_{t+1}) - Φ_{t+1}(S_{t+1}) \mid S_t = s, A_t = a ] \\
&= \EXP[ c^1_t(s,a,S_{t+1}) - Φ_t(s) + V^1_{t+1}(S_{t+1}) \mid
S_t = s, A_t = a] \\
&= Q^{\pi,1}_t(s,a) - Φ_t(s),
\end{align*}$$
where $(a)$ follows from property 2 and the induction hypothesis.

Now, 
$$ \begin{align*}
  V^{\pi,2}_t(s) &= Q^{\pi,2}_t(s,\pi(s)) \\
  &= Q^{\pi,1}_t(s,\pi(s) - Φ_t(s) \\ 
  &= V^{\pi,1}_t(s) - Φ_t(s). 
\end{align*}$$

This proves the induction step.
</div>
</details>

By almost an analogous argument, we can show that the optimal value functions
also satisfy a similar relationship.

::: highlight :::

Corollary #cor:reward-shaping

: Under conditions of @thm:reward-shaping,
    $$\begin{equation}\label{eq:result-opt}
        Q^{2}_t(s,a) = Q^{1}_t(s,a) - Φ_t(s)
        \quad\text{and}\quad
        V^{2}_t(s) = V^{1}_t(s) - Φ_t(s).
    \end{equation}$$

:::

Remark #rem:advantage

:   The advantage (or benefit) function given by
       $$ B_t(s,a) := Q_t(s,a) - V_t(s) $$
    measures the relative cost of choosing action $a$ over the optimal action.
    An implication of \\eqref{eq:result-opt} is that reward shaping does not
    change the advantage function!

Remark #rem:difference

:   Another implication of @thm:reward-shaping and @cor:reward-shaping is that
    for any policy $π$, 
    $$ V^{\pi,2}_t(s) - V^{2}_t(s) = V^{\pi,1}_t(s) - V^1_t(s). $$
    Thus, reward shaping also preserves near-optimality; i.e., if a policy is
    approximately optimal in model $M^1$, then it is approximately optimal in
    model $M^2$ as well. 

# Generalization to discounted models

Now consider a finite horizon discounted cost problem, where the performance
of a policy $π$ is given by 
$$ 
J(π) = \EXP\Bigl[ \sum_{t=1}^{T-1} γ^{t-1} c_t(S_t, A_t) + γ^T c_T(S_T)
       \Bigr]. 
$$

As argued in [the introduction to discounted models][discounted], the dynamic
prgram for this case is given by 

$$ V_{T}(s) = c_T(s) $$
and for $t \in \{T-1, \dots, 1\}$:
$$ \begin{align*}
  Q_t(s,a) &= c(s,a) + γ \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\
  V_t(s) &= \min_{a \in \ALPHABET A} Q_t(s,a).
\end{align*} $$

[discounted]: ../mdp-functional#discounted-cost

For such models, we have the following.

::: highlight :::

Corollary #cor:discounted

:   For discounted cost models, the results of [Theorem #](#thm:reward-shaping)
    and [Corollary #](#cor:reward-shaping) continue to hold if condition 2 is
    replaced by

    2.  For $t \in \{1, \dots, T-1\}$,

        $$ c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) + γ Φ_{t+1}(s_{+}) - Φ_t(s). $$

:::


Remark #rem:inf

:   If the cost function is time homogeneous, [Corollary #](#cor:discounted)
    extends naturally to infinite horizon models with a time-homogeneous potential
    function. A remarkable feature is that if the potential function is chosen
    as the value function, i.e., $Φ(s) = V(s)$, then the value function of the
    modified cost $\tilde c(s,a,s_{+})$ is zero!


[martingale]: ../../inf-mdp/martingale-approach

# Examples

As an example of reward shaping, see the notes on [inventory
management][inventory]. Also see the notes on [martingale approach to stochastic
control][martingale] for an iteresting relationship between reward shaping and
martingales.

# References {-}

The idea of _reward shaping_ was proposed by @Skinner1938 to synthesize
complex behavior by guiding animals to perform simple functions (see
[Skinner's Box
Experiment](https://en.wikipedia.org/wiki/Operant_conditioning_chamber)). The
formal description of reward shaping comes from @Porteus1975, who established
a result similar to @Ng1999, and called it the transformation method.
@Porteus1975 also describes transformations of the dynamics which preserve the
optimal policy.

@cor:discounted was also re-established by @Ng1999, who aslo provided a
partial converse. The results of @Porteus1975 and @Ng1999 were restricted to
time-homogeneous potential functions. The generalization to time-varying
potential functions was presented in @Devlin2012. 

The partial converse of @cor:reward-shaping established by @Ng1999 states that
the shaping presented in @thm:reward-shaping is the _only_ additive cost
transformation that that preserves the set of optimal policy. However, this
converse was derived under the assumption that the transition dynamics are
_complete_ (see @Ng1999). A similar converse under a weaker set of assumptions
on the transition dynamics is established in @Jenner2022. 

For a discussion on practical considerations in using reward shaping in
reinforcement learning, see @Grzes2009 and @Devlin2014. As a counter-point,
@Wiewiora2003 shows that the advantages of reward shaping can also be achieved
by simply adding the potential function to the $Q$-function initialization. 



[inventory]: ../../inf-mdp/inventory-management

---

