<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/</link>
    <description>Recent content on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://adityam.github.io/stochastic-control/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Quadratic Regulation (LQR)</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/lqr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/lqr/</guid>
      <description>Note: To be consistent with the notation used in linear systems, we denote the state and action by lowercase \(x\) and \(u\), even for stochastic systems (unlike the notation used for other models where we use uppercase \(X\) and \(U\) for state and actions to emphasize the fact they are random variables).
We start by considering a determinisitc linear system with state \(x_t \in \reals^n\) and control actions \(u_t \in \reals^m\) and dynamics \[ x_{t+1} = A_t x_t + B_t u_t,\] where \(A_t \in \reals^{n \times n}\) and \(B_t \in \reals^{n \times m}\) are known matrices.</description>
    </item>
    
    <item>
      <title>Overview of adaptive control for linear systems</title>
      <link>https://adityam.github.io/stochastic-control/adaptive-control/linear-adaptive-control/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/adaptive-control/linear-adaptive-control/</guid>
      <description>Adaptive control is a umbrella term which is often used to describe algorithms which are able to control an unknown system (i.e., a system with unknown dynamics). In this section, we focus on adaptive control of linear systems. The simplest setup is as follows.
Consider a standard linear quadratic regulator where the dynamics are \[ x_{t+1} = A_θ x_t + B_θ u_t + w_t \] where the matrices \(A_θ\) and \(B_θ\) are parameterized by a parameter \(θ\) which takes values in a set \(Θ\).</description>
    </item>
    
    <item>
      <title>Prelim: Risk Sensitive Utility</title>
      <link>https://adityam.github.io/stochastic-control/risk-sensitive/risk-sensitive-utility/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/risk-sensitive/risk-sensitive-utility/</guid>
      <description>Risk sensitivity is relative to the idea of utility. The value of a sum of money \(z\) to a decision maker may not be proportional to \(z\) itself but may be some general increasing function \(\mathsf{U}(z)\), known as the utility function. For example, in the example on optimal gambling considered earlier, we had assumed that the utility for wealth \(z\) is \(\log z\). If a decision maker has utility function \(\mathsf{U}\), then the value of a random outcome \(Z\) will be defined by the expected utility \(\EXP[\mathsf{U}(Z)]\).</description>
    </item>
    
    <item>
      <title>Prelim: Stochastic approximation</title>
      <link>https://adityam.github.io/stochastic-control/rl/stochastic-approximation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/rl/stochastic-approximation/</guid>
      <description>Suppose we have a function \(f \colon \reals^d \to \reals^d\) is such that the equation \[ f(θ) = 0 \] has a unique root \(θ = θ^*\). There are many methods for determining the value of \(θ\) by successive approximation where start with an initial guess \(θ_0\) and then recursively obtain a new value \(θ_{k+1}\) as a function of the previously obtained \(θ_0, \dots, θ_{k}\), the values \(f(θ_1), \dots, f(θ_{k})\), and possibly those of the derivatives \(f&amp;#39;(θ_0), \dots, f&amp;#39;(θ_{k})\), etc.</description>
    </item>
    
    <item>
      <title>Prelim: Stochastic optimization</title>
      <link>https://adityam.github.io/stochastic-control/stochastic/stochastic-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/stochastic/stochastic-optimization/</guid>
      <description>Let’s start with the simplest optimization problem. A decision maker has to choose an action \(a \in \ALPHABET A\). Upon choosing the action \(a\), the decision maker incurs a cost \(c(a)\). What action should the decision maker pick to minimize the cost?
Formally, the above optimization problem may be written as \[ \begin{equation} \label{eq:basic} \min_{a \in \ALPHABET A} c(a). \end{equation}\]
When the action space \(\ALPHABET A\) is finite, say \(\ALPHABET A = \{1, \dots, m\}\), solving the optimization problem \eqref{eq:basic} is conceptually straight-forward: enumerate the cost of all possible actions, i.</description>
    </item>
    
    <item>
      <title>Theory: Basic model of a POMDP</title>
      <link>https://adityam.github.io/stochastic-control/pomdp/pomdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/pomdp/pomdp/</guid>
      <description>So far, we have considered a setup where the decision maker perfectly observes the state of the system. In many applications, the decision maker may not directly observe the state of the system but only observe a noisy version of it. Such systems are modeled as partially observable Markov decision processes (POMDPs). We will describe the simplest model of POMDPs, which builds upon the model of MDPs descibed earlier.
We assume that the system has a state \(S_t \in \ALPHABET S\), control input \(A_t \in \ALPHABET A\), and process noise \(W_t \in \ALPHABET W\).</description>
    </item>
    
    <item>
      <title>Theory: Infinite horizon discounted MDP</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</guid>
      <description>A common way to approximate systems that run for a very large horizon is to assume that they run for an infinite horizon. There is an inherent homogeneity over time for infinite horizon system: the future depends only on the current state and not on the current time. Due to this homogeneity over time, we expect that the optimal policy should also be time-homogeneous. Therefore, the optimal policy for an infinite-horizon system should be easier to implement than the optimal policy for a finite horizon system, especially so when the horizon is large.</description>
    </item>
    
    <item>
      <title>Theory: Optimality of threshold policies in optimal stopping</title>
      <link>https://adityam.github.io/stochastic-control/optimal-stopping/monotonicity-optimal-stopping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/optimal-stopping/monotonicity-optimal-stopping/</guid>
      <description>Let \(\{X_t\}_{t \ge 1}\) be a Markov chain. At each time \(t\), a decision maker observes the state \(X_t\) of the Markov chain and decides whether to continue or stop the process. If the decision maker decides to continue, he incurs a continuation cost \(c_t(X_t)\) and the state evolves. If the DM decides to stop, he incurs a stopping cost of \(s_t(X_t)\) and the problem is terminated. The objective is to determine an optimal stopping time \(\tau\) to minimize \[J(\tau) := \EXP\bigg[ \sum_{t=1}^{\tau-1} c_t(X_t) + s_\tau(X_\tau) \bigg].</description>
    </item>
    
    <item>
      <title>Example: Inventory Management (revisited)</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/inventory-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/inventory-management/</guid>
      <description>TL;DR One of the potential benefits of modeling a system as infinite horizon discounted cost MDP is that it can be simpler to identify an optimal policy. We illustrate this using the inventory management example.
Consider the model for inventory management and assume that it runs for an infinite horizon. We assume that the per-step cost is given by \[c(s,a,s_{+}) = p a + γ h(s), \] where \[ h(s) = \begin{cases} c_h s, &amp;amp; \text{if $s \ge 0$} \\ -c_s s, &amp;amp; \text{if $s &amp;lt; 0$}, \end{cases}\] where \(a\) is the per-unit holding cost, \(b\) is the per-unit shortage cost, and \(p\) is the per-unit procurement cost.</description>
    </item>
    
    <item>
      <title>Example: The Newsvendor Problem</title>
      <link>https://adityam.github.io/stochastic-control/stochastic/newsvendor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/stochastic/newsvendor/</guid>
      <description>Image credit: https://americangallery.wordpress.com/category/cafferty-james-h/  TL;DR The newsvendor problem is a simple model of stochastic optimization problem where a decision has to be made when there is uncertainty about the outcome. It also shows that for some stochastic optimization problems it is possible to obtain the qualitative properties of the nature of optimal solution.
Each morning, a newsvendor has to decide how many newspapers to buy before knowing the demand during the day.</description>
    </item>
    
    <item>
      <title>Infinite horizon Linear Quadratic Regulation (LQR)</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/inf-lqr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/inf-lqr/</guid>
      <description>1 Preliminaries We first start with some properties of deterministic time-homogeous linear system: \[\begin{equation} \label{eq:simple} x_{t+1} = A x_{t-1}. \end{equation}\] The solution to this system is given by \(x_t = A^t x_0\). It obviously has an equilibrium point of \(x = 0\). This will be the unique equilibrium point if \(A\) is non-singular, when the only solution of \(x = Ax\) is \(x = 0\). Suppose this is true. One may now ask whether this equilibrium is stable in that \(x_t \to 0\) with increasing \(t\) for any \(x_0\).</description>
    </item>
    
    <item>
      <title>Theory: Q-learning</title>
      <link>https://adityam.github.io/stochastic-control/rl/q-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/rl/q-learning/</guid>
      <description>Consider an MDP with finite state space \(\ALPHABET S\) and action space \(\ALPHABET A\), where \(|\ALPHABET S| = n\) and \(|\ALPHABET A| = m\). Recall the value iteration algorithm for infinite horizon MDPs. We start with an arbitrary initial \(V_0\) and then recursively compute \(V_{k+1} = \mathcal B V_k\). We first observe that the value iteration can be written in terms of the \(Q\)-function rather than the value function as follows: start with an arbitrary initial \(Q_0\) and the recursively compute \[ \begin{equation} \label{eq:Q} Q_{k+1}(s,a) = c(s,a) + γ \sum_{s&amp;#39; \in \ALPHABET S} P_{ss&amp;#39;}(a) \min_{a&amp;#39; \in \ALPHABET A} Q_k(s&amp;#39;,a&amp;#39;), \qquad s \in \ALPHABET S, a \in \ALPHABET A, k \ge 0 \end{equation} \]</description>
    </item>
    
    <item>
      <title>Theory: Risk Sensitive MDP</title>
      <link>https://adityam.github.io/stochastic-control/risk-sensitive/risk-sensitive-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/risk-sensitive/risk-sensitive-mdp/</guid>
      <description>1 Finite horizon setup Consider the matrix formulation of an MDP with state space \(\ALPHABET X\), action space \(\ALPHABET U\), per-step cost \(c \colon \ALPHABET X × \ALPHABET U \to \reals\), and controlled transition matrix \(P\). However, instead of the risk neutral optimization criteria that we consider previously, we consider a risk-sensitive objective. In particular, the performance of any (possibly non-Markovian) strategy \(g = (g_1, \dots, g_T)\) is given by \[ \bar J_θ(g) = \frac{1}{θ} \log \EXP\Bigl[ \exp\Bigl( θ \sum_{t=1}^T c(X_t, U_t) \Bigr) \Bigr].</description>
    </item>
    
    <item>
      <title>Example: Service Migration in Mobile Edge Computing</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/service-migration-in-mec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/service-migration-in-mec/</guid>
      <description>TL;DR Another benefit of infinite horizon models is that it is possible to prove structural properties of the optimal policy which might not hold for a finite horizon model. We illustrate this using a model for service migration in mobile edge computing.
There are many mobile applications which consist of a front-end component running on a mobile device and a back-end component running on a cloud, where the cloud provides additional data processing and computing resources.</description>
    </item>
    
    <item>
      <title>Example: Sequential hypothesis testing</title>
      <link>https://adityam.github.io/stochastic-control/pomdp/sequential-hypothesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/pomdp/sequential-hypothesis/</guid>
      <description>Consider a decision maker (DM) that makes a series of i.i.d. observations which may be distributed according to PDF \(f_0\) or \(f_1\). Let \(Y_t\) denote the observaion at time \(t\). The DM wants to differentiate between two hypothesis: \[\begin{gather*} h_0 : Y_t \sim f_0 \\ h_1 : Y_t \sim f_1 \end{gather*}\] Typically, we think of \(h_0\) as the normal situation (or the null hypothesis) and \(h_1\) as an anomaly. For example, the hypothesis may be \[ h_0: Y_t \sim {\cal N}(0, σ^2) \quad h_1: Y_t \sim {\cal N}(μ, σ^2) \] or \[ h_0: Y_t \sim \text{Ber}(p) \quad h_1: Y_t \sim \text{Ber}(q).</description>
    </item>
    
    <item>
      <title>Partially observed linear quadratic regulator</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/lqg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/lqg/</guid>
      <description>Consider a stochastic linear system as in the case of LQR. The system has state \(x_t \in \reals^n\) and actions \(u_t \in \reals^m\). The initial state \(x_1\) has zero mean and finite variance \(\Sigma^x_1\). The system dynamics are given by \[ x_{t+1} = A_t x_t + B_t u_t + w_t, \] where \(A_t \in \reals^{n×n}\) and \(B_t \in \reals^{n×m}\) are known matrices and \(\{w_t\}_{t \ge 1}\) is \(\reals^n\)-valued i.i.d. noise process with zero mean and finite variance \(\Sigma^w\).</description>
    </item>
    
    <item>
      <title>Theory: Computational complexity of value iteration</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/complexity-vi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/complexity-vi/</guid>
      <description>How many computations are needed to run the value or policy iteration algorithm to obtain a policy that in within \(ε\) of the optimal? In this section, we provide an answer for this question for the value iteration algorithm.
Conisder an MDP with a finite state space \(\ALPHABET S = \{1, \dots, n \}\) with a finite non-empty set of actions \(\ALPHABET A(s)\) available at each \(s \in \ALPHABET S\). Each action set consists of \(M_s\) actions with a total number of \(M = \sum_{s=1}^n M_s\) actions.</description>
    </item>
    
    <item>
      <title>Theory: Linear Programming Formulation and Constrained MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/linear-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/linear-programming/</guid>
      <description>Note: Throughout this section, we assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite and \(|\ALPHABET X|= n\) and \(|\ALPHABET U| = m\).
We know that if \(V \le \mathcal B V\) then \(V \le V^* = \mathcal B V^*\). Thus, \(V^*\) is the “largest” \(V\) that satisfies the constraint \(V \le \mathcal B V\). This constraint can be written as a finite system of linear equations: \[\begin{equation} \label{eq:constaints} V(x) \le c(x,u) + β\sum_{y \in \ALPHABET X} P(y|x,u) V(y), \qquad x \in \ALPHABET X, u \in \ALPHABET U.</description>
    </item>
    
    <item>
      <title>Theory: A Martingale Principle of Optimal Control</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/martingale-approach/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/martingale-approach/</guid>
      <description>Consider a discounted cost MDP (with bounded rewards) and fix a Markov policy \(g \colon \ALPHABET X \to \reals\). Suppose for a bounded function \(Φ \colon \ALPHABET X \to \reals\), we define a process \(\{M_t\}_{t \ge 1}\) starting at \(M_1 = 0\) and its increments \(ΔM_t = M_{t+1} - M_t\) given by \[ ΔM_t = c(X_t, g(X_t)) + β Φ(X_{t+1}) - Φ(X_t). \]
 Proposition  If \(\{M_t\}_{t\ge1}\) is a submartingale for all policies \(g\) and, for some policy \(g^*\), \(\{M_t\}_{t \ge 1}\) is a martingale, then \(g^*\) is an optimal policy and \(Φ(x) = V^{g^*}(x) = V(x)\).</description>
    </item>
    
    <item>
      <title>Example: Optimal choice of the best alternative</title>
      <link>https://adityam.github.io/stochastic-control/optimal-stopping/optimal-choice/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/optimal-stopping/optimal-choice/</guid>
      <description>A decision maker (DM) wants to select the best alternative from a set of \(T\) alternatives. The DM evaluates the alternatives sequentially. After evaluating alternative \(t\), the DM knows whether alternative \(t\) was the best alternative so far or not. Based on this information, the DM must decide whether to choose alternative \(t\) and stop the search, or to permanently reject alternative \(t\) and evaluate remaining alternatives. The DM may reject the last alternative and not make a choice at all.</description>
    </item>
    
    <item>
      <title>Theory: Basic model of an MDP</title>
      <link>https://adityam.github.io/stochastic-control/mdp/mdp-functional/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/mdp-functional/</guid>
      <description>Markov decision processes (MDP) are the simplest model of a stochastic control system. The dynamic behavior of an MDP is modeled by an equation of the form \[ \begin{equation} S_{t+1} = f_t(S_t, A_t, W_t) \label{eq:state} \end{equation}\] where \(S_t \in \ALPHABET S\) is the state, \(A_t \in \ALPHABET A\) is the control input, and \(W_t \in \ALPHABET W\) is the noise. An agent/controller observes the state and chooses the control input \(A_t\).</description>
    </item>
    
    <item>
      <title>Example: Call options</title>
      <link>https://adityam.github.io/stochastic-control/optimal-stopping/call-options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/optimal-stopping/call-options/</guid>
      <description>An investor has a call option to buy one share of a stock at a fixed price \(p\) dollars and has \(T\) days to exercise this option. For simplicity, we assume that the investor makes a decision at the beginning of each day.
The investory may decide not to exercise the option but if he does exercise the option when the stock price is \(x\), he effectively gets \((x-p)\).
Assume that the price of the stoch varies with independent increments, i.</description>
    </item>
    
    <item>
      <title>Prelim: Change of Measure</title>
      <link>https://adityam.github.io/stochastic-control/risk-sensitive/change-of-measure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/risk-sensitive/change-of-measure/</guid>
      <description>1 Change of measure of a single random variable.  Theorem 1  Let \((\Omega, \mathcal F, P)\) be a probability space and \(\Lambda\) be an almost surely non-negative random variable such that \(\EXP[\Lambda] = 1\). For any \(A \in \mathcal F\), define \[ P^\dagger(A) = \int_A \Lambda(\omega) dP(\omega). \] Then,
 \(P^\dagger\) is a probability measure. For any random variable \(X\), \[ \EXP^\dagger[X] = \EXP[ \Lambda X]. \] If \(\Lambda\) is almost surely positive, then \[ \EXP[X] = \EXP^\dagger \left[ \frac{X}{\Lambda} \right].</description>
    </item>
    
    <item>
      <title>Theory: Lipschitz MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</guid>
      <description>1 Preliminaries 1.1 Lipschitz continuous functions Given two metric spaces \((\ALPHABET X, d_X)\) and \((\ALPHABET Y, d_Y)\), the Lipschitz constant of function \(f \colon \ALPHABET X \to \ALPHABET Y\) is defined by \[ \| f\|_{L} = \sup_{x_1 \neq x_2} \left\{ \frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } : x_1, x_2 \in \ALPHABET X \right\} \in [0, ∞]. \] The function is called Lipschitz continuous if its Lipschitz constant is finite.</description>
    </item>
    
    <item>
      <title>Theory: State aggregation in MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</guid>
      <description>1 State quantization Consider an MDP with continuous state space \(\ALPHABET X\) and finite action space \(\ALPHABET U\). We denote this MDP by \(M = (\ALPHABET X, \ALPHABET U, c, p)\), where for simplicity we assume that the \(p\) is the density of the transition kernel.
Since the state space is continuous, in general, we cannot compute the value functions exactly. The simplest way to proceed is to discretize the state space \(\ALPHABET X\).</description>
    </item>
    
    <item>
      <title>Example: Optimal Gambling</title>
      <link>https://adityam.github.io/stochastic-control/mdp/optimal-gambling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/optimal-gambling/</guid>
      <description>Image credit: http://commons.wikimedia.org/wiki/File:Gambling-ca-1800.jpg  TL;DR This stylized model of optimal gambling was introduced by Kelly (1956) to highlight a relationship between channel capacity (which had been proposed recently by Shannon), and gambling. Our motivation for studying this model is to use it as an illustrative example to show that sometimes it is possible to identify the optimal strategy and value function of MDPs in closed form.
Imagine a gambler who goes to a casino with an initial fortune of \(s_1\) dollars and places bets over time and must leave after \(T\) bets.</description>
    </item>
    
    <item>
      <title>Example: Inventory Management</title>
      <link>https://adityam.github.io/stochastic-control/mdp/inventory-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/inventory-management/</guid>
      <description>Image credit: https://hbr.org/2015/06/inventory-management-in-the-age-of-big-data  TL;DR The inventory management example illustrates that a dynamic programming formulation is useful even when a closed form solution does not exist. This model also introduces the idea of post-decision state, which is useful in many contexts.
Imagine a retail store that stockpiles products in its warehouse to meet random demand. Suppose the store procures new stocks at the end of each day (and that there is no lead time and stocks are available next morning).</description>
    </item>
    
    <item>
      <title>Numerics: Matrix formulation of Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/mdp/mdp-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/mdp-matrix/</guid>
      <description>Image credit: https://etc.usf.edu/clipart/28400/28480/young_boy_28480.htm  1 MDPs as controlled Markov chains In this section, we present a matrix formulation for finite state finite action MDPs, which is useful for computing the solutions numerically. Let’s start with the function model described earlier and assume that \(\ALPHABET S\) and \(\ALPHABET A\) are finite sets and that the cost function and the probability distribution of \(\{W_t\}_{t \ge 1}\) are time-homogeneous. Then, the following is a fundamental property of MDPs:</description>
    </item>
    
    <item>
      <title>Prelim: Stochastic dominance</title>
      <link>https://adityam.github.io/stochastic-control/mdp/stochastic-dominance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/stochastic-dominance/</guid>
      <description>Stochastic dominance is a partial order on random variables. Let \(\ALPHABET S\) be a totally ordered finite set, say \(\{1, \dots, n\}\) and let \(\Delta(\ALPHABET S)\) denote the state of pmfs on \(\ALPHABET S\).
 Definition  Suppose \(S^1\) and \(S^2\) are \(\ALPHABET S\) valued random variables where \(S^1 \sim \mu^1\) and \(S^2 \sim \mu^2\). We say \(S^1\) stochastically dominates \(S^2\) if for any \(s \in \ALPHABET S\), \[\begin{equation}\label{eq:inc-prob} \PR(S^1 \ge s) \ge \PR(S^2 \ge s).</description>
    </item>
    
    <item>
      <title>Simplest model of a networked control systems</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/ncs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/ncs/</guid>
      <description>Networked control systems (NCS) refer to systems where there a communication network between different components of the control system. In this section, we consider the simplest setup of a NCS where there is a communication channel between the sensor and the controller. There are multiple ways to model the communication channel and we choose the simplest model—the communication channel is an i.i.d. packet drop channel. The details will be explained later.</description>
    </item>
    
    <item>
      <title>Theory: Approximate dynamic programming</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/adp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/adp/</guid>
      <description>The value and policy iteration algorithms for discounted cost MDPs rely on exact computation of the Bellman update \(W = \mathcal B V\) and the corresponding optimal policy \(g\) such that \(\mathcal B V = \mathcal B_π V\). Suppose we cannot compute these updates exactly, but can find approximate solutions \(W\) and \(g\) such that \[ \NORM{W - \mathcal B V} \le δ \quad\text{and}\quad \NORM{\mathcal B_π V - \mathcal B V} \le ε\] where \(δ\) and \(ε\) are positive constants.</description>
    </item>
    
    <item>
      <title>Networked control systems with communication cost</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/ncs-with-cost/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/ncs-with-cost/</guid>
      <description>In this section, we extend the model of a simple NCS to one where there a cost associated with sending data from the sensor to the controller. This cost may be due to considerations of transmit power at the sensor or it may be a viewed as a Lagrange multiplier if multiple sensors are sharing the same communication channel with limited bandwidth.
As before, we consider a linear system with state \(x_t \in \reals^n\) and control action \(u_t \in \reals^n\) where the initial state \(x_1\) has zero mean and finite variance \(Σ^x_1\).</description>
    </item>
    
    <item>
      <title>Theory: Monotone value functions and policies</title>
      <link>https://adityam.github.io/stochastic-control/mdp/monotonicity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/monotonicity/</guid>
      <description>Consider the matrix formulation of MDPs and suppose the state space \(\ALPHABET S\) is totally ordered. In many applications, it is useful to know if the value function is increasing (or decreasing) in state.
 Theorem  Consider an MDP where the state space \(\ALPHABET S\) is totally ordered. Suppose the following conditions are satisfied.
C1. For every \(a \in \ALPHABET A\), the per-step cost \(c_t(s,a)\) is weakly inceasing in \(s\).</description>
    </item>
    
    <item>
      <title>Example: Power-delay trade-off in wireless communication</title>
      <link>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff/</guid>
      <description>TL;DR This stylized example of power-delay trade-off in wireless communications illustrates that a dynamic programming formulation can be used to identify qualitative properties of the value function and optimal policies.
In a cell phone, higher layer applications such as voicecall, email, browsers, etc. generate data packets. These packets are buffered in a queue and the transmission protocol decides how many packets to transmit at each time depending the number of packets in the queue and the quality of the wireless channel.</description>
    </item>
    
    <item>
      <title>Theory: Reward Shaping</title>
      <link>https://adityam.github.io/stochastic-control/mdp/reward-shaping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/reward-shaping/</guid>
      <description>What are the conditions under which two MDPs which have the same dynamics but different cost functions have the same optimal policy? This is an important question in reinforcement learning (where one often shapes the reward function to speed up learning) and inverse reinforcement learning (where one learns the reward function from the behavior of an expert). The following result provides a complete answer to this question.
Let \(M^1\) and \(M^2\) denote two MDPs on the same state space \(\ALPHABET S\) and action space \(\ALPHABET A\).</description>
    </item>
    
    <item>
      <title>Linear Exponential of Quadratic Gaussian (LEQG)</title>
      <link>https://adityam.github.io/stochastic-control/risk-sensitive/leqg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/risk-sensitive/leqg/</guid>
      <description>1 Preliminaries  Lemma 1  Suppose that \(Q(z,w)\) is a quadratic function of vectors \(z\) and \(w\), positive definite in \(w\).
Let \(Q_{ww} = ∂^2 Q(z,w)/∂w^2\). Since \(Q(z,w)\) is a quadratic function, \(Q_{ww}\) does not depend on \(z\). Since \(Q\) is positive definite in \(w\), \(Q_{ww} &amp;gt; 0\).
Suppose \(w \in \reals^r\). Define \(q = \log[ (2π)^{r/2} \det(Q_{ww})^{-1/2}]\). Then, for a fixed value of \(z\) \[ \int \exp\bigl[ -Q(z,w)\bigr] dw = \exp\bigl[ q - \inf_{w \in \reals^r} Q(z,w) \bigr].</description>
    </item>
    
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>https://adityam.github.io/stochastic-control/appendix/rkhs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/rkhs/</guid>
      <description>1 Linear Operators  Definition 1  (Linear operator) Let \(\mathcal F\) and \(\mathcal G\) be normed vector spaces over \(\reals\). A function \(A \colon \mathcal F \to \mathcal G\) is called a linear operator if it satisfies the following properties:
 Honogeneity: For any \(α \in \reals\) and \(f \in \mathcal F\), \(A(αf) = α (Af)\). Additivity: For any \(f,g \in \mathcal F\), \(A(f + g) = Af + Ag\).</description>
    </item>
    
    <item>
      <title>Vectorization</title>
      <link>https://adityam.github.io/stochastic-control/appendix/vectorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/vectorization/</guid>
      <description>Vectorization is a linear transformation that converts a matrix to a column vector. For example, \[\VEC\left(\MATRIX{a &amp;amp; b \\ c &amp;amp; d }\right) = \MATRIX{a \\ c \\ b \\ d}.\]
Vectorization is often used to express matrix multiplication as a linear transformation on matrices. In particular, we have the following three properties:
\(\VEC(ABC) = (C^\TRANS \otimes A) \VEC(B).\) \(\VEC(ABC) = (I \otimes AB)\VEC(C).\) \(\VEC(ABC) = (C^\TRANS B^\TRANS \otimes I)\VEC(A).\)  Another useful formulation is the following</description>
    </item>
    
    <item>
      <title>Positive definite matrices</title>
      <link>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</guid>
      <description>[WARNING] Citeproc: citation Abbasi-Yadkori2011 not found 1 Definite and basic properties  Definition  A \(n \times n\) symmetric matrix \(M\) is called
 positive definite (written as \(M &amp;gt; 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x &amp;gt; 0.\]
 positive semi definite (written as \(M \ge 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x \ge 0.</description>
    </item>
    
    <item>
      <title>Singular value decomposition</title>
      <link>https://adityam.github.io/stochastic-control/appendix/singular-value-decomposition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/singular-value-decomposition/</guid>
      <description>Recall that if \(A\) is a symmetric \(n × n\) matrix, then \(A\) has real eigenvalues \(λ_1, \dots, λ_n\) (possibly repeated) and \(\reals^n\) has an orthonormal basis \(v_1, \dots, v_n\), where each \(v_i\) is an eigenvector of \(A\) with eigenvalue \(λ_i\). Then, \[ A = V D V^{-1} \] where \(V\) is the matrix whose columns are \(v_1, \dots, v_n\) and \(D\) is a diagonal matrix whose diagonals are \(λ_1, \dots, λ_n\).</description>
    </item>
    
    <item>
      <title>Sub-Gaussian random variables</title>
      <link>https://adityam.github.io/stochastic-control/appendix/sub-gaussian/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/sub-gaussian/</guid>
      <description>1 Prelim: Concentration inequality of sum of Gaussian random variables Let \(\phi(\cdot)\) denote the density of \(\mathcal{N}(0,1)\) Gaussian random variable: \[ \phi(x) = \frac{1}{\sqrt{2π}} \exp\biggl( - \frac{x^2}{2} \biggr). \]
Note that if \(X \sim \mathcal{N}(μ,σ^2)\), then the density of \(X\) is \[ \frac{1}{σ}\phi\biggl( \frac{x-μ}{σ} \biggr) = \frac{1}{\sqrt{2π}\,σ} \exp\biggl( - \frac{(x-μ)^2}{2 σ^2} \biggr). \]
The tails of Gaussian random variables decay fast which can be quantified using the following inequality.
Mills inequality.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/coursework/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/coursework/</guid>
      <description>Project   The report must not be any longer than 10-15 pages. The grade distribution for different parts of the report are as follows.
 Background, presentation of the problem setting, and literature overview: 20% Summary of the results, proof outlines, and examples to illustrate the main results: 50% Critical evaluation of the contributions: 20% Clarity of the presentation: 10%  The potential project topics are listed below. You can also pick up on the discussion of the literature in the reference section of the notes.</description>
    </item>
    
    <item>
      <title>Assignment 1</title>
      <link>https://adityam.github.io/stochastic-control/assignments/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/01/</guid>
      <description>Exercise 1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.
 Exercise 2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.
 Exercise 3 from the notes on the newsvendor problem.</description>
    </item>
    
    <item>
      <title>Assignment 1 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/01/</guid>
      <description>Problem 1 We first observe a simplification that can be done for this model, which makes the calculations a little easier. In principle, we need to compute \[ Q(s,a) = \EXP[ c(s,a,W) | S = s] = \sum_{w \in \ALPHABET W} P(W = w | S = s) c(s,a,w). \]
This means that we need to compute \(P(W|S)\). However, we can avoid that step by observing that \[P(W|S) = \dfrac{ P(S,W) }{ P(S) }.</description>
    </item>
    
    <item>
      <title>Assignment 2</title>
      <link>https://adityam.github.io/stochastic-control/assignments/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/02/</guid>
      <description>Inventory Management. Consider a variation of the inventory management problem considered in class. Suppose that the inventory \(S_t\) and the actions \(A_t\) takes values in the set \(\mathbb S = \{0, 1, \dots, L \}\), and the dynamics are given by \[ S_{t+1} = \bigl[ S_t + A_t - W_t \bigr]_0^L, \] where \([ s ]_{0}^L\) is a function which clips the values between \(0\) and \(L\), i.e., \[ [ s ]_0^L = \begin{cases} 0, &amp;amp; \text{if $s &amp;lt; 0$} \\ s, &amp;amp; \text{if $0 \le s \le L$} \\ L, &amp;amp; \text{if $s &amp;gt; L$}.</description>
    </item>
    
    <item>
      <title>Assignment 2 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/02/</guid>
      <description>0.1 Problem 1 The result follows by writing the maximization in a nested manner \[\begin{align*} V_t(a) &amp;amp;= \max_{(u_1, \dots u_t) \ge 0 : \sum_{i=1}^t u_i = a} u_1 \cdots u_t \\ &amp;amp;= \max_{u_t \in [0,a]} \Bigl\{ \max_{(u_1, \dots, u_{t-1}) \ge 0 : \sum_{i=1}^{t-1} u_i = a-u_t} u_1 \cdots u_{t-1} \Bigr\} \cdot u_t \\ &amp;amp; = \max_{u_t \in [0,a]} V_t(a-u_t) \cdot u_t \end{align*}\]
 We use induction to show that \(V_t(a) = (a/t)^t\).</description>
    </item>
    
    <item>
      <title>Assignment 3</title>
      <link>https://adityam.github.io/stochastic-control/assignments/03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/03/</guid>
      <description>Service rate control in queueing systems
Consider a queueing system where \(S_t \in \{0, 1, \dots, n\}\) denotes the number of jobs in the queue at time \(t\), \(W_t \in \integers_{\ge 0}\) denotes the number of new job arrivals in time \(t\), \(Z_t \in \{0,1\}\) denotes whether the job being processed gets completed in time \(t\). The dynamics of the system are given by \[ S_{t+1} = \bigl[ [ S_t - Z_t]^{+} + W_t \bigr]^{n}_{0} \] where \([s]^{+} = \max\{s,0\}\), and \([s]_{0}^n\) is the clip function (defined in Assignment 2).</description>
    </item>
    
    <item>
      <title>Assignment 3 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/03/</guid>
      <description>0.1 Problem 1 The code for computing the optimal policy is available here. The code is written for ease of reading, not efficiency.
using Distributions # Parameters n, m = 8, 3 λ = 0.5 μ = [0.2, 0.5, 0.8] q = [1, 3, 8] h = 1 R = 10 B = 15 # State spaces S = 0:n A = 1:m Z = 0:1 # Dynamics f(s,z,w) = min( max(s-z, 0) + w, n) # Arrival probability W = Poisson(λ) Pw(w) = pdf(W,w) # Departure probability Z = [Bernoulli(μ[a]) for a in A] Pz(z,a) = pdf(Z[a],z) # Per-step cost.</description>
    </item>
    
    <item>
      <title>Assignment 4</title>
      <link>https://adityam.github.io/stochastic-control/assignments/04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/04/</guid>
      <description>Exercise 1 of the notes on power-delay trade-off in wireless communication.
 Exercise 1 (parts a–c; part d is for reference and you don’t need to submit a solution for part d) of the notes on monotonicity in MDPs.
  </description>
    </item>
    
    <item>
      <title>Assignment 4 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/04/</guid>
      <description>0.1 Problem 1 I had a solution to this problem but the TA found a mistake in my solution. Having thought about it a bit, I realize that as stated the problem cannot be solved.
There are urban legends of students solving unsolved problems thinking they were homework questions. That was not my intention! Let me attempt to explain what led to the mistake and how it can be fixed. This requires some background knowledge of communication systems.</description>
    </item>
    
    <item>
      <title>Assignment 5 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/05/</guid>
      <description>0.1 Problem 1 For the reward maximization case, the dynamic program is given as follows:
\[V_T(x) = s_T(x)\] and for \(t \in \{T-1,\dots, 1\}\), we have \[V_t(x) = \max\{ s_t(x), c_t(x) + \EXP[V_{t+1}(X_{t+1} | X_t = x]. \]
We define the benefit funciton as before. i.e., \[B_t(x) = c_t(x) + \EXP[ V_{t+1}(X_{t+1}) | X_t = x] - s_t(x). \] Then, it is optimal to stop whenever \(B_t(x) \le 0\).
Now, we write the value function in terms of the benefit function as follows:</description>
    </item>
    
    <item>
      <title>Assignment 6 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/06/</guid>
      <description>0.0.1 Problem 1 We start by the right side of the inequality. We want to show that \[V_{k+1} + \bar{\delta}_{k+1} \leq V_k + \bar{\delta}_k.\] We know from the value iteration algorithm proof that \[\mathcal{B} V_k - V_k \leq (1 - \beta) \bar{\delta}_k\] Rewriting this equation, we get \[\begin{equation} V_k + \bar{\delta}_k \geq \mathcal{B} V_k + \beta \bar{\delta}_k \implies V_k + \bar{\delta}_k \geq V_{k+1} + \beta \bar{\delta}_k. \label{eq:one} \end{equation}\]
By definition, we have \[\begin{align} \bar{\delta}_{k+1} &amp;amp;= \frac{\beta}{1-\beta} \max_x \{ V_{k+1}(x) - V_k(x) \} = \frac{\beta}{1-\beta} \| V_{k+1} - V_k \| \notag \\ &amp;amp;= \frac{\beta}{1-\beta} \| \mathcal{B}V_k - \mathcal{B}V_{k-1} \| \notag \\ &amp;amp;\leq \frac{\beta^2}{1-\beta} \| V_{k} - V_{k-1} \| = \beta \bar{\delta}_k.</description>
    </item>
    
    <item>
      <title>Assignment 7 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/07/</guid>
      <description>0.0.1 Problem 1 We are given Bernoulli measures \(μ = a δ_x + (1-a) δ_y\) and \(ν = b δ_x + (1-b)δ_y\). Now consider \[\begin{align*} K(μ, ν) &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } \left| \int_X f dμ - \int_X f dν \right| \\ &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } \left| \int_X f(z)( a δ_x(z) + (1 - a) δ_y(z) ) dz - \int_X f(z)( b δ_x(z) + (1 - b) δ_y(z) ) dz \right| \\ &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } \left| a f(x) + (1-a) f(y) - \big( b f(x) + (1-b) f(y) \big) \right| \\ &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } | a-b | | f(x) - f(y) | \\ &amp;amp;= |a - b | \sup_{ f \colon \| f \|_L \le 1 } \left| f(x) - f(y) \right | \\ &amp;amp;= |a -b | d_X(x,y).</description>
    </item>
    
    <item>
      <title>Course Notes</title>
      <link>https://adityam.github.io/stochastic-control/notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/notes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lectures</title>
      <link>https://adityam.github.io/stochastic-control/lectures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/lectures/</guid>
      <description>Whenever possible, I will post notes on some of the material covered in class, but that is not guaranteed. This is a graduate class and you are responsible for taking notes in class and reading the appropriate chapters of the textbooks.
The notes will be updated as we move along in the course. Please check the dates on the first page to keep track. If you find any typos/mistakes in the notes, please let me know.</description>
    </item>
    
    <item>
      <title>Term Projects for Winter 2020</title>
      <link>https://adityam.github.io/stochastic-control/projects/2020-winter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/projects/2020-winter/</guid>
      <description>Alex Dunyak: An Overview of the Stochastic Dual Dynamic Programming Method
 Alper Oker: Review of “Two Characterizations of Optimality in Dynamic Progrmaming
 Amro Al Baali: Paper Summary: Adaptive Controllers Over Finite Parameter Sets
 Erfan Seyedsalehi: The Options Framework for Temporal Abstraction
 Fatih Gurturk: Average cost error bounds of finite state approximations of Markov decision processes
 Feras Al Taha: Witsenhausen’s Counterexample
 Imen Jendoubi: Average cost MDPs</description>
    </item>
    
  </channel>
</rss>
