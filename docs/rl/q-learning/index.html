<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="reinforcement learning,Q-learning,stochastic approximation" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Theory: Q-learning</div>

<p>Consider an MDP with finite state space <span class="math inline">\(\ALPHABET S\)</span> and action space <span class="math inline">\(\ALPHABET A\)</span>, where <span class="math inline">\(|\ALPHABET S| = n\)</span> and <span class="math inline">\(|\ALPHABET A| = m\)</span>. Recall the <a href="../../inf-mdp/discounted-mdp/#value-iteration-algorithm">value iteration algorithm</a> for infinite horizon MDPs. We start with an arbitrary initial <span class="math inline">\(V_0\)</span> and then recursively compute <span class="math inline">\(V_{k+1} = \mathcal B V_k\)</span>. We first observe that the value iteration can be written in terms of the <span class="math inline">\(Q\)</span>-function rather than the value function as follows: start with an arbitrary initial <span class="math inline">\(Q_0\)</span> and the recursively compute <span class="math display">\[ \begin{equation} \label{eq:Q}
  Q_{k+1}(s,a) = c(s,a) + γ \sum_{s&#39; \in \ALPHABET S} P_{ss&#39;}(a) \min_{a&#39; \in
  \ALPHABET A} Q_k(s&#39;,a&#39;),
  \qquad s \in \ALPHABET S, a \in \ALPHABET A, k \ge 0
\end{equation} \]</span></p>
<p>Suppose that the transition probabilities are unknown, but we have access to a simulator which can be used the sample the next state for any current state and action. Then, one can compute the fixed point of \eqref{eq:Q} using a stochastic approximation variant known as <strong>Q-learning</strong>.</p>
<h1 data-number="1" id="simple-q-learning-algorithm"><span class="header-section-number">1</span> Simple Q-learning algorithm</h1>
<p>Let <span class="math inline">\(Ψ\)</span> denote the simulator, where we use the notation <span class="math inline">\(S_{+} \sim Ψ(s,a)\)</span> to denote that the next state <span class="math inline">\(S_{+}\)</span> is sampled using the simulator with the current state input <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span>. Now, consider the following stochastic approximation algorithm: <span class="math display">\[\begin{equation} \label{eq:QL}
Q_{k+1}(s,a) = Q_k(s,a) + a_k\bigl[
  c(s,a) + γ \min_{a&#39; \in \ALPHABET A} Q_k(Ψ(s,a), a&#39;)  - Q_k(s,a) \bigr]
\end{equation} \]</span> <span class="math inline">\(s \in \ALPHABET S\)</span>, <span class="math inline">\(a \in \ALPHABET A\)</span>, where <span class="math inline">\(Ψ(s,a)\)</span> is an independently sampled next state from the simulator. Note that in the above equation we are assuming that we update the <span class="math inline">\(Q\)</span> function for all values of state action pairs <span class="math inline">\((s,a)\)</span> at each iteration.</p>
<p>The iteration \eqref{eq:QL} may be viewed as a <a href="../stochastic-approximation">stochastic approximation</a> algorithm where <span class="math inline">\(θ_k = Q_k \in \reals^{n×m}\)</span> and <span class="math inline">\(f \colon \reals^{n×m} \to \reals^{n×m}\)</span> given by <span class="math inline">\(f(Q) = [ f_{sa}(Q) ]\)</span> where <span class="math display">\[
  f_{sa}(Q) = c(s,a) + γ \sum_{s&#39; \in \ALPHABET S}
  P_{ss&#39;}(a) \min_{a&#39; \in \ALPHABET A} Q(s&#39;,a&#39;) - Q(s,a). 
\]</span></p>
<p>Define the martingale <span class="math inline">\(\{W_k\}_{k \ge 0}\)</span> by <span class="math inline">\(W_{k+1} = [W_{k+1}]_{sa}\)</span>, where <span class="math display">\[
  [W_{k+1}]_{sa} = γ \Bigl(
    \min_{a&#39;} Q_n(Ψ(s,a), a&#39;) - \sum_{s&#39; \in \ALPHABET Y} P_{ss&#39;}(a)
    \min_{a&#39;} Q_n(s&#39;, a&#39;) \Bigr), 
  \quad s \in \ALPHABET S, a \in \ALPHABET  A.
\]</span></p>
<p>Then iteration \eqref{eq:QL} may be viewed as the standard stochastic approximation algorithm where <span class="math display">\[
  Q_{k+1} = Q_k + a_k[ f(Q_k) + W_{k+1} ].
\]</span></p>
<p>To check that Q-learning converge, we need to check the convergence conditions of <a href="../stochastic-approximation">stochastic approximation</a>. Since we sample <span class="math inline">\(Ψ\)</span> independently at each step, the noise <span class="math inline">\(\{W_k\}\)</span> is a martingale difference sequence which has bounded variance due to fact that the per-step cost is bounded. Thus, we show that Q-learning converges, we need to check the asymptotic stability properties of the ODE: <span class="math display">\[ \begin{equation} \label{eq:Q-ODE}
  \dot Q(t) = f(Q(t))
\end{equation} \]</span> and <span class="math display">\[ \begin{equation} \label{eq:Q-ODE-inf}
  \dot Q(t) = f_∞(Q(t))
\end{equation} \]</span> where <span class="math inline">\(f_∞(Q)\)</span> is given by <span class="math inline">\(f_∞(Q) = [f_∞(Q)]_{sa}\)</span> with <span class="math display">\[
  [f_∞(Q)]_{sa} = 
  γ \sum_{s&#39; \in \ALPHABET S}
  P_{ss&#39;}(a) \min_{a&#39; \in \ALPHABET A} Q(s&#39;,a&#39;) - Q(s,a),
  \quad s \in \ALPHABET S, a \in \ALPHABET  A.
\]</span></p>
<p>We will use the following general property to prove the global asymptotic stability of \eqref{eq:Q-ODE} and \eqref{eq:Q-ODE-inf}.</p>
<div class="highlight">
<dl>
<dt><span id="lemma:1"></span><span id="lemma:ODE" class="pandoc-numbering-text lemma"><strong>Lemma 1</strong></span></dt>
<dd><p>Let <span class="math inline">\(T \colon \reals^d \to \reals^d\)</span> to be <span class="math inline">\(p\)</span>-norm contraction. Then the ODE <span class="math display">\[
  \dot θ(t) = T(θ(t)) - θ(t)
   \]</span> has a unique equilibrium point which is global asymptotically stable.</p>
<p>In particular, suppose <span class="math inline">\(θ^*\)</span> is the unique fixed poit of <span class="math inline">\(T\)</span>. Then <span class="math inline">\(L(θ) =  \| θ - θ^* \|_{p}\)</span> is a Lyapunov function and <span class="math inline">\(θ(t) \to θ^*\)</span>.</p>
</dd>
</dl>
</div>
<p>This was originally proved in <span class="citation" data-cites="Borkar1997"><a href="#ref-Borkar1997" role="doc-biblioref">Borkar and Soumyanatha</a> (<a href="#ref-Borkar1997" role="doc-biblioref">1997</a>)</span> .</p>
<h4 class="unnumbered" id="proof">Proof</h4>
<p>Let <span class="math inline">\(\text{sgn}(θ)\)</span> denote the sign function that is <span class="math inline">\(+1\)</span> is <span class="math inline">\(θ &gt; 0\)</span>, <span class="math inline">\(0\)</span> if <span class="math inline">\(θ = 0\)</span>, and <span class="math inline">\(-1\)</span> is <span class="math inline">\(θ &lt; 0\)</span>. Assume that <span class="math inline">\(1 \le p &lt; ∞\)</span>. By the chain rule</p>
<p><span class="math display">\[\begin{align*}
  \frac{dL(θ)}{dt} &amp;= \sum_{i=1}^d \frac{∂L}{∂θ_i} \frac{dθ_i}{dt} \\
  &amp;= \biggl[ \sum_{j = 1}^d | θ_i - θ^*_i |^p \biggr]^{(1-p)/p}
  \sum_{i=1}^d \text{sign}(θ_i - θ^*_i) | θ_i - θ^*_i |^{1-p} 
  \bigl( T_i(θ) - θ_i \bigr)
\end{align*}\]</span> Note that the term outside the summation is <span class="math inline">\(\|θ - θ^*\|_{p}^{1-p}\)</span> and we can write <span class="math inline">\(T_i(θ) - θ_i\)</span> as <span class="math inline">\(T_i(θ) - T_i(θ^*) - (θ_i - θ^*_i)\)</span> (because <span class="math inline">\(T(θ^*) = θ^*\)</span>. Making these substitutions in the above equation, we get <span class="math display">\[
  \frac{dL(θ)}{dt} = \| θ - θ^*\|_p^{1-p} \biggl[
  \sum_{i=1}^d \text{sgn}(θ_i - θ^*_i) | θ_i - θ^*_i|^{p-1}
  \bigl( T_i(θ) - T_i(θ^*) \bigr) - \| θ - θ^* \|_{p}
\]</span> By Holder’s inequality <span class="math display">\[\begin{align*}
  \sum_{i=1}^d &amp; \text{sgn}(θ_i - θ^*_i) | θ_i - θ^*_i|^{p-1}
  \bigl( T_i(θ) - T_i(θ^*) \bigr) - \| θ - θ^* \|_{p}
  \\
  &amp;\le 
  \biggl[ \sum_{i=1}^d \bigl(\text{sgn}(θ_i - θ^*_i) | θ_i - θ^*_i|^{p-1}
  \bigr)^{p/(p-1)} \biggr]^{(p-1)/p}
  \biggl[ \sum_{i=1}^d \bigl( T_i(θ) - T_i(θ^*) \bigr)^p \biggr]^{1/p}
  \\
  &amp;= \| θ - θ^* \|_{[}^{p-1}\, \| T(θ) - T(θ^*) \|_{p}.
\end{align*}\]</span> Subsituting this in the previous equation, we get <span class="math display">\[ \frac{dL(θ)}{dt} \le \| T(θ) - T(θ^*)\|_p - \| θ - θ^* \|_{p}. \]</span></p>
<p>Note that we can subsitute <span class="math inline">\(p=∞\)</span> in the above expression because <span class="math inline">\(\|z\|_p \to \|z \|_{∞}\)</span> uniformly on compact sets. Since <span class="math inline">\(T\)</span> is a <span class="math inline">\(p\)</span>-norm contraction, <span class="math display">\[ \| T(θ) - T(θ^*) \|_{p} \le γ \| θ - θ^* \|_{p}. \]</span> Substituting this in the previous equation, we get <span class="math display">\[
  \frac{dL(θ)}{dt} \le 
  \le - (1-γ) \| θ(t) - θ^* \|_p = -(1-γ) L(θ).
\]</span> Thus, <span class="math inline">\(L(θ) \to 0\)</span> (or equivalently, <span class="math inline">\(\| θ(t) - θ^* \|_{p} \to 0\)</span> as <span class="math inline">\(t \to ∞\)</span>. <span class="math inline">\(\Box\)</span></p>
<hr />
<p>Define the operator <span class="math inline">\(F(Q) = [F_{sa}(Q)]\)</span> by <span class="math display">\[ \begin{equation} \label{eq:F}
  F_{sa}(Q) = c(s,a) + γ \sum_{s&#39; \in \ALPHABET S} P_{ss&#39;}(a) \min_{a&#39;} Q(s&#39;,a&#39;),
\end{equation} \]</span> which may be thought as an analogue of the Bellman operator for the <span class="math inline">\(Q\)</span>-function. Similarly define <span class="math inline">\(F_∞(Q) = [F_{∞}(Q)]_{sa}\)</span> by <span class="math display">\[
  [F_∞(Q)]_{sa} = γ \sum_{s&#39; \in \ALPHABET S} P_{ss&#39;}(a) \min_{a&#39;} Q(s&#39;,a&#39;),
\]</span></p>
<p>Then, we have have that <span class="math display">\[
  f(Q) = F(Q) - Q
  \quad\text{and}\quad
  f_∞(Q) = F_∞(Q) - Q
\]</span> where both <span class="math inline">\(F\)</span> and <span class="math inline">\(F_∞\)</span> are sup-norm contractions. (It is immediate from definition that <span class="math inline">\(F_∞\)</span> is a sup-norm contraction. <span class="math inline">\(F(Q) = c + F_∞(Q)\)</span> is therefore also a sup-norm contraction). Therefore, <a href="#lemma:ODE" title="Lemma 1"><span class="pandoc-numbering-link lemma">Lemma 1</span></a> implies that both ODEs \eqref{eq:Q-ODE} and \eqref{eq:Q-ODE-inf} have unique global asymptotically stable equilibrium points. Moreover, <span class="math inline">\(f_∞(0) = 0\)</span>. Therefore origin is the unique equilibrium point of \eqref{eq:Q-ODE-inf}. Therefore, we satisfy the conditions of <a href="../stochastic-approximation">stochastic approximation</a>. Hence, if the step sizes <span class="math inline">\(\{a_k\}_{k \ge 0}\)</span> satisfy either conditions (TS) or (BS), then Q-learning converges in the appropriate sense as described in the notes on <a href="../stochastic-approximation">stochastic approximation</a>.</p>
<h1 data-number="2" id="a-single-trajectory-q-learning-algorithm"><span class="header-section-number">2</span> A single trajectory Q-learning algorithm</h1>
<p>The previous version of Q-learning algorithm assumes that we have access to a simulator which samples the output for different choices of state-action pairs. In some applications, such a simulator is not available. Rather, one has to learn simply by interacting with the environment. The next variant of Q-learning does that. This variant is an <em>off-policy</em> learning algorithm, which means that the learner is following a behavioral property (which needs to be sufficiently exploring) while learning the optimal policy (on the side).</p>
<p>The algorithm presented in the previous section assumed that we update the <span class="math inline">\(Q\)</span>-function for all state action pairs at each time. A more practical variation is the following. Let <span class="math inline">\(g\)</span> be some random policy such that <span class="math display">\[ \PR(A_k = a | S_k = s) &gt; 0 \]</span> for all state-action pairs <span class="math inline">\((s,a)\)</span>. Let <span class="math inline">\(\{x_k, u_k, c_k\}_{k \ge 0}\)</span> denote the sequence of states, actions, and costs obtained when executing the policy <span class="math inline">\(g\)</span>. Then, the “on-trajectory” variation of \eqref{eq:QL} is the following: <span class="math display">\[ \begin{equation} \label{eq:Q-traj}
  Q_{k+1}(x_k, u_k) = Q_k(x_k, u_k) + a_k(x_k, u_k)\bigl[
  c_k + γ \min_{a&#39; \in \ALPHABET W}Q_k(x_{t+1}, a&#39;) - Q_k(x_k, u_k) \bigr].
\end{equation} \]</span> This means that, at the <span class="math inline">\((t+1)\)</span>-th update, only the component <span class="math inline">\((x_k, u_k)\)</span> is updated.</p>
<p>The convergence analysis of this algorithm relies on the following result from stochastic approximation (see <span class="citation" data-cites="Jaakkola1994">(<a href="#ref-Jaakkola1994" role="doc-biblioref">Jaakkola et al. 1994</a>)</span>)</p>
<div class="highlight">
<dl>
<dt><span id="lemma:2"></span><span id="lemma:Jaakkola" class="pandoc-numbering-text lemma"><strong>Lemma 2</strong></span></dt>
<dd><p>A <span class="math inline">\(\reals^d\)</span> valued stochastic process <span class="math inline">\(\{Δ_k\}_{k \ge 0}\)</span> given by <span class="math display">\[  \begin{equation} \label{eq:DT}
Δ_{k+1}(i) = Δ_k(i) + a_k(i) (W_k(i) - Δ_k(i)), \quad i \in \{1, \dots
  d\}, 
  \end{equation} \]</span> converges to zero almost surely under the following assumptions:</p>
<ul>
<li><span class="math inline">\(0 \le a_k(i) \le 1\)</span>, <span class="math inline">\(\sum_{k} a_k(i) = ∞\)</span>, and <span class="math inline">\(\sum_{k} a_k^2(i) &lt; ∞\)</span>, for all <span class="math inline">\(i \in \{1, \dots, d\}\)</span>.</li>
<li>Let <span class="math inline">\(\mathcal F_k\)</span> denote the increasing family of <span class="math inline">\(σ\)</span>-fields <span class="math display">\[ \mathcal F_k = σ(Δ_{1:k}, W_{1:k}), \quad k \ge 0. \]</span> Then, for some norm <span class="math inline">\(\| ⋅ \|\)</span> and for all <span class="math inline">\(i \in \{1, \dots, d\}\)</span>, <span class="math display">\[
   \| \EXP[ W_k(i) | \mathcal F_k] \| \le γ \| Δ_k \|, \quad
   \text{where } γ \in (0, 1)
\]</span> and <span class="math display">\[ \text{var}( W_k(i) | \mathcal F_k) \le C_0(1 + \| Δ_k \|)^2, 
  \quad \text{where $C$ is some constant}.
\]</span></li>
</ul>
</dd>
</dl>
</div>
<p>To apply the above result, define the stochastic process <span class="math inline">\(\{Δ_k\}_{k \ge 1}\)</span> where <span class="math inline">\(Δ_k \in \reals^{n×m}\)</span> and is given by <span class="math display">\[ Δ_k(s,a) = Q_k(s,a) - Q^*(s,a) \]</span> where <span class="math inline">\(Q^*\)</span> is the optimal <span class="math inline">\(Q\)</span>-function. Then the iteration \eqref{eq:Q-traj} is of the form \eqref{eq:DT} where <span class="math display">\[
  W_k(x_k,u_k) = c_k + γ \min_{a&#39; \in \ALPHABET A}Q_k(x_{k+1}, a&#39;) - Q^*(x_k,
  u_k).
\]</span> Now, note that <span class="math display">\[
  \EXP[ W_k(x_k, u_k) ] = (F Q_k)(x_k, u_k) - Q^*(x_k,u_k) = 
  (F Q)(x_k,u_k) - (F Q^*)(x_k,u_k),
\]</span> where the operator <span class="math inline">\(F\)</span> is defined in \eqref{eq:F} and we have used the fact that <span class="math inline">\(Q^* = FQ^*\)</span>. From the contraction property of <span class="math inline">\(F\)</span>, we get that <span class="math display">\[
  \EXP[ W_k(x_k, u_k) ] \le γ \| Q_k - Q^* \|_∞ = γ \| Δ_k \|_∞. 
\]</span></p>
<p>Finally, <span class="math display">\[\begin{align*}
  \text{var}[ W_k(s) | \mathcal F_k ] &amp;=
  \EXP[ c_k + γ \min_{a&#39; \in \ALPHABET A} Q_{t}(x_{t+1}, a&#39;) - (HQ_k)(x_k,u_k)
  )^2 | \mathcal F_k ] \\
  &amp;= \text{var}( c_k +  γ \min_{a&#39; \in \ALPHABET A} Q_{t}(x_{t+1}, a&#39;) | \mathcal F_k]
\end{align*}\]</span> which is bounded because of the fact that <span class="math inline">\(c\)</span> (and therefore <span class="math inline">\(Q\)</span>) are bounded. Then, the iteration \eqref{eq:Q-traj} satisfies the properties of <a href="#lemma:Jaakkola" title="Lemma 2"><span class="pandoc-numbering-link lemma">Lemma 2</span></a>. Therefore <span class="math inline">\(Δ_k \to 0\)</span> or, equivalently, <span class="math inline">\(Q_k \to Q^*\)</span>, almost surely.</p>
<h1 class="unnumbered" id="references">References</h1>
<p>The Q-learning algorithm is due to <span class="citation" data-cites="Watkins1992"><a href="#ref-Watkins1992" role="doc-biblioref">Watkins and Dayan</a> (<a href="#ref-Watkins1992" role="doc-biblioref">1992</a>)</span>. The proof here is from <span class="citation" data-cites="Borkar2000"><a href="#ref-Borkar2000" role="doc-biblioref">Borkar and Meyn</a> (<a href="#ref-Borkar2000" role="doc-biblioref">2000</a>)</span>. Also see <span class="citation" data-cites="Tsitsiklis1994"><a href="#ref-Tsitsiklis1994" role="doc-biblioref">Tsitsiklis</a> (<a href="#ref-Tsitsiklis1994" role="doc-biblioref">1994</a>)</span> for a asynchronous version of Q-learning algorithm.</p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Borkar2000" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Borkar, V.S. and Meyn, S.P.</span> 2000. The o.d.e. Method for convergence of stochastic approximation and reinforcement learning. <em><span>SIAM</span> Journal on Control and Optimization</em> <em>38</em>, 2, 447–469. DOI: <a href="https://doi.org/10.1137/s0363012997331639">10.1137/s0363012997331639</a>.</p>
</div>
<div id="ref-Borkar1997" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Borkar, V.S. and Soumyanatha, K.</span> 1997. An analog scheme for fixed point computation. I. theory. <em><span>IEEE</span> Transactions on Circuits and Systems I: Fundamental Theory and Applications</em> <em>44</em>, 4, 351–355. DOI: <a href="https://doi.org/10.1109/81.563625">10.1109/81.563625</a>.</p>
</div>
<div id="ref-Jaakkola1994" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Jaakkola, T., Jordan, M.I., and Singh, S.P.</span> 1994. On the convergence of stochastic iterative dynamic programming algorithms. <em>Neural Computation</em> <em>6</em>, 6, 1185–1201. DOI: <a href="https://doi.org/10.1162/neco.1994.6.6.1185">10.1162/neco.1994.6.6.1185</a>.</p>
</div>
<div id="ref-Tsitsiklis1994" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Tsitsiklis, J.N.</span> 1994. Asynchronous stochastic approximation and q-learning. <em>Machine Learning</em> <em>16</em>, 3, 185–202. DOI: <a href="https://doi.org/10.1007/bf00993306">10.1007/bf00993306</a>.</p>
</div>
<div id="ref-Watkins1992" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Watkins, C.J.C.H. and Dayan, P.</span> 1992. Q-learning. <em>Machine Learning</em> <em>8</em>, 3-4, 279–292. DOI: <a href="https://doi.org/10.1007/bf00992698">10.1007/bf00992698</a>.</p>
</div>
</div>


<p class="categories">
This entry 

 was last updated on 16 Dec 2021</p>



    </div>
  </body>
</html>


